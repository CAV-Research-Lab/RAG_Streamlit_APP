=== Metadata ===
{
    "file_name": "Lecture 3 Mission Planning.pdf",
    "file_path": "/Users/mf0016/Desktop/soe_RAG/resources/Lecture 3 Mission Planning.pdf",
    "status": "Processed"
}

=== Content ===
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  Mission Planning in Intelligent Vehicles  Introduction to Decision-Making in Intelligent Vehicles As we are going to explore mission planning, it's crucial that we first grasp the larger framework within which the decision-making functionality in autonomous driving operates. At its core, autonomous driving is about making intelligent and informed decisions independently. This decision-making process can be broken down into three interconnected components: mission planning, behavioural planning, and motion planning. Let's define these components to set the stage for our in-depth exploration of mission planning and its subsequent elements. First, we have mission planning. This is where the high-level objectives for the intelligent (autonomous) vehicle are set. It's akin to planning a road trip, where we determine our start point, destination, and overall route. In the context of intelligent vehicles, mission planning involves deciding the broader goal-oriented tasks such as where to go, what route to take, and when to start and stop. Next is behavioural planning. This is where the vehicle decides its behaviour based on the environment and its objectives. In our road trip analogy, behavioural planning would involve decisions like when to overtake a slower vehicle, which lane to be in, or when to stop for refuelling. Lastly, we have motion planning. This involves determining the specific control actions to execute the chosen behaviour. In the context of our road trip, it's like deciding the exact steering angle, acceleration, or brake pressure required to carry out our decisions. Together, these elements shape the decision-making process of an intelligent vehicle, enabling it to navigate its environment effectively and safely. The focus of this lecture is specifically on the first component - mission planning - and how it serves as the foundation for all subsequent decisions made by an autonomous vehicle. Introduction to Mission Planning What is mission planning? Mission planning is the high-level strategic decision-making process that an intelligent vehicle employs to plan its journey from start to finish. It is the initial step taken when a destination is entered into the vehicle's system, marking the beginning of the autonomous journey. But don't let the simplicity of this definition fool you. Mission planning is not just about getting from point A to B. It is about integrating a wide range of subsystems, such as sensing and perception systems we studied earlier, and using them to create a detailed, step-by-step plan. This plan must consider variables such as current traffic conditions, weather conditions, road restrictions, and more to ensure a safe and efficient trip. Now, why is mission planning an essential part of an autonomous vehicle's decision-making process? Well, mission planning essentially provides the vehicle with a roadmap of its journey. It helps the vehicle understand its broader context, which is vital for making strategic decisions along the way. For example, should the vehicle take the highway or a smaller side road? Should it avoid a certain area due to heavy traffic or road works? These types of decisions are made during the mission planning process. In other words, mission planning lays the foundation for all the actions that an intelligent vehicle takes, from the moment you set your destination until the moment you reach it. It is a cornerstone in the world of autonomous driving, and understanding it is the first step towards building truly intelligent vehicles.  To better understand Mission Planning, let's break down its fundamental components. Each piece plays an integral part in crafting a mission plan that effectively navigates the twists and turns of real-world driving. Firstly, we must understand our objectives. Consider this - if you're programming a vehicle to drive from Guildford to Oxford, what's the ultimate goal? Reaching Oxford, right? So, understanding the destination, the endpoint of our journey, is the critical first component of mission planning. Next, we need to consider the environmental variables. Imagine there's a huge traffic jam in M25. How should the vehicle respond? How do these conditions impact the overall plan? Real-time traffic data, weather conditions, road restrictions, and even factors like fuel levels and vehicle health, all come into play here. Can you think of other variables that we might need to consider?  
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  The third component is the creation of a multi-step itinerary. It's not enough to know that we are going from Guildford to Oxford - we need a detailed, step-by-step plan on how to get there. This plan needs to be dynamic, able to adapt to real-time conditions and changes. Let's think about it - if the original plan was to take a route through A3, but now there's a traffic jam, what should the vehicle do? In essence, mission planning is not a one-size-fits-all process. It is a complex task that involves understanding our objectives, considering many variables, creating a dynamic itinerary, and ensuring we have contingencies in place for unexpected scenarios. It is about finding the best path from A to B, while also being ready to navigate any detours along the way. So, what detours can you think of on the journey to autonomous driving? As we learn mission planning, it is crucial to understand the significant role that it plays in autonomous driving. In fact, mission planning is at the heart of every decision an intelligent vehicle makes on its journey. Mission planning is what allows intelligent vehicles to interact cohesively with their environment. Mission planning uses data about these environmental conditions to make decisions. It is like having a highly experienced driver who can read the road, weather, and traffic conditions to determine the best course of action. Another crucial aspect is how mission planning serves as a foundation for other decision-making processes in the vehicle. Once we have the strategic plan in place, we can start making lower-level decisions, like behavioural and motion planning, which dictate the immediate actions of the vehicle. For instance, once the vehicle knows its route, it can then decide when to change lanes, when to speed up or slow down, and when to overtake another vehicle. Lastly, mission planning plays a vital role in ensuring efficiency, safety, and smooth driving. A well-planned mission minimises risks, reduces travel time, and optimises energy consumption, resulting in a safer, more comfortable ride. So, to sum it up, mission planning enables interaction with the environment, serves as the foundation for lower-level decisions, and ultimately leads to safer and more efficient driving.  Introduction to Search Algorithms in Mission Planning  Now let’s talk about an integral part of the mission planning process - search algorithms. These algorithms are the 'brains' behind how an intelligent vehicle creates and adjusts its mission plan. But what exactly are search algorithms? Search algorithms are systematic methods used to explore and analyse data. They help us navigate through a vast array of possibilities to find the most optimal solution. In the context of mission planning, they help us find the best route from Point A to Point B. Let's think about a real-world scenario to illustrate this. Let's say you are planning a road trip. You would typically use a navigation application, input your start and end locations, and the app would provide you with the best route. But how does the app determine what's the 'best' route? Is it the shortest? The quickest? The one with the least traffic? This is where search algorithms come into play. They analyse various parameters such as distance, time, and traffic conditions, to provide you with the most optimal route. Now, in the world of intelligent vehicles, search algorithms play a similar role. They analyse data from various sources, such as maps, traffic data, and sensor data, to identify the best route for the vehicle to follow. Moreover, as conditions change, the algorithm adjusts the plan in real time to maintain an optimal path. In essence, search algorithms are the backbone of mission planning. They not only help establish the initial plan but also ensure that the vehicle can adapt and react in real time to any changes or challenges that arise on the road. So, with an understanding of what search algorithms are and their role in mission planning, can you guess how these algorithms might impact the overall efficiency and safety of an autonomous vehicle's journey? Types of Search Algorithms Building on our understanding of search algorithms, let's dive a bit deeper and explore the two main types: uninformed search algorithms and informed search algorithms. Let's start with uninformed search algorithms. Sometimes called 'blind' search algorithms, these methods are not privy to any additional information about the end goal beyond the problem definition. They systematically explore all possibilities until they find a solution. Imagine trying to find your way out of a maze without a map – you would probably try every 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  path, turn every corner until you eventually find the exit. That's akin to how uninformed search algorithms work. Now, on to informed search algorithms. These are a bit more 'intelligent'. They have access to extra information or 'heuristics' about the problem, which helps guide the search process towards a solution. Going back to our maze analogy, imagine if you had a map of the maze. You would be able to make more informed decisions about which paths to take, saving time and effort. That's essentially how informed search algorithms work. So, what's the key difference between the two? Well, it lies in the amount of information they have access to and how they use it. Uninformed search algorithms are like explorers navigating unexplored territories, while informed search algorithms are like guided tourists, leveraging knowledge of the area to find their way efficiently. Now, let's discuss their use cases. Uninformed search algorithms are useful when you have little to no additional information about the problem. They ensure that no possibility is left unexplored, making them a thorough, albeit sometimes slow and resource-intensive, approach. On the other hand, informed search algorithms shine when you have extra information that can guide the search process. They are typically faster and more efficient as they can avoid exploring irrelevant paths. In the context of mission planning for intelligent vehicles, informed search algorithms might leverage data such as real-time traffic information, weather conditions, and road quality to optimise the route planning. So, can you think of a scenario where an uninformed search algorithm might be more beneficial than an informed one? Or vice versa. Let me answer the question: A scenario where an uninformed search algorithm might be more beneficial could be when the intelligent vehicle is navigating an area where very little prior data or information is available. For example, if a vehicle is traversing a newly built road or a rural area where traffic data, road conditions, and other related information is not readily available, an uninformed search algorithm could be beneficial. Here, the algorithm could explore all possible paths without bias, and over time, gather data that could be used for future trips. Conversely, in a dense urban environment with complex road networks and dynamic conditions like traffic congestion, an informed search algorithm would be more advantageous. These algorithms could leverage real-time data about traffic flow, road work, or even public events to optimise the vehicle's path. They could anticipate congestion or obstacles and reroute the vehicle, accordingly, thereby ensuring more efficient and time-saving navigation. These examples highlight how the choice between uninformed and informed search algorithms depends largely on the specific context and available data. A fundamental understanding of both types allows for their strategic application in different scenarios within the realm of intelligent vehicle navigation. Fundamental of Search Algorithms Before learning the concept of searching algorithms for mission-planning, there are two foundational concepts that you must first understand: actions and states. In the context of intelligent vehicles, a state can be defined as a specific condition or circumstance in which the vehicle finds itself at a particular moment. For instance, a state could be the vehicle's current location, the condition of its surroundings, or the status of its internal systems. It is the vehicle's present status in its operating environment. For example, 'being in Guildford', 'being on a highway,' or 'being in the middle of an intersection' can be considered states. On the other hand, an 'action' refers to the operations or manoeuvres that an autonomous vehicle can undertake to transition from one state to another. In the simplest terms, it is what the vehicle does in response to its state. Actions could include turning left, accelerating, stopping, or even activating the windshield wipers. For example, 'driving from Guildford to Oxford', 'changing lanes,' or 'stopping at a red light' are all actions. Understanding the concept of states and actions is fundamental to comprehending the decision-making process of intelligent vehicles. Each action an intelligent vehicle takes is aimed at transitioning from its current state to a new, desirable state – all in pursuit of achieving its ultimate goal. Now, with these definitions in mind, let's consider a practical scenario to show how actions and states play into the decision-making of an intelligent vehicle on a journey...". Following 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  this introduction, we proceed with the “trip in Romania” example, showing you how the concepts of actions and states are applied in a real-world mission-planning. Imagine our intelligent vehicle is in the city of Arad of Romania (see Figure 1). Our vehicle has a mission: to transport its passengers to Bucharest in time for a flight the following day. This is the goal, and any actions that don't lead to this outcome can be disregarded. This goal formulation, based on the current situation, serves as the starting point in our problem-solving process. So, let's define the goal as reaching at Bucharest on time.   
 Figure 1 A simpliﬁed road map of part of Romania, with road distances in miles. Actions are transitions between different states, so the challenge for the vehicle is to determine which actions will lead to the goal state (Bucharest). Before it can do this, it needs to decide what sorts of actions and states to consider. If it was to try to consider actions at the level of "move the left foot forward 18 inches" or "turn the steering wheel six degrees left," it would never find its way out of the parking lot, because constructing a solution at that level of detail would be an intractable problem.  Problem formulation is the process of deciding what actions and states to consider and follows goal formulation. We will discuss this process in more detail. For now, let’s assume that the agent will consider actions at the level of driving from one major town to another. The states that it will consider therefore correspond to being in a particular town. Now, the vehicle decides on the goal of reaching Bucharest and contemplates its next move from Arad. It has three options: head towards Sibiu, Timisoara, or Zerind. Without further information, it wouldn't know which route to choose. Here's where the map comes in. A map, in vehicle’s memory, provides the intelligent vehicle with information about possible states and actions. It can use this information to consider various hypothetical journeys and try to find one that leads to Bucharest. The process of examining different sequences of actions to reach a known state is called search. A search algorithm takes the problem as input and provides a solution as an action sequence. Once a solution is found, the recommended actions are executed. This is called the execution phase. The process – formulate, search, execute – is the essence of decision making in intelligent vehicles. In the following, we will explore these processes in greater depth, equipping you with the knowledge to understand how intelligent vehicles solve complex problems and make decisions. Formulating search problems  Understanding the journey from Arad to Bucharest is not just about knowing where you are and where you want to go. It involves a more complex process we refer to as formulating a search problem. In order to formulate a search problem, an intelligent vehicle needs to have several pieces of information: The first information is the initial state which is the starting point, or where the vehicle currently is, for instance, 'being in Arad in Romania'. Next is the state space of the problem which is the set of all states reachable from the initial state by any sequence of actions. Another important information in formulating a search problem is the possible actions which are the various manoeuvres the vehicle can make to transition from 

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  one state to another, such as 'driving from Arad to Timisoara'. We often term the description of an action as an 'operator'. This denotes the expected outcome of a particular action in a certain state. A 'path' in this context refers to a specific sequence of actions that lead from one state to another. The initial state and possible actions two components together define the state space of the problem. It is noted that the state space is the set of all states that the vehicle can potentially reach from its initial state through any sequence of actions. Next, we need the goal test which is a method that allows the vehicle to ascertain whether a state is a goal state. For instance, in the example, reaching Bucharest would satisfy the goal test. Finally, we may need a way to evaluate the cost of a path. For instance, one path may be shorter but riskier, while another longer but safer. This function, often denoted by 𝑔 and called cost function, assigns a cost to each path based on certain criteria, such as distance, time, or fuel efficiency. Together, these elements define a search problem, and finding a solution to this problem means identifying a path that satisfies the goal test. For more complex problems where there are multiple possible states, our formulation needs a slight adjustment. We consider an initial set of states and a set of operators that specify possible actions for each state in the set. A path now connects sets of states, and a solution is a path that leads to a set where all states satisfy the goal test. Now, let’s understand the workings of a search algorithm by examining a simple problem: driving from Arad to Bucharest using a specific roadmap, shown in Figure 1. In this scenario, we define our state space by location or city, giving us 20 unique states. Our initial state is 'in Arad,' and the goal test asks: 'Are we in Bucharest?' The operators, or actions available, correspond to the various routes between cities. For instance, one solution could be the path from Arad to Sibiu, then Rimnicu Vilcea, Pitesti, and finally to Bucharest. But that is not the only solution, we could also find a path through Lugoj and Craiova. To determine the optimal solution, we need a path cost function. This function could consider several parameters like the total distance or expected travel time. For simplicity, we will count the number of steps or cities passed through as our cost. Therefore, the path via Sibiu and Fagaras with a cost of 3 steps is our best solution. But how do we decide what to include in our state and operator descriptions? Let's compare our simple state description 'in Arad' to a real-world road trip. An actual trip includes numerous variables like companions, weather, the vehicle's speed, gas level, nearby law enforcement, the time of day, and many more. Yet, we simplify the state to only consider the location because these variables do not affect the route selection to Bucharest. This process of simplification is known as abstraction. Similarly, we abstract the actions. For example, driving from Arad to Zerind changes not just the vehicle's location, but also time, fuel, and environmental impact. But we only account for the location change. We even omit some actions altogether, like turning on the radio or slowing down for law enforcement. But the question you may ask is that how do we determine the right level of abstraction? We find an abstraction to be valid when a solution to the abstract problem corresponds to a solution to the more detailed problem. It's also useful when the abstract actions are easy to execute. For instance, if we solve the abstract problem by identifying the path from Arad to Bucharest via specific cities, the more detailed path – including actions like turning the radio on or off – remains a solution.  In essence, effective abstraction strikes a balance between removing irrelevant details, maintaining validity, and ensuring easy action execution. It is important to create useful abstractions that prevent intelligent agents from being overwhelmed by the complexity of the real world. Best-first search   How a search algorithm solves the route-finding problem from Arad to Bucharest? To answer this question, we start with the initial state, 'in Arad.' Our first step is to ascertain whether this is the goal state – by looking at the map given in Figure 1, it is obvious, in this case, it does not. But it is essential to perform this check to avoid problems like 'starting in Arad, get to Arad.' Since 'in Arad' is not our goal state, we need to explore other states. We achieve this by applying the operators to the current state to generate a new set of states. The process is 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  called expanding the state. This expansion process leads us to three states, 'in Sibiu,' 'in Timisoara,' and 'in Zerind,' reflecting the direct one-step routes from Arad. Now we face multiple possibilities and must choose one to explore further - this is the essence of search. In this case, we get three new states, "in Sibiu," "in Timisoara," and "in Zerind," because there is a direct one step route from Arad to these three cities. If there were only one possibility, we would just take it and continue. But whenever there are multiple possibilities, we must make a choice about which one to consider further. You may ask how do we decide which state from current state set to expand next? A very general approach is called best-first-search, in which we choose a state with minimum value of some evaluation function. By employing different evaluation functions, we get different specific algorithms, which we will discuss them later. We continue the process of selection, testing, and expansion until we either find a solution or exhaust all expandable states. The sequence in which states are expanded is dictated by the search strategy. It's useful to visualise this search process as a tree superimposed over the state space. The root of the search tree corresponds to the initial state, while the leaf nodes correspond to states that have yet to be expanded or those that did not generate new states upon expansion. At each step, the search algorithm selects a leaf node for expansion. However, it's important to distinguish between the state space and the search tree. For the route-finding problem, there are only 20 states in the state space, one for each city. But there are an infinite number of paths in this state space, so the search tree has an infinite number of nodes. For example, in Figure 2, the branch Arad-Sibiu-Arad continues Arad-Sibiu-Arad-Sibiu-Arad, and so on, indefinitely. Obviously, a good search algorithm avoids following such paths.         Figure 2 Three par=al search trees for ﬁnding a route from Arad to Bucharest. Nodes that have been expanded are lavender with bold leBers; nodes on the fron=er that have been generated but not yet expanded are in green; the set of states corresponding to these two types of nodes are said to have been reached. Nodes that could be generated next are shown in faint dashed lines. No=ce in the boBom tree there is a cycle from Arad to Sibiu to Arad; that can’t be an op=mal path, so search should not con=nue from there.    

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  Data structure for search trees In our exploration of search algorithms, we willl represent a node as a data structure encompassing five components: 1. The corresponding state in the state space. 2. The parent node in the search tree that generated this node. 3. The operator applied to generate this node. 4. The node's depth, which is the number of nodes from the root to this node. 5. The path cost from the initial state to this node. Remember, nodes and states serve distinct roles. A node is essentially a bookkeeping tool, representing the search tree for a specific problem as generated by a specific algorithm. In contrast, a state depicts a real-world configuration or multiple configurations. Thus, nodes possess attributes like depth and parents, but states do not. It is also possible for different nodes to contain the same state if the state results from different action sequences. Usually, a function, named EXPAND function, calculates the components of the generated nodes. Alongside nodes, we also have a collection of nodes, called the fringe or frontier, waiting to be expanded. A simple representation of this would be a set of nodes, and the search strategy would function to select the next node for expansion from this set. However, for computational efficiency, we represent this collection as a queue to avoid potentially having to examine each set element. We say that any state that has had a node generated for it has been reached (whether or not that node has been expanded).  Figure 3 shows a sequence of search trees generated by a graph search on the Romania problem of the Figure 1. At each stage, we expand every node on the frontier, extending every path with all applicable actions that do not result in a state that has already been reached. Notice that at the third stage, the topmost city (Oradea) has two successors, both of which have already been reached by other paths, so no paths are extended from Oradea.   Figure 3 A sequence of search trees generated by a graph search on the Romania problem of the Figure 1. Measuring performance of search algorithms Evaluating the effectiveness of a search algorithm involves multiple metrics. Firstly, does the algorithm find a solution at all? Secondly, is the solution found of high quality, meaning does it have a low path cost? Lastly, what are the time and memory resources required by the algorithm to find the solution? These resources contribute to the search cost. Therefore, the total cost of the search combines the path cost and the search cost. To illustrate, if we consider the problem of finding a route from Arad to Bucharest, the path cost could be proportional to the total mileage of the path, potentially with some extra cost for varying road conditions. The search cost is context-dependent. In a static environment, it would be zero as the performance measure is independent of time. However, if the goal is to reach Bucharest urgently, the environment becomes semi-dynamic, where prolonged deliberation incurs additional cost. The search cost may then vary linearly with computation time, and the challenge is to combine these different measures, as there is not a clear "exchange rate" between them. This demands the agent balance the resources devoted to search and those devoted to execution.  For simple problems with small state spaces, finding the solution with the lowest path cost is straightforward. For larger, more complex problems, there is a trade-off: the intelligent vehicle can search extensively for an optimal solution or search less and accept a slightly higher path cost solution. 

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  The heart of search algorithm lies in identifying the best search strategy for a problem. We assess these strategies using four key criteria: 1- Completeness: Does the strategy always find a solution when one exists? 2- Time Complexity: How much time is required to find a solution? 3- Space Complexity: What is the memory requirement to execute the search? 4- Optimality: Does the strategy find the highest-quality solution when multiple solutions exist?" Complexity analysis and O( ) notation Computer Scientist are often faced with the task of comparing algorithms to see how fast they run or how much memory they require. There are two approaches to this task. The first is benchmarking – running the algorithm on a computer and measuring speed in seconds and memory consumption in bytes. Ultimately, this is what really matters, but a benchmark can be unsatisfactory because it is so specific: it measures the performance for example a particular program written in a particular language, running on a particular computer, with a particular compiler and particular input data.  From the single result that the benchmark provides, it can be difficult to predict how well the algorithm would do on a different compiler, computer, or data set. The second approach relies on a mathematical analysis of algorithms, independent of the particular implementation and input, known as Asymptotic analysis. Asymptotic analysis provides a mathematical approach to examine algorithms independently of their specific implementations or inputs. For instance, consider a program that computes the sum of a sequence of numbers as shown here:  
  The first step in this analysis is to identify a parameter that characterises the size of the input. Here, the length of the sequence, or '𝑛', serves this role. The next step abstracts the implementation to determine a measure that reflects the running time of the algorithm without depending on a specific compiler or machine. For SUMMATION function, we could measure the number of lines of code executed, additions, assignments, array references, or branches executed by the algorithm. This gives us the total number of steps as a function of the input size. In a simplified measure, SUMMATION has 𝑇(𝑛)	=	2𝑛	+	2. However, not all algorithms are as straightforward as SUMMATION. We often encounter difficulties in characterising the number of steps taken by an algorithm as it is rare to find a parameter like 𝑛 that completely characterises the number of steps taken by an algorithm. Usually, the best we can do is computing worst-case 𝑇!"#$%(𝑛) or average case 𝑇&'((𝑛) scenarios. For average cases, an assumed distribution of inputs is needed. Additionally, many algorithms resist exact analysis, necessitating the use of approximation. In this context, we say SUMMATION is 𝑂(𝑛), indicating that its measure is at most a constant times 𝑛, with exceptions for some small values of n.  This ′𝑂()	′ notation yields an asymptotic analysis. We can confidently say that as 𝑛 approaches infinity, an 𝑂(𝑛) algorithm performs better than an 𝑂(𝑛)). While ′𝑂()	′ notation simplifies analysis by abstracting over constant factors, it sacrifices precision. An 𝑂(𝑛)) algorithm will always lag behind an 𝑂(𝑛) algorithm in the long run, but specific constants can lead to exceptions. For example, an 𝑂(𝑛)) algorithm will always be worse than an 𝑂(𝑛) in the long 

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  run, but if the two algorithms are 𝑇(𝑛)+1) and 𝑇(	100𝑛+1000), then the 𝑂(𝑛)) algorithm is actually better for 𝑛	<	110. Despite these nuances, asymptotic analysis remains a widely-used tool for analysing algorithms. By abstracting over the exact number of operations and the specific input content, the analysis becomes mathematically feasible. The Big O notation strikes a balance between precision and ease of analysis.  Note: In the context of the SUMMATION function provided, the 𝑇(𝑛)	=	2𝑛	+	2 is a simplified estimate of the number of basic operations the algorithm performs. Here's how it breaks down: 1. "𝑠𝑢𝑚	←	0" is one operation. 2. For every item in the sequence (n items), two operations are performed: § One operation is the addition: "𝑠𝑢𝑚	←	𝑠𝑢𝑚	+	𝑠𝑒𝑞𝑢𝑒𝑛𝑐𝑒[𝑖]" § The other operation is the increment of ′𝑖′ in the loop: "𝑖	←	𝑖	+	1" So, for 𝑛 items, there are 2𝑛 operations. 3. "𝑟𝑒𝑡𝑢𝑟𝑛	𝑠𝑢𝑚" is another operation. Adding all these operations up, we get 1 (for "𝑠𝑢𝑚	←	0") +	2𝑛 (for the loop) + 1 (for "𝑟𝑒𝑡𝑢𝑟𝑛	𝑠𝑢𝑚") =	2𝑛	+	2 operations in total. Uninformed Search Algorithms This section explores six distinct strategies classified under the umbrella of uninformed search. The term 'uninformed' implies these algorithms lack specific information about the number of steps or the path cost from the current state to the desired goal. Their only capability is distinguishing between a goal state and a non-goal state, leading to their alternate name, 'blind search'. To illustrate, let's revisit our route-finding scenario. From the initial state in Arad, we have three possible actions leading to new states: Sibiu, Timisoara, and Zerind. An uninformed search strategy possesses no inherent preference amongst these, lacking the insight to deduce that the goal, Bucharest, lies southeast of Arad, and Sibiu would be the most probable choice considering its direction. Strategies utilising such insight are referred to as informed or heuristic search strategies, which we will discuss them later in this lecture note. Although uninformed search methods might seem less effective due to their lack of information utilisation, they hold significant importance. There exist numerous problems where additional information is either unavailable or irrelevant, underscoring the utility of these strategies. The six uninformed search strategies that we will learn in this lecture are:  1- Breadth-first search  2- Uniform cost search  3- Depth-first search  4- Depth-limited search  5- Iterative deepening search  6- Bidirectional search  Breadth-first search algorithm Breadth-first search is a simple but effective strategy. In this method, the root node is expanded first, followed by all its successors, their successors, and so forth. Essentially, all nodes at a depth of 𝑑 in the search tree are expanded before nodes at depth 𝑑+1. What makes breadth-first search systematic is its approach to considering all paths of length 1, then length 2, and so on. If we envision this progression in a binary tree, as depicted in Figure 3, we see that this search method is assured to find a solution if one exists. Among multiple solutions, it always finds the shallowest goal state first. Assessing this against our four criteria for performance measurement of a search algorithm, breadth-first search is complete, and it is optimal provided the path cost is a nondecreasing function of the depth of the node. (This condition is usually satisfied only when all operators have the same cost). So far, the news about breadth-first search has been good. To see why it is not always the strategy of choice, we must consider the amount of time and memory it takes to complete a search. For a clearer understanding, let's consider a hypothetical state space where every state can produce 𝑏 new states, where 𝑏 is the branching factor. The root of the search tree generates 𝑏 nodes at the first level, each of which generates 𝑏 more nodes, giving us 𝑏) nodes 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  at the second level, and so on. If a solution has a path length of 𝑑, the maximum number of nodes expanded before finding a solution would be 1+𝑏+𝑏)+𝑏*+...+𝑏+. Complexity analysis raises eyebrows when it encounters an exponential complexity bound like 𝑂(𝑏+).   Figure 4 Breadth-ﬁrst search on a simple binary tree. At each stage, the node to be expanded next is indicated by the triangular marker. Table 1 shows the time and memory required for a breadth-first search with a branching factor of 𝑏=10 for varying solution depths 𝑑. As all leaf nodes of the tree need to be stored simultaneously, space complexity mirrors time complexity. The figure assumes that 1000 nodes can be checked and expanded per second, and a node requires 100 bytes of storage. Two vital takeaways here are, firstly, the memory requirements pose a greater challenge than execution time. For instance, an 18-minute wait for a depth 6 search might be tolerable, but the corresponding memory requirement of 111 megabytes might not be feasible. Similarly, while waiting for 31 hours for a depth 8 solution might be manageable, having access to the 11 gigabytes of memory needed might not be possible. Thankfully, less memory-intensive search strategies do exist. Secondly, time requirements are not to be dismissed. For instance, given our assumptions, an uninformed search might take up to 35 years to find a solution at depth 12. Even with hardware that is 100 times faster in 10 years, it will still take 128 days to find a solution at depth 12 - and 35 years for a solution at depth 14. Unfortunately, there are no other uninformed search strategies that can do better. Generally, problems with exponential complexity can only be solved for the smallest instances.  Table 1 Time and memory requirements for breadth-ﬁrst search. The ﬁgures shown assume branching factor b = 10; 1000 nodes/second; 1 00 bytes/node. 
  Note: In the context of search algorithms, a "shallowest node" refers to a node that is nearest to the root of the search tree. In other words, it has the shortest path from the root node or the starting point. When we talk about the "shallowest goal state" it means the goal state that can be reached by traversing the least number of edges from the start node. This is often the goal of breadth-first search strategies, as they explore all the neighbours of a node before moving on to their neighbours’ neighbours and so on. Uniform-cost search algorithm When all actions have the same cost, an appropriate strategy is breadth-first search. However, when actions have different costs, an obvious choice is to use uniform-cost search where the evaluation function is the cost of the path from the root to the current node. The idea is that while breadth-first search identifies the shallowest goal state, it may not always lead to the 

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  least-cost solution with a general path cost function. Here, we introduce the uniform cost search, which improves upon breadth-first by consistently expanding the lowest-cost node on the frontier, rather than the shallowest node. To understand its superiority, let's delve into a scenario. Consider our Romania trip, where the problem is to get from Sibiu to Bucharest (Figure 5). The successors of Sibiu are Rimnicu Vilcea and Fagaras, with costs 80 and 99, respectively. The least-cost node which is Rimnicu Vilcea is expanded next, adding Pitesti with cost 80+97=177. The least-cost node is now Fagaras, so it is expanded which adds Bucharest with cost 99+211=310. Bucharest is the goal state, but the algorithm tests for goals only when it expands a node, not when it generates a node, so it has not yet detected that this is a path to the goal. The algorithm continues on, choosing Pitesti for expansion next and adding a second path to Bucharest with cost 80+97+101=278. It has a lower cost, so it replaces the previous path with the new one. It turns out this node now has the lowest cost, so it is considered that a goal has been found. Note that if we had checked for a goal upon generating a node rather than when expanding the lowest-cost node, then we would have returned a higher-cost path, the one through Fagaras.  
 Figure 5 Part of the Romania state space, selected to illustrate uniform cost search. Uniform cost search ensures the cheapest solution under one simple condition: a path's cost must not decrease as we progress along the path. Essentially, each node's path cost should be nondecreasing. This requirement is plausible when the path cost of a node represents the total costs of the operators that form the path. With every operator having a non-negative cost, the path's cost can never decrease as we traverse, allowing the uniform-cost search to discover the cheapest path without exploring the entire search tree. However, if an operator were to have a negative cost, identifying the optimal solution would necessitate exhaustive exploration of all nodes, as even a long and costly path could stumble upon a high negative cost step and transform into the best path. Uniform-cost search is complete and is cost-optimal, because the first solution it finds will have a cost that is at least as low as the cost of any other node in the frontier. Depth-first search algorithm  Depth-first search always expands one of the nodes at the deepest level of the tree. Only when the search hits a dead end which is a non-goal node with no successors does the search go back and expand nodes at shallower levels. As a result, the successors of an expanded node always go even deeper into the search tree, as shown in Figure 6. Depth-first search is commended for its low memory requirements. As can be seen in the figure, it only needs to store a single path from the root to a leaf node, in addition to the remaining unexpanded sibling nodes for each node on this path. This results in storage requirements of only 𝑏𝑚 nodes for a state space with a branching factor 𝑏 and maximum depth 𝑚. This is substantially less compared to the 𝑏𝑑 nodes required by breadth-first search when the shallowest goal is at depth 𝑑.  In terms of time complexity, depth-first search is 𝑂(𝑏,). It may sometimes outperform breadth-first search in scenarios with numerous solutions because it can potentially find a solution after exploring only a small portion of the state space. On the other hand, breadth-first search would 

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  still need to examine all the paths of length 𝑑 before looking into paths of length 𝑑+1. However, depth-first search 's worst-case time complexity is still 𝑂(𝑏𝑚). One notable disadvantage of depth-first search is its propensity to get side-tracked down the incorrect path. Many problems have deep or even infinite search trees, and depth-first search may not recover from a wrong choice at one of the top nodes. Consequently, the search might continue going deeper without backtracking, even when a solution exists at a shallower level. As a result, depth-first search may get trapped in an infinite loop without finding a solution or may discover a solution path longer than the optimal path. Therefore, depth-first search is neither complete nor optimal. Hence, it is generally advisable to avoid depth-first search for search trees with large or infinite maximum depths.  
 Figure 6 A dozen steps (leU to right, top to boBom) in the progress of a depth-ﬁrst search on a binary tree from start state A to goal M. The fron=er is in green, with a triangle marking the node to be expanded next. Previously explained nodes are lavender, and poten=al future nodes have faint dashed lines. Explained nodes with no descendants in the fron=er (very faint lines) can be discarded.  Depth-limited search algorithm    Depth-limited search mitigates the issues associated with depth-first search by applying a cap on the maximum depth of a path. This cap, or cut-off, can be enforced either through a dedicated depth-limited search algorithm or a general search algorithm using operators that monitor the depth. Let's use the map of Romania in Figure 1 as an example again. Given there are 20 cities, we know that if a solution exists, it will have a maximum length of 19. We can enforce the depth cut-off by introducing operators such as: 'If you are in city A and have travelled a path of less than 19 steps, then generate a new state in city B, increasing the path length by one'. With these modified operators, we ensure a solution will be found if it exists. With this new operator set, we are guaranteed to find the solution if it exists, but we are still not guaranteed to find the shortest solution. Depth-limited search is complete but not optimal. If we choose a depth limit that is too small, then depth-limited search is not even complete.  The time and space complexity of depth-limited search is like depth-first search. It takes 𝑂(𝑏-) time and 𝑂(𝑏𝑙) space, where 𝑙 is the depth limit.  Iterative deepening search algorithm    Finding the right depth limit for depth-limited search can be challenging. Let's consider our Romania example again. Initially, 19 seemed like a reasonable depth limit, but upon careful 

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  analysis of the map, we can see that any city is reachable from any other city in a maximum of 9 steps. This number, known as the diameter of the state space, offers a more efficient depth limit, leading to a more proficient depth-limited search. However, it is worth noting that for most problems, a suitable depth limit will not be apparent until the problem is solved. This is where Iterative Deepening Search steps in. This strategy avoids the issue of choosing an optimal depth limit by exploring all potential depth limits: starting from depth 0, then moving to depth 1, depth 2, and so forth. The working of this algorithm can be seen in Figure 7. In essence, iterative deepening search merges the advantages of depth-first and breadth-first searches. It is optimal and complete, akin to breadth-first search, but maintains the modest memory requirements of depth-first search. The state expansion order resembles that of breadth-first search, with the exception that some states are expanded multiple times. At first glance, iterative deepening search might appear wasteful due to the repeated expansion of states. However, for most problems, the overhead associated with this multiple expansion is relatively small. This is primarily because, in an exponential search tree, nearly all the nodes reside at the bottom level. As such, the multiple expansions of upper-level nodes do not significantly affect the overall process. Interestingly, the higher the branching factor, the lower the overhead of repeatedly expanded states. Even with a branching factor of 2, iterative deepening search only takes roughly twice as long as a complete breadth-first search. Therefore, its time complexity remains 𝑂(𝑏+), and the space complexity is also 𝑂(𝑏𝑑). Generally, when dealing with a vast search space and an unknown solution depth, iterative deepening is the preferred search method.  
 Figure 7 Four itera=ons of itera=ve deepening search for goal M on a binary tree, with the depth limit varying from 0 to 3. Note the interior odes from a single path. The triangle marks the node to expand next; green nodes with dark outlines are on the fron=er; the very faint nodes provably can’t be part of a solu=on with this depth limit.  

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  Bidirectional search algorithm The core principle behind bidirectional search is the simultaneous exploration from both the initial state, moving forward, and the goal, moving backward. The process ceases when these two explorations converge in the middle. This approach becomes particularly useful for problems where the branching factor, denoted as 𝑏, is equal in both directions. Assuming a solution depth of 𝑑, we find the solution in 𝑂(2𝑏!") = 𝑂(𝑏!") steps. This occurs because both forward and backward searches only need to traverse half the distance. Despite its theoretical appeal, bidirectional search presents several practical considerations: § The most immediate question pertains to the idea of searching backwards from the goal. We define the predecessors of a node 𝑛 as all nodes that list 𝑛 as a successor. Thus, backward searching involves the successive generation of predecessors starting from the goal node. § When all operators are reversible, the predecessor and successor sets align. However, calculating predecessors can be a challenging task for certain problems. § What if multiple goal states exist? If there is a defined list of goal states, we can apply a predecessor function to the state set in the same way we use a successor function in multiple-state search. If we only have a goal set description, it might be possible to deduce the potential descriptions of 'sets of states that could generate the goal set', but this is a complex process. For instance, consider chess: what are the predecessor states of the checkmate goal? § Lastly, we need to decide on the type of search conducted in each half. For example, one option is two Breadth-First Searches. Is this the best option? Informed Search Algorithms The main advantage of informed search algorithms lies in their ability to make more informed decisions using a heuristic function during the search which significantly improves efficiency and performance. A heuristic function estimates the cheapest cost from the state at node 𝑛 to a goal state. Heuristic Function, ℎ(𝑛), is an estimated cost of the cheapest path from the state at node 𝑛 to a goal state. Among the informed search algorithms, we will concentrate on key variants including Best-First Search, Greedy Best-First Search, A* Search. Best-first search operates by selecting the most promising node based on a specific evaluation function. It is flexible and can be adapted into more specific forms such as greedy best-first search, which always expands the closest node to the goal, and A* search, which considers both the cost to reach the current node and the estimated cost to get from the current node to the goal. These informed search algorithms significantly impact a variety of applications, particularly where computational resources are limited, and efficiency is key.  In this part of lecture, you will learn the mechanics of these informed search algorithms, understanding how they use heuristic knowledge to optimise the search process, and how their thoughtful strategies bring improved efficiency to the mission-planning task of an intelligent vehicle. Best first search algorithm In the previous section, it was discussed how knowledge can be applied in the formulation of a search problem in terms of states and operators. Once we have a well-defined problem, our options for incorporating additional knowledge become slightly more constrained, especially if we plan to use the uninformed search algorithms discussed. Here is the pseudocode for the best-first search algorithm:  function BEST-FIRST-SEARCH (problem, EVAL-Fn)    Input: problem, an object defining the problem to be solved          EVAL-Fn, an evaluation function that estimates the cost to reach the goal from a given node Queueing-Fn ← a function that orders nodes by EVAL-Fn return GENERAL-SEARCH (problem, Queueing-Fn)  
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  The primary location where knowledge can be incorporated in this context is within the node expansion function, known as queuing function. This function determines the node to be expanded next, typically guided by an evaluation function. This evaluation function yields a number, providing an indication of the desirability of expanding a node. When the nodes are ordered to ensure that the one with the most favourable evaluation is expanded first, the strategy that emerges is called the best-first search. The term 'best-first search' may seem slightly inaccurate because if we truly could expand the 'best' node first, it wouldn't be a search but rather a straightforward march towards the goal.  What we can realistically do is choose the node that appears to be the best according to the evaluation function. While the evaluation function is designed to be insightful, it can sometimes lead the search astray. Regardless, we persist with the term 'best-first search' as the alternatives prove to be too cumbersome. Much like the family of uninformed search algorithms that utilise different ordering functions, there is a similar family of best-first search algorithms that employ various evaluation functions. These algorithms typically use an estimated measure of the cost of the solution, striving to minimise it. It was previously discussed one such measure: the use of the path cost to decide which path to extend. This measure, however, does not necessarily direct the search towards the goal. To maintain a focus on the goal, the measure must incorporate an estimate of the cost of the path from the current state to the closest goal state. Greedy best-first search algorithm One of the simplest best-first search strategies is to minimise the estimated cost to reach the goal. That is, the node whose state is judged to be closest to the goal state is always expanded first. A best-first search that uses heuristic function ℎ(𝑛) to select the next node to expand is called greedy search. So, the evaluation function of a greedy search is 𝑓(𝑛)=ℎ(𝑛). For most problems, the cost of reaching the goal from a particular state can be estimated but cannot be determined exactly. Let us see how this works for route-finding problems in Romania. We use the straight-line distance heuristic, which we call ℎ./0. If the goal is Bucharest, we need to know the straight-line distances to Bucharest, which are shown in the Table 2. For example, heuristic value at node Arad is 366 (ℎ./0=366). Notice that the values of  ℎ./0 cannot be computed from problem description itself. Moreover, it takes a certain amount of world knowledge to know that ℎ./0 is correlated with equal road distance and is, therefore, a useful heuristic.    Table 2 Values of hSLD - straight line distance to Bucharest 
 Figure 8 shows the purpose of a greedy best-first search using ℎ./0 to find a path from Arad to Bucharest. The first node to be expanded from Arad will be Sibiu because the heuristic says it is closer to Bucharest than Zerind or Timisoara. The next node to be expanded will be Fagaras because it is now closest according to heuristic, Fagaras in turn generates Bucharest, which is the goal. For this particular problem, greedy best-first search using ℎ./0finds a solution without ever expanding a node that is not on the solution path. The solution that it found does not have optimal cost, however: the path via Sibiu and Fagaras to Bucharest is 32 miles longer than the path through Rimnicu-Vileea and Pitesti. This is why the algorithm is called “greedy” – on each iteration it tries to get as close to a goal as it can, but greediness can lead to worse results than being careful.   

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah   
 Figure 8 Stages in a greedy best-ﬁrst tree like search for Bucharest with the straight-line distance heuris=c hSLD. Nodes are labelled with their h-values. A* search algorithm The most common informed search algorithm is A* search which is a best-first search that uses the evaluation function: 𝑓(𝑛)	=	𝑔(𝑛)	+	ℎ(𝑛) Where 𝑔(𝑛) is the path cost from the initial state to node 𝑛 and ℎ(𝑛) is the estimated cost of shortest path from 𝑛 to a goal state. So, we have 𝑓(𝑛)	which is the estimated cost of the best path that continues from node n to a goal state. A* search is complete. Whether A* is cost-optimal, depends on certain properties of the heuristic. A key property is admissibility. An admissible heuristic is one that never overestimate the cost to reach a goal. Therefore, an admissible heuristic is optimistic. With an admissible heuristic, A* is cost-optimal, which we can show with a proof by contradiction. However, the proof is out of scope of this lecture and if you are interested in understanding the proof, it is recommend reading the reference given in the reading list of this lecture. Note that with an inadmissible heuristic, A* may or may not be cost-optimal.  Figure 9 shows the progress of an A* search with the goal of reaching Bucharest. The values of 𝑔 are computed from the action cost of the Figure 1 and the values of ℎ are given in Table 2. Note that Bucharest first appears on the frontier at step (e), but it is not selected for expansion and thus not detected as a solution because at 𝑓	=450 it is not the lowest-cost node on the frontier – that would be Pitesti, at 𝑓	=	417. Another way to say this is that there might be a solution through Pitesti whose cost is as low as 417, so the algorithm will not settle for a solution that cost 450. At step (f), a different path to Bucharest is now the lowest-cost node, at 𝑓	=	418, so it is selected and detected as the optimal solution.    

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah   Figure 9 Stages in an A* search for Bucharest, Nodes are labelled with 𝑓=𝑔+ℎ. The h values are the straight-line distances to Bucharest taken from Table 2. The g values are given in Figure 1.  Search contours and pruning A useful way to visualise a search is to draw contours in the state space, just like the contours in a topographic map. Figure 10 shows an example. Inside the counter labelled 400, all nodes have evaluation values of less than 400 (𝑓(𝑛)=𝑔(𝑛)+ℎ(𝑛)≤400), and so on. Then because A* expands the frontier nodes of lowest f-cost, we can see that an A* in search fans out from the start node which adds nodes concentric bands of increasing f-cost. With uniform-cost search, we also have contours, but g-cost, not 𝑔+ℎ. The contours with uniform-cost search will be circular around the start state, spreading out equally in all directions with no preference towards the goal. With A* search using a good heuristic the 𝑔+ℎ bands will stretch towards a goal state and become more narrowly focused on an optimal path. It should be clear that as you extend a path, the g costs are monotonic which means the path cost always increases as you go along a path, because actions are always positive. Therefore, you get concentric contour lines that don’t cross each other, and if you choose to draw the lines fine enough, you can put a line between any two nodes on any path. A* is efficient because it prunes away search tree nodes that are not necessary for finding an optimal solution. In Figure 11, it is shown that Timisoara has value of 𝑓	=	329 and Zerind has value of 𝑓	=	374. Even though they are children of the root and would be among the first 

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  nodes expanded by uniform—cost or breadth-first search, they are never expanded by A*   search because the solution with value of 𝑓=253 is found first. The concept of pruning which means eliminating possibilities from consideration without having to examine them is important for many areas of AI.   
 Figure 10 Map of Romania showing contours at 𝑓=380, 𝑓=400, and 𝑓=420, with Arad as the start state. Nodes inside a given contour have 𝑓=𝑔+ℎ costs less than or equal to the contour value.  
 Figure 11 The concept of pruning in A* search That A* search is complete, cost-optimal and optimally efficient among all such algorithms is rather satisfying. Unfortunately, it does not mean that A* is the answer to all our searching needs. The catch is that for many problems, the number of nodes expanded can be exponential in the length of solution which makes computational time and memory cumbersome. For instance, consider the cases in which all nodes have the same path-costs. In this case, all nodes will be visited by A*. Weighted A*  A* search has many good qualities, but it expands a lot of nodes. We can explore fewer nodes if we are willing to accept solutions that are suboptimal but are good enough –what we call satisficing solutions. If we allow A* search to use an inadmissible heuristic - one that may overestimate – then we risk missing the optimal solution, but the heuristic can potentially be more accurate, thereby reducing the number of nodes expanded. For example, road engineers know the concept of a detour index, which is a multiplier applied to the straight-line distance to account for the typical curvature of roads. A detour index of 1.3 means that if two cities are 10 miles apart in straight-line distance, a good estimate of the best path between them is 13 miles. For most localities, the detour index ranges between 1.2 and 1.6.    We can apply the idea of detour index to any problem, not just ones involving roads, with an approach called weighted A* search where we might the heuristic value more heavily, giving us evaluating function 𝑓(𝑛)=	𝑔(𝑛)+	𝑊×ℎ(𝑛), for some 𝑊>1. Figure 12 shows a search problem on a grid world. In (a), an A* search finds the optimal solution, but has to explore a large portion of the state space to find it. In the figure small dots are states that were reached 

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  by each search. In (b), a weighted A* search finds a solution that is slightly costlier, but the search time is much faster. We see that the weighted search focuses the contour if reach states towards a goal state. That means that fewer states are explored, but if the optimal path ever strays outside of the weighted search’s contour as it does in this case then the optimal path will not be found.   We have considered searches that evaluate states by combining 𝑔 and ℎ various ways. However, weighted A* search can be generalisation of others as shown below.  Weighted A* search: 𝑓(𝑛)=	𝑔(𝑛)+	𝑊×ℎ(𝑛) W=1 → A* search: 𝑓(𝑛)=	𝑔(𝑛)+	ℎ(𝑛) W=0 → uniform-cost search: 𝑓(𝑛)=	𝑔(𝑛) W= ∞	 → greedy best-first search: 𝑓(𝑛)=	ℎ(𝑛)    
 Figure 12 Two searches on the same grid: (a) an A* search and (b) a weighted A* search with W = 2 Summary Let's take a moment to recap and consolidate what we have learned. We started by defining mission planning as a fundamental aspect of intelligent vehicle decision-making. We learned how it involves generating and executing a sequence of actions to reach a specific goal, much like planning a road trip from start to finish. From there, we delved into the world of search algorithms, uncovering their role as the 'brains' behind how an intelligent vehicle creates and adjusts its mission plan. We explored the differences between uninformed and informed search algorithms, understanding how each type operates and when each might be used. We discovered that uninformed search algorithms methodically explore all possible paths and are particularly beneficial when there's little to no prior information about the environment. Meanwhile, informed search algorithms use additional information to optimise their search, proving effective in dynamic environments with readily available real-time data. In the context of autonomous driving, understanding these concepts is crucial. The integration of mission planning and search algorithms does not just enable an intelligent vehicle to navigate from point A to B. It allows the vehicle to learn, adapt and make intelligent decisions, enhancing its safety, efficiency, and overall performance. Remember, the goal of intelligent vehicles is not just automation, but autonomy - the ability to make informed and intelligent decisions independently. By understanding the intricacies of mission planning and search algorithms, we are one step closer to achieving this goal. It was also discussed the fundamentals of search algorithms, laying the foundation for mission planning strategies. We started by understanding the concept of a search algorithm, which involves exploring a state space to identify a sequence of actions leading from an initial state to a goal state. We used the example of a route-finding problem from Arad to Bucharest, where the search process involved expanding states, choosing a state to further consider, and continuing until a solution was found or no more states could be expanded. 

Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  We delved into the importance of nodes and states in the search process, elaborating that nodes are the bookkeeping data structures for the search tree, while states represent world configurations. We also explained the concept of the fringe or frontier, which includes nodes awaiting expansion. Performance measurement of search algorithms was another key aspect that was discussed, highlighting three crucial factors - finding a solution, the quality of the solution (path cost), and search cost. We further examined four criteria to evaluate search strategies: completeness, time complexity, space complexity, and optimality. We then explored the idea of asymptotic analysis for complexity analysis of search algorithms, using Big O notation. This analysis provides an approximation of an algorithm's time complexity, offering a generalised understanding of its efficiency as input size grows. However, it's crucial to note that Big O notation may overlook specific scenarios involving smaller input sizes or substantial constant factors. Throughout this lecture, we explored the strategies that an intelligent vehicle might utilise when an optimal action is not immediately obvious. In such cases, the vehicle must consider potential sequences of actions – a process we've come to know as 'search'. To initiate this search, an intelligent vehicle must first define a goal, which then shapes the problem to be solved. A problem is characterised by four components: the initial state, a set of operators, a goal test function, and a path cost function. The environment of the problem manifests itself in a state space.  A solution is defined as a path through this state space, starting from the initial state and leading to a goal state. We evaluated search algorithms based on their completeness, optimality, time complexity, and space complexity. These complexities are influenced by b - the branching factor in the state space, and d - the depth of the shallowest solution.  Now let’s review together the uninformed search algorithms that was discussed in this lectuer.  Breadth-first search gives priority to the shallowest node in the search tree. While it's complete and optimal for unit-cost operators, its time and space complexity of 𝑂(𝑏+) often makes it impractical due to space constraints. Uniform-cost search expands the least-cost leaf node first. It's complete and retains its optimality even when operators have different costs. However, like breadth-first search, it carries a time and space complexity of 𝑂(𝑏+). Depth-first search prioritises the deepest node in the search tree. Despite its time complexity of 𝑂(𝑏,) and space complexity of 𝑂(𝑏𝑚), where m is the maximum depth, it is neither complete nor optimal, making it impractical in scenarios with large or infinite depth search trees. Depth-limited search sets a boundary on the extent a depth-first search can explore. If the limit matches the depth of the shallowest goal state, both time and space complexities are minimised. Iterative deepening search employs depth-limited search with escalating limits until a goal is found. It's complete, optimal, and has a time complexity of 𝑂(𝑏+) and space complexity of 𝑂(𝑏𝑑). Bidirectional search can drastically reduce time complexity when applicable, but its memory demands might be impractical. In summary, these uninformed search strategies provide the backbone for decision-making processes in intelligent vehicles when the next best move isn't immediately clear. In our exploration of search algorithms, a significant focus was placed on informed search strategies, which stand out due to their use of additional information or heuristics to guide the search towards the goal more efficiently. These algorithms contrast with uninformed search strategies by leveraging knowledge about the problem domain, which significantly improves search efficiency in complex environments. The Best First Search Algorithm serves as the cornerstone of informed search strategies, prioritising nodes based on a heuristic evaluation function that estimates the cost from the 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  node to the goal. This approach enables the algorithm to explore paths that are more likely to lead to the goal state, thereby reducing the search space and time. Building on this foundation, the Greedy Best-First Search Algorithm emerges as a variant that exclusively considers the heuristic's estimate to the goal, often leading to faster goal location. However, its focus on immediate cost reduction can sometimes overlook more optimal paths, trading completeness and optimality for speed. The A* search algorithm addresses these limitations by combining the cost to reach a node and the heuristic estimate from the node to the goal, striking a balance between exploration and goal direction. This makes A* both complete and optimal, assuming the heuristic is admissible and consistent. It is widely regarded as one of the most efficient and effective search algorithms for pathfinding and graph traversal. To further enhance the A* algorithm's efficiency, techniques such as Search Contours and Pruning are employed. These methods help in eliminating paths that are unlikely to lead to the goal, thereby reducing the search space. Search contours guide the search in a more focused manner, while pruning removes redundant or non-promising paths. Lastly, the Weighted A Algorithm* extends the A* by introducing a weight factor into the heuristic component, allowing for a tunable trade-off between search speed and optimality. This variation can significantly speed up the search process at the cost of potentially less optimal paths, proving invaluable in time-sensitive applications where a near-optimal solution is acceptable. To sum up, the fundamentals of search algorithms provide vital tools for mission planning, enabling efficient problem-solving and optimisation. Yet, one must be mindful of their complexities and the potential trade-offs in different scenarios.   
