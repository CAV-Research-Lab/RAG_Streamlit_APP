=== Metadata ===
{
    "file_name": "Lecture 2 Sensing, Perception and Localisation Systems 2.pdf",
    "file_path": "/Users/mf0016/Desktop/soe_RAG/resources/Lecture 2 Sensing, Perception and Localisation Systems 2.pdf",
    "status": "Processed"
}

=== Content ===
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  Sensing, Perception and Localisation Systems in Intelligent Vehicles Sensing Systems in Intelligent Vehicles What exactly do we mean by a 'sensing system'? When we discuss intelligent vehicles, the sensing system can be likened to the vehicle's interface with the world. It is a collection of various types of sensors that work together to gather crucial data from the vehicle's environment. This data can span various dimensions - from identifying nearby objects and their velocities to recognising specific road conditions and interpreting traffic signs. The collected data is like the raw sensory input our brains receive from our eyes, ears, and skin. However, for this data to be meaningful and usable, it must be processed and analysed, which is accomplished by other systems in the vehicle.  Let's consider why a sensing system is so vital in the context of autonomous vehicles. Simply, without a sensing system, an intelligent vehicle would be 'blind,' unable to perceive its environment, or make informed decisions based on that perception. Sensing systems, therefore, act as the foundational block of any intelligent vehicle. They are the first critical step towards perceiving, understanding, and effectively interacting with the world. As we will see throughout this module, their role extends beyond simple data collection - they provide the critical inputs that allow an intelligent vehicle to operate in diverse conditions and situations safely and effectively. Anatomy of a Sensing System When we talk about a sensing system, we refer to a cohesive collection of different types of sensors, each with its unique role and capabilities, working together to provide a holistic understanding of the vehicle's environment. Let's start with Cameras. Much like human eyes, cameras capture visual cues from the surroundings, enabling recognition of objects, lane markings, and traffic signals, among other visual elements. Next in our system comes Lidar, an acronym for Light Detection and Ranging. It works by firing rapid pulses of light and measuring the time it takes for the light to return, creating a detailed 3D map of the environment. We also have Radar, short for Radio Detection and Ranging, which, like Lidar, measures distance but uses radio waves. It's particularly good at detecting the speed and distance of moving objects, even in conditions of reduced visibility. Now let's move on to RTK-GPS, which stands for Real-Time Kinematic Global Positioning System. It provides precise location data, essential for accurate vehicle positioning and navigation. Our sensing system also includes Ultrasonic sensors. These sensors emit sound waves and detect their echo after bouncing off nearby objects, making them excellent for close-range detection tasks like parking assistance. Inertial Measurement Units, or IMUs, are our 'motion detectors.' They measure the vehicle's velocity and orientation changes, giving vital information about the vehicle's own movement. Finally, we have the V2X Communications. V2X, or 'Vehicle to Everything,' allows the vehicle to communicate with other vehicles, infrastructure, and devices in its vicinity, greatly expanding its awareness beyond its immediate surroundings. Each of these 'instruments' plays its unique 'tune,' contributing to the symphony of data that enables an intelligent vehicle to perceive and interact with its environment effectively and safely. We'll look at each of these components in more detail in the following subsections. Cameras in Sensing Systems Moving forward, let's focus on the first key player in our sensing system ensemble - the Camera. Cameras serve as the 'eyes' of an intelligent and autonomous vehicle to capture visual information from the environment much as human eyes do. In autonomous vehicles, cameras detect colour and contrast to identify and interpret traffic signs, detect lane markings, and identify obstacles such as other vehicles, pedestrians, and cyclists. Advanced techniques like image recognition and machine learning algorithms enable 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  cameras to classify and interpret these visual inputs, contributing to the decision-making process of the vehicle.  One of the main advantages of cameras is their ability to capture a wide range of colour and texture information that some other sensors cannot. This allows for a more detailed understanding of the environment, such as distinguishing between a red traffic light and a green one. However, cameras are not without limitations. They are susceptible to various environmental conditions. For instance, their performance can be significantly impacted by poor lighting conditions, heavy rain or fog, and direct sunlight or glare. Also, cameras provide a 2D perspective, which can make depth estimation challenging compared to 3D sensing technologies like Lidar. Let's look at a real-world example of camera use in autonomous vehicles. Tesla's Autopilot system relies heavily on cameras. They use an array of eight cameras providing 360-degree visibility around the car up to 250 meters of range. These cameras, combined with advanced image processing algorithms, allow Tesla vehicles to identify and react to various on-road scenarios. In the next few sections, we will study deeper the other instruments in the sensing system orchestra, but keep in mind the critical role that cameras play in shaping an intelligent vehicle's understanding of its surroundings. Lidars in Sensing Systems Lidar is like the 'depth perception' of our sensing ensemble which creates a 3D representation of the environment. Lidar works by emitting pulses of laser light and then measuring the time it takes for each pulse to bounce back after hitting an object. By doing so, it can calculate the distance between the sensor and the object, thereby creating a detailed 'point cloud' which is a 3D representation of the surrounding environment. Lidar holds several advantages in the autonomous vehicle landscape. It provides high-resolution, three-dimensional information about the surrounding environment, and it's capable of functioning well in different lighting conditions, something cameras might struggle with. However, like every technology, Lidar comes with its own set of limitations. Lidar sensors are often more expensive and more complex to integrate into vehicle designs compared to other sensor types. Moreover, their performance can be significantly affected by adverse weather conditions such as fog, rain, or snow, which can scatter the laser light pulses. For a real-world application, let's look at Waymo, the self-driving technology development company. Waymo vehicles use Lidar sensors as a crucial part of their sensor suite. They use the detailed 3D maps created by Lidar to navigate their environment and identify objects like pedestrians, cyclists, other vehicles, and even small objects on the road. Keep in mind that each component, Lidar included, is a unique piece of the puzzle, creating a robust and comprehensive framework for our intelligent vehicle to perceive and interact with its environment. Radars in Sensing Systems Radar is an acronym for Radio Detection and Ranging. Radar operates as the 'long-distance runner' in our sensory system, resilient against conditions that can affect other sensors. In its role, Radar emits radio waves that bounce off objects in the environment. By measuring the time, it takes for these waves to return, Radar can calculate the distance to objects and their speed, with an excellent capability to detect moving objects even at a significant distance. There are several strengths associated with Radar. Its long-range detection capabilities and resilience in poor visibility conditions, such as rain, fog, or even through obstacles, make it a reliable sensor in the autonomous vehicle sensor suite. However, no technology is without its drawbacks. Radar provides less detail in its detections compared to sensors like Lidar or cameras. While it's effective at detecting the presence, distance, and speed of objects, it can struggle with accurately identifying the type of object. For real-world implementation, let's consider the case of Mercedes-Benz. Their Driver Assistance Systems extensively use Radar sensors to detect vehicles ahead, even in poor visibility conditions, enabling features like adaptive cruise control and emergency braking. In 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  general, Radar's contribution bolsters intelligent vehicles' resilience, enabling them to navigate safely through different driving conditions. RTK-GPS in Sensing Systems Shifting our focus to another crucial piece of the sensing puzzle, we now arrive at RTK-GPS. RTK-GPS, short for Real-Time Kinematic Global Positioning System, acts as the 'global navigator' for our autonomous vehicles and provids precise location data. RTK-GPS uses standard GPS signals but enhances their accuracy with additional correction data. The result is a very accurate measurement of the vehicle's position, down to centimetres, in real-time. This precision is critical for tasks like lane-level navigation and precise control of the vehicle. The main advantage of RTK-GPS is its impressive accuracy. Where regular GPS might offer meter-level accuracy, RTK-GPS refines this to the centimetre level, a vital requirement for safe autonomous navigation. However, RTK-GPS also has its own limitations. Its performance can be affected by factors such as buildings or trees blocking the signals (known as the 'urban canyon' effect), and it requires a separate correction data stream, which could be a limitation in some areas. Turning to real-world applications, John Deere is a notable example of RTK-GPS use. They use RTK-GPS in their autonomous tractors for precision farming which enabls exact seeding and harvesting down to the centimetre. As we peel back the layers of our sensing systems, remember that RTK-GPS acts as the pinpoint in our intelligent vehicles' world, ensuring they know exactly where they are at any given moment. IMUs in Sensing Systems Inertial Measurement Units, or IMUs, may be less visible than other sensing systems, but they play a pivotal role in the proper functioning of our intelligent vehicles. IMUs are devices that measure and report on a vehicle's velocity, orientation, and gravitational forces, using a combination of accelerometers, gyroscopes, and sometimes magnetometers. They help the vehicle understand its motion and changes in direction. To put it simply, if the vehicle were a human, the IMU would be its sense of balance and movement. The information provided by the IMU is crucial for many applications, including navigation, stabilising the vehicle's movements, and assisting with tasks that demand precise positioning and control. This is particularly important in conditions where other sensors may struggle, such as poor visibility or GPS-denied environments. However, IMUs are not without their limitations. For example, even small errors in measurement can accumulate over time, leading to a phenomenon known as 'drift.' Therefore, it's common for IMUs to be used in combination with other sensors to correct for this drift and maintain an accurate understanding of the vehicle's position and orientation. On the real-world application front, Tesla's Autopilot is an excellent case study. While Tesla relies heavily on cameras and radar, they also incorporate IMUs to help the vehicle understand its motion and orientation, further enriching the perception capabilities of their autonomous vehicles. It worth remembering that the importance of the often-underappreciated IMU. It stands as a testament to the fact that in the complex world of autonomous vehicles, every sensor has its unique and invaluable contribution. V2X Communications in Sensing Systems Now let’s learn about V2X Communications, the 'social network' of intelligent vehicles, facilitating interaction with other road entities. V2X, or 'Vehicle-to-Everything,' encompasses a set of technologies that allow vehicles to communicate not only with each other (V2V or 'Vehicle-to-Vehicle') but also with infrastructure (V2I or 'Vehicle-to-Infrastructure'), pedestrians (V2P), and the network (V2N). This communication enhances the autonomous vehicle's awareness beyond its immediate sensor reach, providing a broader perspective on its environment. V2X Communications' main advantage lies in their ability to extend the perception of the vehicle beyond what the onboard sensors can see. For example, they can receive information about a pedestrian intending to cross the street around a blind corner or traffic conditions from miles away. However, V2X also faces several challenges. For one, it requires widespread adoption and infrastructure to function optimally. Privacy and security are also major concerns, as it involves sharing location and other sensitive data. 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah   On the real-world application front, Cadillac's CTS sedans stand as a noteworthy example. They were among the first vehicles to be equipped with V2V communication technology, allowing them to share information about their speed and location with other equipped vehicles to enhance road safety. You need to remember that V2X Communications offer intelligent vehicles a unique 'sixth sense,' connecting them with the broader world and enriching their perception capabilities. Summary of Sensing Systems From the visual insight of cameras to the depth perception of Lidars, the long-range capabilities of Radars to the precise localisation offered by RTK-GPS, and finally the extended situational awareness provided by V2X communications - each plays a unique role in making autonomous navigation a reality. Remember, the effectiveness of an autonomous vehicle's sensing system lies not in the strengths of individual components but in their combined input, creating a comprehensive understanding of the vehicle's surroundings and enabling it to make safe and efficient decisions. As technology evolves, we can anticipate enhancements in sensor capabilities, improved robustness, and more integration between different types of sensors, leading to even better perception systems.  There's also a growing interest in leveraging Artificial Intelligence for sensor data processing, offering promise for even more accurate perception and decision-making. Moreover, V2X communication is set to revolutionise the landscape further, extending our vehicles' awareness beyond their immediate vicinity and opening up many possibilities for traffic management, road safety, and even autonomous vehicle swarm intelligence. This is just the beginning of our adventure into the world of intelligent vehicles. Stay curious and keep exploring! Perception Systems in Intelligent Vehicles A perception system in an autonomous vehicle refers to the suite of processes and algorithms that interpret the raw data collected by various sensors, such as cameras, Lidars, Radars, RTK-GPS, ultrasonic sensors, IMUs, and V2X communications. It's the bridge between the raw sensor data and the decision-making and control systems that actually drive the vehicle. Why is this so crucial? The perception system is the key to transforming a vehicle from a mere machine into an intelligent entity capable of navigating the complex, dynamic world of on-road driving. It's responsible for understanding and making sense of the environment surrounding the vehicle, including identifying other vehicles, pedestrians, cyclists, road signs, lane markings, traffic lights, and any other relevant features. Without a robust perception system, an autonomous vehicle would be 'blind', unable to make sense of its surroundings or make safe, efficient driving decisions. As we explore deeper into this fascinating topic, you'll gain an appreciation of the intricate balance between data collection and interpretation, and how they intertwine to enable the remarkable feat of autonomous driving. The Sensing-Perception Connection As we venture further into the workings of perception systems, we must first understand their roots - the sensory inputs. The perception system is like an architect, constructing an understanding of the environment from the 'bricks' provided by the various sensors on board an intelligent vehicle. Each sensor we discussed - be it cameras, Lidars, Radars, RTK-GPS, Ultrasonic sensors, IMUs, or V2X communications - contributes a unique piece of the puzzle. Cameras provide visual data, Lidars offer depth perception, Radars excel at distance and velocity measurements, RTK-GPS provides precise localisation, ultrasonic sensors help with close-range detection, and IMUs provide a sense of the vehicle's own movement. V2X, as we know, extends the perception beyond the immediate vicinity. However, the data from any single sensor is seldom sufficient for safe and reliable autonomous operation. That's where the magic of data fusion comes into play. The perception system synthesises all these diverse sensory inputs, merging them into a comprehensive, coherent understanding of the vehicle's environment. This process, known as sensor fusion, mitigates the limitations of individual sensors and leverages their combined strengths for a more robust 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  and complete perception. For instance, Lidar data might compensate for a camera's inability to gauge distances accurately, or IMU data could provide critical motion information in a GPS-denied environment. In the simple term, the perception system is the core that brings all the sensory information together. Each sensor type provides its own specific type of data. But the data from each sensor on its own is not enough for a complete understanding of the environment. This is where sensor fusion comes in. Sensor fusion is the process of combining the data from all the different sensors to create a complete and accurate understanding of the vehicle's surroundings. It's this full picture that helps the autonomous vehicle make the best decisions for safe and efficient driving. Understanding the Environment Let’s continue with learning some of the key tasks that perception systems carry out.  Let's start with object detection and classification. Think of a busy city street; it's packed with different types of objects, from cars and buses to cyclists, pedestrians, traffic lights, signs, and more. The first step for a perception system is to detect these objects - to identify that there is 'something' in the environment that needs further examination. This is object detection, akin to spotting various figures in a busy photograph. But detection is just half the battle won. The system then needs to understand what these objects are - this is the classification part. For instance, knowing that a particular object is a pedestrian versus a vehicle versus a road sign is crucial, as each requires different reactions from the vehicle. Then comes the second task - scene understanding and interpretation. This is the 'next level' of perception. Here, the system needs to comprehend the broader context of the scene. What are the detected and classified objects doing? Are they moving or stationary? If they are moving, what is their trajectory? Are there any traffic rules displayed that the vehicle needs to follow? For instance, it's not enough for the system to identify that there's a pedestrian on the side of the road. It needs to determine whether the pedestrian is about to cross the street or is just standing there. Based on this interpretation, the vehicle can decide whether to slow down, stop, or continue at its current speed. In sum, object detection and classification provide the building blocks that the perception system uses. Scene understanding and interpretation then take this data and add context to it, painting a comprehensive picture of the vehicle's surroundings that informs the subsequent decision-making and control tasks. It's a fascinating interplay of data and interpretation, each critically dependent on the other. Perception in Action - Dynamic Environment Modelling As we move deeper into our exploration of perception systems, let's consider how they handle a crucial challenge: dynamic environment modelling. This refers to the task of perceiving not only stationary or static objects but also moving or dynamic ones, and then predicting their future movements. So, what is the difference between static and dynamic objects? Quite simply, static objects are those that remain stationary relative to the vehicle, such as road signs, traffic lights, or parked cars. On the other hand, dynamic objects are those that are moving relative to the vehicle, such as other vehicles, pedestrians, or cyclists. Understanding this distinction is crucial because dynamic objects present a higher degree of uncertainty and risk in autonomous driving. Once the perception system identifies an object as dynamic, it must then estimate that object's motion. This involves determining the object's current speed and direction, akin to noticing a car speeding down a highway or a pedestrian crossing the road. But the perception system doesn't stop there. It also needs to predict where these dynamic objects might move next. This is a bit like a chess player thinking a few moves ahead, anticipating the opponent's moves based on their current position and movement. In autonomous vehicles, accurate prediction helps avoid potential accidents and allows for smoother navigation. Therefore, dynamic environment modelling is about understanding the motion of objects around the vehicle and using that understanding to predict how the environment might change in the near future. It's an essential aspect of perception that ensures the vehicle can respond appropriately to the continuously changing world around it. 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  Challenges in Perception Systems Autonomous vehicles don't operate in a perfectly controlled environment. They navigate the real world, which is teeming with variability and uncertainty. This makes the task of perception, while fascinating, also incredibly challenging. Let's take a closer look at these challenges and understand why robustness is a vital trait for any perception system. Firstly, the term 'variability' refers to the diverse and ever-changing nature of the world around us. It covers a wide array of factors. Think of varying weather conditions such as clear skies, rain, snow or fog. Or consider different lighting conditions such as bright daylight, dusk, night-time, or the glare of oncoming headlights. Even the simple act of recognising objects can be fraught with variability, given the many forms that a single object, like a car or a pedestrian, can take. Adding to this complexity is 'uncertainty'. Uncertainty arises due to inherent limitations in sensing and perception technologies. For instance, sensor noise might cause the system to misinterpret an object, or occlusions might hide certain objects from view. Furthermore, the future behaviour of dynamic objects is fundamentally uncertain. Will that pedestrian cross the street or not? Will that car change lanes or stay in its current one? Given these challenges, a key requirement for any perception system is 'robustness'. Robustness is the system's ability to handle the vast range of real-world variabilities and uncertainties, and still function effectively. For example, a robust system would be able to detect and classify objects accurately, even in poor lighting or adverse weather conditions. It would handle sensor noise and uncertainties, making the best possible predictions about dynamic objects. While variability and uncertainty make perception a tough nut to crack, they also make it a compelling area of study. It's a field where striving for robustness against these challenges can lead to significant advancements in the safe and efficient operation of autonomous vehicles. The Future of Perception - AI and Machine Learning When talking about the future of perception in autonomous vehicles, two terms take centre stage: Artificial Intelligence (AI) and Machine Learning (ML). These technologies have the potential to revolutionise perception systems, helping them become more accurate, efficient, and robust. AI's role in perception systems is twofold. Firstly, it helps in improving the accuracy of detection and classification tasks. AI algorithms, especially those based on deep learning, have shown tremendous capability in accurately recognising and classifying objects in an image or a 3D point cloud. Secondly, AI also plays a pivotal role in predicting the future behaviour of dynamic objects, a task with many uncertainties. Machine Learning, a subset of AI, is particularly promising. ML algorithms can 'learn' patterns directly from the data, which can be very beneficial for perception tasks. They can learn to recognise different types of objects, understand their context, and even predict their future movements, all from raw sensor data. But this isn't just theoretical. There are real-world case studies where AI and ML have significantly improved perception capabilities. For instance, companies like Waymo, Tesla, and NVIDIA have extensively used deep learning algorithms to improve the accuracy and efficiency of their perception systems. These AI-driven systems are better able to handle the variability and uncertainty we discussed in the previous slide. In brief, AI and Machine Learning are not just future prospects; they are here now, driving major improvements in the perception capabilities of autonomous vehicles. The continuous evolution of these technologies promises even greater advancements, bringing us closer to the goal of fully autonomous and safe driving. Perception and Decision Making As we explore the role of perception in autonomous vehicles, it's crucial to understand how it fits into the larger picture - especially its relationship with decision-making which will be the focus of this module. The perception system is not an isolated entity. It is, in fact, a vital part of a chain that begins with sensing and ends with vehicle control.  
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  First let me ask you what good is sensory information if the vehicle cannot make sense of it and act accordingly? Here is the role of the perception system. Once the sensing system gathers raw data from the environment, the perception system interprets this data to understand the state of the environment - it 'perceives' the world around the vehicle. This perceived information is then passed on to the decision-making system, also known as the planning system. This system uses the information from the perception system to make informed decisions about the vehicle's future actions. For example, if the perception system detects a pedestrian crossing the road ahead, the decision-making system might decide to slow down or stop the vehicle. The accuracy of perception, therefore, directly impacts the safety and efficiency of vehicle operation. An inaccurate perception could result in incorrect decisions, which can have serious consequences. Suppose the perception system misclassifies a pedestrian as a stationary object. In that case, the decision-making system might not slow down or stop the vehicle, potentially leading to a dangerous situation. In general, perception is not just about understanding the world around the vehicle; it's also about providing accurate and reliable information for decision-making. The safer and more efficient the perception system, the better the vehicle can navigate through the world, highlighting the critical importance of accurate perception for the safe operation of autonomous vehicles. As we come to the end of our exploration into the perception systems of intelligent vehicles, it's time to reflect on the key points we've discussed and what lies ahead in this exciting field. In this part of lecture, we've learned that perception systems are a fundamental component of autonomous vehicles, bridging the gap between raw sensor data and actionable insights for decision-making. We've explored the range of sensors feeding data into these systems, and the important role of sensor fusion in providing a comprehensive picture of the surrounding environment. We've explored the various tasks perception systems, from object detection and classification to understanding and predicting the environment's dynamic nature. We've also considered the challenges in perception, particularly the complexity and unpredictability of real-world conditions. Looking forward, the future of perception systems for intelligent vehicles is bright. Advances in AI and machine learning, along with improvements in sensor technology, promise even better perception capabilities. We're also likely to see more integration, with perception systems increasingly interfacing with other vehicle systems for a more holistic approach to autonomous driving. In conclusion, the world of perception systems is a dynamic and evolving one, full of challenges but also full of immense possibilities. As we continue to explore and innovate, we move closer to the ultimate goal of creating fully autonomous vehicles that can navigate our complex world with the same skill, if not more, as a human driver. Localisation Systems in Intelligent Vehicles Let’s start with this question: what exactly is localisation in the context of autonomous vehicles? Localisation, in essence, is the science of determining the exact position and orientation of our vehicle within its environment. It forms an integral part of the autonomous vehicle technology stack, as it ensures the vehicle understands exactly where it is at any given moment. You might ask, why is it so important? Localisation is the cornerstone that connects our vehicle's perception of the world around it to the decision-making process about how it should move. If the vehicle doesn't know its precise location, it can't plan its path effectively, and the risk of accidents increases significantly. Therefore, a robust and precise localisation system is vital for the safe operation of intelligent vehicles. In the subsequent sections, we will explore more about different localisation methods, their merits and limitations, and the role of advanced technologies in improving localisation accuracy.  The Role of Localisation in the Autonomous Vehicle Stack Let's start with the placement and importance of localisation in the autonomous vehicle functionality stack. The autonomous vehicle stack, or as we often call it, the 'self-driving software stack,' is a series of layers, each performing a specific functionality. These layers 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  work together to help an autonomous vehicle perceive its surroundings, make decisions, and navigate safely. Localisation plays a pivotal role in this stack, acting as the bridge between the perception systems and planning/control systems. Once the sensors have gathered information about the environment, and the perception system has interpreted it, the next step is to use this data to make informed decisions about the vehicle's path. However, before these decisions can be made, the vehicle needs to know its exact location within the environment. And that's where localisation comes in. Localisation uses the data from various sensors and other sources to accurately determine the vehicle's position and orientation. The more precise this localisation, the more accurately the vehicle can plan its path and control its actions. The safety and efficiency of the autonomous operation are significantly enhanced when the vehicle knows exactly where it is at any given moment. Next, we'll look into various methods used for localisation, discussing their strengths, weaknesses, and much more. So, let's dive in and explore the fascinating world of localisation in autonomous vehicles. Methods of Localisation Localisation in intelligent vehicles isn't confined to a single technique or method. It's a multi-faceted concept, utilising a range of techniques to ensure the most accurate positioning possible. Here, we'll take an overview of these different localisation methods, each with their unique strengths and limitations. The first and most common method we're going to discuss is GPS-based localisation. Global Positioning System or GPS, as it is widely known, is a satellite-based system that provides geolocation and time information anywhere on or near the Earth. However, as we discussed before, while GPS is widely available, it is not always precise or reliable, especially in urban environments with tall buildings, known as 'urban canyons'. Next, we have Sensor-based localisation. Sensor-based localisation utilises data from various onboard sensors like cameras, Lidars, and radars, often in combination with GPS, to determine the vehicle's location. This method is more robust in challenging environments but also more complex to implement. Finally, there's Simultaneous Localisation and Mapping (SLAM), an advanced technique that maps an unknown environment while keeping track of the vehicle's location within it. SLAM can provide very accurate localisation but is computationally intense. In the following section, we'll delve deeper into each of these methods, exploring their working principles, strengths, and limitations. As we move forward, you'll see how these different methods complement each other in providing precise localisation for safe and efficient autonomous driving. GPS-based Localisation GPS forms the backbone of most localisation systems, including those in intelligent vehicles. Originally developed for military use, GPS has found its way into countless civilian applications, from navigation aids in mobile phones to tracking devices in logistics. In its core, GPS localisation involves the use of signals from multiple satellites to determine the vehicle's position on Earth. Each GPS satellite broadcasts a signal that includes the satellite's location and the precise time the signal was transmitted. The GPS receiver in the vehicle picks up these signals, calculates the time, it took for each signal to arrive, and thus determines the distance to each satellite. With the known location of the satellites and the calculated distances, the receiver can pinpoint the vehicle's position using a process called trilateration.  However, despite its widespread usage, GPS is not without its limitations. While GPS can provide reasonably accurate localisation in open areas, its performance can significantly degrade in urban canyons, where tall buildings obstruct the signals from the satellites, causing multi-path propagation and signal blockages. It also performs poorly in indoor or underground locations. Furthermore, the typical GPS receivers used in consumer devices are not sufficiently accurate for precise localisation required in autonomous driving. Therefore, we see the use of Differential GPS or DGPS and Real-Time Kinematic or RTK positioning techniques in high-end systems. These techniques correct the GPS signals for various errors, resulting in significantly improved accuracy, often up to a few centimetres. 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah   In the next section, we will examine sensor-based localisation, another key method used in autonomous vehicles, to address some of the limitations of GPS-based localisation. Keep in mind that the most robust and accurate localisation systems in intelligent vehicles often involve a fusion of multiple methods, capitalising on the strengths of each while mitigating their weaknesses. Sensor-based Localisation Sensor-based localisation forms an integral part of the autonomous vehicles' localisation system, utilising a multitude of sensors, each offering unique information about the vehicle's surroundings. By synergistically using data from these sensors, we can establish a robust localisation system that stands up to a variety of environments and situations. Let's start with cameras, the eyes of an autonomous vehicle. Cameras capture visual data from the environment, which can be processed using computer vision algorithms to identify notable features or landmarks and their relative position to the vehicle. However, cameras are highly susceptible to variations in lighting and weather conditions. They can struggle in darkness, glare, fog, or heavy rain. Lidar uses lasers to map the environment around the vehicle. Lidar systems provide high-resolution, three-dimensional information about the vehicle's surroundings. However, their performance can degrade in adverse weather conditions like rain or snow. Radars use radio waves to detect objects and their distance and velocity. Radars are less affected by weather and lighting conditions but offer less detailed information compared to Lidar. IMUs measure the vehicle's velocity, orientation, and gravitational forces, helping maintain a continuous estimate of the vehicle's position over time, a process known as dead reckoning. However, errors in dead reckoning tend to accumulate over time, so it's used in conjunction with other techniques. Finally, ultrasonic sensors are used for close-range detection and localisation, especially useful in slow-speed scenarios like parking. It's crucial to note that these sensors do not work in isolation. Sensor fusion algorithms are used to combine the information from different sensors, enhancing the accuracy and reliability of the localisation. Advanced Localisation Techniques As we dive into advanced localisation techniques, we encounter the concept of 'Simultaneous Localisation and Mapping,' commonly known as SLAM. Originating in the field of robotics, SLAM has found considerable utility in the realm of autonomous vehicles. SLAM is a process through which a device can create a map of its environment while simultaneously tracking its position within it. It's like being dropped in an unknown maze, finding your way through it, and drawing a map of it, all at the same time. The concept may seem daunting, but it's central to autonomous vehicles navigating in unfamiliar environments. In the context of autonomous vehicles, SLAM uses data from Lidar, Radar, and cameras, as well as inertial sensors, to create high-fidelity maps of the environment. These maps can include static features like the road, buildings, and trees, as well as dynamic objects like pedestrians, cyclists, and other vehicles. The vehicle then uses these maps to accurately track its location and movement. Next in line, we have 'Visual Odometry' and 'Inertial Odometry'. 'Visual Odometry' estimates the relative position and orientation of the vehicle by analysing the images from one or more cameras. 'Inertial Odometry', on the other hand, uses data from IMUs to estimate the vehicle's motion. When combined, Visual-Inertial Odometry can provide high-precision localisation. However, these techniques also come with their own challenges, such as computational complexity and sensitivity to errors in sensor data. Therefore, the choice of techniques and their implementation must be carefully done, considering the specific requirements and constraints of the application.  Localisation in Action In this section, we'll illustrate how localisation systems come to life within real-world autonomous vehicles. We will look at how localisation systems actively contribute to the safe operation of autonomous vehicles, enhancing their capabilities of understanding and interacting with the world around them. 
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah   Let's start with Waymo. Waymo's vehicles make use of high-resolution Lidar-based maps along with real-time sensor data for localisation. These pre-built maps contain the rich detail of the environment, including road profiles, buildings, and landmarks. The autonomous vehicle uses its onboard sensors to scan the environment and matches this data with the pre-built maps to determine its exact position. By constantly cross-referencing sensor data with these detailed maps, Waymo vehicles maintain a highly accurate understanding of their location, direction, and speed, even while travelling at highway speeds. Another fascinating example is Tesla's 'Autopilot' system. Unlike Waymo, Tesla primarily relies on cameras and 'Visual Odometry' techniques for localisation. It uses a combination of multiple cameras to form a coherent understanding of the vehicle's surroundings and its place within it. Tesla's software then processes this visual data to calculate the distance travelled and changes in direction, which enables the vehicle to track its location and navigate the road safely. However, it's important to note that these methods are not without challenges. Lidar-based mapping can be expensive and may not be reliable in all weather conditions. Visual odometry, on the other hand, relies heavily on camera data, making it susceptible to issues like lighting changes and occlusions. Despite these challenges, the persistent pursuit of improving localisation techniques continues. In the next slide, we'll look at some of the future trends in localisation systems and how they might shape the future of autonomous vehicles. The Future of Localisation Here, we're going to discuss what the future might hold for localisation in the realm of autonomous vehicles. As this is a rapidly evolving field, we can expect to see many breakthroughs in the years to come. Firstly, as autonomous vehicles become more common, so too will the generation and sharing of data. This is where technologies like V2X communication will play a pivotal role. As we discussed before, V2X systems allow vehicles to share data about their environment, position, speed, and more with other vehicles and infrastructure. This shared data could enhance localisation systems, allowing vehicles to better understand their environment and even predict the actions of other road users. Advancements in AI and machine learning will also significantly contribute to improvement of localisation systems. Machine learning algorithms can learn and improve over time, making them increasingly accurate in predicting and adjusting to changes in the environment. This means they could be used to enhance localisation systems by improving the accuracy and reliability of environmental modelling and position estimation. Moreover, we'll touch on the concept of multi-modal sensor fusion, which involves integrating data from different types of sensors to improve the performance and robustness of localisation systems. As sensors continue to advance, and new types of sensors are developed, we can expect to see more sophisticated and reliable multi-modal sensor fusion techniques being employed in localisation systems. Lastly, let’s look at some real-world examples of companies at the cutting edge of these future trends. These companies are pushing the boundaries of what's possible with localisation, and their work will likely have a significant impact on the future of autonomous vehicles. It's an exciting time to be studying localisation in the context of autonomous vehicles.  We've covered quite a lot in this lecture, so let's take a moment to review the most important points we've learned about the role of localisation in intelligent vehicles. First and foremost, we learned about the importance of localisation as a functionality module in autonomous vehicles. It's vital in providing the vehicle with a sense of its precise position in the world, which is essential for navigation and safety. We discussed different methods of localisation, from the traditional GPS-based systems to more advanced sensor-based localisation techniques. Each method has its strengths and weaknesses, and they often work best when combined together, which leads us to the concept of sensor fusion.  
Intelligent Vehicle Design (ENGM298) – Prof Saber Fallah  Sensor fusion is a key aspect of modern localisation systems. It involves integrating data from multiple types of sensors, such as cameras, lidar, radar, and ultrasonic sensors, to achieve a more comprehensive and accurate understanding of the vehicle's surroundings and its position within it. We then dove into the realm of advanced localisation techniques. Here, we talked about SLAM (Simultaneous Localisation and Mapping) which represent the cutting-edge of localisation technology. These techniques provide a more dynamic and adaptable approach to localisation, which is crucial in complex and unpredictable driving environments. Lastly, we glimpsed into the future of localisation. With advances in AI, machine learning, and sensor technology, the future of localisation looks promising. Technologies like V2X communication and multi-modal sensor fusion are set to take localisation to new levels of accuracy and reliability. Remember, the field of intelligent vehicles is dynamic and ever-evolving. Stay curious and keep learning, and you'll be well-equipped to contribute to the exciting advancements that lie ahead in this industry.                
