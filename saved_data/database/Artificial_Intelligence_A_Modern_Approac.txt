=== Metadata ===
{
    "file_name": "Artificial_Intelligence_A_Modern_Approac.pdf",
    "file_path": "/Users/mf0016/Desktop/soe_RAG/resources/Artificial_Intelligence_A_Modern_Approac.pdf",
    "status": "Processed"
}

=== Content ===
Artificia l Intelligenc e
A Moder n Approac h
Stuar t J. Russel l and Peter Norvi g
Contributing  writers:
John F. Canny , Jitendr a M. Malik , Dougla s D. Edward s
Prentice  Hall,  Englewood  Cliffs,  New  Jersey  07632
Library  of Congress  Cataloging­in­Publication  Data
Russell , Stuar t J. (Stuar t Jonathan )
Artificia l intelligenc e : a moder n approach / Stuar t Russell , Peter Norvig .
p. cm.
Include s bibliographica l reference s and index .
ISBN 0­13­103805­ 2
1. Artificia l intelligenc e I. Norvig , Peter . II. Title .
Q335.R8 6 1995
006.3­dc2 0 94­3644 4
CIP
Publisher : Alan Apt
Productio n Editor : Mon a Pompil i
Developmenta l Editor : Sondr a Chave z
Cove r Designers : Stuar t Russel l and Peter Norvi g
Productio n Coordinator : Lori Bulwi n
Editoria l Assistant : Shirle y McGuir e
© 1995 by Prentice­Hall , Inc.
A Simo n & Schuste r Compan y
Englewoo d Cliffs , New Jerse y 0763 2
The autho r and publishe r of this book have used their best effort s in preparin g this book . Thes e effort s
includ e the development , research , and testin g of the theorie s and program s to determin e their
effectiveness . The autho r and publishe r shall not be liabl e in any even t for incidenta l or consequentia l
damage s in connectio n with , or arisin g out of, the furnishing , performance , or use of these programs .
All right s reserved . No part of this book may be
reproduced , in any form or by any means ,
withou t permissio n in writin g from the publisher .
Printe d in the Unite d State s of Americ a
10 98765432 1
ISBN D­IH­IQBSOS­ E
Prentice­Hal l Internationa l (UK ) Limited , London
Prentice­Hal l of Australi a Pty. Limited , Sydney
Prentice­Hal l Canada , Inc., Toronto
Prentice­Hal l Hispanoamericana , S.A. , Mexico
Prentice­Hal l of India Privat e Limited , New  Delhi
Prentice­Hal l of Japan , Inc., Tokyo
Simo n & Schuste r Asia Pte. Ltd., Singapore
Editor a Prentice­Hal l do Brasil , Ltda. , Rio de Janeiro
Prefac e
There are man y textbook s that offe r an introductio n to artificia l intelligenc e (AI) . This text has
five principa l feature s that togethe r distinguis h it from othe r texts .
1. Unified  presentation  of the field.
Some text s are organize d from a historica l perspective , describin g each of the majo r
problem s and solution s that have been uncovere d in 40 year s of AI research . Althoug h
there is valu e to this perspective , the resul t is to give the impressio n of a doze n or so barel y
relate d subfields , each with its own technique s and problems . We have chose n to presen t
AI as a unifie d field , workin g on a commo n proble m in variou s guises . This has entaile d
some reinterpretatio n of past research , showin g how it fits withi n a commo n framewor k
and how it relate s to othe r work that was historicall y separate . It has also led us to includ e
materia l not normall y covere d in AI texts .
2. Intelligent  agent design.
The unifyin g them e of the book is the concep t of an intelligent  agent.  In this view , the
proble m of AI is to describ e and build agent s that receiv e percept s from the environmen t
and perfor m actions . Eac h such agen t is implemente d by a functio n that map s percept s
to actions , and we cove r differen t way s to represen t these functions , such as productio n
systems , reactiv e agents , logica l planners , neura l networks , and decision­theoreti c systems .
We explai n the role of learnin g as extendin g the reach of the designe r into unknow n environ ­
ments , and show how it constrain s agen t design , favorin g explici t knowledg e representatio n
and reasoning . We treat robotic s and visio n not as independentl y define d problems , but
as occurrin g in the servic e of goal achievement . We stres s the importanc e of the task
environmen t characteristic s in determinin g the appropriat e agen t design .
3. Comprehensive  and up­to­date  coverage.
We cove r area s that are sometime s underemphasized , includin g reasonin g unde r uncer ­
tainty , learning , neura l networks , natura l language , vision , robotics , and philosophica l
foundations . We cove r man y of the more recen t idea s in the field , includin g simulate d
annealing , memory­bounde d search , globa l ontologies , dynami c and adaptiv e probabilisti c
(Bayesian ) networks , computationa l learnin g theory , and reinforcemen t learning . We also
provid e extensiv e note s and reference s on the historica l source s and curren t literatur e for
the main ideas in each chapter .
4. Equal  emphasis  on theory  and practice.
Theor y and practic e are give n equa l emphasis . All materia l is grounde d in first principle s
with rigorou s theoretica l analysi s wher e appropriate , but the poin t of the theor y is to get the
concept s acros s and explai n how they are used in actual , fielde d systems . The reade r of this
book will come away with an appreciatio n for the basic concept s and mathematica l method s
of AI, and also with an idea of what  can and canno t be done with today' s technology , at
what cost, and usin g wha t techniques .
5. Understanding  through  implementation.
The principle s of intelligen t agen t desig n are clarifie d by using them to actuall y build agents .
Chapte r 2 provide s an overvie w of agen t design , includin g a basic agen t and environmen t
vii
Vlll Prefac e
project . Subsequen t chapter s includ e programmin g exercise s that ask the studen t to add >.
capabilitie s to the agent , makin g it behav e mor e and mor e interestingl y and (we hope )
intelligently . Algorithm s are presente d at three level s of detail : pros e description s and !
pseudo­cod e in the text, and complet e Commo n Lisp program s availabl e on the Interne t or
on flopp y disk. All the agen t program s are interoperabl e and work in a unifor m framewor k
for simulate d environments .
This book is primaril y intende d for use in an undergraduat e cours e or cours e sequence . It
can also be used in a graduate­leve l cours e (perhap s with the additio n of som e of the primar y
source s suggeste d in the bibliographica l notes) . Becaus e of its comprehensiv e coverag e and the
large numbe r of detaile d algorithms , it is usefu l as a primar y referenc e volum e for AI graduat e
student s and professional s wishin g to branc h out beyon d their own subfield . We also hope that
AI researcher s coul d benefi t from thinkin g abou t the unifyin g approac h we advocate .
The only prerequisit e is familiarit y with basic concept s of compute r scienc e (algorithms ,
data structures , complexity ) at a sophomor e level . Freshma n calculu s is usefu l for understandin g
neura l network s and adaptiv e probabilisti c network s in detail . Som e experienc e with nonnumeri c
programmin g is desirable , but can be picke d up in a few week s study . We provid e implementation s
of all algorithm s in Commo n Lisp (see Appendi x B), but othe r language s such as Scheme , Prolog ,
Smalltalk , C++ , or ML coul d be used instead .
Overvie w of the book
The book is divide d into eight parts . Part 1, "Artificia l Intelligence, " sets the stage for all the others ,
and offer s a view of the AI enterpris e base d aroun d the idea of intelligen t agents—system s that
can decid e what to do and do it. Part II, "Proble m Solving, " concentrate s on method s for decidin g
what to do whe n one need s to think ahea d severa l steps , for exampl e in navigatin g acros s countr y
or playin g chess . Part III, "Knowledg e and Reasoning, " discusse s way s to represen t knowledg e
abou t the world—ho w it works , what  it is currentl y like, wha t one's action s migh t do—an d how
to reaso n logicall y with that knowledge . Par t IV, "Actin g Logically, " then discusse s how to
use thes e reasonin g method s to decid e wha t to do, particularl y by constructin g plans.  Part V,
"Uncertai n Knowledg e and Reasoning, " is analogou s to Parts III and IV, but it concentrate s on
reasonin g and decision­makin g in the presenc e of uncertainty  abou t the world , as migh t be faced ,
for example , by a syste m for medica l diagnosi s and treatment .
Together , Parts II to V describ e that part of the intelligen t agen t responsibl e for reachin g
decisions . Part VI, "Learning, " describe s method s for generatin g the knowledg e require d by these
decision­makin g components ; it also introduce s a new kind of component , the neural  network,
and its associate d learnin g procedures . Par t VII, "Communicating , Perceiving , and Acting, "
describe s way s in whic h an intelligen t agen t can perceiv e its environmen t so as to know wha t is
going on, whethe r by vision , touch , hearing , or understandin g language ; and way s in whic h it can
turn its plan s into real actions , eithe r as robo t motio n or as natura l languag e utterances . Finally ,
Part VIII, "Conclusions, " analyse s the past and futur e of AI, and provide s som e light amusemen t
by discussin g wha t AI reall y is and why it has alread y succeede d to som e degree , and airin g the
view s of those philosopher s who believ e that AI can neve r succee d at all.
Prefac e
Using this book
This is a big book ; coverin g all the chapter s and the project s woul d take two semesters . You will
notic e that the book is divide d into 27 chapters , whic h make s it easy to selec t the appropriat e
materia l for any chose n cours e of study . Each chapte r can be covere d in approximatel y one week .
Some reasonabl e choice s for a variet y of quarte r and semeste r course s are as follows :
• One­quarter  general  introductory  course:
Chapter s 1, 2, 3, 6, 7, 9, 11, 14, 15, 18, 22.
• One­semester  general  introductory  course:
Chapter s 1, 2, 3, 4, 6, 7, 9, 11, 13, 14, 15, 18, 19, 22, 24, 26, 27.
• One­quarter  course  with  concentration on search  and planning:
Chapter s 1, 2, 3, 4, 5, 6, 7, 9, 11, 12,13 .
• One­quarter  course  with  concentration on  reasoning  and expert  systems:
Chapter s 1,2, 3, 6, 7, 8,9, 10,11,14 , 15,16 .
• One­quarter  course  with  concentration on  natural  language:
Chapter s 1, 2, 3, 6, 7, 8, 9, 14, 15, 22, 23, 26, 27.
• One­semester  course  with  concentration  on learning  and neural  networks:
Chapter s 1, 2, 3, 4, 6, 7, 9, 14, 15, 16, 17,18 , 19, 20, 21.
• One­semester  course  with  concentration  on vision  and robotics:
Chapter s 1, 2, 3, 4, 6, 7, 11, 13, 14, 15, 16, 17, 24, 25, 20.
These sequence s coul d be used for both undergraduat e and graduat e courses . The relevan t parts
of the book coul d also be used to provid e the first phas e of graduat e specialt y courses . For
example , Part VI coul d be used in conjunctio n with reading s from the literatur e in a cours e on
machin e learning .
We have decide d not to designat e certai n section s as "optional " or certai n exercise s as
"difficult, " as individua l taste s and background s vary widely . Exercise s requirin g significan t
programmin g are marke d with a keyboar d icon , and thos e requirin g som e investigatio n of the
literatur e are marke d with a book icon . Altogether , over 300 exercise s are included . Som e of
them are large enoug h to be considere d term projects . Man y of the exercise s can best be solve d
by takin g advantag e of the code repository , whic h is describe d in Appendi x B. Throughou t the
book , importan t point s are marke d with a pointing  icon.
If you have any comment s on the book , we'd like to hear from you. Appendi x B include s
informatio n on how to contac t us.
Acknowledgement s
Jitendr a Mali k wrot e mos t of Chapte r 24 (Vision ) and John Cann y wrot e most of Chapte r
25 (Robotics) . Dou g Edward s researche d the Historica l Note s section s for all chapter s and wrot e
much of them . Tim Huan g helpe d with formattin g of the diagram s and algorithms . Maryan n
Simmon s prepare d the 3­D mode l from whic h the cove r illustratio n was produced , and Lisa
Mari e Sardegn a did the postprocessin g for the final image . Ala n Apt, Mon a Pompili , and Sondr a
Chave z at Prentic e Hall tried their best to keep us on schedul e and mad e man y helpfu l suggestion s
on desig n and content .
Prefac e
Stuar t woul d like to than k his parents , brother , and siste r for their encouragemen t and their
patienc e at his extende d absence . He hope s to be hom e for Christmas . He woul d also like to
thank Loy Sheflot t for her patienc e and support . He hope s to be hom e som e time tomorro w
afternoon . His intellectua l debt to his Ph.D . advisor , Michae l Genesereth , is eviden t throughou t
the book . RUG S (Russell' s Unusua l Grou p of Students ) have been unusuall y helpful .
Peter woul d like to than k his parent s (Torste n and Gerda ) for gettin g him started , his adviso r
(Bob Wilensky) , supervisor s (Bill Wood s and Bob Sproull ) and employe r (Sun Microsystems )
for supportin g his work in AI, and his wife (Kris ) and friend s for encouragin g and toleratin g him
throug h the long hour s of writing .
Befor e publication , draft s of this book were used in 26 course s by abou t 1000 students .
Both of us deepl y appreciat e the man y comment s of these student s and instructor s (and othe r
reviewers) . We can't than k them all individually , but we woul d like to acknowledg e the especiall y
helpfu l comment s of these people :
Tony Barrett , Howar d Beck , John Binder , Larr y Bookman , Chri s Brown , Laure n
Burka , Murra y Campbell , Anil Chakravarthy , Robert o Cipolla , Doug Edwards , Kut­
luhan Erol, Jeffre y Forbes , John Fosler , Bob Futrelle , Sabin e Glesner , Barbar a Grosz ,
Steve Hanks , Otha r Hansson , Jim Hendler , Tim Huang , Seth Hutchinson , Dan Ju­
rafsky , Lesli e Pack Kaelbling , Keij i Kanazawa , Surekh a Kasibhatla,  Simo n Kasif ,
Daphn e Roller , Rich Korf , Jame s Kurien , John Lazzaro , Jaso n Leatherman , Jon
LeBlanc , Jim Martin , And y Mayer , Stev e Minton , Leor a Morgenstern , Ron Musick ,
Stuar t Nelson , Stev e Omohundro , Ron Parr , Ton y Passera , Michae l Pazzani , Ira
Pohl, Marth a Pollack , Bruc e Porter , Malcol m Pradhan , Lorrain e Prior , Greg Provan ,
Phili p Resnik , Richar d Scherl , Danie l Sleator , Rober t Sproull , Lynn Stein , Devik a
Subramanian , Rich Sutton , Jonatha n Tash , Austi n Tate , Mar k Torrance , Randal l
Upham , Jim Waldo , Bonni e Webber , Michae l Wellman , Dan Weld , Richar d Yen ,
Shlom o Zilberstein .
Summar y of Content s
i
ii
in
IVArtificia l Intelligenc e 1
1 Introduction................................................................ . 3
2 Intelligen t Agents........................................................... . 31
Problem­solvin g 5 3
3 Solvin g Problem s by Searchin g ............................................. . 55
4 Informe d Searc h Method s .................................................. . 92
5 Gam e Playing............................................................... . 122
Knowledg e and reasonin g 14 9
6 Agent s that Reaso n Logically............................................... . 151
7 First­Orde r Logic........................................................... . 185
8 Buildin g a Knowledg e Base ................................................. . 217
9 Inferenc e in First­Orde r Logic.............................................. . 265
10 Logica l Reasonin g Systems.................................................. . 297
Actin g logicall y 33 5
11 Planning.................................................................... . 337
12 Practica l Plannin g .......................................................... . 367
13 Plannin g and Acting........................................................ . 392
Uncertai n knowledg e and reasonin g 41 3
14 Uncertainty................................................................. . 415
15 Probabilisti c Reasonin g Systems............................................ . 436
16 Makin g Simpl e Decision s ................................................... . 471
17 Makin g Comple x Decision s ................................................. . 498
Learnin g 52 3
18 Learnin g from Observations................................................ . 525
19 Learnin g in Neura l and Belie f Networks.................................... . 563
20 Reinforcemen t Learning.................................................... . 598
21 Knowledg e in Learning..................................................... . 625
Communicating , perceiving , and actin g 64 9
22 Agent s that Communicat e .................................................. . 651
23 Practica l Natura l Languag e Processin g ..................................... . 691
24 Perceptio n .................................................................. . 724
25 Robotics.................................................................... . 773
VIII Conclusion s 81 5
26 Philosophica l Foundation s .................................................. . 817
27 AI: Presen t and Futur e ..................................................... . 842
A Complexit y analysi s and O() notation....................................... . 851
B Note s on Language s and Algorithms........................................ . 854
Bibliograph y 85 9
Index 90 5VI
VII
Content s
I Artificia l Intelligenc e 1
1 Introductio n 3
1.1 Wha t is AI? ................................... . 4
Actin g humanly : The Turin g Test approac h ................... . 5
Thinkin g humanly : The cognitiv e modellin g approac h ............. . 6
Thinkin g rationally : The laws of though t approac h ............... . 6
Actin g rationally : The rationa l agen t approac h ................. . 7
1.2 Th e Foundation s of Artificia l Intelligenc e .................... . 8
Philosoph y (428 B.C.­present ) ......................... . 8
Mathematic s (c. 800­present ) .......................... . 11
Psycholog y (1879­present ) ........................... . 12
Compute r engineerin g (1940­present ) ..................... . 14
Linguistic s (1957­present ) ........................... . 15
1.3 Th e Histor y of Artificia l Intelligenc e ...................... . 16
The gestatio n of artificia l intelligenc e (1943­1956) . .............. . 16
Early enthusiasm , grea t expectation s (1952­1969 ) ............... . 17
A dose of realit y (1966­1974 ) .......................... . 20
Knowledge­base d systems : The key to power ? (1969­1979) . ......... . 22
AI become s an industr y (1980­1988 ) ...................... . 24
The retur n of neura l network s (1986­present ) ................. . 24
Recen t event s (1987­present ) .......................... . 25
1.4 Th e State of the Art ............................... . 26
1.5 Summar y ..................................... . 27
Bibliographica l and Historica l Note s .......................... . 28
Exercise s ........................................ . 28
2 Intelligen t Agent s 3 1
2.1 Introductio n ................................... . 31
2.2 Ho w Agent s Shoul d Act ............................. . 31
The ideal mappin g from percep t sequence s to action s ............. . 34
Autonom y .................................... . 35
2.3 Structur e of Intelligen t Agent s .......................... . 35
Agen t program s ................................. . 37
Why not just look up the answers ? ....................... . 38
An exampl e ................................... . 39
Simpl e refle x agent s ............................... . 40
Agent s that keep track of the worl d ....................... . 41
Goal­base d agent s ................................ . 42
Utility­base d agent s ............................... . 44
2.4 Environment s .................................. . 45
XIV Content s
Propertie s of environment s ........................... . 46
Environmen t program s ............................. . 47
2.5 Summar y ..................................... . 49
Bibliographica l and Historica l Note s .......................... . 50
Exercise s ........................................ . 50
II Problem­solvin g 5 3
3 Solvin g Problem s by Searchin g 5 5
3.1 Problem­Solvin g Agent s ............................. . 55
3.2 Formulatin g Problem s .............................. . 57
Knowledg e and proble m types ......................... . 58
Well­define d problem s and solution s ...................... . 60
Measurin g problem­solvin g performanc e .................... . 61
Choosin g state s and action s ........................... . 61
3.3 Exampl e Problem s ................................ . 63
Toy problem s .................................. . 63
Real­worl d problem s .............................. . 68
3.4 Searchin g for Solution s ............................. . 70
Generatin g actio n sequence s ............................ . 70
Data structure s for searc h trees ......................... . 72
3.5 Searc h Strategie s ................................. . 73
Breadth­firs t searc h ............................... . 74
Unifor m cost searc h ............................... . 75
Depth­firs t searc h ................................ . 77
Depth­limite d searc h ............................... . 78
Iterativ e deepenin g searc h ............................ . 78
Bidirectiona l searc h ............................... . 80
Comparin g searc h strategie s ........................... . 81
3.6 Avoidin g Repeate d State s ............................ . 82
3.7 Constrain t Satisfactio n Searc h .......................... . 83
3.8 Summar y ..................................... . 85
Bibliographica l and Historica l Note s .......................... . 86
Exercise s ........................................ . 87
4 Informe d Searc h Method s 9 2
4.1 Best­Firs t Searc h ................................. . 92
Minimiz e estimate d cost to reach a goal: Greed y searc h ............ . 93
Minimizin g the total path cost: A* searc h ................... . 96
4.2 Heuristi c Function s ............................... . 101
The effec t of heuristi c accurac y on performanc e ................ . 102
Inventin g heuristi c function s ........................... . 103
Heuristic s for constrain t satisfactio n problem s ................. . 104
4.3 Memor y Bounde d Searc h ............................ . 106
Contents_______________________________________________________x v
Iterativ e deepenin g A* searc h (IDA* ) ...................... . 106
SMA * searc h .................................. . 107
4.4 Iterativ e Improvemen t Algorithm s ........................11 1
Hill­climbin g searc h ................................11 1
Simulate d annealin g ................................11 3
Application s in constrain t satisfactio n problem s .................11 4
4.5 Summar y ..................................... . 115
Bibliographica l and Historica l Note s .......................... . 115
Exercise s ........................................ . 118
5 Gam e Playin g 12 2
5.1 Introduction : Game s as Searc h Problem s .................... . 122
5.2 Perfec t Decision s in Two­Perso n Game s .................... . 123
5.3 Imperfec t Decision s ............................... . 126
Evaluatio n function s ............................... . 127
Cuttin g off searc h ................................ . 129
5.4 Alpha­Bet a Prunin g ............................... . 129
Effectivenes s of alpha­bet a prunin g ....................... . 131
5.5 Game s That Includ e an Elemen t of Chanc e ................... . 133
Positio n evaluatio n in game s with chanc e node s ................ . 135
Complexit y of expectiminima x ......................... . 135
5.6 State­of­the­Ar t Gam e Program s ........................ . 136
Ches s ....................................... . 137
Checker s or Draught s .............................. . 138
Othell o ...................................... . 138
Backgammo n .................................. . 139
Go ........................................ . 139
5.7 Discussio n .................................... . 139
5.8 Summar y ..................................... . 141
Bibliographica l and Historica l Note s .......................... . 141
Exercise s ........................................ . 145
III Knowledg e and reasonin g 14 9
6 Agent s that Reaso n Logicall y 15 1
6.1 A  Knowledge­Base d Agen t ........................... . 151
6.2 Th e Wumpu s Worl d Environmen t ........................ . 153
Specifying  the environmen t ........................... . 154
Actin g and reasonin g in the wumpu s worl d ................... . 155
6.3 Representation , Reasoning , and Logi c ..................... . 157
Representatio n .................................. . 160
Inferenc e ..................................... . 163
Logic s ...................................... . 165
6.4 Prepositiona l Logic : A Very Simpl e Logi c ................... . 166
XVI Content s
Synta x ...................................... . 166
Semantic s .................................... . 168
Validit y and inferenc e .............................. . 169
Model s ...................................... . 170
Rule s of inferenc e for propositiona l logic .................... . 171
Complexit y of prepositiona l inferenc e ..................... . 173
6.5 A n Agen t for the Wumpu s Worl d ........................ . 174
The knowledg e base ............................... . 174
Findin g the wumpu s ............................... . 175
Translatin g knowledg e into actio n ........................ . 176
Problem s with the propositiona l agen t ..................... . 176
6.6 Summar y ..................................... . 178
Bibliographica l and Historica l Note s .......................... . 178
Exercise s ........................................ . 180
7 First­Orde r Logi c 18 5
7.1 Synta x and Semantic s .............................. . 186
Term s ...................................... . 188
Atomi c sentence s ................................ . 189
Comple x sentence s ............................... . 189
Quantifier s .................................... . 189
Equalit y ..................................... . 193
7.2 Extension s and Notationa l Variation s ...................... . 194
Higher­orde r logic ................................ . 195
Functiona l and predicat e expression s usin g the A operato r ........... . 195
The uniquenes s quantifie r 3! .......................... . 196
The uniquenes s operato r / ............................ . 196
Notationa l variations............................... . 196
7.3 Usin g First­Orde r Logi c ............................. . 197
The kinshi p domai n ............................... . 197
Axioms , definitions , and theorem s ....................... . 198
The domai n of sets ................................ . 199
Specia l notation s for sets, lists and arithmeti c .................. . 200
Askin g question s and gettin g answer s ...................... . 200
7.4 Logica l Agent s for the Wumpu s Worl d ..................... . 201
7.5 A  Simpl e Refle x Agen t ............................. . 202
Limitation s of simpl e refle x agents  ....................... . 203
7.6 Representin g Chang e in the Worl d ....................... . 203
Situatio n calculu s ................................ . 204
Keepin g track of locatio n ............................ . 206
7.7 Deducin g Hidde n Propertie s of the Worl d . ................... . 208
7.8 Preference s Amon g Action s ........................... . 210
7.9 Towar d a Goal­Base d Agen t ........................... . 211
7.10 Summar y ..................................... . 211
Content s xvn
Bibliographica l and Historica l Note s .......................... . 212
Exercise s ........................................ . 213
8 Buildin g a Knowledg e Base 21 7
8.1 Propertie s of Good and Bad Knowledg e Base s ................. . 218
8.2 Knowledg e Engineerin g ............................. . 221
8.3 Th e Electroni c Circuit s Domai n ......................... . 223
Decid e wha t to talk abou t ............................ . 223
Decid e on a vocabular y ............................. . 224
Encod e genera l rules ............................... . 225
Encod e the specifi c instanc e ........................... . 225
Pose querie s to the inferenc e procedur e ..................... . 226
8.4 Genera l Ontolog y ................................ . 226
Representin g Categorie s ............................. . 229
Measure s ..................................... . 231
Composit e object s ................................ . 233
Representin g chang e with event s ........................ . 234
Times , intervals , and action s ........................... . 238
Object s revisite d ................................. . 240
Substance s and object s ............................. . 241
Menta l event s and menta l object s ........................ . 243
Knowledg e and actio n .............................. . 247
8.5 Th e Grocer y Shoppin g Worl d .......................... . 247
Complet e descriptio n of the shoppin g simulatio n ................ . 248
Organizin g knowledg e .............................. . 249
Menu­plannin g .................................. . 249
Navigatin g .................................... . 252
Gatherin g .................................... . 253
Communicatin g ................................. . 254
Payin g ...................................... . 255
8.6 Summar y ..................................... . 256
Bibliographica l and Historica l Note s .......................... . 256
Exercise s ........................................ . 261
9 Inferenc e in First­Orde r Logi c 26 5
9.1 Inferenc e Rule s Involvin g Quantifier s ...................... . 265
9.2 A n Exampl e Proo f ................................ . 266
9.3 Generalize d Modu s Ponen s ........................... . 269
Canonica l form ................................. . 270
Unificatio n .................................... . 270
Sampl e proo f revisite d .............................. . 271
9.4 Forwar d and Backwar d Chainin g ........................ . 272
Forward­chainin g algorith m ........................... . 273
Backward­chainin g algorith m .......................... . 275
XV11 1 Content s
9.5 Completenes s .................................. . 276
9.6 Resolution : A Complet e Inferenc e Procedur e .................. . 277
The resolutio n inferenc e rule .......................... . 278
Canonica l form s for resolutio n ......................... . 278
Resolutio n proof s ................................ . 279
Conversio n to Norma l Form ........................... . 281
Exampl e proo f .................................. . 282
Dealin g with equalit y .............................. . 284
Resolutio n strategie s ............................... . 284
9.7 Completenes s of resolutio n ........................... . 286
9.8 Summar y ..................................... . 290
Bibliographica l and Historica l Note s .......................... . 291
Exercise s ........................................ . 294
10 Logica l Reasonin g System s 29 7
10.1 Introductio n ................................... . 297
10.2 Indexing , Retrieval , and Unificatio n ....................... . 299
Implementin g sentence s and term s ....................... . 299
Store and fetch .................................. . 299
Table­base d indexin g .............................. . 300
Tree­base d indexin g ............................... . 301
The unificatio n algorith m ............................ . 302
10.3 Logi c Programmin g System s .......................... . 304
The Prolo g languag e ............................... . 304
Implementatio n ................................. . 305
Compilatio n of logic program s ......................... . 306
Othe r logic programmin g language s ...................... . 308
Advance d contro l facilitie s ........................... . 308
10.4 Theore m Prover s ................................. . 310
Desig n of a theore m prove r ........................... . 310
Extendin g Prolo g .................................31 1
Theore m prover s as assistant s .......................... . 312
Practica l uses of theore m prover s ........................ . 313
10.5 Forward­Chainin g Productio n System s ......................31 3
Matc h phas e ................................... . 314
Conflic t resolutio n phas e ............................ . 315
Practica l uses of productio n system s ...................... . 316
10.6 Fram e System s and Semanti c Network s . .................... . 316
Synta x and semantic s of semanti c network s .................. . 317
Inheritanc e with exception s ........................... . 319
Multipl e inheritanc e ............................... . 320
Inheritanc e and chang e ............................. . 320
Implementatio n of semanti c network s ...................... . 321
Expressivenes s of semanti c network s ...................... . 323
IContent s _________________________________________________ _ xix
10.7 Descriptio n Logic s ................................ . 323
Practica l uses of descriptio n logic s ....................... . 325
10.8 Managin g Retractions , Assumptions , and Explanation s ............ . 325
10.9 Summar y ..................................... . 327
Bibliographica l and Historica l Note s .......................... . 328
Exercise s ........................................ . 332
IV Actin g logicall y 33 5
11 Plannin g 33 7
11.1 A Simpl e Plannin g Agen t ............................ . 337
11.2 Fro m Proble m Solvin g to Plannin g ....................... . 338
11.3 Plannin g in Situatio n Calculu s . ......................... . 341
11.4 Basi c Representation s for Plannin g ....................... . 343
Representation s for states and goals ....................... . 343
Representation s for action s ........................... . 344
Situatio n space and plan space ......................... . 345
Representation s for plans ............................ . 346
Solution s ..................................... . 349
11.5 A Partial­Orde r Plannin g Exampl e ....................... . 349
11.6 A Partial­Orde r Plannin g Algorith m ...................... . 355
11.7 Plannin g with Partiall y Instantiate d Operator s ................. . 357
11.8 Knowledg e Engineerin g for Plannin g ...................... . 359
The block s worl d ................................ . 359
Shakey' s worl d .................................. . 360
11.9 Summar y ..................................... . 362
Bibliographica l and Historica l Note s .......................... . 363
Exercise s ........................................ . 364
12 Practica l Plannin g 36 7
12.1 Practica l Planner s ................................ . 367
Spacecraf t assembly , integration , and verificatio n ................ . 367
Job shop schedulin g ............................... . 369
Schedulin g for space mission s .......................... . 369
Buildings , aircraf t carriers , and beer factorie s .................. . 371
12.2 Hierarchica l Decompositio n ........................... . 371
Extendin g the languag e ............................. . 372
Modifyin g the planne r .............................. . 374
12.3 Analysi s of Hierarchica l Decompositio n .................... . 375
Decompositio n and sharin g ........................... . 379
Decompositio n versu s approximatio n ...................... . 380
12.4 Mor e Expressiv e Operato r Description s . .................... . 381
Conditiona l effects  ................................ . 381
Negate d and disjunctiv e goals .......................... . 382
XX Content s
Universa l quantificatio n ............................. . 383
A planne r for expressiv e operato r description s ................. . 384
12.5 Resourc e Constraint s .............................. . 386
Usin g measure s in plannin g ........................... . 386
Tempora l constraints............................... . 388
12.6 Summar y ..................................... . 388
Bibliographica l and Historica l Note s .......................... . 389
Exercise s ........................................ . 390
13 Plannin g and Actin g 39 2
13.1 Conditiona l Plannin g .............................. . 393
The natur e of conditiona l plans ......................... . 393
An algorith m for generatin g conditiona l plans ................. . 395
Extendin g the plan languag e ........................... . 398
13.2 A Simpl e Replannin g Agen t ........................... . 401
Simpl e replannin g with executio n monitoring.................. . 402
13.3 Full y Integrate d Plannin g and Executio n .................... . 403
13.4 Discussio n and Extension s ........................... . 407
Comparin g conditiona l plannin g and replannin g ................ . 407
Coercio n and abstractio n ............................ . 409
13.5 Summar y ..................................... . 410
Bibliographica l and Historica l Note s .......................... . 411
Exercise s ........................................ . 412
V Uncertai n knowledg e and reasonin g 41 3
14 Uncertaint y 41 5
14.1 Actin g unde r Uncertaint y ............................ . 415
Handlin g uncertai n knowledg e ......................... . 416
Uncertaint y and rationa l decision s . ....................... . 418
Desig n for a decision­theoreti c agen t ...................... . 419
14.2 Basi c Probabilit y Notatio n . ........................... . 420
Prior probabilit y ................................. . 420
Conditiona l probabilit y ............................. . 421
14.3 Th e Axiom s of Probabilit y ........................... . 422
Why the axiom s of probabilit y are reasonabl e ................. . 423
The joint probabilit y distributio n ........................ . 425
14.4 Bayes ' Rule and Its Use ............................. . 426
Applyin g Bayes ' rule: The simpl e case ..................... . 426
Normalization  .................................. . 427
Usin g Bayes ' rule: Combinin g evidenc e .................... . 428
14.5 Wher e Do Probabilitie s Com e From ? ...................... . 430
14.6 Summar y ..................................... . 431
Bibliographica l and Historica l Note s .......................... . 431
Content s xxi
Exercise s ........................................ . 433
15 Probabilisti c Reasonin g System s 43 6
15.1 Representin g Knowledg e in an Uncertai n Domai n ............... . 436
15.2 Th e Semantic s of Belie f Network s ....................... . 438
Representin g the joint probabilit y distributio n ................. . 439
Conditiona l independenc e relation s in belie f network s ............. . 444
15.3 Inferenc e in Belie f Network s .......................... . 445
The natur e of probabilisti c inference s ...................... . 446
An algorith m for answerin g querie s ....................... . 447
15.4 Inferenc e in Multipl y Connecte d Belie f Network s ............... . 453
Clusterin g method s ............................... . 453
Cutse t conditionin g method s .......................... . 454
Stochasti c simulatio n method s ......................... . 455
15.5 Knowledg e Engineerin g for Uncertai n Reasonin g ............... . 456
Case study : The Pathfinde r syste m ....................... . 457
15.6 Othe r Approache s to Uncertain  Reasonin g ................... . 458
Defaul t reasonin g ................................ . 459
Rule­base d method s for uncertai n reasonin g .................. . 460
Representin g ignorance : Dempster­Shafe r theor y ............... . 462
Representin g vagueness : Fuzz y sets and fuzz y logic .............. . 463
15.7 Summar y ..................................... . 464
Bibliographica l and Historica l Note s .......................... . 464
Exercise s ........................................ . 467
16 Makin g Simpl e Decision s 47 1
16.1 Combinin g Belief s and Desire s Unde r Uncertaint y ............... . 471
16.2 Th e Basi s of Utilit y Theor y ........................... . 473
Constraint s on rationa l preference s ....................... . 473
... and then there was Utilit y .......................... . 474
16.3 Utilit y Function s ................................. . 475
The utilit y of mone y ............................... . 476
Utilit y scale s and utilit y assessmen t ....................... . 478
16.4 Multiattribut e utilit y function s .......................... . 480
Dominanc e .................................... . 481
Preferenc e structur e and multiattribut e utilit y .................. . 483
16.5 Decisio n Network s ................................ . 484
Representin g a decisio n proble m using decisio n network s ........... . 484
Evaluatin g decisio n network s .......................... . 486
16.6 Th e Valu e of Informatio n ............................ . 487
A simpl e exampl e ................................ . 487
A genera l formul a ................................ . 488
Propertie s of the valu e of informatio n ...................... . 489
Implementin g an information­gatherin g agen t ................. . 490
xxii Content s
16.7 Decision­Theoreti c Exper t System s ....................... . 491
16.8 Summar y ..................................... . 493
Bibliographica l and Historica l Note s .......................... . 493
Exercise s ........................................ . 495
17 Makin g Comple x Decision s 49 8
17.1 Sequentia l Decisio n Problem s .......................... . 498
17.2 Valu e Iteratio n .................................. . 502
17.3 Polic y Iteratio n . ................................. . 505
17.4 Decision­Theoreti c Agen t Desig n ........................ . 508
The decisio n cycl e of a rationa l agen t ...................... . 508
Sensin g in uncertai n world s ........................... . 510
17.5 Dynami c Belie f Network s ............................ . 514
17.6 Dynami c Decisio n Network s .......................... . 516
Discussio n .....................................51 8
17.7 Summar y ..................................... . 519
Bibliographica l and Historica l Note s .......................... . 520
Exercise s ........................................ . 521
VI Learnin g 52 3
18 Learnin g from Observation s 52 5
18.1 A Genera l Mode l of Learnin g Agent s ...................... . 525
Component s of the performanc e elemen t .................... . 527
Representatio n of the component s ........................ . 528
Availabl e feedbac k ................................ . 528
Prior knowledg e ................................. . 528
Bringin g it all togethe r .............................. . 529
18.2 Inductiv e Learnin g ................................ . 529
18.3 Learnin g Decisio n Tree s ............................. . 531
Decisio n trees as performanc e element s ..................... . 531
Expressivenes s of decisio n trees ......................... . 532
Inducin g decisio n trees from example s ..................... . 534
Assessin g the performanc e of the learnin g algorith m .............. . 538
Practica l uses of decisio n tree learnin g ..................... . 538
18.4 Usin g Informatio n Theor y ............................ . 540
Nois e and overfillin g ............................... . 542
Broadenin g the applicabilit y of decisio n Irees .................. . 543
18.5 Learnin g Genera l Logica l Description s ..................... . 544
Hypothese s .................................... . 544
Example s ..................................... . 545
Current­besl­hypolhesi s searc h ......................... . 546
Least­commitmen t searc h ............................ . 549
Discussio n .................................... . 552
Content s XXll l
18.6 Wh y Learnin g Works : Computationa l Learnin g Theor y ............ . 552
How man y example s are needed ? ........................ . 553
Learnin g decisio n lists .............................. . 555
Discussio n .................................... . 557
18.7 Summar y ..................................... . 558
Bibliographica l and Historica l Note s .......................... . 559
Exercise s ........................................ . 560
19 Learnin g in Neura l and Belie f Network s 56 3
19.1 Ho w the Brai n Work s .............................. . 564
Comparin g brain s with digita l computer s .................... . 565
19.2 Neura l Network s ................................. . 567
Notatio n ..................................... . 567
Simpl e computin g element s ........................... . 567
Networ k structure s ................................ . 570
Optima l networ k structur e ............................ . 572
19.3 Perceptron s ................................... . 573
What perceptron s can represen t ......................... . 573
Learnin g linearl y separabl e function s ...................... . 575
19.4 Multilaye r Feed­Forwar d Network s ....................... . 578
Back­propagatio n learnin g ............................ . 578
Back­propagatio n as gradien t descen t searc h .................. . 580
Discussio n .................................... . 583
19.5 Application s of Neura l Network s ........................ . 584
Pronunciatio n .................................. . 585
Handwritte n characte r recognitio n ....................... . 586
Drivin g ...................................... . 586
19.6 Bayesia n Method s for Learnin g Belie f Network s ................ . 588
Bayesia n learnin g ................................ . 588
Belie f networ k learnin g problem s ........................ . 589
Learnin g network s with fixed structur e ..................... . 589
A compariso n of belie f network s and neura l network s ............. . 592
19.7 Summar y ..................................... . 593
Bibliographica l and Historica l Note s .......................... . 594
Exercise s ........................................ . 596
20 Reinforcemen t Learnin g 59 8
20.1 Introductio n ................................... . 598
20.2 Passiv e Learnin g in a Know n Environmen t ................... . 600
Nai'v e updatin g .................................. . 601
Adaptiv e dynami c programmin g ........................ . 603
Tempora l differenc e learnin g .......................... . 604
20.3 Passiv e Learnin g in an Unknow n Environmen t ................. . 605
20.4 Activ e Learnin g in an Unknow n Environmen t ................. . 607
XXI V Content s
20.5 Exploratio n ................................... . 609
20.6 Learnin g an Action­Valu e Functio n ....................... . 612
20.7 Generalizatio n in Reinforcemen t Learnin g ................... . 615
Application s to game­playin g .......................... . 617
Applicatio n to robo t contro l ............................61 7
20.8 Geneti c Algorithm s and Evolutionar y Programmin g .............. . 619
20.9 Summar y ..................................... . 621
Bibliographica l and Historica l Note s .......................... . 622
Exercise s ........................................ . 623
21 Knowledg e in Learnin g 62 5
21.1 Knowledg e in Learnin g ............................. . 625
Some simpl e example s ............................. . 626
Some genera l scheme s .............................. . 627
21.2 Explanation­Base d Learnin g .......................... . 629
Extractin g genera l rules from example s ..................... . 630
Improvin g efficienc y ............................... . 631
21.3 Learnin g Usin g Relevanc e Informatio n ..................... . 633
Determinin g the hypothesi s space ........................ . 633
Learnin g and usin g relevanc e informatio n ................... . 634
21.4 Inductiv e Logi c Programmin g .......................... . 636
An exampl e ................................... . 637
Invers e resolutio n ................................ . 639
Top­dow n learnin g method s ........................... . 641
21.5 Summar y ..................................... . 644
Bibliographica l and Historica l Note s .......................... . 645
Exercise s ........................................ . 647
VII Communicating , perceiving , and actin g 64 9
22 Agent s that Communicat e 65 1
22.1 Communicatio n as Actio n ............................ . 652
Fundamental s of languag e ............................ . 654
The componen t steps of communicatio n .................... . 655
Two model s of communicatio n ......................... . 659
22.2 Type s of Communicatin g Agent s ........................ . 659
Communicatin g usin g Tell and Ask ....................... . 660
Communicatin g usin g forma l languag e ..................... . 661
An agen t that communicate s ........................... . 662
22.3 A Forma l Gramma r for a Subse t of Englis h ................... . 662
The Lexico n of £o ................................ . 664
The Gramma r of £Q ............................... . 664
22.4 Syntacti c Analysi s (Parsing ) ........................... . 664
22.5 Definit e Claus e Gramma r (DCG ) ........................ . 667
Content s xxv
22.6 Augmentin g a Gramma r ............................. . 668
Verb Subcategorizatio n ............................. . 669
Generativ e Capacit y of Augmente d Grammar s ................. . 671
22.7 Semanti c Interpretatio n ............................. . 672
Semantic s as DCG Augmentation s ....................... . 673
The semantic s of "Joh n love s Mary " ...................... . 673
The semantic s of £\ ............................... . 675
Convertin g quasi­logica l form to logica l form ................. . 677
Pragmati c Interpretatio n ............................. . 678
22.8 Ambiguit y and Disambiguatio n ......................... . 680
Disambiguatio n ................................. . 682
22.9 A Communicatin g Agen t ............................ . 683
22.10 Summar y ..................................... . 684
Bibliographica l and Historica l Note s .......................... . 685
Exercise s ........................................ . 688
23 Practica l Natura l Languag e Processin g 69 1
23.1 Practica l Application s .............................. . 691
Machin e translatio n ............................... . 691
Databas e acces s ................................. . 693
Informatio n retrieva l ............................... . 694
Text categorizatio n ................................ . 695
Extractin g data from text ............................ . 696
23.2 Efficien t Parsin g ................................. . 696
Extractin g parse s from the chart : Packin g .................... . 701
23.3 Scalin g Up the Lexico n ............................. . 703
23.4 Scalin g Up the Gramma r ............................ . 705
Nomina l compound s and appositio n ...................... . 706
Adjectiv e phrase s ................................ . 707
Determiner s ................................... . 708
Noun phrase s revisite d .............................. . 709
Clausa l complement s .............................. . 710
Relativ e clause s ................................. . 710
Question s .....................................71 1
Handlin g agrammatica l string s ......................... . 712
23.5 Ambiguit y .................................... . 712
Syntacti c evidenc e ................................ . 713
Lexica l evidenc e ..................................71 3
Semanti c evidenc e ................................ . 713
Metonym y .................................... . 714
Metapho r ..................................... . 715
23.6 Discours e Understandin g ............................ . 715
The structur e of coheren t discours e ....................... . 717
23.7 Summar y ..................................... . 719
xxvi Content s
Bibliographica l and Historica l Note s .......................... . 720
Exercise s ........................................ . 721
24 Perceptio n 72 4
24.1 Introductio n ................................... . 724
24.2 Imag e Formatio n ................................. . 725
Pinhol e camer a .................................. . 725
Lens system s ................................... . 727
Photometr y of imag e formatio n ......................... . 729
Spectrophotometr y of imag e formatio n ..................... . 730
24.3 Image­Processin g Operation s for Early Visio n ................. . 730
Convolutio n with linea r filter s .......................... . 732
Edge detectio n .................................. . 733
24.4 Extractin g 3­D Informatio n Usin g Visio n .................... . 734
Motio n ...................................... . 735
Binocula r stereopsi s ............................... . 737
Textur e gradient s ................................. . 742
Shadin g ..................................... . 743
Contou r ..................................... . 745
24.5 Usin g Visio n for Manipulatio n and Navigatio n ................. . 749
24.6 Objec t Representatio n and Recognitio n ..................... . 751
The alignmen t metho d .............................. . 752
Usin g projectiv e invariant s ........................... . 754
24.7 Speec h Recognitio n ............................... . 757
Signa l processin g ................................ . 758
Definin g the overal l speec h recognitio n mode l ................. . 760
The languag e model : P(words ) ......................... . 760
The acousti c model : P(signallwords ) ...................... . 762
Puttin g the model s togethe r ........................... . 764
The searc h algorith m .............................. . 765
Trainin g the mode l ................................ . 766
24.8 Summar y ..................................... . 767
Bibliographica l and Historica l Note s .......................... . 767
Exercise s ........................................ . 771
25 Robotic s 77 3
25.1 Introductio n ................................... . 773
25.2 Tasks : Wha t Are Robot s Goo d For? . ...................... . 774
Manufacturin g and material s handlin g ..................... . 774
Gofe r robot s ................................... . 775
Hazardou s environment s ............................. . 775
Telepresenc e and virtua l realit y ......................... . 776
Augmentatio n of huma n abilitie s ........................ . 776
25.3 Parts : Wha t Are Robot s Mad e Of? ....................... . 777
Content s _________________________________________________________xxvi i
Effectors : Tool s for actio n ............................ . 777
Sensors : Tool s for perceptio n .......................... . 782
25.4 Architecture s ................................... . 786
Classica l architectur e .............................. . 787
Situate d automat a ................................ . 788
25.5 Configuratio n Spaces : A Framewor k for Analysi s ............... . 790
Generalize d configuratio n spac e ......................... . 792
Recognizabl e Sets ................................ . 795
25.6 Navigatio n and Motio n Plannin g ........................ . 796
Cell decompositio n ............................... . 796
Skeletonizatio n method s ............................. . 798
Fine­motio n plannin g .............................. . 802
Landmark­base d navigatio n ........................... . 805
Onlin e algorithm s ................................ . 806
25.7 Summar y ..................................... . 809
Bibliographica l and Historica l Note s .......................... . 809
Exercise s ........................................ . 811
VIII Conclusion s 81 5
26 Philosophica l Foundation s 81 7
26.1 Th e Big Question s ................................ . 817
26.2 Foundation s of Reasonin g and Perceptio n ................... . 819
26.3 On the Possibilit y of Achievin g Intelligen t Behavio r .............. . 822
The mathematica l objectio n ........................... . 824
The argumen t from informalit y ......................... . 826
26.4 Intentionalit y and Consciousnes s ........................ . 830
The Chines e Room ............................... . 831
The Brai n Prosthesi s Experimen t ........................ . 835
Discussio n .................................... . 836
26.5 Summar y ..................................... . 837
Bibliographica l and Historica l Note s .......................... . 838
Exercise s ........................................ . 840
27 AI: Presen t and Futur e 84 2
27.1 Hav e We Succeede d Yet? ............................ . 842
27.2 Wha t Exactl y Are We Tryin g to Do? ...................... . 845
27.3 Wha t If We Do Succeed ? ............................ . 848
A Complexit y analysi s and O() notatio n 85 1
A.I Asymptoti c Analysi s ............................... . 851
A.2 Inherentl y Hard Problem s ............................ . 852
Bibliographica l and Historica l Note s .......................... . 853
XXV11 1 Content s
B Note s on Language s and Algorithm s 85 4
B.I Definin g Language s with Backus­Nau r Form (BNF ) .............. . 854
B.2 Describin g Algorithm s with Pseudo­Cod e ................... . 855
Nondeterminis m ................................. . 855
Static variable s .................................. . 856
Function s as value s ............................... . 856
B.3 Th e Cod e Repositor y .............................. . 857
B.4 Comment s .................................... . 857
Bibliograph y
Index859
905
Parti
ARTIFICIA L INTELLIGENC E
The two chapter s in this part introduc e the subjec t of Artificia l Intelligenc e or AI
and our approac h to the subject : that AI is the stud y of agents  that exist in an
environmen t and perceiv e and act.
Sectio n The Foundation s of Artificia l Intelligenc e
and subtractin g machin e calle d the Pascaline . Leibni z improve d on this in 1694 , buildin g a
mechanica l devic e that multiplie d by doin g repeate d addition . Progres s stalle d for over a centur y
until Charle s Babbag e (1792­1871 ) dreame d that logarith m table s coul d be compute d by machine .
He designe d a machin e for this task , but neve r complete d the project . Instead , he turne d to the
desig n of the Analytica l Engine , for whic h Babbag e invente d the idea s of addressabl e memory ,
store d programs , and conditiona l jumps . Althoug h the idea of programmabl e machine s was
not new—i n 1805 , Josep h Mari e Jacquar d invente d a loom that coul d be programme d usin g
punche d cards—Babbage' s machin e was the first artifac t possessin g the characteristic s necessar y
for universa l computation . Babbage' s colleagu e Ada Lovelace , daughte r of the poet Lord Byron ,
wrote program s for the Analytica l Engin e and even speculate d that the machin e coul d play ches s
or compos e music . Lovelac e was the world' s first programmer , and the first of man y to endur e
massiv e cost overrun s and to have an ambitiou s projec t ultimatel y abandoned. " Babbage' s basic
desig n was prove n viabl e by Doro n Swad e and his colleagues , who built a workin g mode l usin g
only the mechanica l technique s availabl e at Babbage' s time (Swade , 1993) . Babbag e had the
right idea , but lacke d the organizationa l skill s to get his machin e built .
AI also owe s a debt to the softwar e side of compute r science , whic h has supplie d the
operatin g systems , programmin g languages , and tools neede d to writ e moder n program s (and
paper s abou t them) . But this is one area wher e the debt has been repaid : wor k in AI has pioneere d
many idea s that have mad e their way back to "mainstream " compute r science , includin g time
sharing , interactiv e interpreters , the linke d list data type , automati c storag e management , and
some of the key concept s of object­oriente d programmin g and integrate d progra m developmen t
environment s with graphica l user interfaces .
Linguistic s (1957­present )
In 1957 , B. F. Skinne r publishe d Verbal  Behavior.  This was a comprehensive , detaile d accoun t
of the behavioris t approac h to languag e learning , writte n by the foremos t exper t in the field . But
curiously , a revie w of the book becam e as well­know n as the book itself , and serve d to almos t kill
off interes t in behaviorism . The autho r of the revie w was Noa m Chomsky , who had just publishe d
a book on his own theory , Syntactic  Structures.  Chomsk y showe d how the behavioris t theor y did
not addres s the notio n of creativit y in language—i t did not explai n how a child coul d understan d
and mak e up sentence s that he or she had neve r hear d before . Chomsky' s theory—base d on
syntacti c model s goin g back to the India n linguis t Panin i (c. 350 B.C.)—coul d explai n this, and
unlik e previou s theories , it was forma l enoug h that it coul d in principl e be programmed .
Later development s in linguistic s showe d the proble m to be considerabl y mor e comple x
than it seeme d in 1957 . Languag e is ambiguou s and leave s muc h unsaid . Thi s mean s that
understandin g languag e require s an understandin g of the subjec t matte r and context , not just an
understandin g of the structur e of sentences . This may seem obvious , but it was not appreciate d
until the early 1960s . Muc h of the early work in knowledg e representatio n (the stud y of how to
put knowledg e into a form that a compute r can reaso n with ) was tied to languag e and informe d
by researc h in linguistics , whic h was connecte d in turn to decade s of work on the philosophica l
analysi s of language .
She also gave her nam e to Ada , the U.S. Departmen t of Defense' s all­purpos e programmin g language .
1INTRODUCTIO N
In which  we try  to explain  why we  consider artificial intelligence to  be a  subject  most
worthy  of study,  and in which  we try to decide  what  exactly  it is, this  being a  good
thing  to decide before  embarking.
Humankin d has give n itsel f the scientifi c nam e hom o sapiens —ma n the wise—becaus e our
menta l capacitie s are so importan t to our everyda y live s and our sens e of self. Th e field of
artificia l intelligence , or AI, attempt s to understan d intelligen t entities . Thus , one reaso n to
study it is to learn mor e abou t ourselves . Bu t unlik e philosoph y and psychology , whic h are
also concerne d with intelligence , AI strive s to build  intelligen t entitie s as well as understan d
them . Anothe r reaso n to stud y AI is that these constructe d intelligen t entitie s are interestin g and
usefu l in their own right . AI has produce d man y significan t and impressiv e product s even at this
early stag e in its development . Althoug h no one can predic t the futur e in detail , it is clear that
computer s with human­leve l intelligenc e (or better ) woul d have a huge impac t on our everyda y
lives and on the futur e cours e of civilization .
AI addresse s one of the ultimat e puzzles . How is it possibl e for a slow , tiny brain , whethe r
biologica l or electronic , to perceive , understand , predict , and manipulat e a worl d far large r and
more complicate d than itself ? How do we go abou t makin g somethin g with thos e properties ?
These are hard questions , but unlik e the searc h for faster­than­ligh t trave l or an antigravit y device ,
the researche r in AI has solid evidenc e that the ques t is possible . All the researche r has to do is
look in the mirro r to see an exampl e of an intelligen t system .
AI is one of the newes t disciplines . It was formall y initiate d in 1956 , whe n the nam e
was coined , althoug h at that poin t work had been unde r way for abou t five years . Alon g with
moder n genetics , it is regularl y cited as the "fiel d I woul d most like to be in" by scientist s in othe r
disciplines . A studen t in physic s migh t reasonabl y feel that all the good ideas have alread y been
taken by Galileo , Newton , Einstein , and the rest, and that it take s man y year s of stud y befor e one
can contribut e new ideas . AI, on the othe r hand , still has opening s for a full­tim e Einstein .
The stud y of intelligenc e is also one of the oldes t disciplines . For over 2000 years , philoso ­
phers have tried to understan d how seeing , learning , remembering , and reasonin g could , or should ,
Chapte r Introductio n
be done. ' The adven t of usabl e computer s in the early 1950 s turne d the learne d but armchai r
speculatio n concernin g these menta l facultie s into a real experimenta l and theoretica l discipline .
Many felt that the new "Electroni c Super­Brains " had unlimite d potentia l for intelligence . "Faste r
Than Einstein " was a typica l headline . But as well as providin g a vehicl e for creatin g artificiall y
intelligen t entities , the compute r provide s a tool for testin g theorie s of intelligence , and man y
theorie s faile d to withstan d the test— a case of "out of the armchair , into the fire." AI has turne d
out to be more difficul t than man y at first imagined , and mode m idea s are muc h richer , mor e
subtle , and more interestin g as a result .
AI currentl y encompasse s a huge variet y of subfields , from general­purpos e areas such as
perceptio n and logica l reasoning , to specifi c task s such as playin g chess , provin g mathematica l
theorems , writin g poetry , and diagnosin g diseases . Often , scientist s in othe r field s mov e graduall y
into artificia l intelligence , wher e they find the tools and vocabular y to systematiz e and automat e
the intellectua l tasks on whic h they have been workin g all their lives . Similarly , worker s in AI
can choos e to appl y their method s to any area of huma n intellectua l endeavor . In this sense , it is
truly a universa l field .
1.1 WHA T is AI?
RATIONALIT YWe have now explaine d why AI is exciting , but we have not said wha t it is. We coul d just say,
"Well , it has to do with smar t programs , so let's get on and write some. " But the histor y of scienc e
show s that it is helpfu l to aim at the right goals . Earl y alchemists , lookin g for a potio n for eterna l
life and a metho d to turn lead into gold , were probabl y off on the wron g foot. Only whe n the aim ;
changed , to that of findin g explici t theorie s that gave accurat e prediction s of the terrestria l world , j
in the sam e way that early astronom y predicte d the apparen t motion s of the stars and planets , i
could the scientifi c metho d emerg e and productiv e scienc e take place .
Definition s of artificia l intelligenc e accordin g to eigh t recen t textbook s are show n in Fig­ j
ure 1.1. Thes e definition s vary alon g two main dimensions . The ones on top are concerne d
with thought  processes  and reasoning,  wherea s the ones on the botto m addres s behavior.  Also, !
the definition s on the left measur e succes s in term s of human  performance , wherea s the ones 1
on the right measur e agains t an ideal  concep t of intelligence , whic h we will call rationality . A!
syste m is rationa l if it does the right thing . This give s us four possibl e goals to pursu e in artificia l j
intelligence , as seen in the captio n of Figur e 1.1.
Historically , all four approache s have been followed . As one migh t expect , a tensio n exists l
betwee n approache s centere d aroun d human s and approache s centere d aroun d rationality.2 A!
human­centere d approac h mus t be an empirica l science , involvin g hypothesi s and experimental ]
1 A more  recen t branc h of philosoph y is concerne d with provin g that AI is impossible . We will retur n to this interestin g j
viewpoin t in Chapte r 26.
2 We shoul d poin t out that by distinguishin g betwee n human  and rational  behavior , we are not suggestin g that human s 1
are necessaril y "irrational " in the sens e of "emotionall y unstable " or "insane. " One merel y need note that we ofte n mak e I
mistakes ; we are not all ches s grandmaster s even thoug h we may know all the rule s of chess ; and unfortunately , not]
everyon e gets an A on the exam . Som e systemati c error s in huma n reasonin g are cataloge d by Kahnema n et al. (1982).
Sectio n 1.1 Wha t is Al?
"The excitin g new effor t to mak e computer s
think . . . machines  with  minds,  in the full
and litera l sense " (Haugeland , 1985 )
"[The automatio n of] activitie s that we asso­
ciate with huma n thinking , activitie s such as
decision­making , proble m solving , learnin g
..."(Bellman , 1978 )
"The art of creatin g machine s that perfor m
function s that requir e intelligenc e when per­
forme d by people " (Kurzweil , 1990 )
"The stud y of how to mak e computer s do
thing s at which , at the moment , peopl e are
better " (Ric h and Knight , 1 99 1 )"The stud y of menta l facultie s throug h the
use of computationa l models "
(Charnia k and McDermott , 1985 )
"The stud y of the computation s that mak e
it possibl e to perceive , reason , and act"
(Winston , 1992 )
"A field of stud y that seek s to explai n and
emulat e intelligen t behavio r in term s of
computationa l processes " (Schalkoff , 1 990)
"The branc h of compute r scienc e that is con­
cerne d wit h the automatio n of intelligen t
behavior " (Luge r and Stubblefield , 1993 )
Figur e 1.1 Som e definition s of AI. The y are organize d into four categories :
System s that think like humans .
System s that act like humans .System s that think rationally .
System s that act rationally .
confirmation . A rationalis t approac h involve s a combinatio n of mathematic s and engineering .
Peopl e in each grou p sometime s cast aspersion s on wor k done in the othe r groups , but the truth
is that each directio n has yielde d valuabl e insights . Let us look at each in more detail .
TURIN G TEST
KNOWLEDG E
REPRESENTATIO N
AUTOMATE D
REASONIN G
MACHIN E LEARNIN G
LActin g humanly : The Turin g Test approac h
The Turin g Test , propose d by Ala n Turin g (1950) , was designe d to provid e a satisfactor y
operationa l definitio n of intelligence . Turin g define d intelligen t behavio r as the abilit y to achiev e
human­leve l performanc e in all cognitiv e tasks , sufficien t to fool an interrogator . Roughl y
speaking , the test he propose d is that the compute r shoul d be interrogate d by a huma n via a
teletype , and passe s the test if the interrogato r canno t tell if there is a compute r or a huma n at the
other end. Chapte r 26 discusse s the detail s of the test, and whethe r or not a compute r is reall y
intelligen t if it passes . For now , programmin g a compute r to pass the test provide s plent y to work
on. The compute r woul d need to posses s the followin g capabilities :
0 natura l languag e processin g to enabl e it to communicat e successfull y in Englis h (or som e
other huma n language) ;
<C> knowledg e representatio n to store informatio n provide d befor e or durin g the interrogation ;
<) automate d reasonin g to use the store d informatio n to answe r question s and to draw new
conclusions ;
<) machin e learnin g to adap t to new circumstance s and to detec t and extrapolat e patterns .
Turing' s test deliberatel y avoide d direc t physica l interactio n betwee n the interrogato r and the
computer , becaus e physical  simulatio n of a perso n is unnecessar y for intelligence . However ,
Chapte r 1. Introductio n
TOTAL TURIN G TEST th e so­calle d tota l Turin g Test include s a vide o signa l so that the interrogato r can test the
subject' s perceptua l abilities , as wel l as the opportunit y for the interrogato r to pass physica l
object s "throug h the hatch. " To pass the total Turin g Test, the compute r will need
COMPUTE R VISIO N < ) compute r visio n to perceiv e objects , and
ROBOTIC S ( > robotic s to mov e them about .
Withi n AI, there has not been a big effor t to try to pass the Turin g test. The issue of actin g
like a huma n come s up primaril y whe n AI program s have to interac t with people , as whe n an
exper t syste m explain s how it cam e to its diagnosis , or a natura l languag e processin g syste m has
a dialogu e with a user. Thes e program s mus t behav e accordin g to certai n norma l convention s of
huma n interactio n in orde r to mak e themselve s understood . The underlyin g representatio n and
reasonin g in such a syste m may or may not be base d on a huma n model .
COGNITIV E SCIENC EThinkin g humanly : The cognitiv e modellin g approac h
If we are goin g to say that a give n progra m think s like a human , we mus t have som e way of
determinin g how human s think . We need to get inside  the actua l working s of huma n minds .
There are two way s to do this: throug h introspection—tryin g to catc h our own thought s as they
go by—o r throug h psychologica l experiments . Onc e we have a sufficientl y precis e theor y of
the mind , it become s possibl e to expres s the theor y as a compute r program . If the program' s
input/outpu t and timin g behavio r matche s huma n behavior , that is evidenc e that som e of the
program' s mechanism s may also be operatin g in humans . For example , Newel l and Simon , who
develope d GPS , the "Genera l Proble m Solver " (Newel l and Simon , 1961) , were not conten t to
have their progra m correctl y solv e problems . The y were mor e concerne d with comparin g the
trace of its reasonin g step s to trace s of huma n subject s solvin g the sam e problems . Thi s is in
contras t to othe r researcher s of the same time (suc h as Wan g (I960)) , who were concerne d with
gettin g the righ t answer s regardles s of how human s migh t do it. The interdisciplinar y field of
cognitiv e scienc e bring s togethe r compute r model s from AI and experimenta l technique s from
psycholog y to try to construc t precis e and testabl e theorie s of the working s of the huma n mind .
Althoug h cognitiv e scienc e is a fascinatin g field in itself , we are not goin g to be discussin g
it all that muc h in this book . We will occasionall y commen t on similaritie s or difference s betwee n
AI technique s and huma n cognition . Rea l cognitiv e science , however , is necessaril y base d on
experimenta l investigatio n of actua l human s or animals , and we assum e that the reade r only has
acces s to a compute r for experimentation . We will simpl y note that AI and cognitiv e scienc e
continu e to fertiliz e each other , especiall y in the area s of vision , natura l language , and learning .
The histor y of psychologica l theorie s of cognitio n is briefl y covere d on page 12.
SYLLOGISM S
LThinkin g rationally : The laws of though t approac h
The Gree k philosophe r Aristotl e was one of the first to attemp t to codif y "righ t thinking, " that is,
irrefutabl e reasonin g processes . His famou s syllogism s provide d pattern s for argumen t structure s
that alway s gave correc t conclusion s give n correc t premises . For example , "Socrate s is a man ;
Sectio n 1.1. Wha t is AI?
LOGIC
LOGICIS Tall men are mortal ; therefor e Socrate s is mortal. " Thes e laws of though t were suppose d to gover n
the operatio n of the mind , and initiate d the field of logic .
The developmen t of forma l logic in the late nineteent h and early twentiet h centuries , whic h
we describ e in more detai l in Chapte r 6, provide d a precis e notatio n for statement s abou t all kind s
of thing s in the worl d and the relation s betwee n them . (Contras t this with ordinar y arithmeti c
notation , whic h provide s mainl y for equalit y and inequalit y statement s abou t numbers. ) By 1965 ,
program s existe d that could , give n enoug h time and memory , take a descriptio n of a proble m
in logica l notatio n and find the solutio n to the problem , if one exists . (If there is no solution ,
the progra m migh t neve r stop lookin g for it.) The so­calle d logicis t traditio n withi n artificia l
intelligenc e hope s to build on such program s to creat e intelligen t systems .
There are two main obstacle s to this approach . First , it is not easy to take informa l
knowledg e and state it in the forma l term s require d by logica l notation , particularl y whe n the
knowledg e is less than 100% certain . Second , ther e is a big differenc e betwee n bein g able to
solve a proble m "in principle " and doin g so in practice . Eve n problem s with just a few doze n
facts can exhaus t the computationa l resource s of any compute r unles s it has som e guidanc e as to
whic h reasonin g steps to try first . Althoug h both of these obstacle s appl y to any attemp t to build
computationa l reasonin g systems , they appeare d first in the logicis t traditio n becaus e the powe r
of the representatio n and reasonin g system s are well­define d and fairl y well understood .
AGEN TActin g rationally : The rationa l agen t approac h
Actin g rationall y mean s actin g so as to achiev e one's goals , give n one's beliefs . An agen t is just
somethin g that perceives  and acts . (Thi s may be an unusua l use of the word , but you will get
used to it.) In this approach , AI is viewe d as the stud y and constructio n of rationa l agents .
In the "law s of thought " approac h to AI, the whol e emphasi s was on correc t inferences .
Makin g correc t inference s is sometime s part  of bein g a rationa l agent , becaus e one way to act
rationall y is to reaso n logicall y to the conclusio n that a give n actio n will achiev e one' s goals ,
and then to act on that conclusion . On the othe r hand , correc t inferenc e is not all of rationality ,
becaus e ther e are ofte n situation s wher e ther e is no provabl y correc t thin g to do, yet somethin g
must still be done . Ther e are also way s of actin g rationall y that canno t be reasonabl y said to
involv e inference . For example , pullin g one' s hand off of a hot stov e is a refle x actio n that is
more successfu l than a slowe r actio n take n after carefu l deliberation .
All the "cognitiv e skills " neede d for the Turin g Test are there to allow rationa l actions . Thus ,
we need the abilit y to represen t knowledg e and reaso n with it becaus e this enable s us to reac h
good decision s in a wide variet y of situations . We need to be able to generat e comprehensibl e
sentence s in natura l languag e becaus e sayin g those sentence s help s us get by in a comple x society .
We need learnin g not just for erudition , but becaus e havin g a bette r idea of how the worl d work s
enable s us to generat e more effectiv e strategie s for dealin g with it. We need visua l perceptio n not
just becaus e seein g is fun, but in orde r to get a bette r idea of wha t an actio n migh t achieve—fo r
example , bein g able to see a tasty morse l help s one to mov e towar d it.
The stud y of AI as rationa l agen t desig n therefor e has two advantages . First , it is mor e
genera l than the "law s of thought " approach , becaus e correc t inferenc e is only a usefu l mechanis m
for achievin g rationality , and not a necessar y one. Second , it is mor e amenabl e to scientifi c
Chapte r 1. Introductio n
LIMITE D
RATIONALIT Ydevelopmen t than approache s base d on huma n behavio r or huma n thought , becaus e the standar d
of rationalit y is clearl y define d and completel y general . Huma n behavior , on the othe r hand ,
is well­adapte d for one specifi c environmen t and is the product , in part, of a complicate d and
largel y unknow n evolutionar y proces s that still may be far from achievin g perfection . This
book  will therefore  concentrate  on general  principles  of rational  agents,  and  on components  for
constructing  them.  We will see that despit e the apparen t simplicit y with whic h the proble m can
be stated , an enormou s variet y of issue s com e up whe n we try to solv e it. Chapte r 2 outline s
some of these issue s in more detail .
One importan t poin t to keep in mind : we will see befor e too long that achievin g perfec t
rationality—alway s doin g the righ t thing—i s not possibl e in complicate d environments . The
computationa l demand s are just too high . However , for mos t of the book , we will adop t the
workin g hypothesi s that understandin g perfec t decisio n makin g is a good plac e to start . It
simplifie s the proble m and provide s the appropriat e settin g for most of the foundationa l materia l
in the field . Chapter s 5 and 17 deal explicitl y with the issu e of limite d rationality —actin g
appropriatel y whe n there is not enoug h time to do all the computation s one migh t like.
1.2 TH E FOUNDATION S OF ARTIFICIA L INTELLIGENC E
In this sectio n and the next , we provid e a brief histor y of AI. Althoug h AI itsel f is a youn g field ,
it has inherite d man y ideas , viewpoints , and technique s from othe r disciplines . From over 2000
years of traditio n in philosophy , theorie s of reasonin g and learnin g have emerged , alon g with the
viewpoin t that the mind is constitute d by the operatio n of a physica l system . From over 400 year s
of mathematics , we have forma l theorie s of logic , probability , decisio n making , and computation .
From psychology , we have the tools with whic h to investigat e the huma n mind , and a scientifi c
languag e withi n whic h to expres s the resultin g theories . Fro m linguistics , we have theorie s of
the structur e and meanin g of language . Finally , from compute r science , we have the tools with
which to mak e AI a reality .
Like any history , this one is force d to concentrat e on a smal l numbe r of peopl e and events ,
and ignor e other s that were also important . We choos e to arrang e event s to tell the story of how
the variou s intellectua l component s of moder n AI came into being . We certainl y woul d not wish
to give the impression , however , that the discipline s from whic h the component s cam e have all
been workin g towar d AI as their ultimat e fruition .
Philosoph y (428 B.C.­present )
The safes t characterizatio n of the Europea n philosophica l traditio n is that it consist s of a serie s
of footnote s to Plato .
—Alfre d Nort h Whitehea d
We begi n with the birth of Plat o in 428 B.C . His writing s rang e acros s politics , mathematics ,
physics , astronomy , and severa l branche s of philosophy . Together , Plato , his teache r Socrates ,
I
Sectio n 1.2. Th e Foundation s of Artificia l Intelligenc e
DUALIS M
MATERIALIS M
EMPIRICIS T
INDUCTIO Nand his studen t Aristotl e laid the foundatio n for muc h of wester n though t and culture . Th e
philosophe r Huber t Dreyfu s (1979 , p. 67) says that "The story of artificia l intelligenc e migh t well
begin aroun d 450 B.C. " whe n Plato reporte d a dialogu e in whic h Socrate s asks Euthyphro,3 "I
want to know wha t is characteristi c of piety whic h make s all action s pious.. . that I may have it
to turn to, and to use as a standar d whereb y to judg e your action s and thos e of othe r men."4 In
other words , Socrate s was askin g for an algorithm  to distinguis h piety from non­piety . Aristotl e
went on to try to formulat e more precisel y the laws governin g the rationa l part of the mind . He
develope d an informa l syste m of syllogism s for prope r reasoning , whic h in principl e allowe d one
to mechanicall y generat e conclusions , give n initia l premises . Aristotl e did not believ e all parts
of the mind were governe d by logica l processes ; he also had a notio n of intuitiv e reason .
Now that we have the idea of a set of rules that can describ e the workin g of (at least part
of) the mind , the next step is to conside r the mind as a physica l system . We have to wait for
Rene Descarte s (1596­1650 ) for a clear discussio n of the distinctio n betwee n min d and matter ,
and the problem s that arise . One proble m with a purel y physica l conceptio n of the mind is that
it seem s to leav e little room for free will : if the mind is governe d entirel y by physica l laws , then
it has no more free will than a rock "deciding " to fall towar d the cente r of the earth . Althoug h a
stron g advocat e of the powe r of reasoning , Descarte s was also a proponen t of dualism . He held
that there is a part of the mind (or soul or spirit ) that is outsid e of nature , exemp t from physica l
laws. On the othe r hand , he felt that animal s did not posses s this dualis t quality ; they coul d be
considere d as if they were machines .
An alternativ e to dualis m is materialism , whic h hold s that all the worl d (includin g the
brain and mind ) operat e accordin g to physica l law.5 Wilhel m Leibni z (1646­1716 ) was probabl y
the first to take the materialis t positio n to its logica l conclusio n and buil d a mechanica l devic e
intende d to carry out menta l operations . Unfortunately , his formulatio n of logic was so weak that
his mechanica l concep t generato r coul d not produc e interestin g results .
It is also possibl e to adop t an intermediat e position , in whic h one accept s that the min d
has a physica l basis , but denie s that it can be explained  by a reductio n to ordinar y physica l
processes . Menta l processe s and consciousnes s are therefor e part of the physica l world , but
inherentl y unknowable ; they are beyon d rationa l understanding . Som e philosopher s critica l of
AI have adopte d exactl y this position , as we discus s in Chapte r 26.
Barrin g thes e possibl e objection s to the aim s of AI, philosoph y had thus establishe d a
traditio n in whic h the min d was conceive d of as a physica l devic e operatin g principall y by
reasonin g with the knowledg e that it contained . Th e nex t proble m is then to establis h the
sourc e of knowledge . The empiricis t movement , startin g with Franci s Bacon' s (1561­1626 )
Novwn  Organum,6 is characterize d by the dictu m of John Lock e (1632­1704) : "Nothin g is in
the understanding , whic h was not first in the senses. " Davi d Hume' s (1711­1776 ) A Treatise
of Human  Nature  (Hume , 1978 ) propose d wha t is now know n as the principl e of induction :
3 Th e Euthyphro  describe s the event s just befor e the trial of Socrate s in 399 B.C . Dreyfu s has clearl y erred in placin g it
51 year s earlier .
4 Not e that othe r translation s have "goodness/good " instea d of "piety/pious. "
5 In this view , the perceptio n of "free will " arise s becaus e the deterministi c generatio n of behavio r is constitute d by the
operatio n of the min d selectin g amon g wha t appea r to be the possibl e course s of action . The y remai n "possible " becaus e
the brai n does not have acces s to its own futur e states .
6 An updat e of Aristotle' s organon,  or instrumen t of thought .
10 Chapte r 1. Introductio n
LOGICA L POSITIVIS M
OBSERVATIO N
SENTENCE S
CONFIRMATIO N
THEOR Y
MEANS­END S
ANALYSI Sthat genera l rule s are acquire d by exposur e to repeate d association s betwee n thei r elements .
The theor y was give n mor e forma l shap e by Bertran d Russel l (1872­1970 ) who introduce d
logica l positivism . Thi s doctrin e hold s that all knowledg e can be characterize d by logica l
theorie s connected , ultimately , to observatio n sentence s that correspon d to sensor y inputs.7 The
confirmatio n theor y of Rudol f Carna p and Carl Hempe l attempte d to establis h the natur e of the
connectio n betwee n the observatio n sentence s and the mor e genera l theories—i n othe r words , to
understan d how knowledg e can be acquire d from experience .
The fina l elemen t in the philosophica l pictur e of the min d is the connectio n betwee n
knowledg e and action . Wha t form shoul d this connectio n take , and how can particula r action s
be justified ? Thes e question s are vita l to AI, becaus e only by understandin g how action s are
justifie d can we understan d how to buil d an agen t whos e action s are justifiable , or rational .
Aristotl e provide s an elegan t answe r in the Nicomachean  Ethics  (Boo k III. 3, 1112b) :
We deliberat e not abou t ends , but abou t means . For a docto r does not deliberat e whethe r he
shall heal , nor an orato r whethe r he shal l persuade , nor a statesma n whethe r he shal l produc e
law and order , nor does any one else deliberat e abou t his end . The y assum e the end and
conside r how and by wha t mean s it is attained , and if it seem s easil y and best produce d
thereby ; whil e if it is achieve d by one mean s only they conside r how  it will be achieve d by
this and by wha t mean s this will be achieved , till they com e to the first cause , whic h in the
order of discover y is last ... and wha t is last in the orde r of analysi s seem s to be firs t in the
order of becoming . An d if we com e on an impossibility , we give up the search , e.g. if we
need mone y and this canno t be got: but if a thin g appear s possibl e we try to do it.
Aristotle' s approac h (wit h a few mino r refinements ) was implemente d 2300 year s later by Newel l
and Simo n in their GPS program , abou t whic h they writ e (Newel l and Simon , 1972) :
The main method s of GPS jointl y embod y the heuristi c of means­end s analysis . Means­end s
analysi s is typifie d by the followin g kind of common­sens e argument :
I wan t to take my son to nurser y school . What' s the differenc e betwee n wha t I
have and wha t I want ? One of distance . Wha t change s distance ? My automobile .
My automobil e won' t work . Wha t is neede d to mak e it work ? A new battery .
Wha t has new  batteries ? An auto repai r shop . I wan t the repai r shop to put in a
new battery ; but the shop doesn' t kno w I need one. Wha t is the difficulty ? One
of communication . Wha t allow s communication ? A telephon e ... and so on.
This kind of analysis—classifyin g thing s in term s of the function s they serv e and oscillatin g
amon g ends , function s required , and mean s that perfor m them—form s the basi c syste m of
heuristi c of GPS .
Means­end s analysi s is useful , but does not say wha t to do whe n severa l action s will achiev e the
goal, or whe n no actio n will completel y achiev e it. Arnauld , a followe r of Descartes , correctl y
describe d a quantitativ e formul a for decidin g wha t actio n to take in case s like this (see Chapte r 16).
John Stuar t Mill' s (1806­1873 ) book Utilitarianism  (Mill , 1863 ) amplifie s on this idea. The more
forma l theor y of decision s is discusse d in the followin g section .
7 In this picture , all meaningfu l statement s can be verifie d or falsifie d eithe r by analyzin g the meanin g of the word s or
by carryin g out experiments . Becaus e this rule s out mos t of metaphysics , as was the intention , logica l positivis m was
unpopula r in som e circles .
Sectio n 1.2. Th e Foundation s of Artificia l Intelligenc e 11
Mathematic s (c. 800­present )
Philosopher s stake d out mos t of the importan t idea s of AI, but to mak e the leap to a forma l
scienc e require d a level of mathematica l formalizatio n in three main areas : computation , logic ,
ALGORITH M an d probability . The notio n of expressin g a computatio n as a forma l algorith m goes back to
al­Khowarazmi , an Ara b mathematicia n of the nint h century , whos e writing s also introduce d
Europ e to Arabi c numeral s and algebra.
Logic goes back at least to Aristotle , but it was a philosophica l rathe r than mathematica l
subjec t unti l Georg e Bool e (1815­1864 ) introduce d his forma l languag e for makin g logica l
inferenc e in 1847 . Boole' s approac h was incomplete , but good enoug h that other s fille d in the
gaps. In 1879 , Gottlo b Freg e (1848­1925 ) produce d a logi c that , excep t for som e notationa l
changes , form s the first­orde r logic that is used toda y as the mos t basic knowledg e representatio n
system.8 Alfre d Tarsk i (1902­1983 ) introduce d a theor y of referenc e that show s how to relat e
the object s in a logic to object s in the real world . The next step was to determin e the limit s of
what coul d be done with logic and computation .
Davi d Hilber t (1862­1943) , a grea t mathematicia n in his own right , is mos t remembere d
for the problem s he did not solve . In 1900 , he presente d a list of 23 problem s that he correctl y
predicte d woul d occup y mathematician s for the bulk of the century . Th e fina l proble m asks
if there is an algorith m for decidin g the truth of any logica l propositio n involvin g the natura l
numbers—th e famou s Entscheidungsproblem,  or decisio n problem . Essentially , Hilber t was
askin g if there were fundamenta l limit s to the powe r of effectiv e proo f procedures . In 1930 , Kurt
Gode l (1906­1978 ) showe d that there exist s an effectiv e procedur e to prov e any true statemen t in
the first­orde r logic of Freg e and Russell ; but first­orde r logi c coul d not captur e the principl e of
mathematica l inductio n neede d to characteriz e the natura l numbers . In 1931 , he showe d that real
TNHCEora=METENES S limit s do exist . His incompletenes s theore m showe d that in any languag e expressiv e enoug h
to describ e the propertie s of the natura l numbers , there are true statement s that are undecidable :
their truth canno t be establishe d by any algorithm .
This fundamenta l resul t can also be interprete d as showin g that there are som e function s
on the integer s that canno t be represente d by an algorithm—tha t is, they canno t be computed .
This motivate d Ala n Turin g (1912­1954 ) to try to characteriz e exactl y whic h function s are
capabl e of bein g computed . Thi s notio n is actuall y slightl y problematic , becaus e the notio n
of a computatio n or effectiv e procedur e reall y canno t be give n a forma l definition . However ,
the Church­Turin g thesis , whic h state s that the Turin g machin e (Turing , 1936 ) is capabl e of
computin g any computabl e function , is generall y accepte d as providin g a sufficien t definition .
Turin g also showe d that there were som e function s that no Turin g machin e can compute . For
example , no machin e can tell in general  whethe r a give n progra m will retur n an answe r on a
given input , or run forever .
Althoug h undecidabilit y and noncomputabilit y are importan t to an understandin g of com ­
WTRACTABILIT Y putation , the notio n of intractabilit y has had a muc h greate r impact . Roughl y speaking ,
a class of problem s is calle d intractabl e if the time require d to solv e instance s of the clas s
grow s at least exponentiall y with the size of the instances . The distinctio n betwee n polynomia l
and exponentia l growt h in complexit y was first emphasize d in the mid­1960 s (Cobham , 1964 ;
Edmonds , 1965) . It is importan t becaus e exponentia l growt h mean s that even moderate­size d in­
To understan d why Frege' s notatio n was not universall y adopted , see the cove r of this book .
12 Chapte r 1. Introductio n
stance s canno t be solve d in any reasonabl e time . Therefore , one shoul d striv e to divid e the overall
proble m of generatin g intelligen t behavio r into tractabl e subproblem s rathe r than intractabl e ones .
REDUCTIO N Th e secon d importan t concep t in the theor y of complexit y is reduction , whic h also emerge d in
the 1960 s (Dantzig , 1960 ; Edmonds , 1962) . A reductio n is a genera l transformatio n from one
class of problem s to another , such that solution s to the first class can be foun d by reducin g them
to problem s of the secon d class and solvin g the latte r problems .
NP COMPLETENES S Ho w can one recogniz e an intractabl e problem ? The theor y of NP­completeness , pioneere d
by Steve n Coo k (1971 ) and Richar d Karp (1972) , provide s a method . Coo k and Karp showe d
the existenc e of large classe s of canonica l combinatoria l searc h and reasonin g problem s that
are NP­complete . An y proble m class to whic h an NP­complet e proble m class can be reduce d
is likel y to be intractable . (Althoug h it has not yet been prove d that NP­complet e problem s
are necessaril y intractable , few theoretician s believ e otherwise. ) Thes e result s contras t sharpl y
with the "Electroni c Super­Brain " enthusias m accompanyin g the adven t of computers . Despit e
the ever­increasin g spee d of computers , subtlet y and carefu l use of resource s will characteriz e
intelligen t systems . Put crudely , the worl d is an extremely  large proble m instance !
Beside s logi c and computation , the third grea t contributio n of mathematic s to AI is the j
theor y of probability . Th e Italia n Gerolam o Cardan o (1501­1576 ) first frame d the idea of I
probability , describin g it in term s of the possibl e outcome s of gamblin g events . Befor e his time , j
the outcome s of gamblin g game s were seen as the will of the gods rathe r than the whim of chance , i
Probabilit y quickl y becam e an invaluabl e part of all the quantitativ e sciences , helpin g to deal
with uncertai n measurement s and incomplet e theories . Pierr e Ferma t (1601­1665) , Blais e Pasca l I
(1623­1662) , Jame s Bernoull i (1654­1705) , Pierr e Laplac e (1749­1827) , and other s advance d j
the theor y and introduce d new statistica l methods . Bernoull i also frame d an alternativ e view ]
of probability , as a subjectiv e "degre e of belief " rathe r than an objectiv e ratio of outcomes. !
Subjectiv e probabilitie s therefor e can be update d as new evidenc e is obtained . Thoma s Baye s j
(1702­1761 ) propose d a rule for updatin g subjectiv e probabilitie s in the ligh t of new evidence !
(publishe d posthumousl y in 1763) . Bayes ' rule , and the subsequen t field of Bayesia n analysis, !
form the basis of the moder n approac h to uncertai n reasonin g in AI systems . Debat e still rage s j
betwee n supporter s of the objectiv e and subjectiv e view s of probability , but it is not clear if the!
differenc e has grea t significanc e for AI. Both version s obey the sam e set of axioms . Savage's J
(1954 ) Foundations  of Statistics  give s a good introductio n to the field .
As with logic , a connectio n mus t be mad e betwee n probabilisti c reasonin g and action. !
DECISIO N THEOR Y Decisio n theory , pioneere d by John Von Neuman n and Oska r Morgenster n (1944) , combines !
probabilit y theor y with utilit y theor y (whic h provide s a forma l and complet e framewor k forl
specifyin g the preference s of an agent ) to give the first genera l theor y that can distinguis h good !
action s from bad ones . Decisio n theor y is the mathematica l successo r to utilitarianism , and ]
provide s the theoretica l basis for man y of the agen t design s in this book .
Psycholog y (1879­present )
Scientifi c psycholog y can be said to have begu n with the work of the Germa n physicis t Herman n i
von Helmholt z (1821­1894 ) and his studen t Wilhel m Wund t (1832­1920) . Helmholt z applie d
the scientifi c metho d to the stud y of huma n vision , and his Handbook  of Physiological  Optics  \
Sectio n 1.2. Th e Foundation s of Artificia l Intelligenc e 13
BEHAVIORIS M
COGNITIV E
PSYCHOLOG Yis even now describe d as "the singl e mos t importan t treatis e on the physic s and physiolog y of
huma n visio n to this day" (Nalwa , 1993 , p.15) . In 1879 , the same year that Freg e launche d first ­
order logic , Wund t opene d the first laborator y of experimenta l psycholog y at the Universit y of
Leipzig . Wund t insiste d on carefull y controlle d experiment s in whic h his worker s woul d perfor m
a perceptua l or associativ e task whil e introspectin g on thei r though t processes . Th e carefu l
control s wen t a long way to make psycholog y a science , but as the methodolog y spread , a curiou s
phenomeno n arose : each laborator y woul d repor t introspectiv e data that just happene d to matc h
the theorie s tint were popula r in that laboratory . The behavioris m movemen t of John Watso n
(1878­1958 ) aid Edwar d Lee Thorndik e (1874­1949 ) rebelle d agains t this subjectivism , rejectin g
any theor y involvin g menta l processe s on the ground s that introspectio n could not provid e reliabl e
evidence . Behiviorist s insiste d on studyin g only objectiv e measure s of the percept s (or stimulus)
given to an anima l and its resultin g action s (or response).  Menta l construct s such as knowledge ,
beliefs , goals , md reasonin g steps were dismisse d as unscientifi c "folkpsychology. " Behavioris m
discovere d a let abou t rats and pigeons , but had less succes s understandin g humans . Nevertheless ,
it had a stronghol d on psycholog y (especiall y in the Unite d States ) from abou t 1920 to 1960 .
The view that the brai n possesse s and processe s information , whic h is the principa l char ­
acteristi c of cognitiv e psychology , can be trace d back at least to the work s of Willia m James9
(1842­1910) . Helmholt z also insiste d that perceptio n involve d a form of unconsciou s logica l in­
ference . The cognitiv e viewpoin t was largel y eclipse d by behavioris m unti l 1943 , whe n Kennet h
Craik publishe d The Nature of  Explanation.  Crai k put back the missin g menta l step betwee n
stimulu s and response . He claime d that beliefs , goals , and reasonin g steps coul d be usefu l valid
component s of a theor y of huma n behavior , and are just as scientifi c as, say, usin g pressur e and
temperatur e to talk abou t gases , despit e their bein g mad e of molecule s that have neither . Crai k
specifie d the tlree key steps of a knowledge­base d agent : (1) the stimulu s mus t be translate d into
an interna l representation , (2) the representatio n is manipulate d by cognitiv e processe s to deriv e
new interna l representations , and (3) thes e are in turn retranslate d back into action . He clearl y
explaine d why this was a good desig n for an agent :
If the orgmis m carrie s a "small­scal e model " of externa l realit y and of its own possibl e action s
withi n its head , it is able to try out variou s alternatives , conclud e whic h is the best of them ,
react to fitur e situation s befor e they arise , utiliz e the knowledg e of past event s in dealin g with
the presen t and future , and in ever y way to reac t in a muc h fuller , safer , and more competen t
manne r to the emergencie s whic h face it. (Craik , 1943 )
An agen t designe d this way can, for example , plan a long trip by considerin g variou s possi ­
ble routes , comparin g them , and choosin g the best one, all befor e startin g the journey . Sinc e
the 1960s , the information­processin g view has dominate d psychology . It it now almos t take n
for grante d amon g man y psychologist s that "a cognitiv e theor y shoul d be like a compute r pro­
gram " (Andersen , 1980) . By this it is mean t that the theor y shoul d describ e cognitio n as consistin g
of well­define j transformatio n processe s operatin g at the leve l of the informatio n carrie d by the
inpu t signals .
For mos t of the early histor y of AI and cognitiv e science , no significan t distinctio n was
draw n betwee n the two fields , and it was commo n to see AI program s describe d as psychologica l
9 Willia m Jame s was the brothe r of novelis t Henr y James . It is said that Henr y wrot e fictio n as if it were psycholog y
and Willia m wrot ; psycholog y as if it were fiction .
14 Chapte r Introductio n
result s withou t any claim as to the exac t huma n behavio r they were modelling . In the last decad e
or so, however , the methodologica l distinction s have becom e clearer , and mos t wor k now falls
into one field or the other .
Compute r engineerin g (1940­present )
For artificia l intelligenc e to succeed , we need two things : intelligenc e and an artifact . Th e
compute r has been unanimousl y acclaime d as the artifact  with the best chanc e of demonstratin g
intelligence . The moder n digita l electroni c compute r was invente d independentl y and almos t
simultaneousl y by scientist s in three countrie s embattle d in Worl d War II. The first operationa l
moder n compute r was the Heat h Robinson,10 buil t in 1940 by Alan Turing' s team for the singl e
purpos e of decipherin g Germa n messages . Whe n the German s switche d to a more sophisticate d
code, the electromechanica l relay s in the Robinso n prove d to be too slow , and a new machin e
called the Colossu s was buil t from vacuu m tubes . It was complete d in 1943 , and by the end of
the war, ten Colossu s machine s were in everyda y use.
The first operationa l programmable  compute r was the Z­3, the inventio n of Konra d Zuse
in German y in 1941 . Zus e invente d floating­poin t number s for the Z­3, and wen t on in 1945 to
develo p Plankalkul , the first high­leve l programmin g language . Althoug h Zuse receive d som e
suppor t from the Thir d Reic h to appl y his machin e to aircraf t design , the militar y hierarch y did
not attac h as muc h importanc e to computin g as did its counterpar t in Britain .
In the Unite d States , the first electronic  computer , the ABC , was assemble d by Joh n
Atanasof f and his graduat e studen t Cliffor d Berr y betwee n 1940 and 1942 at Iowa State University .
The projec t receive d little suppor t and was abandone d after Atanasof f becam e involve d in militar y
researc h in Washington . Two othe r compute r project s were starte d as secre t militar y research :
the Mark I, If, and III computer s were develope d at Harvar d by a team unde r Howar d Aiken ; and
the ENIA C was develope d at the Universit y of Pennsylvani a by a team includin g John Mauchl y
and John Eckert . ENIA C was the first general­purpose , electronic , digita l computer . One of its
first application s was computin g artiller y firin g tables . A successor , the EDVAC , followe d John
Von Neumann' s suggestio n to use a store d program , so that technician s woul d not have to scurr y
abou t changin g patch cord s to run a new program .
But perhap s the mos t critica l breakthroug h was the IBM 701, buil t in 1952 by Nathanie l
Rocheste r and his group . This was the first compute r to yield a profi t for its manufacturer . IBM
went on to becom e one of the world' s larges t corporations , and sales of computer s have grow n to j
$150 billion/year . In the Unite d States , the compute r industr y (includin g softwar e and services ) j
now account s for abou t 10% of the gros s nationa l product .
Each generatio n of compute r hardwar e has brough t an increas e in spee d and capacity , and I
a decreas e in price . Compute r engineerin g has been remarkabl y successful , regularl y doublin g j
performanc e ever y two years , with no immediat e end in sigh t for this rate of increase . Massivel y j
paralle l machine s promis e to add severa l more zero s to the overal l throughpu t achievable .
Of course,  ther e wer e calculatin g device s befor e the electroni c computer . Th e abacu s \
is roughl y 7000 year s old. In the mid­17t h century , Blais e Pasca l buil t a mechanica l addin g 1
10 Heat h Robinso n was a cartoonis t famou s for his depiction s of whimsica l and absurdl y complicate d contraption s for
everyda y tasks such as butterin g toast .
sectio n 1.2. Th e Foundation s of Artificia l Intelligenc e 1 5
and subtractin g machin e calle d the Pascaline . Leibni z improve d on this in 1694 . buildin g a
mechanica l devic e that multiplie d by doin g repeate d addition . Progres s stalle d for over a centur y
unti 1 Charle s Babbag e (1792­1871 ) dreame d that logarith m table s coul d be compute d by machine .
He designe d a machin e for this task , but neve r complete d the project . Instead , he turne d to the
desig n of the Analytica l Engine , for whic h Babbag e invente d the idea s of addressabl e memory .
stored programs , and conditiona l jumps . Althoug h the idea of programmabl e machine s was
not new—i n 1805 . Josep h Mari e Jacquar d invente d a loom that coul d be programme d usin g
punche d cards—Babbage' s machin e was the first artifac t possessin g the characteristic s necessar y
for universa l computation . Babbage' s colleagu e Ada Lovelace , daughte r of the poet Lord Byron ,
wrot e program s for the Analytica l Engin e and even speculate d that the machin e coul d play ches s
or compos e music . Lovelac e was the world' s first programmer , and the first of man y to endur e
massiv e cost overrun s and to have an ambitiou s projec t ultimatel y abandoned. " Babbage' s basi c
desig n was prove n viabl e by Doro n Swad e and his colleagues , who buil t a workin g mode l usin g
only the mechanica l technique s availabl e at Babbage' s tim e (Swade . 1993) . Babbag e had the
right idea , but lacke d the organizationa l skill s to get his machin e built .
AI also owe s a deb t to the softwar e side of compute r science , whic h has supplie d the
operatin g systems , programmin g languages , and tool s neede d to writ e moder n program s (and
paper s abou t them) . But this is one area wher e the debt has been repaid : wor k in AI has pioneere d
many idea s that have mad e thei r way back to "mainstream " compute r science , includin g tim e
sharing , interactiv e interpreters , the linke d list data type , automati c storag e management , and
some of the key concept s of object­oriente d programmin g and integrate d progra m developmen t
environment s with graphica l user interfaces .
Linguistic s (1957­present )
In 1957 . B. F. Skinne r publishe d Verbal  Behavior.  Thi s was a comprehensive , detaile d accoun t
of the behavioris t approac h to languag e learning , writte n by the foremos t exper t in the field . But
curiously , a revie w of the book becam e as well­know n as the book itself , and serve d to almos t kill
off interes t in behaviorism . The autho r of the revie w was Noa m Chomsky , who had just publishe d
a book on his own theory . Syntactic  Structures.  Chomsk y showe d how the behavioris t theor y did
not addres s the notio n of creativit y in language—i t did not explai n how a chil d coul d understan d
and mak e up sentence s that he or she had neve r hear d before . Chomsky' s theory—base d on
syntacti c model s goin g back to the India n linguis t Panin i (c. 350 B.C.)—coul d explai n this , and
unlik e previou s theories , it was forma l enoug h that it coul d in principl e be programmed .
Later development s in linguistic s showe d the proble m to be considerabl y mor e comple x
than it seeme d in 1957 . Languag e is ambiguou s and leave s muc h unsaid . Thi s mean s tha t
understandin g languag e require s an understandin g of the subjec t matte r and context , not just an
understandin g of the structur e of sentences . Thi s may seem obvious , but it was not appreciate d
until the early 1960s . Muc h of the early wor k in knowledg e representatio n (the stud y of how to
put knowledg e into a form that a compute r can reaso n with ) was tied to languag e and informe d
by researc h in linguistics , whic h was connecte d in turn to decade s of wor k on the philosophica l
analysi s of language .
She also gave her nam e to Ada . the U.S . Departmen t of Defense' s all­purpos e programmin g language .
16 Chapte r 1. Introductio n
Moder n linguistic s and AI were "born " at abou t the same time , so linguistic s does not play
a larg e foundationa l role in the growt h of AI. Instead , the two grew up together , intersectin g
in a hybri d fiel d calle d computationa l linguistic s or natura l languag e processing , whic h
concentrate s on the proble m of languag e use.
1.3 TH E HISTOR Y OF ARTIFICIA L INTELLIGENC E
With the backgroun d materia l behin d us, we are now read y to outlin e the developmen t of AI
proper . We coul d do this by identifyin g loosel y define d and overlappin g phase s in its development ,
or by chroniclin g the variou s differen t and intertwine d conceptua l thread s that mak e up the field .
In this section , we will take the forme r approach , at the risk of doin g som e degre e of violenc e
to the real relationship s amon g subfields . The histor y of each subfiel d is covere d in individua l
chapter s later in the book .
The gestatio n of artificia l intelligenc e (1943­1956 )
The first wor k that is now generall y recognize d as AI was don e by Warre n McCulloc h and
Walte r Pitts (1943) . The y drew on thre e sources : knowledg e of the basi c physiolog y and
functio n of neuron s in the brain ; the forma l analysi s of propositiona l logic due to Russel l and
Whitehead ; and Turing' s theor y of computation . The y propose d a mode l of artificia l neuron s in
whic h each neuro n is characterize d as bein g "on" or "off, " with a switc h to "on" occurrin g in
respons e to stimulatio n by a sufficien t numbe r of neighborin g neurons . The state of a neuro n
was conceive d of as "factuall y equivalen t to a propositio n whic h propose d its adequat e stimulus. "
They showed , for example , that any computabl e functio n coul d be compute d by som e networ k
of connecte d neurons , an d that all the logica l connective s coul d be implemented  by simpl e
net structures . McCulloc h and Pitts also suggeste d that suitabl y define d network s coul d learn .
Donal d Hebb (1949 ) demonstrate d a simpl e updatin g rule for modifyin g the connectio n strength s
betwee n neurons , such that learnin g coul d take place .
The work of McCulloc h and Pitts was arguabl y the forerunne r of both the logicis t traditio n i
in AI and the connectionis t tradition . In the earl y 1950s , Claud e Shanno n (1950 ) and Ala n
Turin g (1953 ) were writin g ches s program s for von Neumann­styl e conventiona l computers.12
At the sam e time , two graduat e student s in the Princeto n mathematic s department , Marvi n
Minsk y and Dea n Edmonds , buil t the first neura l networ k compute r in 1951 . The SNARC , as
it was called , used 3000 vacuu m tube s and a surplu s automati c pilo t mechanis m from a B­24
bombe r to simulat e a networ k of 40 neurons . Minsky' s Ph.D . committe e was skeptica l whethe r
this kind of work shoul d be considere d mathematics , but von Neuman n was on the committe e
and reportedl y said , "If it isn' t now it will be someday. " Ironically , Minsk y was later to prov e
theorem s that contribute d to the demis e of muc h of neura l networ k researc h durin g the 1970s .
12 Shanno n actuall y had no real compute r to wor k with , and Turin g was eventuall y denie d acces s to his own team' s
computer s by the Britis h government , on the ground s that researc h into artificia l intelligenc e was surel y frivolous .
Sectio n 1.3. Th e Histor y of Artificia l Intelligenc e 17
Princeto n was hom e to anothe r influentia l figur e in AI, John McCarthy . Afte r graduation ,
McCarth y move d to Dartmout h College , whic h was to becom e the officia l birthplac e of the
field. McCarth y convince d Minsky , Claud e Shannon , and Nathanie l Rocheste r to help him brin g
togethe r U.S. researcher s intereste d in automat a theory , neura l nets , and the stud y of intelligence .
They organize d a two­mont h worksho p at Dartmout h in the summe r of 1956 . All togethe r there
were ten attendees , includin g Trenchar d Mor e from Princeton , Arthu r Samue l from IBM , and
Ray Solomonof f and Olive r Selfridg e from MIT .
Two researcher s from Carnegi e Tech,13 Alie n Newel l and Herber t Simon , rathe r stole the
show . Althoug h the other s had idea s and in som e case s program s for particula r application s
such as checkers , Newel l and Simo n alread y had a reasonin g program , the Logi c Theoris t (LT) ,
abou t whic h Simo n claimed , "We have invente d a compute r progra m capabl e of thinkin g non­
numerically , and thereb y solve d the venerabl e mind­bod y problem."14 Soo n after the workshop ,
the progra m was able to prov e mos t of the theorem s in Chapte r 2 of Russel l and Whitehead' s
Principia  Mathematica.  Russel l was reportedl y delighte d whe n Simo n showe d him that the pro­
gram had com e up with a proo f for one theore m that was shorte r than the one in Principia.  The
editor s of the Journal of  Symbolic  Logic  were less impressed ; they rejecte d a pape r coauthore d
by Newell , Simon , and Logi c Theorist .
The Dartmout h worksho p did not lead to any new breakthroughs , but it did introduc e all
the majo r figure s to each other . For the next 20 years , the field woul d be dominate d by thes e
peopl e and their student s and colleague s at MIT , CMU , Stanford , and IBM . Perhap s the mos t
lastin g thing to come out of the worksho p was an agreemen t to adop t McCarthy' s new nam e for
the field : artificia l intelligence .
Early enthusiasm , grea t expectation s (1952­1969 )
The early year s of AI were full of successes—i n a limite d way. Give n the primitiv e computer s
and programmin g tool s of the time , and the fact that only a few year s earlie r computer s were
seen as thing s that coul d do arithmeti c and no more , it was astonishin g wheneve r a compute r did
anythin g remotel y clever . The intellectua l establishment , by and large , preferre d to believ e that "a
machin e can neve r do X" (see Chapte r 26 for a long list of X's gathere d by Turing) . AI researcher s
naturall y responde d by demonstratin g one X after another . Som e moder n AI researcher s refer to
this perio d as the "Look , Ma, no hands! " era.
Newel l and Simon' s early succes s was followe d up with the Genera l Proble m Solver ,
or GPS . Unlik e Logi c Theorist , this progra m was designe d from the star t to imitat e huma n
problem­solvin g protocols . Withi n the limite d class of puzzle s it coul d handle , it turne d out that
the orde r in whic h the progra m considere d subgoal s and possibl e action s was simila r to the way
human s approache d the sam e problems . Thus , GPS was probabl y the first progra m to embod y
the "thinkin g humanly " approach . The combinatio n of AI and cognitiv e scienc e has continue d
at CMU up to the presen t day.
13 Now Carnegi e Mello n Universit y (CMU) .
14 Newel l and Simo n also invente d a list­processin g language , IPL, to write LT. The y had no compiler , and translate d it
into machin e code by hand . To avoi d errors , they worke d in parallel , callin g out binar y number s to each othe r as they
wrote each instructio n to mak e sure they agreed .
Chapte r 1. Introductio n
At IBM , Nathanie l Rocheste r and his colleague s produce d some of the first AI programs .
Herber t Gelernte r (1959 ) constructe d the Geometr y Theore m Prover . Lik e the Logi c Theorist ,
it prove d theorem s usin g explicitl y represente d axioms . Gelernte r soon foun d that there wer e
too man y possibl e reasonin g path s to follow , mos t of whic h turne d out to be dead ends . To help
focus the search , he adde d the capabilit y to creat e a numerica l representatio n of a diagram— a
particula r case of the genera l theore m to be proved . Befor e the progra m tried to prov e something ,
it coul d first chec k the diagra m to see if it was true in the particula r case.
Startin g in 1952 , Arthu r Samue l wrot e a serie s of program s for checker s (draughts ) that
eventuall y learne d to play tournament­leve l checkers . Alon g the way, he disprove d the idea that
computer s can only do what  they are told to, as his progra m quickl y learne d to play a bette r gam e
than its creator . The progra m was demonstrate d on televisio n in Februar y 1956 , creatin g a very
stron g impression . Like Turing , Samue l had troubl e findin g compute r time . Workin g at night , he
used machine s that were still on the testin g floor at IBM' s manufacturin g plant . Chapte r 5 cover s
game playing , and Chapte r 20 describe s and expand s on the learnin g technique s used by Samuel .
John McCarth y move d from Dartmout h to MIT and there mad e three crucia l contribution s
in one histori c year: 1958 . In MIT AI Lab Mem o No. 1, McCarth y define d the high­leve l languag e
Lisp, whic h was to becom e the dominan t AI programmin g language . Lisp is the second­oldes t
languag e in curren t use.15 Wit h Lisp , McCarth y had the tool he needed , but acces s to scarc e and
expensiv e computin g resource s was also a seriou s problem . Thus , he and other s at MIT invente d
time sharing . Afte r gettin g an experimenta l time­sharin g syste m up at MIT , McCarth y eventuall y
attracte d the interes t of a grou p of MIT grad s who forme d Digita l Equipmen t Corporation , whic h
was to becom e the world' s secon d larges t compute r manufacturer , thank s to their time­sharin g
minicomputers . Als o in 1958 , McCarth y publishe d a pape r entitle d Programs  with  Common
Sense,  in whic h he describe d the Advic e Taker , a hypothetica l progra m that can be seen as the
first complete  AI system . Lik e the Logi c Theoris t and Geometr y Theore m Prover , McCarthy' s
progra m was designe d to use knowledg e to searc h for solution s to problems . But unlik e the others ,
it was to embod y genera l knowledg e of the world . For example , he showe d how som e simpl e
axiom s woul d enabl e the progra m to generat e a plan to drive to the airpor t to catch a plane . The
progra m was also designe d so that it coul d accep t new axiom s in the norma l cours e of operation ,
thereb y allowin g it to achiev e competenc e in new areas without  being  reprogrammed.  The Advic e
Taker thus embodie d the central  principle s of knowledg e representatio n and reasoning : that it
is usefu l to have a formal , explici t representatio n of the worl d and the way an agent' s action s
affect  the world , and to be able to manipulat e these representation s with deductiv e processes . It
is remarkabl e how muc h of the 1958 pape r remain s relevan t after more than 35 years .
1958 also marke d the year that Marvi n Minsk y move d to MIT . For year s he and McCarth y
were inseparabl e as they define d the field together . Bu t they grew apar t as McCarth y stresse d
representatio n and reasonin g in forma l logic , wherea s Minsky  was mor e intereste d in gettin g
program s to work , and eventuall y develope d an anti­logica l outlook . In 1963 , McCarth y took
the opportunit y to go to Stanfor d and start the AI lab there . Hi s researc h agend a of usin g
logic to buil d the ultimat e Advic e Take r was advance d by J. A. Robinson' s discover y of the
resolutio n metho d (a complete  theorem­provin g algorith m for first­orde r logic ; see Sectio n 9.6).
Work at Stanfor d emphasize d general­purpos e method s for logica l reasoning . Application s of
15 FORTRA N is one year olde r than Lisp .
Sectio n 1.3. Th e Histor y of Artificia l Intelligenc e 19
logic include d Cordel l Green' s questio n answerin g and plannin g system s (Green , 1969b) , and the
Shake y robotic s projec t at the new Stanfor d Researc h Institut e (SRI) . The latte r project , discusse d
furthe r in Chapte r 25, was the first to demonstrat e the complet e integratio n of logica l reasonin g
and physica l activity .
Minsk y supervise d a serie s of student s who chose  limite d problem s that appeare d to requir e
intelligenc e to solve . Thes e limite d domain s becam e know n as microworlds . Jame s Slagle' s
SAIN T progra m (1963a ) was able to solv e closed­for m integratio n problem s typica l of first­yea r
colleg e calculu s courses . Tom Evans' s ANALOG Y progra m (1968 ) solve d geometri c analog y
problem s that appea r in IQ tests , such as the one in Figur e 1.2. Bertra m Raphael' s (1968 ) SIR
(Semanti c Informatio n Retrieval ) was able to accep t inpu t statement s in a very restricte d subse t
of Englis h and answe r question s thereon . Danie l Bobrow' s STUDEN T progra m (1967 ) solve d
algebr a story problem s such as
If the numbe r of customer s Tom gets is twic e the squar e of 20 percen t of the numbe r of
advertisement s he runs , and the numbe r of advertisement s he runs is 45, wha t is the numbe r
of customer s Tom gets?
is to:
Figur e 1.2 A n exampl e proble m solve d by Evans' s ANALOG Y program .
The mos t famou s microworl d was the block s world , whic h consist s of a set of solid block s
place d on a tableto p (or more often , a simulatio n of a tabletop) , as show n in Figur e 1.3. A task
in this worl d is to rearrang e the block s in a certai n way , usin g a robo t hand that can pick up one
block at a time . The block s worl d was hom e to the visio n projec t of Davi d Huffma n (1971) ,
the visio n and constraint­propagatio n work of Davi d Walt z (1975) , the learnin g theor y of Patric k
Winsto n (1970) , the natura l languag e understandin g progra m of Terry Winogra d (1972) , and the
planne r of Scot t Fahlma n (1974) .
Early wor k buildin g on the neura l network s of McCulloc h and Pitts also flourished . The
work of Winogra d and Cowa n (1963 ) showe d how a large numbe r of element s coul d collectivel y
represen t an individua l concept , with a correspondin g increas e in robustnes s and parallelism .
Hebb' s learnin g method s wer e enhance d by Berni e Widro w (Widro w and Hoff , 1960 ; Widrow ,
1962) , who calle d his network s adalines , and by Fran k Rosenblat t (1962 ) with his perceptrons .
20 Chapte r 1. Introductio n
Figur e 1.3 A  scen e from the block s world . A task for the robo t migh t be "Pic k up a big red
block, " expresse d eithe r in natura l languag e or in a forma l notation .
Rosenblat t prove d the famou s perceptro n convergenc e theorem , showin g that his learnin g
algorith m coul d adjus t the connectio n strength s of a perceptro n to matc h any inpu t data, provide d
such a matc h existed . Thes e topic s are covere d in Sectio n 19.3 .
A dose of realit y (1966­1974 )
From the beginning , AI researcher s were not shy in makin g prediction s of their comin g successes .
The followin g statemen t by Herber t Simo n in 1957 is ofte n quoted :
It is not my aim to surpris e or shoc k you—bu t the simples t way I can summariz e is to say
that there are now in the worl d machine s that think , that learn and that create . Moreover , their
abilit y to do thes e thing s is goin g to increas e rapidl y until—i n a visibl e future—th e rang e of
problem s they can handl e will be coextensiv e with the rang e to whic h huma n min d has been
applied .
Althoug h one migh t argu e that term s such as "visibl e future " can be interprete d in variou s ways ,
some of Simon' s prediction s wer e mor e concrete . In 1958 , he predicte d that withi n 10 year s
a compute r woul d be ches s champion , and an importan t new mathematica l theore m woul d be
prove d by machine . Claim s such as thes e turne d out to be wildl y optimistic . The barrie r that
faced almos t all AI researc h project s was that method s that suffice d for demonstration s on one or
two simpl e example s turne d out to fail miserabl y whe n tried out on wide r selection s of problem s
and on mor e difficul t problems .
The firs t kin d of difficult y aros e becaus e earl y program s ofte n containe d littl e or no
knowledg e of their subjec t matter , and succeede d by mean s of simpl e syntacti c manipulations .
Weizenbaum' s ELIZ A progra m (1965) , whic h coul d apparentl y engag e in seriou s conversatio n
Sectio n 1.3. Th e Histor y of Artificia l Intelligenc e 21
MACHIN E EVOLUTIO Non any topic , actuall y just borrowe d and manipulate d the sentence s type d into it by a human .
A typica l story occurre d in early machin e translatio n efforts , whic h were generousl y funde d by
the Nationa l Researc h Counci l in an attemp t to spee d up the translatio n of Russia n scientifi c
paper s in the wake of the Sputni k launc h in 1957 . It was though t initiall y that simpl e syntacti c
transformation s base d on the grammar s of Russia n and English , and wor d replacemen t usin g
an electroni c dictionary , woul d suffic e to preserv e the exac t meaning s of sentences . In fact ,
translatio n require s genera l knowledg e of the subjec t matte r in orde r to resolv e ambiguit y and
establis h the conten t of the sentence . The famou s retranslatio n of "the spiri t is willin g but the
flesh is weak ' as "the vodk a is good but the meat is rotten " illustrate s the difficultie s encountered .
In 1966 , a repor t by an advisor y committe e foun d that "ther e has been no machin e translatio n
of genera l scientifi c text, and non e is in immediat e prospect. " All U.S . governmen t fundin g for
academi c translatio n project s was cancelled .
The secon d kind of difficult y was the intractabilit y of man y of the problem s that AI was
attemptin g tosolve . Mos t of the early AI program s worke d by representin g the basi c facts abou t
a proble m and tryin g out a serie s of steps to solv e it, combinin g differen t combination s of steps
until the righ t one was found . Th e earl y program s wer e feasibl e only becaus e microworld s
containe d veiy few objects . Befor e the theor y of NP­completenes s was developed , it was widel y
though t that "scalin g up" to large r problem s was simpl y a matte r of faste r hardwar e and large r
memories . The optimis m that accompanie d the developmen t of resolutio n theore m proving , for
example , wa< soon dampene d whe n researcher s faile d to prov e theorem s involvin g mor e than a
few doze n facts . The fact that  a program  can find a solution  in principle  does  not mean  that  the
program  contains  an\ of the mechanisms  needed  to find it in practice.
The illusio n of unlimite d computationa l powe r was not confine d to problem­solvin g pro­
grams . Ear h experiment s in machin e evolutio n (now calle d geneti c algorithms ) (Friedberg ,
1958 ; Friedber g et al,, 1959 ) were base d on the undoubtedl y correc t belie f that by makin g an
appropriat e serie s of smal l mutation s to a machin e code program , one can generat e a progra m
with good performanc e for any particula r simpl e task . The idea , then , was to try rando m muta ­
tions and then appl y a selectio n proces s to preserv e mutation s that seeme d to improv e behavior .
Despit e thousand s of hour s of CPU time , almos t no progres s was demonstrated .
Failur e to com e to grip s with the "combinatoria l explosion " was one of the main criticism s
of AI containe d in the Lighthil l repor t (Lighthill , 1973) , whic h forme d the basi s for the decisio n
by the Britis h governmen t to end suppor t for AI researc h in all but two universities . (Ora l
traditio n paint s a somewha t differen t and mor e colorfu l picture , wit h politica l ambition s and
persona l animositie s that canno t be put in print. )
A thir d difficult y aros e becaus e of som e fundamenta l limitation s on the basi c structure s
being used to generat e intelligen t behavior . For example , in 1969 , Minsk y and Papert' s book
Perceptrons  (1969 ) prove d that althoug h perceptron s coul d be show n to learn anythin g they were
capabl e of representing , they coul d represen t very little . In particular , a two­inpu t perceptro n
could not be .raine d to recogniz e whe n its two input s wer e different . Althoug h thei r result s
did not appb to mor e complex , multilaye r networks , researc h fundin g for neura l net researc h
soon dwindle d to almos t nothing . Ironically , the new back­propagatio n learnin g algorithm s for
multilaye r network s that were to caus e an enormou s resurgenc e in neura l net researc h in the late
1980 s were actuall y discovere d first in 1969 (Bryso n and Ho, 1969) .
Chapte r Introductio n
WEA K METHOD S
EXPER T SYSTEM SKnowledge­base d systems : The key to power ? (1969­1979 )
The pictur e of proble m solvin g that had arise n durin g the first decad e of AI researc h was of a
general­purpos e searc h mechanis m tryin g to strin g togethe r elementar y reasonin g step s to find
complet e solutions . Suc h approache s have been calle d wea k methods , becaus e they use wea k
informatio n abou t the domain . For nian y comple x domains , it turn s out that their performanc e is
also weak . The only way aroun d this is to use knowledg e more suite d to makin g large r reasonin g
steps and to solvin g typicall y occurrin g case s in narro w area s of expertise . One migh t say that to
solve a hard problem , you almos t have to know the answe r already .
The DENDRA L progra m (Buchana n et a/., 1969 ) was an early exampl e of this approach . It
was develope d at Stanford , wher e Ed Feigenbau m (a forme r studen t of Herber t Simon) , Bruc e
Buchana n (a philosophe r turne d compute r scientist) , and Joshu a Lederber g (a Nobe l laureat e
geneticist ) teame d up to solv e the proble m of inferrin g molecula r structur e from the informatio n
provide d by a mass spectrometer . The inpu t to the progra m consist s of the elementar y formul a of
the molecul e (e.g. , C^H^NCi) , and the mass spectru m givin g the masse s of the variou s fragment s
of the molecul e generate d whe n it is bombarde d by an electro n beam . For example , the mas s
spectru m migh t contai n a peak at in­  15 correspondin g to the mass of a methy l (CHi ) fragment .
The naiv e versio n of the progra m generate d all possibl e structure s consisten t with the
formula , and then predicte d wha t mas s spectru m woul d be observe d for each , comparin g this
with the actua l spectrum . As one migh t expect , this rapidl y becam e intractabl e for decent­size d
molecules . The DENDRA L researcher s consulte d analytica l chemist s and foun d that they worke d
by lookin g for well­know n pattern s of peak s in the spectru m that suggeste d commo n substructure s
in the molecule . For example , the followin g rule is used to recogniz e a keton e (C=O ) subgroup :
if there are two peak s at A"i and,r > such that
(a) x\ +.\i  = M + 28 (M is the mas s of the whol e molecule) ;
(b) A"i — 28 is a high peak ;
(c) A"2 — 28 is a high peak ;
(d) At leas t one of AI and AT is high .
then ther e is a keton e subgrou p
Havin g recognize d that the molecul e contain s a particula r substructure , the numbe r of possibl e
candidate s is enormousl y reduced . Th e DENDRA L team conclude d that the new syste m was
powerfu l becaus e
All the relevan t theoretica l knowledg e to solv e thes e problem s has been mappe d over from its
genera l form in the [spectru m predictio n component ] ("firs t principles" ) to efficien t specia l
form s ("cookboo k recipes") . (Feigenbau m el al, 1971 )
The significanc e of DENDRA L was that it was arguabl y the first successfu l knowledge­intensive
system : its expertis e derive d from larg e number s of special­purpos e rules . Late r system s also
incorporate d the main them e of McCarthy' s Advic e Take r approach — the clean separatio n of the
knowledg e (in the form of rules ) and the reasonin g component .
With this lesso n in mind , Feigenbau m and other s at Stanfor d bega n the Heuristi c Program ­
ming Projec t (HPP) , to investigat e the exten t to whic h the new methodolog y of exper t system s
could be applie d to othe r area s of huma n expertise . Th e nex t majo r effor t was in the area of
medica l diagnosis . Feigenbaum , Buchanan , and Dr. Edwar d Shortliff e develope d MYCI N to
diagnos e bloo d infections . Wit h abou t 450 rules , MYCI N was able to perfor m as well as som e
Sectio n 1.3. Th e Histor y of Artificia l Intelligenc e 23
FRAME Sexperts , and considerabl y bette r than junio r doctors . It also containe d two majo r difference s from
DENDRAL . First , unlik e the DENDRA L rules , no genera l theoretica l mode l existe d from whic h the
MYCI N rules coul d be deduced . The y had to be acquire d from extensiv e interviewin g of experts ,
who in turn acquire d them from direc t experienc e of cases . Second , the rules had to reflec t the
uncertaint y associate d with medica l knowledge . MYCI N incorporate d a calculu s of uncertaint y
called certaint y factor s (see Chapte r 14), whic h seeme d (at the time ) to fit well with how doctor s
assesse d the impac t of evidenc e on the diagnosis .
Othe r approache s to medica l diagnosi s were also followed . At Rutger s University , Sau l
Amarel' s Computers  in Biomedicine  projec t bega n an ambitiou s attemp t to diagnos e disease s
based on explici t knowledg e of the causa l mechanism s of the diseas e process . Meanwhile , large
group s at MIT and the New Englan d Medica l Cente r were pursuin g an approac h to diagnosi s and
treatmen t base d on the theorie s of probabilit y and utility . Thei r aim was to buil d system s that
gave provabl y optima l medica l recommendations . In medicine , the Stanfor d approac h usin g rules
provide d by doctor s prove d mor e popula r at first . Bu t anothe r probabilisti c reasonin g system ,
PROSPECTO R (Dud a et al., 1979) , generate d enormou s publicit y by recommendin g explorator y
drillin g at a geologica l site that prove d to contai n a large molybdenu m deposit .
The importanc e of domai n knowledg e was also apparen t in the area of understandin g
natura l language . Althoug h Winograd' s SHRDL U syste m for understandin g natura l languag e had
engendere d a goo d deal of excitement , its dependenc e on syntacti c analysi s cause d som e of
the same problem s as occurre d in the early machin e translatio n work . It was able to overcom e
ambiguit y and understan d pronou n references , but this was mainl y becaus e it was designe d
specificall y for one area—th e block s world . Severa l researchers , includin g Eugen e Charniak ,
a fello w graduat e studen t of Winograd' s at MIT , suggeste d that robus t languag e understandin g
woul d requir e genera l knowledg e abou t the worl d and a genera l metho d for usin g that knowledge .
At Yale , the linguist­turned­Al­researche r Roge r Schan k emphasize d this poin t by claiming ,
"Ther e is no such thin g as syntax, " whic h upse t a lot of linguists , but did serv e to start a usefu l
discussion . Schan k and his student s buil t a serie s of program s (Schan k and Abelson , 1977 ;
Schan k and Riesbeck , 1981 ; Dyer , 1983 ) that all had the task of understandin g natura l language .
The emphasis , however , was less on languag e per se  and mor e on the problem s of representin g
and reasonin g with the knowledg e require d for languag e understanding . The problem s include d
representin g stereotypica l situation s (Cullingford , 1981) , describin g huma n memor y organizatio n
(Rieger , 1976 ; Kolodner , 1983) , and understandin g plan s and goal s (Wilensky , 1983) . Willia m
Wood s (1973 ) buil t the LUNA R system , whic h allowe d geologist s to ask question s in Englis h
abou t the rock sample s brough t back by the Apoll o moo n mission . LUNA R was the first natura l
languag e progra m that was used by peopl e othe r than the system' s autho r to get real work done .
Since then , man y natura l languag e program s have been used as interface s to databases .
The widesprea d growt h of application s to real­worl d problem s cause d a concomitan t in­
creas e in the demand s for workabl e knowledg e representatio n schemes . A larg e numbe r of
differen t representatio n language s wer e developed . Som e wer e base d on logic—fo r example ,
the Prolo g languag e becam e popula r in Europe , and the PLANNE R famil y in the Unite d States .
Others , followin g Minsky' s idea of frame s (1975) , adopte d a rathe r mor e structure d approach ,
collectin g togethe r facts abou t particula r objec t and even t types , and arrangin g the type s into a
large taxonomi c hierarch y analogou s to a biologica l taxonomy .
24 Chapte r 1. Introductio n
AI become s an industr y (1980­1988 )
The first successfu l commercia l exper t system , Rl, bega n operatio n at Digita l Equipmen t Cor­
poratio n (McDermott , 1982) . The progra m helpe d configur e order s for new compute r systems ,
and by 1986 , it was savin g the compan y an estimate d $40 millio n a year . By 1988 , DEC' s AI
group had 40 deploye d exper t systems , with more on the way. Du Pont had 100 in use and 500 in
development , savin g an estimate d $10 millio n a year . Nearl y ever y majo r U.S. corporatio n had
its own AI grou p and was eithe r usin g or investigatin g exper t syste m technology .
In 1981 , the Japanes e announce d the "Fift h Generation " project , a 10­yea r plan to build
intelligen t computer s runnin g Prolo g in muc h the same way that ordinar y computer s run machin e
code. The idea was that with the abilit y to mak e million s of inference s per second , computer s
woul d be able to take advantag e of vast store s of rules . The projec t propose d to achiev e full­scal e
natura l languag e understanding , amon g othe r ambitiou s goals .
The Fift h Generatio n projec t fuele d interes t in AI, and by takin g advantag e of fears of j
Japanes e domination , researcher s and corporation s were able to generat e suppor t for a simila r
investmen t in the Unite d States . The Microelectronic s and Compute r Technolog y Corporatio n i
(MCC ) was forme d as a researc h consortiu m to counte r the Japanes e project . In Britain , the
Alve y repor t reinstate d the fundin g that was cut by the Lighthil l report.16 In both cases , AI was
part of a broa d effort , includin g chip desig n and human­interfac e research .
The boomin g AI industr y also include d companie s such as Carnegi e Group , Inference ,
Intellicorp , and Teknowledg e that offere d the softwar e tools to buil d exper t systems , and hard ­
ware companie s such as Lisp Machine s Inc. , Texa s Instruments , Symbolics , and Xero x that ;
were buildin g workstation s optimize d for the developmen t of Lisp programs . Ove r a hundre d
companie s buil t industria l roboti c visio n systems . Overall , the industr y wen t from a few millio n \
in sales in 1980 to $2 billio n in 1988 .
The retur n of neura l network s (1986­present )
Althoug h compute r scienc e had neglecte d the field of neura l network s after Minsk y and Papert' s
Perceptrons  book , wor k had continue d in othe r fields , particularl y physics . Larg e collection s '
of simpl e neuron s coul d be understoo d in muc h the sam e way as larg e collection s of atom s in <
solids . Physicist s such as Hopfiel d (1982 ) used technique s from statistica l mechanic s to analyz e
the storag e and optimizatio n propertie s of networks , leadin g to significan t cross­fertilizatio n of j
ideas . Psychologist s includin g Davi d Rumelhar t and Geof f Hinto n continue d the stud y of neura l
net model s of memory . As we discus s in Chapte r 19, the real impetu s cam e in the mid­1980 s
when at least four differen t group s reinvente d the back­propagatio n learnin g algorith m first foun d
in 1969 by Bryso n and Ho. The algorith m was applie d to man y learnin g problem s in compute r
scienc e and psychology , and the widesprea d disseminatio n of the result s in the collectio n Parallel
Distributed  Processing  (Rumelhar t and McClelland , 1986 ) cause d grea t excitement .
At abou t the same time , som e disillusionmen t was occurrin g concernin g the applicabilit y
of the exper t syste m technolog y derive d from MYCiN­typ e systems. ­ Man y corporation s and
16 To save embarrassment , a new field calle d IKB S (Intelligen t Knowledge­BasedSystems ) was define d becaus e Artificia l
Intelligenc e had been officiall y cancelled .
Sectio n 1.3. Th e Histor y of Artificia l Intelligenc e 25
researc h group s foun d that buildin g a successfu l exper t syste m involve d muc h more than simpl y
buyin g a reasonin g syste m and fillin g it with rules . Som e predicte d an "AI Winter " in whic h AI
fundin g woul d be squeeze d severely . It was perhap s this fear , and the historica l factor s on the
neura l networ k side, that led to a perio d in whic h neura l network s and traditiona l AI were seen
as rival fields , rathe r than as mutuall y supportin g approache s to the same problem .
Recen t event s (1987­present )
Recen t year s have seen a sea chang e in both the conten t and the methodolog y of researc h in
artificia l intelligence.17 It is now mor e commo n to buil d on existin g theorie s than to propos e
brand new ones , to base claim s on rigorou s theorem s or hard experimenta l evidenc e rathe r than
on intuition , and to show relevanc e to real­worl d application s rathe r than toy examples .
The fiel d of speec h recognitio n illustrate s the pattern . In the 1970s , a wid e variet y of
differen t architecture s and approache s were tried . Man y of these were rathe r ad hoc and fragile ,
and were demonstrate d on a few speciall y selecte d examples . In recen t years , approache s base d
on hidde n Marko v model s (HMMs ) have com e to dominat e the area. Two aspect s of HMM s are
relevan t to the presen t discussion . First , they are base d on a rigorou s mathematica l theory . This
has allowe d speec h researcher s to build on severa l decade s of mathematica l result s develope d in
other fields . Second , they are generate d by a proces s of trainin g on a large corpu s of real speec h
data. Thi s ensure s that the performanc e is robust , and in rigorou s blin d tests the HMM s have
been steadil y improvin g thei r scores . Speec h technolog y and the relate d field of handwritte n
characte r recognitio n are alread y makin g the transitio n to widesprea d industria l and consume r
applications .
Anothe r area that seem s to have benefitte d from formalizatio n is planning . Earl y wor k by
Austi n Tate (1977) , followe d up by Davi d Chapma n (1987) , has resulte d in an elegan t synthesi s
of existin g plannin g program s into a simpl e framework . Ther e have been a numbe r of advance s
that buil t upon each othe r rathe r than startin g from scratc h each time . The resul t is that plannin g
system s that were only good for microworld s in the 1970 s are now used for schedulin g of factor y
work and spac e missions , amon g othe r things . See Chapter s 11 and 12 for more details .
Judea Pearl' s (1988 ) Probabilistic  Reasoning  in Intelligent  Systems  marke d a new accep ­
tance of probabilit y and decisio n theor y in AI, followin g a resurgenc e of interes t epitomize d by
Peter Cheeseman' s (1985 ) articl e "In Defens e of Probability. " The belie f networ k formalis m was
invente d to allow efficien t reasonin g abou t the combinatio n of uncertai n evidence . This approac h
largel y overcome s the problem s with probabilisti c reasonin g system s of the 1960 s and 1970s ,
and has com e to dominat e AI researc h on uncertai n reasonin g and exper t systems . Wor k by
Judea Pear l (1982a ) and by Eric Horvit z and Davi d Heckerma n (Horvit z and Heckerman , 1986 ;
Horvit z et al., 1986 ) promote d the idea of normative  exper t systems : one s that act rationall y
accordin g to the laws of decisio n theor y and do not try to imitat e huma n experts . Chapter s 14 to
16 cove r this area .
17 Som e have characterize d this chang e as a victor y of the neats —thos e who thin k that AI theorie s shoul d be grounde d
in mathematica l rigor—ove r the scruffles —thos e who woul d rathe r try out lots of ideas , writ e som e programs , and then
asses s wha t seem s to be working . Both approache s are important . A shif t towar d increase d neatnes s implie s that the field
has reache d a leve l of stabilit y and maturity . (Whethe r that stabilit y will be disrupte d by a new scruff y idea is anothe r
question. )
26 Chapte r Introductio n
Simila r gentl e revolution s have occurre d in robotics , compute r vision , machin e learnin g
(includin g neura l networks) , and knowledg e representation . A bette r understandin g of the prob ­
lems and their complexit y properties , combine d with increase d mathematica l sophistication , has
led to workabl e researc h agenda s and robus t methods . Perhap s encourage d by the progres s in
solvin g the subproblem s of AI, researcher s have also starte d to look at the "whol e agent " proble m
again . The work of Alie n Newell , John Laird , and Paul Rosenbloo m on SOA R (Newell , 1990 ;
Laird et al., 1987 ) is the best­know n exampl e of a complet e agen t architectur e in AI. The so­calle d
"situated " movemen t aims to understan d the working s of agent s embedde d in real environment s
with continuou s sensor y inputs . Man y interestin g result s are comin g out of such work , includin g
the realizatio n that the previousl y isolate d subfield s of AI may need to be reorganize d somewha t
when thei r result s are to be tied togethe r into a singl e agen t design .
1.4 TH E STAT E OF THE ART
Internationa l grandmaste r Arnol d Denke r studie s the piece s on the boar d in fron t of him . He
realize s ther e is no hope ; he mus t resig n the game . His opponent , HITECH , become s the first
compute r progra m to defea t a grandmaste r in a gam e of ches s (Berliner , 1989) .
"I wan t to go from Bosto n to San Francisco, " the travelle r says into the microphone . "Wha t
date will you be travellin g on?" is the reply . The travelle r explain s she want s to go Octobe r 20th ,
nonstop , on the cheapes t availabl e fare, returnin g on Sunday . A speec h understandin g progra m
name d PEGASU S handle s the whol e transaction , whic h result s in a confirme d reservatio n that
saves the travelle r $894 over the regula r coac h fare. Eve n thoug h the speec h recognize r gets one
out of ten word s wrong,18 it is able to recove r from thes e error s becaus e of its understandin g of
how dialog s are put togethe r (Zue et al., 1994) .
An analys t in the Missio n Operation s room of the Jet Propulsio n Laborator y suddenl y
starts payin g attention . A red messag e has flashe d onto the scree n indicatin g an "anomaly " with
the Voyage r spacecraft , whic h is somewher e in the vicinit y of Neptune . Fortunately , the analys t
is able to correc t the proble m from the ground . Operation s personne l believ e the proble m migh t
have been overlooke d had it not been for MARVEL , a real­tim e exper t syste m that monitor s the
massiv e strea m of data transmitte d by the spacecraft , handlin g routin e task s and alertin g the
analyst s to more seriou s problem s (Schwuttke , 1992) .
Cruisin g the highwa y outsid e of Pittsburg h at a comfortabl e 55 mph , the man in the driver' s
seat seem s relaxed . He shoul d be—fo r the past 90 miles , he has not had to touc h the steerin g wheel ,
brake , or accelerator . The real drive r is a roboti c syste m that gather s inpu t from vide o cameras ,
sonar , and lase r rang e finder s attache d to the van. It combine s thes e input s with experienc e
learne d from trainin g runs and succesfull y compute s how to steer the vehicl e (Pomerleau , 1993) .
A leadin g exper t on lymph­nod e patholog y describe s a fiendishl y difficul t case to the
exper t system , and examine s the system' s diagnosis . He scoff s at the system' s response . Onl y
slightl y worried , the creator s of the syste m sugges t he ask the compute r for an explanatio n of
18 Som e othe r existin g system s err only half as ofte n on this task .
Sectio n 1.5. Summar y 27
the diagnosis . The machin e point s out the majo r factor s influencin g its decision , and explain s
the subtl e interactio n of severa l of the symptom s in this case . Th e exper t admit s his error ,
eventuall y (Heckerman , 1991) .
From a camer a perche d on a stree t ligh t abov e the crossroads , the traffi c monito r watche s
the scene . If any human s wer e awak e to read the mai n screen , they woul d see "Citroe n 2CV
turnin g from Place de la Concord e into Champ s Ely sees," "Larg e truc k of unknow n mak e stoppe d
on Plac e de la Concorde, " and so on into the night . And occasionally , "Majo r inciden t on Plac e
de la Concorde , speedin g van collide d with motorcyclist, " and an automati c call to the emergenc y
service s (Kin g et al, 1993 ; Rolle r et al., 1994) .
These are just a few example s of artificia l intelligenc e system s that exis t today . Not magi c
or scienc e fiction—bu t rathe r science , engineering , and mathematics , to whic h this book provide s
an introduction .
i.5 SUMMAR Y
This chapte r define s Al and establishe s the cultura l backgroun d agains t whic h it has developed .
Some of the importan t point s are as follows :
• Differen t peopl e thin k of Al differently . Tw o importan t question s to ask are: Are you
concerne d with thinkin g or behavior ? Do you wan t to mode l humans , or wor k from an
ideal standard ?
• In this book , we adop t the view that intelligenc e is concerne d mainl y with rationa l action .
Ideally , an intelligen t agen t take s the best possibl e actio n in a situation . We will stud y the
proble m of buildin g agent s that are intelligen t in this sense .
• Philosopher s (goin g back to 400 B.C. ) mad e Al conceivabl e by considerin g the idea s that
the min d is in som e way s like a machine , that it operate s on knowledg e encode d in som e
interna l language , and that though t can be used to help arriv e at the righ t action s to take .
• Mathematician s provide d the tool s to manipulat e statement s of logica l certaint y as wel l
as uncertain , probabilisti c statements . The y also set the groundwor k for reasonin g abou t
algorithms .
• Psychologist s strengthene d the idea that human s and othe r animal s can be considere d
informatio n processin g machines . Linguist s showe d that languag e use fits into this model .
• Compute r engineerin g provide d the artifac t that make s Al application s possible . Al pro­
gram s tend to be large , and they coul d not work withou t the grea t advance s in spee d and
memor y that the compute r industr y has provided .
• The histor y of Al has had cycle s of success , misplace d optimism , and resultin g cutback s
in enthusias m and funding . Ther e hav e also been cycle s of introducin g new creativ e
approache s and systematicall y refinin g the best ones .
• Recen t progress  in understandin g the theoretica l basi s for intelligenc e has gon e han d in
hand with improvement s in the capabilitie s of real systems .
28 Chapte r 1. Introductio n
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
Danie l Crevier' s (1993 ) Artificial  Intelligence  give s a complet e histor y of the field , and Raymon d
Kurzweil' s (1990 ) Age of Intelligent  Machines  situate s AI in the broade r contex t of compute r
scienc e and intellectua l histor y in general . Diann e Marti n (1993 ) document s the degre e to whic h
early computer s were endowe d by the medi a with mythica l power s of intelligence .
The methodologica l statu s of artificia l intelligenc e is discusse d in The Sciences  of the Ar­
tificial,  by Herb Simo n (1981) , whic h discusse s researc h area s concerne d with comple x artifacts .
It explain s how AI can be viewe d as both scienc e and mathematics .
Artificial  Intelligence:  The Very Idea,  by John Haugelan d (1985 ) gives a readabl e accoun t of
the philosophica l and practica l problem s of AI. Cognitiv e scienc e is well­describe d by Johnson ­
Laird' s The  Computer  and the  Mind:  An Introduction to  Cognitive  Science.  Bake r (1989 )
cover s the syntacti c part of moder n linguistics , and Chierchi a and McConnell­Gine t (1990 ) cove r
semantics . Alie n (1995 ) cover s linguistic s from the AI poin t of view .
Early AI work is covere d in Feigenbau m and Feldman' s Computers  and Thought,  Minsky' s
Semantic  Information  Processing,  and the Machine  Intelligence  serie s edite d by Donal d Michie .
A large numbe r of influentia l paper s are collecte d in Readings  in Artificial Intelligence  (Webbe r
and Nilsson , 1981) . Earl y paper s on neura l network s are collecte d in Neurocomputing  (Anderso n
and Rosenfeld , 1988) . The Encyclopedia  ofAI  (Shapiro , 1992 ) contain s surve y article s on almos t
every topic in AI. Thes e article s usuall y provid e a good entr y poin t into the researc h literatur e on
each topic . The four­volum e Handbook  of Artificial  Intelligence  (Bar r and Feigenbaum , 1981 )
contain s description s of almos t ever y majo r AI syste m publishe d befor e 1981 .
The mos t recen t wor k appear s in the proceeding s of the majo r AI conferences : the biennia l
Internationa l Join t Conferenc e on AI (IJCAI) , and the annua l Nationa l Conferenc e on AI, more
often know n as AAAI , afte r its sponsorin g organization . The majo r journal s for genera l AI are
Artificial  Intelligence,  Computational  Intelligence,  the IEE E Transactions  on Pattern  Analysis
and Machine  Intelligence,  and the electroni c Journal  of Artificial  Intelligence Research.  Ther e
are also man y journal s devote d to specifi c areas , whic h we cove r in the appropriat e chapters .
Commercia l product s are covere d in the magazine s AI Expert  and PC AI. The main professiona l
societie s for AI are the America n Associatio n for Artificia l Intelligenc e (AAAI) , the ACM Specia l
Interes t Grou p in Artificia l Intelligenc e (SIGART) , and the Societ y for Artificia l Intelligenc e and
Simulatio n of Behaviou r (AISB) . AAAF s AI Magazine  and the SIGART  Bulletin  contai n man y
topica l and tutoria l article s as well as announcement s of conference s and workshops .
EXERCISE S
These exercise s are intende d to stimulat e discussion , and som e migh t be set as term projects .
Alternatively , preliminar y attempt s can be mad e now , and these attempt s can be reviewe d after
completin g the book .
1.1 Rea d Turing' s origina l pape r on AI (Turing , 1950) . In the paper , he discusse s severa l
potentia l objection s to his propose d enterpris e and his test for intelligence . Whic h objection s
I
Sectio n 1.5. Summar y 29
still carry som e weight ? Are his refutation s valid ? Can you thin k of new objection s arisin g from
development s since he wrot e the paper ? In the paper , he predict s that by the year 2000 , a compute r
will have a 30% chanc e of passin g a five­minut e Turin g Test with an unskille d interrogator . Do
you thin k this is reasonable ?
1.2 W e characterize d the definition s of AI alon g two dimensions , huma n vs. idea l and though t
vs. action . But there are othe r dimension s that are wort h considering . One dimensio n is whethe r
we are intereste d in theoretica l result s or in practica l applications . Anothe r is whethe r we inten d
our intelligen t computer s to be consciou s or not. Philosopher s have had a lot to say abou t this
issue , and althoug h mos t AI researcher s are happ y to leav e the question s to the philosophers ,
STRON G AI ther e has been heate d debate . The claim that machine s can be consciou s is calle d the stron g AI
WEAKA I claim ; the wea k AI positio n make s no such claim . Characteriz e the eigh t definition s on page
5 and the seve n followin g definition s accordin g to the four dimension s we have mentione d and
whateve r othe r ones you feel are helpful .
Artificia l intelligenc e is ...
a. "a collectio n of algorithm s that are computationall y tractable , adequat e approximation s of
intractabl y specifie d problems " (Partridge , 1991 )
b. "the enterpris e of constructin g a physica l symbo l syste m that can reliabl y pass the Turin g
Test" (Ginsberg , 1993 )
c. "the field of compute r scienc e that studie s how machine s can be mad e to act intelli ­
gently " (Jackson , 1986 )
d. "a fiel d of stud y that encompasse s computationa l technique s for performin g task s that
apparentl y requir e intelligenc e whe n performe d by humans " (Tanimoto , 1990 )
e. "a very genera l investigatio n of the natur e of intelligenc e and the principle s and mechanism s
require d for understandin g or replicatin g it" (Sharpie s et ai, 1989 )
f. "the gettin g of computer s to do thing s that seem to be intelligent " (Rowe , 1988) .
1.3 Ther e are well­know n classe s of problem s that are intractabl y difficul t for computers ,
and othe r classe s that are provabl y undecidabl e by any computer . Doe s this mea n that AI is
impossible ?
1.4 Suppos e we exten d Evans' s ANALOG Y progra m so that it can scor e 200 on a standar d IQ
test. Woul d we then have a progra m mor e intelligen t than a human ? Explain .
1.5 Examin e the AI literatur e to discove r whethe r or not the followin g task s can currentl y be
solve d by computers :
a. Playin g a decen t gam e of table tenni s (ping­pong) .
b. Drivin g in the cente r of Cairo .
c. Playin g a decen t gam e of bridg e at a competitiv e level .
d. Discoverin g and provin g new mathematica l theorems .
e. Writin g an intentionall y funn y story .
f. Givin g competen t lega l advic e in a specialize d area of law.
g. Translatin g spoke n Englis h into spoke n Swedis h in real time .
30 Chapte r 1. Introductio n
For the currentl y infeasibl e tasks , try to find out wha t the difficultie s are and estimat e whe n they
will be overcome .
1.6 Fin d an articl e writte n by a lay perso n in a reputabl e newspape r or magazin e claimin g
the achievemen t of som e intelligen t capacit y by a machine , wher e the clai m is eithe r wildl y
exaggerate d or false .
1.7 Fact , fiction , and forecast :
a. Fin d a clai m in prin t by a reputabl e philosophe r or scientis t to the effec t that a certai n
capacit y will neve r be exhibite d by computers , wher e that capacit y has now been exhibited .
b. Fin d a claim by a reputabl e compute r scientis t to the effec t that a certai n capacit y woul d
be exhibite d by a date that has sinc e passed , withou t the appearanc e of that capacity .
c. Compar e the accurac y of these prediction s to prediction s in othe r field s such as biomedicine ,
fusio n power , nanotechnology , transportation , or hom e electronics .
1.8 Som e author s have claime d that perceptio n and moto r skill s are the mos t importan t part of
intelligence , and that "higher­level " capacitie s are necessaril y parasitic—simpl e add­on s to thes e
underlyin g facilities . Certainly , mos t of evolutio n and a larg e part of the brai n have been devote d
to perceptio n and moto r skills , wherea s AI has foun d task s such as gam e playin g and logica l
inferenc e to be easier , in man y ways , than perceivin g and actin g in the real world . Do you thin k
that AI's traditiona l focu s on higher­leve l cognitiv e abilitie s is misplaced ?
1.9 "Surel y computer s canno t be intelligent—the y can only do wha t thei r programmer s tell
them. " Is the latte r statemen t true , and does it impl y the former ?
1.10 "Surel y animal s canno t be intelligent—the y can only do wha t thei r gene s tell them. " Is
the latte r statemen t true , and does it impl y the former ?
I
2INTELLIGEN T AGENT S
In which we  discuss  what  an intelligent  agent does,  how  it is related  to its environment,
how it is evaluated,  and how  we might  go about  building  one.
2.1 INTRODUCTIO N
An agen t is anythin g that can be viewe d as perceivin g its environmen t throug h sensor s and actin g
upon that environmen t throug h effectors . A huma n agen t has eyes , ears , and othe r organ s for
sensors , and hands , legs , mouth , and othe r body parts for effectors . A roboti c agen t substitute s
camera s and infrare d rang e finder s for the sensor s and variou s motor s for the effectors . A
softwar e agen t has encode d bit string s as its percept s and actions . A generi c agen t is diagramme d
in Figur e 2.1.
Our aim in this book is to desig n agent s that do a good job of actin g on their environment .
First, we will be a little more precis e abou t wha t we mea n by a good job. The n we will talk abou t
differen t design s for successfu l agents—fillin g in the questio n mar k in Figur e 2.1. We discus s
some of the genera l principle s used in the desig n of agent s throughou t the book , chie f amon g
whic h is the principl e that agent s shoul d know  things . Finally , we show how to coupl e an agen t
to an environmen t and describ e severa l kind s of environments .
How AGENT S SHOUL D ACT
RATIONA L AGEN T A  rationa l agen t is one that does the righ t thing . Obviously , this is bette r than doin g the wron g
thing , but wha t does it mean ? As a first approximation , we will say that the righ t actio n is the
one that will caus e the agen t to be mos t successful . Tha t leave s us with the proble m of decidin g
how and when  to evaluat e the agent' s success .
31
32 Chapte r 2. Intelligen t Agent s
sensors
effector s
Figur e 2.1 Agent s interac t with environment s throug h sensor s and effectors .
PERFORMANC E
MEASUR E
OMNISCIENC EWe use the term performanc e measur e for the how— the criteri a that determin e how
successfu l an agen t is. Obviously , ther e is not one fixe d measur e suitabl e for all agents . We
could ask the agen t for a subjectiv e opinio n of how happ y it is with its own performance , but
some agent s woul d be unabl e to answer , and other s woul d delud e themselves . (Huma n agent s in
particula r are notoriou s for "sou r grapes"—sayin g they did not reall y wan t somethin g after they
are unsuccessfu l at gettin g it.) Therefore , we will insis t on an objectiv e performanc e measur e
impose d by som e authority . In othe r words , we as outsid e observer s establis h a standar d of what
it mean s to be successfu l in an environmen t and use it to measur e the performanc e of agents .
As an example , conside r the case of an agen t that is suppose d to vacuu m a dirty floor . A
plausibl e performanc e measur e woul d be the amoun t of dirt cleane d up in a singl e eight­hou r shift .
A mor e sophisticate d performanc e measur e woul d facto r in the amoun t of electricit y consume d
and the amoun t of nois e generate d as well . A third performanc e measur e migh t give highes t
mark s to an agen t that not only clean s the floo r quietl y and efficiently , but also find s time to go
windsurfin g at the weekend. '
The when  of evaluatin g performanc e is also important . If we measure d how muc h dirt the
agen t had cleane d up in the first hou r of the day, we woul d be rewardin g thos e agent s that start
fast (eve n if they do littl e or no work later on), and punishin g those that work consistently . Thus ,
we wan t to measur e performanc e over the long run, be it an eight­hou r shif t or a lifetime .
We need to be carefu l to distinguis h betwee n rationalit y and omniscience . An omniscien t
agen t know s the actual  outcom e of its actions , and can act accordingly ; but omniscienc e is ;
impossibl e in reality . Conside r the followin g example : I am walkin g alon g the Champ s Elysee s
one day and I see an old frien d acros s the street . Ther e is no traffi c nearb y and I'm not otherwis e
engaged , so, bein g rational , I start to cros s the street . Meanwhile , at 33,00 0 feet, a carg o door
falls off a passin g airliner,2 and befor e I mak e it to the othe r side of the stree t I am flattened . Was
I irrationa l to cross the street ? It is unlikel y that my obituar y woul d read "Idio t attempt s to cros s
1 Ther e is a dange r here for thos e who establis h performanc e measures : you ofte n get wha t you ask for. Tha t is. if
you measur e succes s by the amoun t of dirt cleane d up, then som e cleve r agen t is boun d to brin g in a load of dirt each
morning , quickl y clea n it up, and get a good performanc e score . Wha t you reall y wan t to measur e is how clea n the floo r
is, but determinin g that is mor e difficul t than just weighin g the dirt cleane d up.
2 See N. Henderson . "Ne w doo r latche s urge d for Boein g 747 jumb o jets." Washington  Post,  8/24/89 .
I
Sectio n 2.2. Ho w Agent s Shoul d Act 33
PERCEP T SEQUENC E
IDEAL RATIONA L
AGEN Tstreet. " Rather , this point s out that rationalit y is concerne d with expected  succes s given what  has
been perceived.  Crossin g the stree t was rationa l becaus e mos t of the time the crossin g woul d be
successful , and there was no way I coul d have foresee n the fallin g door . Note that anothe r agen t
that was equippe d with rada r for detectin g fallin g door s or a steel cage stron g enoug h to repe l
them woul d be more successful , but it woul d not be any mor e rational .
In othe r words , we canno t blam e an agen t for failin g to take into accoun t somethin g it could
not perceive , or for failin g to take an actio n (suc h as repellin g the carg o door ) that it is incapabl e
of taking . But relaxin g the requiremen t of perfectio n is not just a questio n of being fair to agents .
The poin t is that if we specif y that an intelligen t agen t shoul d alway s do wha t is actually  the right
thing , it will be impossibl e to desig n an agen t to fulfil l this specification—unles s we improv e the
performanc e of crysta l balls .
In summary , what  is rationa l at any give n time depend s on four things :
• The performanc e measur e that define s degre e of success .
• Everythin g that the agen t has perceive d so far. We will call this complet e perceptua l histor y
the percep t sequence .
• Wha t the agen t know s abou t the environment .
• The action s that the agen t can perform .
This lead s to a definitio n of an idea l rationa l agent : For  each  possible  percept  sequence,  an
ideal  rational  agent  should do  whatever action  is expected  to maximize  its performance  measure,
on the basis of  the evidence  provided  by the  percept  sequence and  whatever built­in  knowledge
the agent  has.
We need to look carefull y at this definition . At first glance , it migh t appea r to allow an
agent to indulg e in som e decidedl y underintelligen t activities . For example , if an agen t does not
look both way s befor e crossin g a busy road , then its percep t sequenc e will not tell it that there is
a large truck approachin g at high speed . The definitio n seem s to say that it woul d be OK for it to
cross the road . In fact, this interpretatio n is wron g on two counts . First , it woul d not be rationa l
to cross the road : the risk of crossin g withou t lookin g is too great . Second , an idea l rationa l
agent woul d have chose n the "looking " actio n befor e steppin g into the street , becaus e lookin g
helps maximiz e the expecte d performance . Doin g action s in order  to obtain  useful  information
is an importan t part of rationalit y and is covere d in dept h in Chapte r 16.
The notio n of an agen t is mean t to be a tool for analyzin g systems , not an absolut e
characterizatio n that divide s the worl d into agent s and non­agents . Conside r a clock . It can be
though t of as just an inanimat e object , or it can be though t of as a simpl e agent . As an agent ,
most clock s alway s do the righ t action : movin g their hand s (or displayin g digits ) in the prope r
fashion . Clock s are a kind of degenerat e agen t in that their percep t sequenc e is empty ; no matte r
what happen s outside , the clock' s actio n shoul d be unaffected .
Well , this is not quite true. If the clock and its owne r take a trip from Californi a to Australia ,
the righ t thin g for the cloc k to do woul d be to turn itsel f back six hours . We do not get upse t at
our clock s for failin g to do this becaus e we realiz e that they are actin g rationally , give n their lack
of perceptua l equipment.3
One of the author s still gets a smal l thril l whe n his compute r successfull y reset s itsel f at dayligh t saving s time .
34 Chapte r 2. Intelligen t Agent s
MAPPIN G
IDEA L MAPPING SThe idea l mappin g from percep t sequence s to action s
Once we realiz e that an agent' s behavio r depend s only on its percep t sequenc e to date, then we can
describ e any particula r agen t by makin g a table of the actio n it takes in respons e to each possibl e
percept  sequence . (Fo r mos t agents , this woul d be a very long list—infinite , in fact, unles s we
place a boun d on the lengt h of percep t sequence s we wan t to consider. ) Suc h a list is calle d
a mappin g from percep t sequence s to actions . We can, in principle , find out whic h mappin g
correctl y describe s an agen t by tryin g out all possibl e percep t sequence s and recordin g whic h
action s the agen t does in response . (If the agen t uses som e randomizatio n in its computations ,
then we woul d have to try some percep t sequence s severa l time s to get a good idea of the agent' s
averag e behavior. ) And if mapping s describ e agents , then idea l mapping s describ e ideal agents .
Specifying  which action an  agent  ought to  take  in response to  any given  percept  sequence  provides
a design  for an ideal  agent.
This does not mean , of course , that we have to creat e an explici t table with an entr y
for ever y possibl e percep t sequence . It is possibl e to defin e a specificatio n of the mappin g
withou t exhaustivel y enumeratin g it. Conside r a very simpl e agent : the square­roo t functio n
on a calculator . The percep t sequenc e for this agen t is a sequenc e of keystroke s representin g a
number , and the actio n is to displa y a numbe r on the displa y screen . The idea l mappin g is that
when the percep t is a positiv e numbe r x, the righ t actio n is to displa y a positiv e numbe r z such
that z2 « x, accurat e to, say, 15 decima l places . Thi s specificatio n of the idea l mappin g does
not requir e the designe r to actuall y construc t a tabl e of squar e roots . Nor does the square­roo t
functio n have to use a table to behav e correctly : Figur e 2.2 show s part of the ideal mappin g and
a simpl e progra m that implement s the mappin g usin g Newton' s method .
The square­roo t exampl e illustrate s the relationshi p betwee n the idea l mappin g and an
ideal agen t design , for a very restricte d task. Wherea s the table is very large , the agen t is a nice, ;
compac t program . It turn s out that it is possibl e to desig n nice , compac t agents  that implemen t j
Percep t x
1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9Actio n z
1.00000000000000 0
1.04880884817015 2
1.09544511501033 2
1.14017542509913 8
1.18321595661992 3
1.22474487139158 9
1.26491106406735 2
1.30384048104053 0
1.34164078649987 4
1 .37840487520902 2functio n SQRT(;C )
z <— 1 .0 /  * initial  guess  * 1
repea t until ]z2 ­ x] < 10~15
z *­ z ­ (z2 ­ x)/(2z)
end
retur n z
Figur e 2.2 Par t of the idea l mappin g for the square­roo t proble m (accurat e to 1 5 digits) , and a
correspondin g progra m that implement s the idea l mapping .
Sectio n 2.3. Structur e of Intelligen t Agent s 35
the idea l mappin g for muc h mor e genera l situations : agent s that can solv e a limitles s variet y of
tasks in a limitles s variet y of environments . Befor e we discus s how to do this, we need to look
at one mor e requiremen t that an intelligen t agen t ough t to satisfy .
Autonom y
There is one mor e thin g to deal with in the definitio n of an idea l rationa l agent : the "built­i n
knowledge " part . If the agent' s action s are base d completel y on built­i n knowledge , such that it
AUTONOM Y nee d pay no attentio n to its percepts , then we say that the agen t lack s autonomy . For example ,
if the cloc k manufacture r was prescien t enoug h to know that the clock' s owne r woul d be goin g
to Australi a at som e particula r date , then a mechanis m coul d be buil t in to adjus t the hand s
automaticall y by six hour s at just the righ t time . This woul d certainl y be successfu l behavior , but
the intelligenc e seem s to belon g to the clock' s designe r rathe r than to the cloc k itself .
An agent' s behavio r can be base d on both its own experienc e and the built­i n knowledg e
,.­.<­.. use d in constructin g the agen t for the particula r environmen t in whic h it operates . A system  is
I*E~ autonomous4 to the extent that  its behavior is  determined  b\ its own  experience.  It woul d be
too stringent , though , to requir e complet e autonom y from the word go: whe n the agen t has had
little or no experience , it woul d have to act randoml y unles s the designe r gave som e assistance .
So, just as evolutio n provide s animal s with enoug h built­i n reflexe s so that they can surviv e long
enoug h to learn for themselves , it woul d be reasonabl e to provid e an artificia l intelligen t agen t
with som e initia l knowledg e as well as an abilit y to learn .
Autonom y not only fits in with our intuition , but it is an exampl e of soun d engineerin g
practices . An agen t that operate s on the basis of built­i n assumption s will only operat e success ­
fully whe n thos e assumption s hold , and thus lack s flexibility . Consider , for example , the lowl y
dung beetle . Afte r diggin g its nest and layin g its eggs , it fetche s a ball of dung from a nearb y heap
to plug the entrance ; if the ball of dung is remove d from its gras p en route,  the beetl e continue s
on and pantomime s pluggin g the nest with the nonexisten t dun g ball , neve r noticin g that it is
missing . Evolutio n has buil t an assumptio n into the beetle' s behavior , and whe n it is violated ,
unsuccessfu l behavio r results . A truly autonomou s intelligen t agen t shoul d be able to operat e
successfull y in a wide variet y of environments , give n sufficien t time to adapt .
STRUCTUR E OF INTELLIGEN T AGENT S
So far we have talke d abou t agent s by describin g thei r behavior —the  actio n that is performe d
after any give n sequenc e of percepts . Now , we will have to bite the bulle t and talk abou t how
AGENTPROGRA M th e inside s work . The job of AI is to desig n the agen t program : a functio n that implement s
the agen t mappin g from percept s to actions . We assum e this progra m will run on som e sort of
ARCHITECTUR E computin g device , whic h we will call the architecture . Obviously , the progra m we choos e has
4 Th e wor d "autonomous " has also com e to mea n somethin g like "not unde r the immediat e contro l of a human, " as in
"autonomou s land vehicle. " We are usin g it in a stronge r sense .
AI All
36 Chapte r 2. Intelligen t Agent s
SOFTWAR E AGENT S
SOFTBOT Sto be one that the architectur e will accep t and run. The architectur e migh t be a plain computer , or
it migh t includ e special­purpos e hardwar e for certai n tasks , such as processin g camer a image s or
filterin g audi o input . It migh t also includ e softwar e that provide s a degre e of insulatio n betwee n
the raw compute r and the agen t program , so that we can progra m at a highe r level . In general ,
the architectur e make s the percept s from the sensor s availabl e to the program , runs the program ,
and feed s the program' s actio n choice s to the effector s as they are generated . The relationshi p
amon g agents , architectures , and program s can be summe d up as follows :
agent  = architecture  + program
Most of this book is abou t designin g agen t programs , althoug h Chapter s 24 and 25 deal directl y
with the architecture .
Befor e we desig n an agen t program , we mus t have a prett y good idea of the possibl e
percept s and actions , wha t goal s or performanc e measur e the agen t is suppose d to achieve , and
what sort of environmen t it will operat e in.5 Thes e com e in a wide variety . Figur e 2.3 show s the
basic element s for a selectio n of agen t types .
It may com e as a surpris e to som e reader s that we includ e in our list of agen t type s som e
program s that seem to operat e in the entirel y artificia l environmen t define d by keyboar d inpu t
and characte r outpu t on a screen . "Surely, " one migh t say, "thi s is not a real environment , is
it?" In fact , wha t matter s is not the distinctio n betwee n "real " and "artificial " environments ,
but the complexit y of the relationshi p amon g the behavio r of the agent , the percep t sequenc e
generate d by the environment , and the goal s that the agen t is suppose d to achieve . Som e "real "
environment s are actuall y quit e simple . For example , a robo t designe d to inspec t parts as they
come by on a conveye r belt can mak e use of a numbe r of simplifyin g assumptions : tha t the
lightin g is alway s just so, that the only thin g on the conveye r belt will be parts of a certai n kind ,
and that there are only two actions—accep t the part or mark it as a reject .
In contrast , som e softwar e agent s (or softwar e robot s or softbots ) exis t in rich , unlimite d
domains . Imagin e a softbo t designe d to fly a fligh t simulato r for a 747 . Th e simulato r is a
very detailed , comple x environment , and the softwar e agen t mus t choos e from a wide variet y of
action s in real time . Or imagin e a softbo t designe d to scan onlin e new s source s and show the
interestin g item s to its customers . To do well , it will need som e natura l languag e processin g
abilities , it will need to learn wha t each custome r is intereste d in, and it will need to dynamicall y
chang e its plan s when , for example , the connectio n for one new s sourc e crashe s or a new one
come s online .
Some environment s blur the distinctio n betwee n "real " and "artificial. " In the ALIV E
environmen t (Mae s et al., 1994) , softwar e agent s are give n as percept s a digitize d camer a imag e
of a room wher e a huma n walk s about . The agen t processe s the camer a imag e and choose s an
action . The environmen t also display s the camer a imag e on a large displa y scree n that the huma n
can watch , and superimpose s on the imag e a compute r graphic s renderin g of the softwar e agent .
One such imag e is a cartoo n dog, whic h has been programme d to mov e towar d the huma n (unles s
he point s to send the dog away ) and to shak e hand s or jump up eagerl y whe n the huma n make s
certai n gestures .
5 For the acronymicall y minded , we call this the PAG E (Percepts , Actions , Goals , Environment ) description . Not e that
the goal s do not necessaril y have to be represente d withi n the agent ; they simpl y describ e the performanc e measur e by
whic h the agen t desig n will be judged .
Sectio n 2.3. Structur e of Intelligen t Agent s 37
Agen t Type
Medica l diagnosi s
syste m
Satellit e imag e
analysi s syste m
Part­pickin g robo t
Refiner y controlle r
Interactiv e Englis h
tutorPercept s
Symptoms ,
findings , patient' s
answer s
Pixel s of varyin g
intensity , color
Pixel s of varyin g
intensit y
Temperature ,
pressur e reading s
Type d word sAction s
Questions , tests ,
treatment s
Print a
categorizatio n of
scene
Pick up parts and
sort into bins
Open , close
valves ; adjus t
temperatur e
Print exercises ,
suggestions ,
correction sGoal s
Health y patient ,
minimiz e costs
Correc t
categorizatio n
Place parts in
correc t bins
Maximiz e purity ,
yield , safet y
Maximiz e
student' s score on
testEnvironmen t
Patient , hospita l
Image s from
orbitin g satellit e
Conveyo r belt
with parts
Refiner y
Set of student s
Figur e 2.3 Example s of agen t type s and their PAG E descriptions .
The most famou s artificia l environmen t is the Turin g Test environment , in whic h the whol e
point is that real and artificia l agent s are on equa l footing , but the environmen t is challengin g
enoug h that it is very difficul t for a softwar e agen t to do as well as a human . Sectio n 2.4 describe s
in more detai l the factor s that make som e environment s more demandin g than others .
Agen t program s
We will be buildin g intelligen t agent s throughou t the book . The y will all have the same skeleton ,
namely , acceptin g percept s from an environmen t and generatin g actions . The early version s of
agent program s will have a very simpl e form (Figur e 2.4) . Eac h will use som e interna l data
structure s that will be update d as new percept s arrive . Thes e data structure s are operate d on by
the agent' s decision­makin g procedure s to generat e an actio n choice , whic h is then passe d to the
architectur e to be executed .
There are two thing s to note abou t this skeleto n program . First , even thoug h we define d
the agen t mappin g as a functio n from percep t sequences  to actions , the agen t progra m receive s
only a singl e percep t as its input . It is up to the agen t to build up the percep t sequenc e in memory ,
if it so desires . In som e environments , it is possibl e to be quit e successfu l withou t storin g
the percep t sequence , and in comple x domains , it is infeasibl e to store the complet e sequence .
38 Chapte r 2. Intelligen t Agent s
functio n SKELETON­AGEN~[(percept)  return s actio n
static : memory,  the agent' s memor y of the worl d
memory  — UPDATE­MEMORY(memory , percept)
action  <— CHOOSE­BEST­ACTION(/tt<?mwy )
memory  — UPDATE­MEMORY(/wemorv , action)
retur n action
Figur e 2.4 A  skeleto n agent . On each invocation , the agent' s memor y is update d to reflec t
the new percept , the best actio n is chosen , and the fact that the actio n was take n is also store d in
memory . The memor y persist s from one invocatio n to the next .
Second , the goal or performanc e measur e is not part of the skeleto n program . Thi s is becaus e
the performanc e measur e is applie d externall y to judg e the behavio r of the agent , and it is often
possibl e to achiev e high performanc e withou t explici t knowledg e of the performanc e measur e
(see, e.g., the square­roo t agent) .
Why not just look up the answers ?
Let us start with the simples t possibl e way we can thin k of to writ e the agen t program— a looku p
table . Figur e 2.5 show s the agen t program . It operate s by keepin g in memor y its entir e percep t
sequence , and usin g it to inde x into table,  whic h contain s the appropriat e actio n for all possibl e
percep t sequences .
It is instructiv e to conside r why this proposa l is doome d to failure :
1. The table neede d for somethin g as simpl e as an agen t that can only play ches s woul d be
abou t 3510° entries .
2. It woul d take quite a long time for the designe r to buil d the table .
3. The agen t has no autonom y at all, becaus e the calculatio n of best action s is entirel y built­in . ]
So if the environmen t change d in som e unexpecte d way , the agen t woul d be lost.
functio n TABLE­DRIVEN­AGENT(percepf ) return s action
static : percepts,  a sequence , initiall y empt y
table,  a table , indexe d by percep t sequences , initiall y full y specifie d
appen d percept  to the end of percepts
action  <— L(3OKVf(percepts,  table)
retur n action
Figur e 2.5 A n agen t base d on a prespecifie d looku p table . It keep s trac k of the percep t
sequenc e and just look s up the best action .
Sectio n 2.3. Structur e of Intelligen t Agent s 39
4. Eve n if we gave the agen t a learnin g mechanis m as well , so that it coul d have a degre e of
autonomy , it woul d take foreve r to learn the righ t valu e for all the table entries .
Despit e all this, TABLE­DRIVEN­AGEN T does  do wha t we want : it implement s the desire d agen t
mapping . It is not enoug h to say, "It can't be intelligent; " the poin t is to understan d why an agen t
that reasons  (as oppose d to lookin g thing s up in a table ) can do even bette r by avoidin g the four
drawback s liste d here .
An exampl e
At this point , it will be helpfu l to conside r a particula r environment , so that our discussio n
can becom e mor e concrete . Mainl y becaus e of its familiarity , and becaus e it involve s a broa d
range of skills , we will look at the job of designin g an automate d taxi driver . We shoul d poin t
out, befor e the reade r become s alarmed , that such a syste m is currentl y somewha t beyon d the
capabilitie s of existin g technology , althoug h mos t of the component s are availabl e in some form.6
The full drivin g task is extremel y open­ended— there  is no limi t to the nove l combination s of
circumstance s that can arise (whic h is anothe r reaso n why we chos e it as a focu s for discussion) .
We mus t first thin k abou t the percepts , actions , goal s and environmen t for the taxi. The y
are summarize d in Figur e 2.6 and discusse d in turn .
Agen t Type
Taxi drive rPercept s
Cameras ,
speedometer , GPS ,
sonar , microphon eAction s
Steer , accelerate ,
brake , talk to
passenge rGoal s
Safe, fast, legal ,
comfortabl e trip,
maximiz e profit sEnvironmen t
Roads , othe r
traffic , pedestrians ,
customer s
Figur e 2.6 Th e taxi drive r agen t type .
The taxi will need to know wher e it is, wha t else is on the road , and how fast it is going .
This informatio n can be obtaine d from the percept s provide d by one or mor e controllabl e TV
cameras , the speedometer , and odometer . To contro l the vehicl e properly , especiall y on curves , it
shoul d have an accelerometer ; it will also need to know the mechanica l state of the vehicle , so it
will need the usua l arra y of engin e and electrica l syste m sensors . It migh t have instrument s that
are not availabl e to the averag e huma n driver : a satellit e globa l positionin g syste m (GPS ) to give
it accurat e positio n informatio n with respec t to an electroni c map ; or infrare d or sona r sensor s to
detec t distance s to othe r cars and obstacles . Finally , it will need a microphon e or keyboar d for
the passenger s to tell it their destination .
The action s availabl e to a taxi drive r will be more or less the same ones availabl e to a huma n
driver : contro l over the engin e throug h the gas peda l and contro l over steerin g and braking . In
addition , it will need outpu t to a scree n or voic e synthesize r to talk back to the passengers , and
perhap s som e way to communicat e with othe r vehicles .
6 See page 26 for a descriptio n of an existin g drivin g robot , or look at the conferenc e proceeding s on Intelligen t Vehicl e
and Highwa y System s (IVHS) .
40 Chapte r 2. Intelligen t Agent s
Wha t performanc e measur e woul d we like our automate d drive r to aspir e to? Desirabl e
qualitie s includ e gettin g to the correc t destination ; minimizin g fuel consumptio n and wea r and
tear; minimizin g the trip time and/o r cost; minimizin g violation s of traffi c laws and disturbance s
to other drivers ; maximizin g safet y and passenge r comfort ; maximizin g profits . Obviously , som e
of these goal s conflict , so there will be trade­off s involved .
Finally , were this a real project , we woul d need to decid e wha t kind of drivin g environmen t
the taxi will face . Shoul d it operat e on local roads , or also on freeways ? Will it be in Souther n
California , wher e snow is seldo m a problem , or in Alaska , wher e it seldo m is not? Will it alway s
be drivin g on the right , or migh t we wan t it to be flexibl e enoug h to driv e on the left in case we
want to operat e taxis in Britai n or Japan ? Obviously , the mor e restricte d the environment , the
easie r the desig n problem .
Now we have to decid e how to buil d a real progra m to implemen t the mappin g from
percept s to action . We will find that differen t aspect s of drivin g sugges t differen t type s of agen t
program . We will conside r four type s of agen t program :
• Simpl e refle x agents
• Agent s that keep track of the worl d
• Goal­base d agent s
• Utility­base d agents
CONDITION­ACTIO N
RULESimpl e refle x agent s
The optio n of constructin g an explici t looku p table is out of the question . The visua l inpu t from
a singl e camer a come s in at the rate of 50 megabyte s per secon d (25 frame s per second , 1000 x
1000 pixel s with 8 bits of colo r and 8 bits of intensit y information) . So the looku p table for an
hour woul d be 260x60x50 M entries .
However , we can summariz e portion s of the table by notin g certai n commonl y occurrin g
input/outpu t associations . For example , if the car in fron t brakes , and its brak e light s com e on,
then the drive r shoul d notic e this and initiat e braking . In othe r words , some processin g is done on
the visua l inpu t to establis h the conditio n we call "The car in fron t is braking" ; then this trigger s
some establishe d connectio n in the agen t progra m to the actio n "initiat e braking" . We call such
a connectio n a condition­actio n rule7 writte n as
if car­in­front­is­bmking  then initiate­braking
Human s also have man y such connections , som e of whic h are learne d response s (as for driving )
and som e of whic h are innat e reflexe s (suc h as blinkin g whe n somethin g approache s the eye) .
In the cours e of the book , we will see severa l differen t way s in whic h such connection s can be
learne d and implemented .
Figur e 2.7 give s the structur e of a simpl e refle x agen t in schemati c form , showin g how
the condition­actio n rule s allo w the agen t to mak e the connectio n from percep t to action . (Do
not worr y if this seem s trivial ; it gets mor e interestin g shortly. ) We use rectangle s to denot e
7 Als o calle d situation­actio n rules , productions , or if­the n rules . The last term is also used by som e author s for
logica l implications , so we will avoi d it altogether .
Sectio n 2.3. Structur e of Intelligen t Agent s 41
Condition­actio n rules
Figur e 2.7 Schemati c diagra m of a simpl e refle x agent .
functio n SiMPLE­REFLEX­AGENT(/?erc<?/?? ) return s action
static : rules,  a set of condition­actio n rules
state  <— lNTERpRET­lNPUT(/7e;re/« )
rtf/e <­ RULE­MATCH(.s?ate , rules)
action  <­ RULE­AcTiON[rw/e ]
retur n action
Figur e 2.8 A  simpl e refle x agent . It work s by findin g a rule whos e conditio n matche s the
curren t situatio n (as define d by the percept ) and then doin g the actio n associate d with that rule .
the curren t interna l state of the agent' s decisio n process , and oval s to represen t the backgroun d
informatio n used in the process . The agen t program , whic h is also very simple , is show n in
Figur e 2.8. Th e INTERPRET­INPU T functio n generate s an abstracte d descriptio n of the curren t
state from the percept , and the RULE­MATC H functio n return s the first rule in the set of rules that
matche s the give n state description . Althoug h such agent s can be implemente d very efficientl y
(see Chapte r 10), their rang e of applicabilit y is very narrow , as we shal l see.
Agent s that keep trac k of the worl d
The simpl e refle x agen t describe d befor e will wor k only if the correc t decisio n can be mad e
on the basi s of the curren t percept . If the car in fron t is a recen t model , and has the centrall y
mounte d brak e ligh t now require d in the Unite d States , then it will be possibl e to tell if it is
brakin g from a singl e image . Unfortunately , olde r model s have differen t configuration s of tail
42 Chapte r 2. Intelligen t Agent s
lights , brak e lights , and turn­signa l lights , and it is not alway s possibl e to tell if the car is braking .
Thus , even for the simpl e brakin g rule , our drive r will have to maintai n som e sort of interna l
INTERNA L STAT E stat e in orde r to choos e an action . Here , the interna l state is not too extensive—i t just need s the
previou s fram e from the camer a to detec t whe n two red light s at the edge of the vehicl e go on or
off simultaneously .
Conside r the followin g mor e obviou s case : fro m time to time , the drive r look s in the
rear­vie w mirro r to chec k on the location s of nearb y vehicles . Whe n the drive r is not lookin g in
the mirror , the vehicle s in the next lane are invisibl e (i.e., the state s in whic h they are presen t and
absen t are indistinguishable) ; but in orde r to decid e on a lane­chang e maneuver , the drive r need s
to know whethe r or not they are there .
The proble m illustrate d by this exampl e arise s becaus e the sensor s do not provid e acces s to
the complet e state of the world . In such cases , the agen t may need to maintai n some interna l state
informatio n in orde r to distinguis h betwee n worl d state s that generat e the sam e perceptua l inpu t
but nonetheles s are significantl y different . Here , "significantl y different " mean s that differen t
action s are appropriat e in the two states .
Updatin g this interna l state informatio n as time goes by require s two kind s of knowledg e to
be encode d in the agen t program . First , we need some informatio n abou t how the worl d evolve s
independentl y of the agent—fo r example , that an overtakin g car generall y will be close r behin d
than it was a momen t ago. Second , we need some informatio n about'ho w the agent' s own action s
affec t the world—fo r example , that whe n the agen t change s lane s to the right , ther e is a gap (at
least temporarily ) in the lane it was in before , or that after drivin g for five minute s northboun d ;
on the freewa y one is usuall y abou t five mile s nort h of wher e one was five minute s ago.
Figur e 2.9 give s the structur e of the refle x agent , showin g how the curren t percep t is j
combine d with the old interna l state to generat e the update d descriptio n of the curren t state . The I
agent progra m is show n in Figur e 2.10 . The interestin g part is the functio n UPDATE­STATE , whic h
is responsibl e for creatin g the new interna l state description . As well as interpretin g the new
percept  in the ligh t of existin g knowledg e abou t the state , it uses informatio n abou t how the worl d j
evolve s to keep track of the unsee n parts of the world , and also mus t know abou t what the agent' s j
action s do to the state of the world . Detaile d example s appea r in Chapter s 7 and 17.
GOAL
SEARC H
PLANNIN GGoal­base d agent s
Knowin g abou t the curren t state of the environmen t is not alway s enoug h to decid e wha t to do. j
For example , at a road junction , the taxi can turn left, right , or go straigh t on. The righ t decisio n j
depend s on wher e the taxi is tryin g to get to. In othe r words , as well as a curren t state description, !
the agen t need s som e sort of goal information , whic h describe s situation s that are desirable— j
for example , bein g at the passenger' s destination . The agen t progra m can combin e this with !
informatio n abou t the result s of possibl e action s (the sam e informatio n as was used to update ]
interna l state in the refle x agent ) in orde r to choos e action s that achiev e the goal . Sometime s j
this will be simple , whe n goal satisfactio n result s immediatel y from a singl e action ; sometimes , j
it will be more tricky , whe n the agen t has to conside r long sequence s of twist s and turn s to find
a way to achiev e the goal . Searc h (Chapter s 3 to 5) and plannin g (Chapter s 11 to 13) are the j
subfield s of AI devote d to findin g actio n sequence s that do achiev e the agent' s goals .
Sectio n 2.3. Structur e of Intelligen t Agent s 43
j;iaijj|jHo w the worl d evolve s JESiji g
Figur e 2.9 A  refle x agen t with  interna l state .
functio n REFLEX­AGENT­WiTH­STATE(percep O return s
static : state,  a descriptio n of the curren t worl d state
rules,  a set of condition­actio n rules
state  <— UPDATE­STATE(.stafe , percept)
rule — RULE­MATCHOtate , rules)
action  — RULE­ACTION[rwfe ]
state <­ UPDATE­STATEGstafe,  action)
retur n action
Figur e 2.10 A  refle x agen t with  interna l state . It work s by findin g a rule whos e conditio n
matche s the curren t situatio n (as define d by the percep t and the store d interna l state ) and then
doing the actio n associate d with that rule.
Notic e that decision­makin g of this kind is fundamentall y differen t from the condition ­
actio n rule s describe d earlier , in that it involve s consideratio n of the future—bot h "Wha t will
happe n if I do such­and­such? " and "Wil l that mak e me happy? " In the refle x agen t designs ,
this informatio n is not explicitl y used , becaus e the designe r has precompute d the correc t actio n
for variou s cases . The refle x agen t brake s whe n it sees brak e lights . A goal­base d agent , in
principle , coul d reaso n that if the car in fron t has its brak e light s on, it will slow down . Fro m
the way the worl d usuall y evolves , the only actio n that will achiev e the goal of not hittin g othe r
cars is to brake . Althoug h the goal­base d agen t appear s less efficient , it is far more flexible . If it
starts to rain, the agen t can updat e its knowledg e of how effectivel y its brake s will operate ; this
will automaticall y caus e all of the relevan t behavior s to be altere d to suit the new conditions . For
the refle x agent , on the othe r hand , we woul d have to rewrit e a large numbe r of condition­actio n
44 Chapte r 2. Intelligen t Agent s
rules . Of course , the goal­base d agen t is also mor e flexibl e with respec t to reachin g differen t
destinations . Simpl y by specifyin g a new destination , we can get the goal­base d agen t to com e
up with a new behavior . The refle x agent' s rules for whe n to turn and whe n to go straigh t will
only work for a singl e destination ; they mus t all be replace d to go somewher e new .
Figur e 2.11 show s the goal­base d agent' s structure . Chapte r 13 contain s detaile d agen t
program s for goal­base d agents .
How the worl d evolve s BIS
What my action s do JgSli i
Figur e 2.11 A n agen t with explici t goals .
UTILIT YUtility­base d agent s
Goals alon e are not reall y enoug h to generat e high­qualit y behavior . For example , there are man y
actio n sequence s that will get the taxi to its destination , thereb y achievin g the goal , but som e I
are quicker , safer , mor e reliable , or cheape r than others . Goal s just provid e a crud e distinctio n
betwee n "happy " and "unhappy " states , wherea s a mor e genera l performanc e measur e shoul d j
allow a compariso n of differen t worl d state s (or sequence s of states ) accordin g to exactl y how ]
happ y they woul d mak e the agen t if they coul d be achieved . Becaus e "happy " does not soun d I
very scientific , the customar y terminolog y is to say that if one worl d state is preferre d to another , ]
then it has highe r utilit y for the agent.8
Utilit y is therefor e a functio n that map s a state9 onto a real number , whic h describe s the j
associate d degre e of happiness . A complet e specificatio n of the utilit y functio n allow s rationa l j
decision s in two kind s of cases wher e goal s have trouble . First , whe n there are conflictin g goals , 1
only some of whic h can be achieve d (for example , spee d and safety) , the utilit y functio n specifie s
the appropriat e trade­off . Second , whe n ther e are severa l goal s that the agen t can aim for, non e
8 The word "utility " here refer s to "the qualit y of bein g useful, " not to the electri c compan y or wate r works .
9 Or sequenc e of states , if we are measurin g the utilit y of an agen t over the long run.
2.4. Environment s 45
of whic h can be achieve d with certainty , utilit y provide s a way in whic h the likelihoo d of succes s
can be weighe d up agains t the importanc e of the goals .
In Chapte r 16, we show that any rationa l agen t can be describe d as possessin g a utilit y
function . An agen t that possesse s an explicit  utilit y functio n therefor e can mak e rationa l decisions ,
but may have to compar e the utilitie s achieve d by differen t course s of actions . Goals , althoug h
cruder , enabl e the agen t to pick an actio n righ t awa y if it satisfie s the goal . In som e cases ,
moreover , a utilit y functio n can be translate d into a set of goals , such that the decision s mad e by
a goal­base d agen t usin g thos e goal s are identica l to thos e mad e by the utility­base d agent .
The overal l utility­base d agen t structur e appear s in Figur e 2.12 . Actua l utility­base d agen t
program s appea l in Chapte r 5, wher e we examin e game­playin g program s that mus t mak e fine
distinction s amoa g variou s boar d positions ; and in Chapte r 17, wher e we tackl e the genera l
proble m of designin g decision­makin g agents .
^ESiS
'\­ySt  How the world evolve s jSi*';:?,^^l 5
What the world
« is like now
...... ......... ......... _ ;;..;, ......
. ........ ... . .... . . .... . ... •'­•• • ­­•*
! jSfwha t my action s do^SSf t
­ '• '•' '•­"• • ''".^ii , . . .... . 1Mj /V.L.:.°:.:.:v.': . ­•..:What it will be like
if 1 do actio n All
Hi
1
> •W>S­Sf^fSS^'t?fff':SyKS
•'f^jKff  utilit y ^JfS^SXKHow happ y 1 will be I
in such a state
.''•^••'.­?S#'J$P"WS'Wr ;
What actio n 1
1 shoul d do now
Ss;s"j?y"™°:"^:Z^0W:'frs°fy'°":>5a::>:­"'= Ar
vf™%:; = =*^si£i^iS£ii5& ^ ­^ii
»?i^viv^vi^^y ^
Figure 2.12 A complet e utility­base d agent .
In this sectio n and in the exercise s at the end of the chapter , you will see how to coupl e an agen t
to an environment . Sectio n 2.3 introduce d severa l differen t kind s of agent s and environments .
In all cases , however , the natur e of the connectio n betwee n them is the same : action s are done
by the agen t on tie environment , whic h in turn provide s percept s to the agent . First , we will
describ e the differen t type s of environment s and how they affec t the desig n of agents . The n we
will describ e environment  program s that can be used as testbed s for agen t programs .
46 Chapte r 2. Intelligen t Agent s
Propertie s of environment s
Environment s com e in severa l flavors . The principa l distinction s to be mad e are as follows :
ACCESSIBL E 0  Accessibl e vs. inaccessible .
If an agent' s sensor y apparatu s give s it acces s to the complet e state of the environment ,
then we say that the environmen t is accessibl e to that agent . An environmen t is effectivel y
accessibl e if the sensor s detec t all aspect s that are relevan t to the choic e of action . An
accessibl e environmen t is convenien t becaus e the agen t need not maintai n any interna l state
to keep track of the world .
DETERMINISTI C 0  Deterministi c vs. nondeterministic .
If the next state of the environmen t is completel y determine d by the curren t state and the
action s selecte d by the agents , then we say the environmen t is deterministic . In principle ,
an agen t need not worr y abou t uncertaint y in an accessible , deterministi c environment . If
the environmen t is inaccessible , however , then it may appear  to be nondeterministic . This
is particularl y true if the environmen t is complex , makin g it hard to keep track of all the
inaccessibl e aspects . Thus , it is often bette r to thin k of an environmen t as deterministi c or
nondeterministic/ro m the point  of view  of the agent.
EPISODI C 0  Episodi c vs. nonepisodic .
In an episodi c environment , the agent' s experienc e is divide d into "episodes. " Each episod e
consist s of the agen t perceivin g and then acting . The qualit y of its actio n depend s just on
the episod e itself , becaus e subsequen t episode s do not depen d on wha t action s occu r in
previou s episodes . Episodi c environment s are muc h simple r becaus e the agen t does not
need to thin k ahead .
STATI C 0  Stati c vs. dynamic .
If the environmen t can chang e whil e an agen t is deliberating , then we say the environmen t
is dynami c for that agent ; otherwis e it is static . Stati c environment s are easy to deal with
becaus e the agen t need not keep lookin g at the worl d whil e it is decidin g on an action ,
nor need it worr y abou t the passag e of time . If the environmen t does not chang e with the
passag e of time but the agent' s performanc e scor e does , then we say the environmen t is
SEMIDYNAMI C semidynamic .
DISCRET E 0  Discret e vs. continuous .
If there are a limite d numbe r of distinct , clearl y define d percept s and action s we say that
the environmen t is discrete . Ches s is discrete—ther e are a fixed numbe r of possibl e move s
on each turn . Tax i drivin g is continuous—th e spee d and locatio n of the taxi and the othe r
vehicle s swee p throug h a rang e of continuou s values.10
We will see that differen t environmen t type s requir e somewha t differen t agen t program s to deal
with them effectively . It will turn out, as you migh t expect , that the hardes t case is inaccessible,
nonepisodic,  dynamic,  and continuous.  It also turn s out that mos t real situation s are so comple x
that whethe r they are really  deterministi c is a moo t point ; for practica l purposes , they mus t be
treate d as nondeterministic .
10 At a fine enoug h leve l of granularity , eve n the taxi drivin g environmen t is discrete , becaus e the camer a imag e is
digitize d to yiel d discret e pixe l values . But any sensibl e agen t progra m woul d have to abstrac t abov e this level , up to a
level of granularit y that is continuous .
Sectio n 2.4. Environment s 47
Figur e 2.13 lists the propertie s of a numbe r of familia r environments . Note that the answer s
can chang e dependin g on how you conceptualiz e the environment s and agents . For example ,
poke r is deterministi c if the agen t can keep track of the orde r of card s in the deck , but it is
nondeterministi c if it cannot . Also , man y environment s are episodi c at highe r level s than the
agent' s individua l actions . For example , a ches s tournamen t consist s of a sequenc e of games ;
each gam e is an episode , becaus e (by and large ) the contributio n of the move s in one gam e to the
agent' s overal l performanc e is not affecte d by the move s in its next game . On the othe r hand ,
move s withi n a singl e gam e certainl y interact , so the agen t need s to look ahea d severa l moves .
Environmen t
Ches s with a cloc k
Ches s withou t a clock
Poke r
Backgammo n
Taxi drivin g
Medica l diagnosi s syste m
Image­analysi s syste m
Part­pickin g robo t
Refiner y controlle r
Interactiv e Englis h tuto rAccessibl e
Yes
Yes
No
Yes
No
No
Yes
No
No
NoDeterministi c
Yes
Yes
No
No
No
No
Yes
No
No
NoEpisodi c
No
No
No
No
No
No
Yes
Yes
No
NoStatic
Semi
Yes
Yes
Yes
No
No
Semi
No
No
NoDiscret e
Yes
Yes
Yes
Yes
No
No
No
No
No
Yes
Figur e 2.13 Example s of environment s and their characteristics .
Environmen t program s
The generi c environmen t progra m in Figur e 2.14 illustrate s the basic relationshi p betwee n agent s
and environments . In this book , we will find it convenien t for many of the example s and exercise s
to use an environmen t simulato r that follow s this progra m structure . The simulato r takes one or
more agent s as inpu t and arrange s to repeatedl y give each agen t the right percept s and receiv e back
an action . The simulato r then update s the environmen t base d on the actions , and possibl y othe r
dynami c processe s in the environmen t that are not considere d to be agent s (rain , for example) .
The environmen t is therefor e define d by the initia l state and the updat e function . Of course,  an
agent that work s in a simulato r ough t also to work in a real environmen t that provide s the same
kinds of percept s and accept s the same kind s of actions .
The RUN­ENVIRONMEN T procedur e correctl y exercise s the agent s in an environment . For
some kind s of agents , such as thos e that engag e in natura l languag e dialogue , it may be sufficien t
simpl y to observ e their behavior . To get more detaile d informatio n abou t agen t performance , we
inser t som e performanc e measuremen t code . The functio n RUN­EVAL­ENVIRONMENT , show n in
Figur e 2.15 , does this; it applie s a performanc e measur e to each agen t and return s a list of the
resultin g scores . The scores  variabl e keep s track of each agent' s score .
In general , the performanc e measur e can depen d on the entir e sequenc e of environmen t
state s generate d durin g the operatio n of the program . Usually , however , the performanc e measur e
48 Chapte r 2. Intelligen t Agent s
procedur e RuN­ENViRONMENT(>tefc , UPDATE­FN , agents,  termination)
inputs : state,  the initia l state of the environmen t
UPDATE­FN , functio n to modif y the environmen t
agents,  a set of agent s
termination,  a predicat e to test whe n we are done
repea t
for each agent  in agents  do
PERCEPT[agent]  <— GEY­PERCEPT(agent,  state)
end
for each agent  in agents  do
ACTlON[agent]  — PROGRAM[agent](PERCEPT[agent})
end
state  <— UPDATE­FN(acft'o«.s , agents,  state)
until termination(state)
Figur e 2.14 Th e basic environmen t simulato r program . It give s each agen t its percept , gets an
actio n from each agent , and then update s the environment .
functio n RuN­EvAL­ENViRONMENT(itare , UPDATE­FN , agents,
termination,  PERFORMANCE­FN ) return s scores
local variables : scores,  a vecto r the same size as agents,  all 0
repea t
for each agent  in agents  do
PERCEPT[agent]  <­ GET­PERCEPT(age«f , state)
end
for each agent  in agents  do
ACTTONfagenr ] *­ PROGRAM[agent](PERCEPT[agent])
end
state ^­ UPDATE­FN(acrio/w , agents, state)
scores  ^­ PERFORMANCE­FN(>c0n?s , agents,  state)
until termination(state)
retur n scores  I  * change  * I
Figur e 2.15 A n environmen t simulato r progra m that keep s track of the performanc e measur e
for each agent .
work s by a simpl e accumulatio n usin g eithe r summation , averaging , or takin g a maximum . For i
example , if the performanc e measur e for a vacuum­cleanin g agen t is the tota l amoun t of dirt ]
cleane d in a shift , scores  will just keep track of how muc h dirt has been cleane d up so far.
RUN­EVAL­ENVIRONMEN T return s the performanc e measur e for a a singl e environment ,
define d by a singl e initia l state and a particula r updat e function . Usually , an agen t is designe d to
Sectio n Summar y 49
ENVIRONMEN T
CLAS Swork in an environmen t class , a whol e set of differen t environments . For example , we desig n
a ches s progra m to play agains t any of a wide collectio n of huma n and machin e opponents . If
we designe d it for a singl e opponent , we migh t be able to take advantag e of specifi c weaknesse s
in that opponent , but that woul d not give us a good progra m for genera l play. Strictl y speaking ,
in orde r to measur e the performanc e of an agent , we need to have an environmen t generato r
that select s particula r environment s (wit h certai n likelihoods ) in whic h to run the agent . We are
then intereste d in the agent' s averag e performanc e over the environmen t class . Thi s is fairl y
straightforwar d to implemen t for a simulate d environment , and Exercise s 2.5 to 2.11 take you
throug h the entir e developmen t of an environmen t and the associate d measuremen t process .
A possibl e confusio n arise s betwee n the state variabl e in the environmen t simulato r and
the state variabl e in the agen t itsel f (see REFLEX ­ AGENT­WITH­STATE) . As a programme r imple ­
mentin g both the environmen t simulato r and the agent , it is temptin g to allow the agen t to peek
at the environmen t simulator' s state variable . This temptatio n mus t be resiste d at all costs ! The
agent' s versio n of the state mus t be constructe d from its percept s alone , withou t acces s to the
complet e state information .
2.5 SUMMAR Y
This chapte r has been somethin g of a whirlwin d tour of AI, whic h we have conceive d of as the
scienc e of agen t design . The majo r point s to recal l are as follows :
• An agen t is somethin g that perceive s and acts in an environment . We split an agen t into
an architectur e and an agen t program .
• An idea l agen t is one that alway s take s the actio n that is expecte d to maximiz e its perfor ­
manc e measure , give n the percep t sequenc e it has seen so far.
• An agen t is autonomou s to the exten t that its actio n choice s depen d on its own experience ,
rathe r than on knowledg e of the environmen t that has been built­i n by the designer .
• An agen t progra m map s from a percep t to an action , whil e updatin g an interna l state .
• Ther e exist s a variet y of basic agen t progra m designs , dependin g on the kind of informatio n
made explici t and used in the decisio n process . The design s vary in efficiency , compactness ,
and flexibility . The appropriat e desig n of the agen t progra m depend s on the percepts ,
actions , goals , and environment .
• Refle x agent s respon d immediatel y to percepts , goal­base d agent s act so that they will
achiev e their goal(s) , and utility­base d agent s try to maximiz e their own "happiness. "
• The proces s of makin g decision s by reasonin g with knowledg e is centra l to AI and to
successfu l agen t design . This mean s that representin g knowledg e is important .
• Som e environment s are mor e demandin g than others . Environment s that are inaccessible ,
nondeterministic , nonepisodic , dynamic , and continuou s are the mos t challenging .
50 Chapte r 2. Intelligen t Agent s
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
The analysi s of rationa l agenc y as a mappin g from percep t sequence s to action s probabl y stem s
ultimatel y from the effor t to identif y rationa l behavio r in the realm of economic s and othe r form s
of reasonin g unde r uncertaint y (covere d in later chapters ) and from the effort s of psychologica l
behaviorist s such as Skinne r (1953 ) to reduc e the psycholog y of organism s strictl y to input/outpu t
or stimulus/respons e mappings . The advanc e from behavioris m to functionalis m in psychology ,
whic h was at least partl y drive n by the applicatio n of the compute r metapho r to agent s (Putnam ,
1960; Lewis , 1966) , introduce d the interna l state of the agen t into the picture . The philosophe r
Danie l Dennet t (1969 ; 1978b ) helpe d to synthesiz e these viewpoint s into a coheren t "intentiona l
stance " towar d agents . A high­level , abstrac t perspectiv e on agenc y is also taken withi n the worl d
of AI in (McCarth y and Hayes , 1969) . Jon Doyl e (1983 ) propose d that rationa l agen t desig n is
the core of AI, and woul d remai n as its missio n whil e othe r topic s in AI woul d spin off to form
new disciplines . Horvit z et al. (1988 ) specificall y sugges t the use of rationalit y conceive d as the
maximizatio n of expecte d utilit y as a basis for AI.
The AI researche r and Nobel­prize­winnin g economis t Herb Simo n drew a clear distinctio n
betwee n rationalit y unde r resourc e limitation s (procedura l rationality ) and rationalit y as makin g
the objectivel y rationa l choic e (substantiv e rationality ) (Simon , 1958) . Chernia k (1986 ) explore s
the minima l level of rationalit y neede d to qualif y an entit y as an agent . Russel l and Wefal d (1991 )
deal explicitl y with the possibilit y of usin g a variet y of agen t architectures . Dung  Beetle Ecol­
ogy (Hansk i and Cambefort , 1991 ) provide s a wealt h of interestin g informatio n on the behavio r
of dung beetles .
EXERCISE S
2.1 Wha t is the differenc e betwee n a performanc e measur e and a utilit y function ?
2.2 Fo r each of the environment s in Figur e 2.3, determin e wha t type of agen t architectur e is
most appropriat e (tabl e lookup , simpl e reflex , goal­base d or utility­based) .
2.3 Choos e a domai n that you are familia r with , and writ e a PAG E descriptio n of an agen t
for the environment . Characteriz e the environmen t as bein g accessible , deterministic , episodic ,
static , and continuou s or not. Wha t agen t architectur e is best for this domain ?
2.4 Whil e driving , whic h is the best policy ?
a. Alway s put your directiona l blinke r on befor e turning ,
b. Neve r use you r blinker ,
c. Loo k in you r mirror s and use your blinke r only if you observ e a car that can observ e you?
Wha t kind of reasonin g did you need to do to arriv e at this polic y (logical , goal­based , or utility ­
based) ? Wha t kind of agen t desig n is necessar y to carry out the polic y (reflex , goal­based , or
utility­based) ?
Sectio n 2.5. Summar y 51
The followin g exercise s all concer n the implementatio n of an environmen t and set of agent s in
the vacuum­cleane r world .
2.5 Implemen t a performance­measurin g environmen t simulato r for the vacuum­cleane r world .
This worl d can be describe d as follows :
<) Percepts : Eac h vacuum­cleane r agen t gets a three­elemen t percept  vecto r on each turn .
The first element , a touc h sensor , shoul d be a 1 if the machin e has bumpe d into somethin g
and a 0 otherwise . The secon d come s from a photosenso r unde r the machine , whic h emit s
a 1 if there is dirt there and a 0 otherwise . The third come s from an infrare d sensor , whic h
emits a 1 whe n the agen t is in its hom e location , and a 0 otherwise .
0 Actions : Ther e are five action s available : go forward , turn righ t by 90°, turn left by 90°,
suck up dirt, and turn off.
<) Goals : The goal for each agen t is to clean up and go home . To be precise , the performanc e
measur e will be 100 point s for each piec e of dirt vacuume d up, minu s 1 poin t for each
actio n taken , and minu s 1000 point s if it is not in the hom e locatio n whe n it turns itself off.
<) Environment : The environmen t consist s of a grid of squares . Som e square s contai n
obstacle s (wall s and furniture ) and other square s are open space . Som e of the open square s
contai n dirt. Each "go forward " actio n move s one squar e unles s there is an obstacl e in that
square , in whic h case the agen t stays wher e it is, but the touc h sensor  goes on. A "suc k up
dirt" actio n alway s clean s up the dirt. A "turn off" comman d ends the simulation .
We can vary the complexit y of the environmen t alon g three dimensions :
<y Roo m shape : In the simples t case , the room is an n x n square , for some fixe d n. We can
make it more difficul t by changin g to a rectangular , L­shaped , or irregularl y shape d room ,
or a serie s of room s connecte d by corridors .
0 Furniture : Placin g furnitur e in the room make s it more comple x than an empt y room . To
the vacuum­cleanin g agent , a piec e of furnitur e canno t be distinguishe d from a wall by
perception ; both appea r as a 1 on the touc h sensor .
0 Dir t placement : In the simples t case , dirt is distribute d uniforml y aroun d the room . But
it is more realisti c for the dirt to predominat e in certai n locations , such as alon g a heavil y
travelle d path to the next room , or in fron t of the couch .
2.6 Implemen t a table­looku p agen t for the specia l case of the vacuum­cleane r worl d consistin g
of a 2 x 2 grid of open squares , in whic h at mos t two square s will contai n dirt. The agen t start s
in the uppe r left corner , facin g to the right . Recal l that a table­looku p agen t consist s of a table of
action s indexe d by a percep t sequence . In this environment , the agen t can alway s complet e its
task in nine or fewe r action s (fou r moves , thre e turns , and two suck­ups) , so the table only need s
entrie s for percep t sequence s up to lengt h nine . At each turn , there are eigh t possibl e percep t
vectors , so the table will be of size 89 = 134,217,728 . Fortunately , we can cut this dow n by
realizin g that the touc h senso r and hom e sensor  input s are not needed ; we can arrang e so that
the agen t neve r bump s into a wall and know s whe n it has returne d home . The n ther e are only
two relevan t percep t vectors , ?0? and ?!?, and the size of the table is at mos t 29 = 512. Run the
environmen t simulato r on the table­looku p agen t in all possibl e world s (how man y are there?) .
Recor d its performanc e scor e for each worl d and its overal l averag e score .
52 Chapte r 2. Intelligen t Agent s
2.7 Implemen t an environmen t for anxm  rectangula r room , wher e each squar e has a 5% chanc e
of containin g dirt, and n and m are chose n at rando m from the rang e 8 to 15, inclusive .
2.8 Desig n and implemen t a pure reflex  agen t for the environmen t of Exercis e 2.7, ignorin g
the requiremen t of returnin g home , and measur e its performance . Explai n why it is impossibl e
to have a refle x agen t that return s homelan d shut s itsel f off. Speculat e on wha t the best possibl e
reflex agen t coul d do. Wha t prevent s a refle x agen t from doin g very well ?
2.9 Desig n and implemen t severa l agent s with interna l state . Measur e their performance . How
close do they com e to the ideal agen t for this environment ?
2.10 Calculat e the size of the table for a table­looku p agen t in the domai n of Exercis e 2.7.
Explai n your calculation . You need not fill in the entrie s for the table .
2.11 Experimen t with changin g the shap e and dirt placemen t of the room , and with addin g
furniture . Measur e you r agents  in thes e new environments . Discus s how their performanc e
migh t be improve d to handl e more comple x geographies .
Part II
PROBLEM­SOLVIN G
In this part we show how an agen t can act by establishin g goals  and considerin g
sequence s of action s that migh t achiev e thos e goals . A goal and a set of mean s
for achievin g the goal is calle d a problem,  and the proces s of explorin g wha t the
mean s can do is calle d search.  We show wha t searc h can do, how it mus t be
modifie d to accoun t for adversaries , and wha t its limitation s are.
I
3SOLVIN G PROBLEM S BY
SEARCHIN G
In which  we look  at how  an agent  can decide  what  to do by systematically considering
the outcomes  of various  sequences  of actions  that  it might  take.
PROBLEM­SOLVIN G
AGEN TIn Chapte r 2, we saw that simpl e refle x agent s are unabl e to plan ahead . They are limite d in wha t
they can do becaus e their action s are determine d only by the curren t percept . Furthermore , they
have no knowledg e of what their action s do nor of what they are tryin g to achieve .
In this chapter , we describ e one kind of goal­base d agen t calle d a problem­solvin g agent .
Problem­solvin g agent s decid e what  to do by findin g sequence s of action s that lead to desirabl e
states . We discus s informall y how the agen t can formulat e an appropriat e view of the proble m it
faces . The proble m type that result s from the formulatio n proces s will depen d on the knowledg e
availabl e to the agent : principally , whethe r it know s the curren t state and the outcome s of actions .
We then defin e mor e precisel y the element s that constitut e a "problem " and its "solution, " and
give severa l example s to illustrat e these definitions . Given  precis e definition s of problems , it
is relativel y straightforwar d to construc t a searc h proces s for findin g solutions . We cove r six
differen t searc h strategie s and show how they can be applie d to a variet y of problems . Chapte r 4
will then cove r searc h strategie s that mak e use of more informatio n abou t the proble m to improv e
the efficienc y of the searc h process .
This chapte r uses concept s from the analysi s of algorithms . Reader s unfamilia r with the
concept s of asymptoti c complexit y and NP­completenes s shoul d consul t Appendi x A.
•LL_PRQBLEM­SOLVIN G AGENT S
Intelligen t agent s are suppose d to act in such a way that the environmen t goes throug h a sequenc e
of state s that maximize s the performanc e measure . In its full generality , this specificatio n is
difficul t to translat e into a successfu l agen t design . As we mentione d in Chapte r 2, the task is
somewha t simplifie d if the agen t can adop t a goal and aim to satisf y it. Let us first look at how
and why an agen t migh t do this.
55
56 Chapte r 3. Solvin g Problem s by Searchin g
GOA L FORMULATIO N
PROBLE M
FORMULATIO NImagin e our agen t in the city of Arad , Romania , towar d the end of a tourin g holiday . The
agent has a ticke t to fly out of Buchares t the followin g day. The ticke t is nonrefundable , the
agent' s visa is abou t to expire , and after tomorrow , there are no seats availabl e for six weeks . Now
the agent' s performanc e measur e contain s man y othe r factor s beside s the cost of the ticke t and
the.undesirabilit y of bein g arreste d aqd deported . For example , it want s to improv e its suntan ,
improv e its Romanian , take in the sights , and so on. All these factor s migh t sugges t any of a vast
array of possibl e actions . Give n the seriousnes s of the situation , however , it shoul d adop t the
goal of drivin g to Bucharest . Action s that resul t in a failur e to reach Buchares t on time can be
rejecte d withou t furthe r consideration . Goal s such as this help organiz e behavio r by limitin g the
objective s that the agen t is tryin g to achieve . Goa l formulation , base d on the curren t situation ,
is the first step in proble m solving . As well as formulatin g a goal , the agen t may wish to decid e
on som e othe r factor s that affec t the desirabilit y of differen t way s of achievin g the goal .
For the purpose s of this chapter , we will conside r a goal to be a set of worl d states—jus t
those state s in whic h the goal is satisfied . Action s can be viewe d as causin g transition s betwee n
world states , so obviousl y the agen t has to find out whic h action s will get it to a goal state . Befor e
it can do this , it needs  to decid e wha t sorts of action s and state s to consider . If it were to try
to conside r action s at the leve l of "mov e the left foot forwar d 18 inches " or "turn the steerin g
whee l six degree s left," it woul d neve r find its way out of the parkin g lot, let alon e to Bucharest ,
becaus e constructin g a solutio n at that level of detai l woul d be an intractabl e problem . Proble m
formulatio n is the proces s of decidin g wha t action s and state s to consider , and follow s goal
formulation . We will discus s this proces s in more detail . For now , let us assum e that the agen t
will conside r action s at the leve l of drivin g from one majo r town to another . The state s it will
conside r therefor e correspon d to bein g in a particula r town. '
Our agen t has now adopte d the goal of drivin g to Bucharest , and is considerin g whic h town
to driv e to from Arad . Ther e are three road s out of Arad , one towar d Sibiu , one to Timisoara ,
and one to Zerind . Non e of these achieve s the goal , so unles s the agen t is very familia r with the
geograph y of Romania , it will not know whic h road to follow.2 In other words , the agen t will not ]
know whic h of its possibl e action s is best , becaus e it does not know enoug h abou t the state that j
result s from takin g each action . If the agen t has no additiona l knowledge , then it is stuck . The i
best it can do is choos e one of the action s at random .
But suppos e the agen t has a map of Romania , eithe r on pape r or in its memory . The poin t I
of a map is to provid e the agen t with informatio n abou t the state s it migh t get itsel f into, and j
the action s it can take . The agen t can use this informatio n to conside r subsequen t stage s of a j
hypothetica l journe y throug h each of the three towns , to try to find a journe y that eventuall y gets j
to Bucharest . Onc e it has foun d a path on the map from Arad to Bucharest , it can achiev e its goal ;
by carryin g out the drivin g action s that correspon d to the legs of the journey . In general , then , an j
agent with severa l immediat e option s of unknow n valu e can decid e wha t to do by first examinin g ;
differen t possibl e sequences  of action s that lead to state s of know n value , and then choosin g the j
best one. This proces s of lookin g for such a sequenc e is calle d search . A searc h algorith m take s
a proble m as inpu t and return s a solutio n in the form of an actio n sequence . Onc e a solutio n is
1 Notic e that thes e state s actuall y correspon d to larg e sets of worl d states , becaus e a worl d state specifie s ever y aspec t
of reality . It is importan t to keep in min d the distinctio n betwee n state s in proble m solvin g and worl d states .
2 We are assumin g that mos t reader s are in the sam e position , and can easil y imagin e themselve s as clueles s as our
agent . We apologiz e to Romania n reader s who are unabl e to take advantag e of this pedagogica l device .
Sectio n 3.2. Formulatin g Problem s 57
EXECUTIO N found , the action s it recommend s can be carrie d out. This is calle d the executio n phase . Thus ,
we have a simpl e "formulate , search , execute " desig n for the agent , as show n in Figur e 3.1. Afte r
formulatin g a goal and a proble m to solve , the agen t call s a searc h procedur e to solv e it. It then
uses the solutio n to guid e its actions , doin g whateve r the solutio n recommend s as the next thing
to do, and then removin g that step from the sequence . Onc e the solutio n has been executed , the
agent will find a new goal .
functio n SIMPLE­PROBLEM­SOLVING­AGENT(P ) return s an actio n
inputs : p, a percep t
static : s, an actio n sequence , initiall y empt y
state,  som e descriptio n of the curren t worl d state
g, a goal , initiall y null
problem,  a proble m formulatio n
state  <— UPDATE­STATE(sfa/e , p)
if s is empt y then
g — FORMULATE­GOAL(,s?afe )
problem  <— FORMULATE­PROBLEM(.stafe , g)
s r­ SE&RCH(  problem)
action  — RECOMMENDATION^ , state)
s <— REMAINDER^ , state)
retur n action
Figur e 3.1 A  simpl e problem­solvin g agent .
We will not discus s the UPDATE­STAT E and FORMULATE­GOA L function s furthe r in this
chapter . Th e next two section s describ e the proces s of proble m formulation , and then the
remainde r of the chapte r is devote d to variou s version s of the SEARC H function . The executio n
phase is usuall y straightforwar d for a simpl e problem­solvin g agent : RECOMMENDATIO N just
takes the first actio n in the sequence , and REMAINDE R return s the rest.
•L2__ FORMULATIN G PROBLEM S
In this section , we will conside r the proble m formulatio n proces s in more detail . First , we will
look at the differen t amount s of knowledg e that an agen t can have concernin g its action s and the
state that it is in. Thi s depend s on how the agen t is connecte d to its environmen t throug h its
percept s and actions . We find that there are four essentiall y differen t type s of problems—single ­
state problems , multiple­stat e problems , contingenc y problems , and exploratio n problems . We
will defin e these type s precisely , in preparatio n for later section s that addres s the solutio n process .
58 Chapte r 3. Solvin g Problem s by Searchin g
Knowledg e and proble m type s
Let us conside r an environmen t somewha t differen t from Romania : the vacuu m worl d from
Exercise s 2.5 to 2.11 in Chapte r 2. We will simplif y it even furthe r for the sake of exposition . Let
the worl d contai n just two locations . Eac h locatio n may or may not contai n dirt, and the agen t
may be in one locatio n or the other . Ther e are 8 possibl e worl d states , as show n in Figur e 3.2.
The agen t has thre e possibl e action s in this versio n of the vacuu m world : Left,  Right,  and Suck.
Assume , for the moment , that suckin g is 100% effective . The goal is to clean up all the dirt. Tha t
is, the goal is equivalen t to the state set {7,8} .
Figur e 3.2 Th e eigh t possibl e state s of the simplifie d vacuu m world .
SINGLE­STAT E
PROBLE M
MULTIPLE­STAT E
PROBLE MFirst, suppos e that the agent' s sensor s give it enoug h informatio n to tell exactl y whic h state
it is in (i.e., the worl d is accessible) ; and suppos e that it know s exactl y wha t each of its action s
does.  The n it can calculat e exactl y whic h state it will be in after any sequenc e of actions . For
example , if its initia l state is 5, then it can calculat e that the actio n sequenc e [Right,Suck]  will get
to a goal state . This is the simples t case , whic h we call a single­stat e problem .
Second , suppos e that the agen t know s all the effect s of its actions , but has limite d acces s
to the worl d state . For example , in the extrem e case , it may have no sensor s at all. In that case,
it know s only that its initia l state is one of the set {1,2,3,4,5,6,7,8} . One migh t suppos e that
the agent' s predicamen t is hopeless , but in fact it can do quit e well . Becaus e it know s wha t its
action s do, it can, for example , calculat e that the actio n Right  will caus e it to be in one of the
states {2,4,6,8} . In fact , the agen t can discove r that the actio n sequenc e [Right,Suck,Left,Suck]
is guarantee d to reac h a goal state no matte r wha t the start state . To summarize : whe n the worl d
is not full y accessible , the agen t mus t reaso n abou t sets of state s that it migh t get to, rathe r than
singl e states . We call this a multiple­stat e problem .
Sectio n 3.2. Formulatin g Problem s 59
CONTINGENC Y
PROBLE M
INTERLEAVIN G
EXPLORATIO N
PROBLE MAlthoug h it migh t seem different , the case of ignoranc e abou t the effect s of action s can be
treate d similarly . Suppose , for example , that the environmen t appear s to be nondeterministi c in
that it obey s Murphy' s Law : the so­calle d Suck  actio n sometimes  deposit s dirt on the carpe t but
only if there  is no dirt there  already?  For example , if the agen t know s it is in state 4, then it
know s that if it sucks , it will reach me of the state s {2,4} . For any known  initia l state , however ,
there is an actio n sequenc e that is guarantee d to reac h a goal state (see Exercis e 3.2).
Sometime s ignoranc e prevent s the agen t from findin g a guarantee d solutio n sequence .
Suppose , for example , that the agen t is in the Murphy' s Law world , and that it has a positio n
senso r and a local dirt sensor , but no senso r capabl e of detectin g dirt in othe r squares . Suppos e
furthe r that the sensor s tell it that it is in one of the state s {1,3} . The agen t migh t formulat e the
actio n sequenc e [Suck,Right,Suck].  Suckin g woul d chang e the state to one of {5,7} , and movin g
right woul d then chang e the state to one of {6,8} . If it is in fact state 6, then the actio n sequenc e
will succeed , but if it is state 8, the plan will fail. If the agen t had chose n the simple r actio n
sequenc e [Suck],  it woul d also succee d som e of the time , but not always . It turn s out there is no
fixed actio n sequenc e that guarantee s a solutio n to this problem .
Obviously , the agen t does  have a way to solve the proble m startin g from one of {1,3} : first
suck, then mov e right , then suck only if  there  is dirt  there.  Thus , solvin g this proble m require s
sensin g during the execution  phase.  Notic e that the agen t mus t now calculat e a whol e tree of
actions , rathe r than a singl e actio n sequence . In general , each branc h of the tree deal s with a
possibl e contingenc y that migh t arise . For this reason , we call this a contingenc y problem .
Many problem s in the real, physica l worl d are contingenc y problems , becaus e exac t predictio n is
impossible . For this reason , man y peopl e keep their eyes open whil e walkin g aroun d or driving .
Single­stat e and multiple­stat e problem s can be handle d by simila r searc h techniques ,
whic h are covere d in this chapte r and the next . Contingenc y problems , on the othe r hand ,
requir e more comple x algorithms , whic h we cove r in Chapte r 13. They also lend themselve s to a
somewha t differen t agen t design , in whic h the agen t can act before  it has foun d a guarantee d plan .
This is usefu l becaus e rathe r than considerin g in advanc e ever y possibl e contingenc y that migh t
arise durin g execution , it is ofte n bette r to actuall y start executin g and see whic h contingencie s
do arise . The agen t can then continu e to solv e the proble m give n the additiona l information . This
type of interleavin g of searc h and executio n is also covere d in Chapte r 13, and for the limite d
case of two­playe r games , in Chapte r 5. For the remainde r of this chapter , we will only conside r
cases wher e guarantee d solution s consis t of a singl e sequenc e of actions .
Finally , conside r the pligh t of an agen t that has no informatio n abou t the effect s of its
actions . This is somewha t equivalen t to bein g lost in a strang e countr y with no map at all, and is
the hardes t task faced by an intelligen t agent.4 The agen t mus t experiment,  graduall y discoverin g
what its action s do and wha t sorts of state s exist . Thi s is a kind of search , but a searc h in the
real worl d rathe r than in a mode l thereof . Takin g a step in the real world , rathe r than in a model ,
may involv e significan t dange r for an ignoran t agent . If it survives , the agen t learn s a "map " of
the environment , whic h it can then use to solv e subsequen t problems . We discus s this kind of
exploratio n proble m in Chapte r 20.
3 We assum e that mos t reader s face simila r problems , and can imagin e themselve s as frustrate d as our agent . We
apologiz e to owner s of modern , efficien t hom e appliance s who canno t take advantag e of this pedagogica l device .
4 It is also the task face d by newbor n babies .
60 Chapte r 3. Solvin g Problem s by Searchin g
PROBLE M
INITIA L STAT E
OPERATO R
SUCCESSO R
FUNCTIO N
STAT E SPAC E
PATH
GOAL TEST
PATH COS T
SOLUTIO NWell­define d problem s and solution s
A proble m is reall y a collectio n of informatio n that the agen t will use to decid e wha t to do. We
will begi n by specifyin g the informatio n neede d to defin e a single­stat e problem .
We have seen that the basic element s of a proble m definitio n are the state s and actions . To
captur e these formally , we need the following :
• ,The initia l state that the agen t know s itsel f to be in.
• The set of possibl e action s availabl e to the agent . The term operato r is used to denot e
the descriptio n of an actio n in term s of whic h state will be reache d by carryin g out the
actio n in a particula r state . (An alternat e formulatio n uses a successo r functio n S. Give n
a particula r state x, S(x)  return s the set of state s reachabl e from x by any singl e action. )
Together , thes e defin e the state spac e of the problem : the set of all state s reachabl e from the
initia l state by any sequenc e of actions . A path in the state spac e is simpl y any sequenc e of
action s leadin g from one state to another . The next elemen t of a proble m is the following :
• The goal test, whic h the agen t can appl y to a singl e state descriptio n to determin e if it is
a goal state . Sometime s there is an explici t set of possibl e goal states , and the test simpl y
check s to see if we have reache d one of them . Sometime s the goal is specifie d by an
abstrac t propert y rathe r than an explicitl y enumerate d set of states . For example , in chess ,
the goal is to reach a state calle d "checkmate, " wher e the opponent' s king can be capture d
on the next mov e no matte r wha t the opponen t does .
Finally , it may be the case that one solutio n is preferabl e to another , even thoug h they both reach
the goal . For example , we migh t prefe r path s with fewe r or less costl y actions .
• A path cost functio n is a functio n that assign s a cost to a path . In all cases we will consider ,
the cost of a path is the sum of the costs of the individua l action s alon g the path . The path
cost functio n is ofte n denote d by g.
Together , the initia l state , operato r set, goal test, and path cost functio n defin e a problem .
Naturally , we can then defin e a datatyp e with whic h to represen t problems :
datatyp e PROBLE M
components : INITIAL­STATE , OPERATORS , GOAL­TEST , PATH­COST­FUNCTIO N
STAT E SET SPAC EInstance s of this datatyp e will be the inpu t to our searc h algorithms . The outpu t of a searc h
algorith m is a solution , that is, a path from the initia l state to a state that satisfie s the goal test.
To deal with multiple­stat e problems , we need to mak e only mino r modifications : a proble m
consist s of an initia l state set; a set of operator s specifyin g for each actio n the set of state s reache d
from any give n state ; and a goal test and path cost functio n as before . An operato r is applie d to
a state set by unionin g the result s of applyin g the operato r to each state in the set. A path now
connect s sets of  states , and a solutio n is now a path that lead s to a set of state s all of which  are
goal states . The state spac e is replace d by the state set spac e (see Figur e 3.7 for an example) .
Problem s of both type s are illustrate d in Sectio n 3.3.
Sectio n 3.2. Formulatin g Problem s 61
Measurin g problem­solvin g performanc e
The effectivenes s of a searc h can be measure d in at least  three ways . First , does it rind a solutio n
SEARC H COST a t all? Second , is it a good solutio n (one with a low path cost) ? Third , wha t is the searc h cost
TOTAL COST associate d with the time and memor y require d to find a solution ? The tota l cost of the searc h is
the sum of the path cost and the searc h cost.5
For the proble m of findin g a rout e from Arad to Bucharest , the path cost migh t be pro­
portiona l to the total mileag e of the path , perhap s with somethin g throw n in for wear and tear
on differen t road surfaces . The searc h cost will depen d on the circumstances . In a stati c en­
vironment , it will be zero becaus e the performanc e measur e is independen t of time . If there is
some urgenc y to get to Bucharest , the environmen t is semidynami c becaus e deliberatin g longe r
will cost more . In this case , the searc h cost migh t vary approximatel y linearl y with computatio n
time (at least for smal l amount s of time) . Thus , to comput e the total cost , it woul d appea r that
we have to add mile s and milliseconds . Thi s is not alway s easy , becaus e ther e is no "officia l
exchang e rate" betwee n the two. The agen t mus t someho w decid e wha t resource s to devot e to
search and wha t resource s to devot e to execution . For problem s with very smal l state spaces , it
is easy to find the solutio n with the lowes t path cost. But for large , complicate d problems , there
is a trade­of f to be made—th e agen t can searc h for a very long time to get an optima l solution ,
or the agen t can searc h for a shorte r time and get a solutio n with a slightl y large r path cost . The
issue of allocatin g resource s will be take n up agai n in Chapte r 16; for now , we concentrat e on
the searc h itself .
Choosin g state s and action s
Now that we have the definition s out of the way , let us start our investigatio n of problem s with
an easy one: "Driv e from Arad to Buchares t usin g the road s in the map in Figur e 3.3." An
appropriat e state spac e has 20 states , wher e each state is define d solel y by location , specifie d as
a city. Thus , the initia l state is "in Arad " and the goal test is "is this Bucharest? " The operator s
correspon d to drivin g alon g the road s betwee n cities .
One solutio n is the path Arad to Sibiu to Rimnic u Vilce a to Pitest i to Bucharest . Ther e are
lots of othe r path s that are also solutions , for example , via Lugo j and Craiova . To decid e whic h
of these solution s is better , we need to know wha t the path cost functio n is measuring : it coul d
be the total mileage , or the expecte d trave l time . Becaus e our curren t map does not specif y eithe r
of these , we will use the numbe r of steps as the cost function . Tha t mean s that the path throug h
Sibiu and Fagaras , with a path cost of 3, is the best possibl e solution .
The real art of proble m solvin g is in decidin g wha t goes into the descriptio n of the state s
and operator s and wha t is left out. Compar e the simpl e state descriptio n we have chosen , "in
Arad, " to an actua l cross­countr y trip, wher e the state of the worl d include s so man y things : the
travellin g companions , wha t is on the radio , wha t there is to look at out of the window , the vehicl e
being used for the trip, how fast it is going , whethe r there are any law enforcemen t officer s nearby ,
what time it is, whethe r the drive r is hungr y or tired or runnin g out of gas, how far it is to the next
5 In theoretica l compute r scienc e and in robotics , the searc h cost (the part you do befor e interactin g with the environment )
is calle d the offlin e cost and the path cost is calle d the onlin e cost .
62 Chapte r 3. Solvin g Problem s by Searchin g
Orade a
Neam t
lasi
Arad
Vaslu i
Dobret a
Giurgi uHirsov a
Efori e
Figur e 3.3 A  simplifie d road map of Romania .
rest stop , the conditio n of the road , the weather , and so on. All thes e consideration s are left out
of state description s becaus e they are irrelevan t to the proble m of findin g a rout e to Bucharest .
ABSTRACTIO N Th e proces s of removin g detai l from a representatio n is calle d abstraction .
As well as abstractin g the state description , we mus t abstrac t the action s themselves . An
action—le t us say a car trip from Ara d to Zerind—ha s man y effects . Beside s changin g the
locatio n of the vehicl e and its occupants , it take s up time , consume s fuel, generate s pollution , and
change s the agen t (as they say, trave l is broadening) . In our formulation , we take into accoun t
only the chang e in location . Also , ther e are man y action s that we will omit altogether : turnin g j
on the radio , lookin g out of the window , slowin g dow n for law enforcemen t officers , and so on.
Can we be mor e precis e abou t definin g the appropriat e leve l of abstraction ? Thin k of the
states and action s we have chose n as correspondin g to sets of detaile d worl d state s and sets of j
detaile d actio n sequences . No w conside r a solutio n to the abstrac t problem : for example , the j
path Arad to Sibiu to Rimnic u Vilce a to Pitest i to Bucharest . This solutio n correspond s to a large |
numbe r of mor e detaile d paths . For example , we coul d driv e with the radio on betwee n Sibi u :
and Rimnic u Vilcea , and then switc h it off for the rest of the trip. Eac h of these mor e detaile d ;
paths is still a solution  to the goal , so the abstractio n is valid . The abstractio n is also useful ,
becaus e carryin g out each of the action s in the solution , such as drivin g from Pitest i to Bucharest ,
is somewha t easie r than the origina l problem . The choic e of a good abstractio n thus involve s
removin g as muc h detai l as possibl e whil e retainin g validit y and ensurin g that the abstrac t action s
are easy to carry out. Wer e it not for the abilit y to construc t usefu l abstractions , intelligen t agent s
woul d be completel y swampe d by the real world .
Sectio n 3.3. Exampl e Problem s 63
V3__EXAMPL E PROBLEM S
TOY PROBLEM S
REAL­WORL D
PROBLEM SThe rang e of task environment s that can be characterize d by well­define d problem s is vast . We
can distinguis h betwee n so­called , toy problems , whic h are intende d to illustrat e or exercis e
variou s problem­solvin g methods , and so­calle d real­worl d problems , whic h tend to be mor e
difficul t and whos e solution s peopl e actuall y care about . In this section , we will give example s of
both. By nature , toy problem s can be give n a concise , exac t description . This mean s that they can
be easil y used by differen t researcher s to compar e the performanc e of algorithms . Real­worl d
problems , on the othe r hand , tend not to have a singl e agreed­upo n description , but we will
attemp t to give the genera l flavo r of their formulations .
8­PUZZL E
SLID,NG sBLOC KToy problem s
The 8­puzzI e
The 8­puzzle , an instanc e of whic h is show n in Figur e 3.4, consist s of a 3x3 boar d with eigh t
numbere d tiles and a blan k space . A tile adjacen t to the blan k spac e can slide into the space .
The objec t is to reach  the configuratio n show n on the righ t of the figure . One importan t trick is
to notic e that rathe r than use operator s such as "mov e the 3 tile into the blan k space, " it is mor e
sensibl e to have operator s such as "the blan k spac e change s place s with the tile to its left." This is
becaus e ther e are fewe r of the latte r kind of operator . This lead s us to the followin g formulation :
<) States : a state descriptio n specifie s the locatio n of each of the eigh t tiles in one of the nine
squares . For efficiency , it is usefu l to includ e the locatio n of the blank .
<> Operators : blan k move s left, right , up, or down .
0 Goa l test: state matche s the goal configuratio n show n in Figur e 3.4.
0 Path cost: each step costs 1, so the path cost is just the lengt h of the path .
The 8­puzzl e belong s to the famil y of sliding­bloc k puzzles . This genera l class is know n
to be NP­complete , so one does not expec t to find method s significantl y bette r than the searc h
5
6
7 I1 4 ;
5 1 I
S 3 !8
2i 1 I
8
7I 2 \
63
4
5
Start Stat e Goa l Stat e
Figur e 3.4 A  typica l instanc e of the 8­puzzle .
64 Chapte r 3. Solvin g Problem s by Searchin g
algorithm s describe d in this chapte r and the next . The 8­puzzl e and its large r cousin , the 15­
puzzle , are the standar d test problem s for new searc h algorithm s in Al.
The 8­queen s proble m
The goal of the 8­queen s proble m is to plac e eigh t queen s on a chessboar d such that no queen
attack s any other . (A quee n attack s any piec e in the sam e row, colum n or diagonal. ) Figur e 3.5
show s an attempte d solutio n that fails : the quee n in the rightmos t colum n is attacke d by the quee n
at top left.
Figur e 3.5 Almos t a solutio n to the 8­queen s problem . (Solutio n is left as an exercise. )
Althoug h efficien t special­purpos e algorithm s exis t for this proble m and the whol e n­
queen s family , it remain s an interestin g test proble m for searc h algorithms . Ther e are two main
kind s of formulation . The incremental  formulatio n involve s placin g queen s one by one, wherea s
the complete­state  formulatio n start s with all 8 queen s on the boar d and move s them around . In .
eithe r case , the path cost is of no interes t becaus e only the fina l state counts ; algorithm s are thus
compare d only on searc h cost . Thus , we have the followin g goal test and path cost:
0 Goa l test: 8 queen s on board , none attacked .
0 Pat h cost: zero .
There are also differen t possibl e state s and operators . Conside r the followin g simple­minde d
formulation :
0 States : any arrangemen t of 0 to 8 queen s on board .
<C> Operators : add a quee n to any square .
In this formulation , we have 648 possibl e sequence s to investigate . A more sensibl e choic e woul d
use the fact that placin g a quee n wher e it is alread y attacke d canno t work , becaus e subsequen t
placing s of othe r queen s will not und o the attack . So we migh t try the following :
Sectio n 3.3. Exampl e Problem s 65
0 States : arrangement s of 0 to 8 queen s with none attacked .
0 Operators : plac e a quee n in the left­mos t empt y colum n such that it is not attacke d by any
other queen .
It is easy to see that the action s give n can generat e only state s with no attacks ; but sometime s
no action s will be possible . For example , afte r makin g the first seve n choice s (left­to­right ) in
Figur e 3.5, there is no actio n availabl e in this formulation . The searc h proces s mus t try anothe r
choice . A quic k calculatio n show s that there are only 2057 possibl e sequence s to investigate . The
right  formulation  makes  a big  difference  to the size of the search  space.  Simila r consideration s
apply for a complete­stat e formulation . For example , we coul d set the proble m up as follows :
0 States : arrangement s of 8 queens , one in each column .
<C> Operators : mov e any attacke d quee n to anothe r squar e in the same column .
This formulatio n woul d allow the algorith m to find a solutio n eventually , but it woul d be bette r
to mov e to an unattacke d squar e if possible .
Cryptarithmeti c
In cryptarithmeti c problems , letter s stan d for digit s and the aim is to find a substitutio n of digit s
for letter s such that the resultin g sum is arithmeticall y correct . Usually , each lette r mus t stan d
for a differen t digit . The followin g is a well­know n example :
FORTY
+ TEN
+ TENSolution :
SIXTY29786
850
850
31486F=2, 0=9, R=7, etc.
The followin g formulatio n is probabl y the simplest :
<C> States : a cryptarithmeti c puzzl e with som e letter s replace d by digits .
0 Operators : replac e all occurrence s of a lette r with a digi t not alread y appearin g in the
puzzle .
<> Goa l test: puzzl e contain s only digits , and represent s a correc t sum .
<C> Path cost: zero . All solution s equall y valid .
A moment' s though t show s that replacin g E by 6 then F by 7 is the same thin g as replacin g F by
7 then E by 6—orde r does not matte r to correctness , so we wan t to avoi d tryin g permutation s of
the same substitutions . One way to do this is to adop t a fixed order , e.g., alphabetica l order . A
bette r choic e is to do whicheve r is the mos t constrained  substitution , that is, the lette r that has
the fewes t lega l possibilitie s give n the constraint s of the puzzle .
The vacuu m worl d
Here we will defin e the simplifie d vacuu m worl d from Figur e 3.2, rathe r than the full versio n
from Chapte r 2. The latte r is deal t with in Exercis e 3.17 .
66 Chapte r 3. Solvin g Problem s by Searchin g
First, let us revie w the single­stat e case with complet e information . We assum e that the
agent know s its locatio n and the location s of all the piece s of dirt, and that the suctio n is still in
good workin g order .
<} States : one of the eigh t state s show n in Figur e 3.2 (or Figur e 3.6).
<) Operators : mov e left, move'right , suck .
<) Goa l test: no dirt left in any square .
<> Path cost: each actio n costs 1.
Figur e 3.6 Diagra m of the simplifie d vacuu m state space . Arcs denot e actions . L = mov e left,
R = mov e riaht , S = suck .
Figur e 3.6 show s the complet e state spac e showin g all the possibl e paths . Solvin g the ;
proble m from any startin g state is simpl y a matte r of followin g arrow s to a goal state . This is the j
case for all problems , of course , but in most , the state spac e is vastl y large r and more tangled .
Now let us conside r the case wher e the agen t has no sensors , but still has to clean up all i
the dirt. Becaus e this is a multiple­stat e problem , we will have the following :
0 Stat e sets: subset s of state s 1­8 show n in Figur e 3.2 (or Figur e 3.6).
<) Operators : mov e left, mov e right , suck .
0 Goa l test: all state s in state set have no dirt.
<) Path cost: each actio n cost s 1.
The start state set is the set of all states , becaus e the agen t has no sensors . A solutio n is any
sequenc e leadin g from the start state set to a set of state s with no dirt (see Figur e 3.7). Simila r
state set space s can be constructe d for the case of uncertaint y abou t action s and uncertaint y abou t
both state s and actions .
ISectio n 3.3. Exampl e Problem s 67
Figur e 3.7 Stat e set spac e for the simplifie d vacuu m worl d with no sensors . Each dashed­lin e
box enclose s a set of states . At any given point , the agen t is withi n a state set but does not know
whic h state of that set it is in. The initia l state set (complet e ignorance ) is the top cente r box.
Action s are represente d by labelle d arcs. Self­loop s are omitte d for clarity .
Missionarie s and cannibal s
The missionarie s and cannibal s proble m is usuall y state d as follows . Thre e missionarie s and
three cannibal s are on one side of a river , alon g with a boat that can hold one or two people . Find
a way to get everyon e to the othe r side, withou t ever leavin g a grou p of missionarie s in one place
outnumbere d by the cannibal s in that place .
This proble m is famou s in AI becaus e it was the subjec t of the first pape r that approache d
proble m formulatio n from an analytica l viewpoin t (Amarel , 1968) . As with travellin g in Romania ,
the real­lif e proble m mus t be greatl y abstracte d befor e we can appl y a problem­solvin g strategy .
68 Chapte r 3. Solvin g Problem s by Searchin g
Imagin e the scen e in real life: thre e member s of the Arawaska n tribe , Alice , Bob , and Charles ,
stand at the edge of the crocodile­infeste d Amazo n river with their new­foun d friends , Xavier ,
Yolanda , and Zelda . All aroun d them birds cry, a rain storm beats down , Tarza n yodels , and so
on. The missionarie s Xavier , Yolanda , and Zeld a are a little worrie d abou t wha t migh t happe n if
one of them were caugh t alon e with two or three of the others , and Alice , Bob , and Charle s are
concerne d that they migh t be in for a long sermo n that they migh t find equall y unpleasant . Both
partie s are not quite sure if the smal l boat they find tied up by the side of the river is up to makin g
the crossin g with two aboard .
To formaliz e the problem , the first step is to forge t abou t the rain, the crocodiles , and all the
other detail s that have no bearin g in the solution . The next step is to decid e wha t the right operato r
set is. We know that the operator s will involv e takin g one or two peopl e acros s the river in the
boat, but we have to decid e if we need a state to represen t the time whe n they are in the boat , or
just when they get to the other side. Becaus e the boat hold s only two people , no "outnumbering "
can occu r in it; hence , only the endpoint s of the crossin g are important . Next , we need to abstrac t
over the individuals . Surely , each of the six is a uniqu e huma n being , but for the purpose s of the
solution , whe n it come s time for a canniba l to get into the boat , it does not matte r if it is Alice ,
Bob, or Charles . Any permutatio n of the three missionarie s or the three cannibal s lead s to the
same outcome . Thes e consideration s lead to the followin g forma l definitio n of the problem :
0 States : a state consist s of an ordere d sequenc e of three number s representin g the numbe r
of missionaries , cannibals , and boat s on the bank of the rive r from whic h they started .
Thus , the start state is (3,3,1) .
0 Operators : from each state the possibl e operator s are to take eithe r one missionary , one
cannibal , two missionaries , two cannibals , or one of each acros s in the boat . Thus , there !
are at mos t five operators , althoug h most state s have fewe r becaus e it is necessar y to avoi d j
illega l states . Not e that if we had chose n to distinguis h betwee n individua l peopl e then j
there woul d be 27 operator s instea d of just 5.
0 Goa l test: reache d state (0,0,0) .
<C> Pat h cost: numbe r of crossings .
This state spac e is smal l enoug h to mak e it a trivia l proble m for a compute r to solve . Peopl e j
have a hard time , however , becaus e some of the necessar y move s appea r retrograde . Presumably , J
human s use som e notio n of "progress " to guid e their search . We will see how such notion s arej
used in the next chapter .
Real­worl d problem s
Rout e findin g
We have alread y seen how rout e findin g is define d in term s of specifie d location s and transitions !
along link s betwee n them . Route­findin g algorithm s are used in a variet y of applications , such !
as routin g in compute r networks , automate d trave l advisor y systems , and airlin e trave l planning !
systems . The last applicatio n is somewha t more complicated , becaus e airlin e trave l has a very 1
comple x path cost, in term s of money , seat quality , time of day, type of airplane , frequent­flye r
Sectio n 3.3. Exampl e Problem s 69
mileag e awards , and so on. Furthermore , the action s in the proble m do not have completel y
know n outcomes : flight s can be late or overbooked , connection s can be missed , and fog or
emergenc y maintenanc e can caus e delays .
TRAVELLIN G
SALESPERSO N
PROBLE MTourin g and travellin g salesperso n problem s
Conside r the problem,  "Visi t ever y city in Figur e 3.3 at leas t once , startin g and endin g in
Bucharest. " Thi s seem s very simila r to rout e finding , becaus e the operator s still correspon d to
trips betwee n adjacen t cities . But for this problem , the state spac e mus t recor d more information .
In additio n to the agent' s location , each state mus t keep track of the set of citie s the agen t has
visited . So the initia l state woul d be "In Bucharest ; visite d {Bucharest}, " a typica l intermediat e
state woul d be "In Vaslui ; visite d {Bucharest,Urziceni,Vaslui}, " and the goal test woul d check if
the agen t is in Buchares t and that all 20 citie s have been visited .
The travellin g salesperso n proble m (TSP ) is a famou s tourin g proble m in whic h each city
must be visite d exactl y once . The aim is to find the shortest  tour.6 The proble m is NP­har d (Karp ,
1972) , but an enormou s amoun t of effor t has been expende d to improv e the capabilitie s of TSP
algorithms . In additio n to plannin g trips for travellin g salespersons , thes e algorithm s have been
used for tasks such as plannin g movement s of automati c circui t boar d drills .
VLS I layou t
The desig n of silico n chip s is one of the mos t comple x engineerin g desig n task s currentl y
undertaken , and we can give only a brief sketc h here . A typica l VLS I chip can have as man y as
a millio n gates , and the positionin g and connection s of ever y gate are crucia l to the successfu l
operatio n of the chip . Computer­aide d desig n tool s are use d in ever y phas e of the process .
Two of the mos t difficul t task s are cell layou t and channe l routing . Thes e com e afte r the
component s and connection s of the circui t have been fixed ; the purpos e is to lay out the circui t
on the chip so as to minimiz e area and connectio n lengths , thereb y maximizin g speed . In cell
layout , the primitiv e component s of the circui t are groupe d into cells , each of whic h perform s
some recognize d function . Each cell has a fixed footprin t (size and shape ) and require s a certai n
numbe r of connection s to each of the othe r cells . The aim is to plac e the cells on the chip so
that they do not overla p and so that there is room for the connectin g wire s to be place d betwee n
the cells . Channe l routin g find s a specifi c rout e for each wire usin g the gaps betwee n the cells .
These searc h problem s are extremel y complex , but definitel y wort h solving . In Chapte r 4, we
will see som e algorithm s capabl e of solvin g them .
Robo t navigatio n
Robo t navigatio n is a generalizatio n of the route­findin g proble m describe d earlier . Rathe r than
a discret e set of routes , a robo t can mov e in a continuou s spac e with (in principle ) an infinit e set
of possibl e action s and states . For a simple , circula r robo t movin g on a flat surface , the spac e
6 Strictl y speaking , this is the travellin g salesperso n optimizatio n problem ; the TSP itsel f asks if a tour exist s with cost
less than som e constant .
70 Chapte r 3. Solvin g Problem s by Searchin g
is essentiall y two­dimensional . Whe n the robo t has arms and legs that mus t also be controlled ,
the searc h spac e become s many­dimensional . Advance d technique s are require d just to mak e
the searc h spac e finite . We examin e som e of thes e method s in Chapte r 25. In additio n to the
complexit y of the problem , real robot s mus t also deal with error s in thei r senso r reading s and
moto r controls .
Assembl y sequencin g
Automati c assembl y of comple x object s by a robo t was firs t demonstrate d by FREDD Y the
robot (Michie , 1972) . Progres s sinc e then has been slow but sure , to the poin t wher e assembl y of
object s such as electri c motor s is economicall y feasible . In assembl y problems , the proble m is to
find an orde r in whic h to assembl e the part s of som e object . If the wron g orde r is chosen , ther e
will be no way to add som e part later in the sequenc e withou t undoin g som e of the work alread y
done . Checkin g a step in the sequenc e for feasibilit y is a comple x geometrica l searc h proble m
closel y relate d to robo t navigation . Thus , the generatio n of lega l successor s is the expensiv e part
of assembl y sequencing , and the use of informe d algorithm s to reduc e searc h is essential .
3.4 SEARCHIN G FOR SOLUTION S
We have seen how to defin e a problem , and how to recogniz e a solution . The remainin g part—
findin g a solution—i s done by a searc h throug h the state space . The idea is to maintai n and exten d
a set of partia l solutio n sequences . In this section , we show how to generat e thes e sequence s and
how to keep track of them usin g suitabl e data structures .
Generatin g actio n sequence s
To solv e the route­findin g proble m from Arad to Bucharest , for example , we start off with just the \
initia l state , Arad . The first step is to test if this is a goal state . Clearl y it is not, but it is importan t |
to chec k so that we can solv e trick problem s like "startin g in Arad , get to Arad. " Becaus e this is ;
not a goal state , we need to conside r som e othe r states . Thi s is done by applyin g the operator s ;
GENERATIN G t o the curren t state , thereb y generatin g a new set of states . The proces s is calle d expandin g the ]
EXPANDIN G state . In this case , we get thre e new states , "in Sibiu, " "in Timisoara, " and "in Zerind, " becaus e j
there is a direc t one­ste p rout e from Arad to thes e thre e cities . If there were only one possibility,;
we woul d just take it and continue . But wheneve r ther e are multipl e possibilities , we mus t mak e j
a choic e abou t whic h one to conside r further .
This is the essenc e of search—choosin g one optio n and puttin g the other s aside for later , in '
case the first choic e does not lead to a solution . Suppos e we choos e Zerind . We chec k to see if it
is a goal state (it is not) , and then expan d it to get "in Arad " and "in Oradea. " We can then choos e
any of these two, or go back and choos e Sibi u or Timisoara . We continu e choosing , testing , and
expandin g unti l a solutio n is found , or unti l ther e are no mor e state s to be expanded . The choic e
SEARC H STRATEG Y o f whic h state to expan d first is determine d by the searc h strategy .
cection 3.4. Searchin g for Solution s 71
SEARC H TREE I t is helpfu l to think of the searc h proces s as buildin g up a searc h tree that is superimpose d
SEARC H NOD E ove r the state space . The root of the searc h tree is a searc h nod e correspondin g to the initia l
state. The leaf node s of the tree correspon d to state s that do not have successor s in the tree,
eithe r becaus e they have not been expande d yet, or becaus e they were expanded , but generate d
the empt y set. At each step , the searc h algorith m choose s one leaf node to expand . Figur e 3.8
show s som e of the expansion s in the searc h tree for rout e findin g from Arad to Bucharest . The
genera l searc h algorith m is describe d informall y in Figur e 3.9.
It is importan t to distinguis h betwee n the state spac e and the searc h tree. For the route ­
findin g problem , there are only 20 state s in the state space , one for each city. But there are an
(a) The initia l state
(b) Afte r expandin g Ara dArad
Arad
(c) Afte r expandin g Sibi uSibiu Timisoar a Zerin d
Arad
Zerin d
Arad Fagara s Orade a Rimnic u Vilce a
Figur e 3.8 Partia l searc h tree for route findin g from Arad to Bucharest .
functio n GENERAL­SEARCH(/?roWem , strategy)  return s a solution , or failur e
initializ e the searc h tree usin g the initia l state of problem
loop do
if there are no candidate s for expansio n then retur n failur e
choos e a leaf node for expansio n accordin g to strategy
if the node contain s a goal state then retur n the correspondin g solutio n
else expan d the node and add the resultin g node s to the searc h tree
end
Figur e 3.9 A n informa l descriptio n of the genera l searc h algorithm .
72 Chapte r 3. Solvin g Problem s by Searchin g
infinit e numbe r of path s in this state space , so the searc h tree has an infinit e numbe r of nodes .
For example , in Figur e 3.8, the branc h Arad­Sibiu­Ara d continue s Arad­Sibiu­Arad­Sibiu ­
Arad , and so on, indefinitely . Obviously , a good searc h algorith m avoid s followin g such paths .
Technique s for doin g this are discusse d in Sectio n 3.6.
PAREN T NOD E
DEPT H
FRING E
FRONTIE R
QUEU EData structure s for searc h trees
There are man y way s to represen t nodes , but in this chapter , we will assum e a node is a data
structur e with five components :
• the state in the state spac e to whic h the node corresponds ;
• the node in the searc h tree that generate d this node (this is calle d the paren t node) ;
• the operato r that was applie d to generat e the node ;
• the numbe r of node s on the path from the root to this node (the dept h of the node) ;
• the path cost of the path from the initia l state to the node .
The node data type is thus :
datatyp e node
components : STATE , PARENT­NODE , OPERATOR , DEPTH , PATH­COS T
It is importan t to remembe r the distinctio n betwee n node s and states . A node is a bookkeepin g
data structur e used to represen t the searc h tree for a particula r proble m instanc e as generate d by
a particula r algorithm . A state represent s a configuratio n (or set of configurations ) of the world .
Thus , node s have depth s and parents , wherea s state s do not. (Furthermore , it is quite possibl e for
two differen t node s to contai n the same state , if that state is generate d via two differen t sequence s
of actions. ) The EXPAN D functio n is responsibl e for calculatin g each of the component s of the
node s it generates .
We also need to represen t the collectio n of node s that are waitin g to be expanded—thi s
collectio n is calle d the fring e or frontier . The simples t representatio n woul d be a set of nodes .
The searc h strateg y then woul d be a functio n that select s the next node to be expande d from
this set. Althoug h this is conceptuall y straightforward , it coul d be computationall y expensive ,
becaus e the strateg y functio n migh t have to look at ever y elemen t of the set to choos e the best
one. Therefore , we will assum e that the collectio n of node s is a implemente d as a queue . The
operation s on a queu e are as follows :
• MAKE­Q\JEUE(Elements)  create s a queu e with the give n elements .
• EMPTY? '(Queue ) return s true only if there are no more element s in the queue .
• REMOVE­FRONT(gweMe ) remove s the elemen t at the fron t of the queu e and return s it.
• QUEUlNG­FN(Elements,Queue)  insert s a set of element s into the queue . Differen t varietie s
of the queuin g functio n produc e differen t varietie s of the searc h algorithm .
With these definitions , we can writ e a more forma l versio n of the genera l searc h algorithm . This
is show n in Figur e 3.10 .
Sectio n 3.5. Searc h Strategie s 73
functio n GENERAL­SEARCH(/?roWem , QUEUING­FN ) return s a solution , or failur e
nodes  *— MAKE­QUEUE(MAKE­NODE(lNiTlAL­STATE[prob;em]) )
loop do
if nodes  is empt y then retur n failur e
node <­ REMOVE­FRONT(rcode.s )
if GOAL­TEST[proWem ] applie d to STATE(woflfe ) succeed s then retur n node
nodes  <­ QuEUlNG­FN(n0<fe.y , E\PAND(node,  OPERATORS\pmblem]))
end
Figur e 3.10 Th e genera l searc h algorithm . (Not e that QuEUiNG­F N is a variabl e whos e valu e
will be a function. )
3.5 SEARC H STRATEGIE S
COMPLETENES S
TIME COMPLEXIT Y
SPAC E COMPLEXIT Y
OPTIMALIT Y
UNARME D
BLIND SEARC H
INFORME D SEARC H
HEURISTI C SEARC HThe majorit y of work in the area of searc h has gone into findin g the righ t searc h strateg y for a
problem . In our stud y of the field we will evaluat e strategie s in term s of four criteria :
{> Completeness : is the strateg y guarantee d to find a solutio n whe n there is one?
<) Tim e complexity : how long does it take to find a solution ?
<) Spac e complexity : how muc h memor y does it need to perfor m the search ?
<} Optimality : doe s the strateg y find the highest­qualit y solutio n whe n ther e are severa l
differen t solutions?7
This sectio n cover s six searc h strategie s that com e unde r the headin g of uninforme d
search . The term mean s that they have no informatio n abou t the numbe r of steps or the path cost
from the curren t state to the goal—al l they can do is distinguis h a goal state from a nongoa l state .
Uninforme d searc h is also sometime s calle d blin d search .
Conside r agai n the route­findin g problem . From the initia l state in Arad , there are three
action s leadin g to three new states : Sibiu , Timisoara , and Zerind . An uninforme d searc h has
no preferenc e amon g these , but a mor e cleve r agen t migh t notic e that the goal , Bucharest , is
southeas t of Arad , and that only Sibi u is in that direction , so it is likel y to be the best choice .
Strategie s that use such consideration s are calle d informe d searc h strategie s or heuristi c searc h
strategies , and they will be covere d in Chapte r 4. No t surprisingly , uninforme d searc h is less
effectiv e than informe d search . Uninforme d searc h is still important , however , becaus e there are
many problem s for whic h there is no additiona l informatio n to consider .
The six uninforme d searc h strategie s are distinguishe d by the order  in whic h node s are
expanded . It turn s out that this differenc e can matte r a grea t deal , as we shall shortl y see.
L7 Thi s is the way "optimality " is used in the theoretica l compute r scienc e literature . Som e AI author s use "optimality "
to refe r to time of executio n and "admissibility " to refe r to solutio n optimality .
74 Chapte r 3. Solvin g Problem s by Searchin g
BREADTH­FIRS T
SEARC H
BRANCHIN G FACTO RBreadth­firs t searc h
One simpl e searc h strateg y is a breadth­firs t search . In this strategy , the root node is expande d
first, then all the node s generate d by the root node are expande d next , and then their  successors ,
and so on. In general , all the node s at dept h d in the searc h tree are expande d befor e the node s at
depth d + 1. Breadth­firs t searc h can be implemente d by callin g the GENERAL­SEARC H algorith m
with a queuin g functio n that puts the newl y generate d state s at the end of the queue , after all the
previousl y generate d states :
functio n BREADTH­FlRST­SEARCH(/wWem ) return s a solutio n or failur e
retur n GENERAL­SEARCH ( problem,ENQUEUE­  AT­END )
Breadth­firs t searc h is a very systemati c strateg y becaus e it consider s all the path s of lengt h 1
first, then all thos e of lengt h 2, and so on. Figur e 3.11 show s the progres s of the searc h on a
simpl e binar y tree. If there is a solution , breadth­firs t searc h is guarantee d to find it, and if there
are severa l solutions , breadth­firs t searc h will alway s find the shallowes t goal state first . In term s
of the four criteria , breadth­firs t searc h is complete , and it is optima l provided  the path  cost is a
nondecreasing  function  of the depth  of the node.  (Thi s conditio n is usuall y satisfie d only whe n
all operator s have the same cost. For the genera l case , see the next section. )
So far, the new s abou t breadth­firs t searc h has been good . To see why it is not alway s the
strateg y of choice , we have to conside r the amoun t of time and memor y it take s to complet e a
search . To do this, we conside r a hypothetica l state spac e wher e ever y state can be expande d to
yield b new states . We say that the branchin g facto r of these state s (and of the searc h tree) is b.
The root of the searc h tree generate s b node s at the first level , each of whic h generate s b mor e
nodes , for a total of b2 at the secon d level . Eac h of these  generate s b mor e nodes , yieldin g b3
node s at the third level , and so on. Now suppos e that the solutio n for this proble m has a path
lengt h of d. Then the maximu m numbe r of node s expande d befor e findin g a solutio n is
1 + b + b2 + b3 + • • • + bd
This is the maximu m number , but the solutio n coul d be foun d at any poin t on the Jth level . In
the best case, therefore , the numbe r woul d be smaller .
Thos e who do complexit y analysi s get nervou s (or excited , if they are the sort of peopl e
who like a challenge ) wheneve r they see an exponentia l complexit y boun d like O(bd). Figur e 3.12
show s why . It show s the time and memor y require d for a breadth­firs t searc h with branchin g
facto r b = 10 and for variou s value s of the solutio n dept h d. The spac e complexit y is the same
as the time complexity , becaus e all the leaf node s of the tree mus t be maintaine d in memor y
Figur e 3.11 Breadth­firs t searc h trees after 0, 1, 2, and 3 node expansions .
Sectio n Searc h Strategie s 75
at the sam e time . Figur e 3.12 assume s that 1000 node s can be goal­checke d and expande d per
second , and that a node require s 100 byte s of storage . Man y puzzle­lik e problem s fit roughl y
withi n these assumption s (giv e or take a facto r of 100) whe n run on a moder n persona l compute r
or workstation .
Dept h
0
2
4
6
8
10
12
14
Figur e 3.12Node s
1
111
11,11 1
106
108
10'°
10' 2
1014
Time and
assum e branchin g facto r bTime
1 millisecon d
. 1 second s
1 1 second s
18 minute s
3 1 hour s
128 day s
35 year s
3500 year s 11 ,
memor y requirement s for breadth­firs t searc h
= 10; 1000 nodes/second ; 1 00 bytes/node .Memor y
100 byte s
1 1 kilobyte s
1 megabyt e
1 1 1 megabyte s
1 1 gigabyte s
1 terabyt e
1 1 1 terabyte s
1 1 1 terabyte s
. Th e figure s show n
UNIFOR M COS T
SEARC HThere are two lesson s to be learne d from Figur e 3.12 . First , the memory requirements  are
a bigger  problem for  breadth­first search  than  the execution  time.  Mos t peopl e have the patienc e
to wait 18 minute s for a dept h 6 searc h to complete , assumin g they care abou t the answer , but not
so man y have the 111 megabyte s of memor y that are required . And althoug h 31 hour s woul d not
be too long to wait for the solutio n to an importan t proble m of dept h 8, very few peopl e indee d
have acces s to the 11 gigabyte s of memor y it woul d take . Fortunately , ther e are othe r searc h
strategie s that requir e less memory .
The secon d lesso n is that the time requirement s are still a majo r factor . If your proble m
has a solutio n at dept h 12, then (give n our assumptions ) it will take 35 year s for an uninforme d
searc h to find it. Of course , if trend s continu e then in 10 years , you will be able to buy a compute r
that is 100 time s faste r for the same price as your curren t one. Eve n with that computer , however ,
it will still take 128 days to find a solutio n at dept h 12—an d 35 year s for a solutio n at dept h
14. Moreover , ther e are no othe r uninforme d searc h strategie s that fare any better . In general,
exponential  complexity search  problems cannot  be solved  for any but the smallest instances.
Unifor m cost searc h
Breadth­firs t searc h find s the shallowest  goal state , but this may not alway s be the least­cos t
solutio n for a genera l path cost function . Unifor m cost searc h modifie s the breadth­firs t strateg y
by alway s expandin g the lowest­cos t node on the fring e (as measure d by the path cost g(n)) ,
rathe r than the lowest­dept h node . It is easy to see that breadth­firs t searc h is just unifor m cost
searc h with g(n) = DEPTH(«) .
When certai n condition s are met , the first solutio n that is foun d is guarantee d to be the
cheapes t solution , becaus e if there were a cheape r path that was a solution , it woul d have been
expande d earlier , and thus woul d have been foun d first . A look at the strateg y in actio n will help
explain . Conside r the route­findin g proble m in Figur e 3.13 . The proble m is to get from S to G,
76 Chapte r 3. Solvin g Problem s by Searchin g
and the cost of each operato r is marked . The strateg y first expand s the initia l state , yieldin g path s
to A, B, and C. Becaus e the path to A is cheapest , it is expande d next , generatin g the path SAG,
whic h is in fact a solution , thoug h not the optima l one. However , the algorith m does not yet
recogniz e this as a solution , becaus e it has cost 11, and thus is burie d in the queu e belo w the path
SB, whic h has cost 5. It seem s a sham e to generat e a solutio n just to bury it deep in the queue ,
but it is necessar y if we wan t to find the optima l solutio n rathe r than just any solution . The next
step is to expan d SB, generatin g SBG,  whic h is now the cheapes t path remainin g in the queue , so
it is goal­checke d and returne d as the solution .
Unifor m cost searc h find s the cheapes t solutio n provide d a simpl e requiremen t is met: the
cost of a path mus t neve r decreas e as we go alon g the path . In othe r words , we insis t that
g(SlJCCESSOR(n) ) > g(n)
for every node n.
The restrictio n to nondecreasin g path cost make s sens e if the path cost of a node is take n to
be the sum of the costs of the operator s that make up the path. If every operato r has a nonnegativ e
cost, then the cost of a path can neve r decreas e as we go alon g the path , and uniform­cos t searc h
can find the cheapes t path withou t explorin g the whol e searc h tree. But if some operato r had a
negativ e cost, then nothin g but an exhaustiv e searc h of all node s woul d find the optima l solution ,
becaus e we woul d neve r know whe n a path , no matte r how long and expensive , is abou t to run
into a step with high negativ e cost and thus becom e the best path overall . (See Exercis e 3.5.)
15
(a)
Figur e 3.13 A  route­findin g problem , (a) The state space , showin g the cost for each operator ,
(b) Progressio n of the search . Eac h node is labelle d with g(n).  At the next step , the goal node
with g = 10 will be selected.
Sectio n 3.5. Searc h Strategie s 77
nEpTH­FIRS T
SEARC HDepth­firs t searc h
Depth­firs t searc h alway s expand s one of the node s at the deepes t leve l of the tree. Onl y whe n
the searc h hits a dead end (a nongoa l nod e with no expansion ) does the searc h go back and
expan d node s at shallowe r levels . This strateg y can be implemente d by GENERAL­SEARC H with
a queuin g functio n that alway s puts the newl y generate d state s at the fron t of the queue . Becaus e
the expande d node was the deepest , its successor s will be even deepe r and are therefor e now the
deepest . The progres s of the searc h is illustrate d in Figur e 3.14 .
Depth­firs t searc h has very modes t memory  requirements . As the figur e shows , it need s
to store only a singl e path from the root to a leaf node , alon g with the remainin g unexpande d
siblin g node s for each node on the path . For a state spac e with branchin g facto r b and maximu m
depth m, depth­firs t searc h require s storag e of only bm nodes , in contras t to the bd that woul d be
require d by breadth­firs t searc h in the case wher e the shallowes t goal is at dept h d. Usin g the
same assumption s as Figur e 3.12 , depth­firs t searc h woul d requir e 12 kilobyte s instea d of 111
terabyte s at dept h d=12,a  facto r of 10 billio n time s less space .
The time complexit y for depth­firs t searc h is O(bm). For problem s that have very man y
solutions , depth­firs t may actuall y be faste r than breadth­first , becaus e it has a good chanc e of
A
Figur e 3.14 Depth­firs t searc h trees for a binar y searc h tree. Node s at dept h 3 are assume d to
have no successors .
78 Chapte r 3. Solvin g Problem s by Searchin g
findin g a solutio n after explorin g only a smal l portio n of the whol e space . Breadth­firs t searc h
woul d still hav e to look at all the path s of lengt h d ­ 1 befor e considerin g any of lengt h d.
Depth­firs t searc h is still O(bm) in the wors t case .
The drawbac k of depth­firs t searc h is that it can get stuc k goin g dow n the wron g path .
Many problem s have very deep or even infinit e searc h trees , so depth­firs t searc h will neve r be
able to recove r from an unluck y choic e at one of the node s near the top of the tree. The searc h
will alway s continu e downwar d withou t backin g up, even whe n a shallo w solutio n exists . Thus ,
on thes e problem s depth­firs t searc h will eithe r get stuc k in an infinit e loop and neve r retur n a
solution , or it may eventuall y find a solutio n path that is longe r than the optima l solution . Tha t
mean s depth­firs t searc h is neithe r complet e nor optimal . Becaus e of this, depth­first  search
should  be avoided  for search  trees  with large  or infinite maximum  depths.
It is trivia l to implemen t depth­firs t searc h with GENERAL­SEARCH :
functio n DEPTH­FlRST­SEARCH(proWem ) return s a solution , or failur e
GENERAL­SEARCH(/7roWem,ENQUEUE­AT­FRONT )
It is also commo n to implemen t depth­firs t searc h with a recursiv e functio n that calls itsel f on
each of its childre n in turn . In this case , the queu e is store d implicitl y in the loca l state of each
invocatio n on the callin g stack .
DEPTH­LIMITE D
SEARC HDepth­limite d searc h
Depth­limite d searc h avoid s the pitfall s of depth­firs t searc h by imposin g a cutof f on the max ­
imum dept h of a path . Thi s cutof f can be implemente d with a specia l depth­limite d searc h
algorithm , or by usin g the genera l searc h algorith m with operator s that keep track of the depth .
For example , on the map of Romania , ther e are 20 cities , so we know that if there is a solution ,
then it mus t be of lengt h 19 at the longest . We can implemen t the dept h cutof f usin g operator s
of the form "If you are in city A and have travelle d a path of less than 19 steps , then generat e
a new state in city B with a path lengt h that is one greater. " Wit h this new operato r set, we are
guarantee d to find the solutio n if it exists , but we are still not guarantee d to find the shortes t
solutio n first : depth­limite d searc h is complet e but not optimal . If we choos e a dept h limi t that
is too small , then depth­limite d searc h is not even complete . The time and spac e complexit y of
depth­limite d searc h is simila r to depth­firs t search . It take s O(b')  time and O(bl)  space , wher e /
is the dept h limit .
Iterativ e deepenin g searc h
The hard part abou t depth­limite d searc h is pickin g a good limit . We picke d 19 as an "obvious "
depth limi t for the Romani a problem , but in fact if we studie d the map carefully , we woul d
discove r that any city can be reache d from any othe r city in at mos t 9 steps . This number , know n
as the diamete r of the state space , give s us a bette r dept h limit , whic h lead s to a mor e efficien t
depth­limite d search . However , for mos t problems , we will not know a good dept h limi t unti l we
have solve d the problem .
Secti'"13.5. Searc h Strategie s 79
£|ERPAENING SEARC HIterativ e deepenin g searc h is a strateg y that sidestep s the issue of choosin g the best dept h
limit by tryin g all possibl e dept h limits : firs t dept h 0, then dept h 1, then dept h 2, and so on.
The algorith m is show n in Figur e 3.15 . In effect , iterativ e deepenin g combine s the benefit s of
depth­firs t and breadth­firs t search . It is optima l and complete , like breadth­firs t search , but has
only the modes t memor y requirement s of depth­firs t search . The orde r of expansio n of state s is
simila r to breadth­first , excep t that som e state s are expande d multipl e times . Figur e 3.16 show s
the first four iteration s of ITERATIVE­DEEPENING­SEARC H on a binar y searc h tree.
Iterativ e deepenin g searc h may seem wasteful , becaus e so man y state s are expande d
multipl e times . For mos t problems , however , the overhea d of this multipl e expansio n is actuall y
functio n iTERATiVE­DEEPENiNG­SEARCH(praWem ) return s a solutio n sequenc e
inputs : problem,  a proble m
for depth  <— 0 to oo do
if DEPTH­LlMITED­SEARCH(proWem , depth)  succeed s then retur n its resul t
end
retur n failur e
Figur e 3.15 Th e iterativ e deepenin g searc h algorithm .
Limi t = 0
Limi t = 1 ®
Limi t = 2
Limi t = 3
Figur e 3.16 Fou r iteration s of iterativ e deepenin g searc h on a binar y tree.
80 Chapte r 3. Solvin g Problem s by Searchin g
rathe r small . Intuitively , the reaso n is that in an exponentia l searc h tree, almos t all of the node s
are in the botto m level , so it does not matte r muc h that the uppe r level s are expande d multipl e
times . Recal l that the numbe r of expansion s in a depth­limite d searc h to dept h d with branchin g
facto r b is
•­To mak e this concrete , for b = 10 and d = 5. the numbe r is
1 + 10+100+1,000+10,000 + 100,000 = 111,11 1
In an iterativ e deepenin g search , the node s on the botto m level are expande d once , thos e on the
next to botto m leve l are expande d twice , and so on, up to the root of the searc h tree, whic h is
expande d d + 1 times . So the total numbe r of expansion s in an iterativ e deepenin g searc h is
(d + 1)1 + (d)b + (d­ \)b2 + ••• + 3bd~2 + 2bd~l + \bd
Again , for b = 10 and d = 5 the numbe r is
6 + 50 + 400 + 3,00 0 + 20,000+100,000 = 123,45 6
All together , an iterativ e deepenin g searc h from dept h 1 all the way dow n to dept h d expand s
only abou t 11 % more node s than a singl e breadth­firs t or depth­limite d searc h to dept h d, whe n
b = 10. The highe r the branchin g factor , the lowe r the overhea d of repeatedl y expande d states ,
but even whe n the branchin g facto r is 2, iterativ e deepenin g searc h only take s abou t twic e as long
as a complet e breadth­firs t search . This mean s that the time complexit y of iterativ e deepenin g is
still O(bd), and the spac e complexit y is O(bd).  In general,  iterative  deepening  is the preferred
search  method  when  there is  a large  search  space  and the depth  of the solution  is not known.
PREDECESSOR SBidirectiona l searc h
The idea behin d bidirectiona l searc h is to simultaneousl y searc h both forwar d from the initia l state j
and backwar d from the goal , and stop whe n the two searche s mee t in the middl e (Figur e 3.17) . j
For problem s wher e the branchin g facto r is b in both directions , bidirectiona l searc h can mak e a j
big difference . If we assum e as usua l that there is a solutio n of dept h d, then the solutio n will j
be foun d in O(2bd/2) = O(bd/2) steps , becaus e the forwar d and backwar d searche s each have to I
go only half way . To mak e this concrete : for b = 10 and d = 6, breadth­firs t searc h generates !
1,111,11 1 nodes , wherea s bidirectiona l searc h succeed s whe n each directio n is at dept h 3, at!
whic h poin t 2,22 2 node s have been generated . Thi s sound s grea t in theory . Severa l issue s need j
to be addresse d befor e the algorith m can be implemented .
• The main questio n is, wha t does it mea n to searc h backward s from the goal ? We defin e j
the predecessor s of a node n to be all thos e node s that have n as a successor . Searchin g j
backward s mean s generatin g predecessor s successivel y startin g from the goal node .
• Whe n all operator s are reversible , the predecesso r and successo r sets are identical ; for j
some problems , however , calculatin g predecessor s can be very difficult .
• Wha t can be done if there are man y possibl e goal states ? If there is an explicit  list of goal
states , such as the two goal state s in Figur e 3.2, then we can appl y a predecesso r functio n
to the state set just as we appl y the successo r functio n in multiple­stat e search . If we only
Sectio n 3.5. Searc h Strategie s 81
Figur e 3.17 A  schemati c view of a bidirectiona l breadth­firs t searc h that is abou t to succeed ,
when a branc h from the start node meet s a branc h from the goal node .
have a description  of the set, it may be possibl e to figur e out the possibl e description s of
"sets of state s that woul d generat e the goal set," but this is a very trick y thin g to do. For
example , wha t are the state s that are the predecessor s of the checkmat e goal in chess ?
• Ther e mus t be an efficien t way to chec k each new node to see if it alread y appear s in the
searc h tree of the othe r half of the search .
• We need to decid e what  kind of searc h is goin g to take plac e in each half . For example ,
Figur e 3.17 show s two breadth­firs t searches . Is this the best choice ?
The O(bdl2) complexit y figur e assume s that the proces s of testin g for intersectio n of the two
frontier s can be done in constan t time (that is, is independen t of the numbe r of states) . This often
can be achieve d with a hash table . In orde r for the two searche s to mee t at all, the node s of at
least one of them mus t all be retaine d in memor y (as with breadth­firs t search) . This mean s that
the spac e complexit y of uninforme d bidirectiona l searc h is O(bdl2).
Comparin g searc h strategie s
Figur e 3.18 compare s the six searc h strategie s in term s of the four evaluatio n criteri a set forth in
Sectio n 3.5.
Criterio n
Time
Spac e
Optimal ?
Complete ?Breadth ­ Uniform ­
First Cos t
bd b"
b" b"
Yes Ye s
Yes Ye sDepth ­ Depth ­
First Limite d
bm b1
bm bl
No N o
No Yes , ifl>dIterativ e
Deepenin g
b"
bd
Yes
YesBidirectiona l
(if applicable )
bdn
Yes
Yes
Figur e 3.18 Evaluatio n of searc h strategies , b is the branchin g factor ; d is the dept h of solution ;
m is the maximu m dept h of the searc h tree; / is the dept h limit .
L
82 Chapte r 3. Solvin g Problem s by Searchin g
3.6 AVOIDIN G REPEATE D STATE S
Up to this point , we have all but ignore d one of the mos t importan t complication s to the searc h
process : the possibilit y of wastin g time by expandin g state s that have alread y been encountere d
and expande d befor e on som e othe r path . For som e problems , this possibilit y neve r come s up;
each state can only be reache d one way . The efficien t formulatio n of the 8­queen s proble m is
efficien t in large part becaus e of this—eac h state can only be derive d throug h one path .
For man y problems , repeate d state s are unavoidable . Thi s include s all problem s wher e
the operator s are reversible , such as route­findin g problem s and the missionarie s and cannibal s
problem . The searc h trees for thes e problem s are infinite , but if we prun e som e of the repeate d
states , we can cut the searc h tree dow n to finit e size, generatin g only the portio n of the tree that
spans the state spac e graph . Eve n whe n the tree is finite , avoidin g repeate d state s can yield an
exponentia l reductio n in searc h cost . The classi c exampl e is show n in Figur e 3.19 . The spac e
contain s only m + 1 states , wher e in is the maximu m depth . Becaus e the tree include s each
possibl e path throug h the space , it has 2m branches .
There are three way s to deal with repeate d states , in increasin g orde r of effectivenes s and
computationa l overhead :
• Do not retur n to the state you just cam e from . Hav e the expan d functio n (or the operato r
set) refus e to generat e any successo r that is the same state as the node' s parent .
• Do not creat e path s with cycle s in them . Hav e the expan d functio n (or the operato r set)
refus e to generat e any successo r of a node that is the same as any of the node' s ancestors .
• Do not generat e any state that was ever generate d before . This require s ever y state that is
generate d to be kept in memory , resultin g in a spac e complexit y of O(bd), potentially . It is
bette r to thin k of this as O(s),  wher e s is the numbe r of state s in the entir e state space .
To implemen t this last option , searc h algorithm s ofte n mak e use of a hash table that store s all
the node s that are generated . This make s checkin g for repeate d state s reasonabl y efficient . The
trade­of f betwee n the cost of storin g and checkin g and the cost of extra searc h depend s on the
problem : the "loopier " the state space , the more likel y it is that checkin g will pay off.
Figur e 3.19 A  state spac e that generate s an exponentiall y large r searc h tree. The left­han d
side show s the state space , in whic h there are two possibl e action s leadin g from A to B, two from
B to C, and so on. The right­han d side show s the correspondin g searc h tree.
cection 3.7. Constrain t Satisfactio n Searc h 83
3 7 CONSTRAIN T SATISFACTIO N SEARC H
DOMAI NA constrain t satisfactio n proble m (or CSP ) is a specia l kind of proble m that satisfie s som e
additiona l structura l propertie s beyon d the basi c requirement s for problem s in general . In a CSP ,
the state s are define d by the value s of a set of variable s and the goal test specifie s a set of
constraint s that the value s mus t obey . For example , the 8­queen s proble m can be viewe d as a
CSP in whic h the variable s are the location s of each of the eigh t queens ; the possibl e value s are
square s on the board ; and the constraint s state that no two queen s can be in the same row, colum n
or diagonal . A solutio n to a CSP specifie s value s for all the variable s such that the constraint s
are satisfied . Cryptarithmeti c and VLS I layou t can also be describe d as CSP s (Exercis e 3.20) .
Man y kind s of desig n and schedulin g problem s can be expresse d as CSPs , so they form a very
importan t subclass . CSP s can be solve d by general­purpos e searc h algorithms , but becaus e of
their specia l structure , algorithm s designe d specificall y for CSP s generall y perfor m muc h better .
Constraint s com e in severa l varieties . Unar y constraint s concer n the valu e of a singl e vari­
able. For example , the variable s correspondin g to the leftmos t digit on any row of acryptarithmeti c
puzzl e are constraine d not to have the valu e 0. Binar y constraint s relat e pair s of variables . The
constraint s in the 8­queen s proble m are all binar y constraints . Higher­orde r constraint s involv e
three or mor e variables—fo r example , the column s in the cryptarithmeti c proble m mus t obey
an additio n constrain t and can involv e severa l variables . Finally , constraint s can be absolute
constraints , violatio n of whic h rule s out a potentia l solution , or preference  constraint s that say
whic h solution s are preferred .
Each variabl e V/ in a CSP has a domai n £>,, whic h is the set of possibl e value s that the
variabl e can take on. The domai n can be discrete  or continuous.  In designin g a car, for instance ,
the variable s migh t includ e componen t weight s (continuous ) and componen t manufacturer s (dis­
crete) . A unar y constrain t specifie s the allowabl e subse t of the domain , and a binar y constrain t
betwee n two variable s specifie s the allowabl e subse t of the cross­produc t of the two domains . In
discret e CSP s wher e the domain s are finite , constraint s can be represente d simpl y by enumeratin g
the allowabl e combination s of values . For example , in the 8­queen s problem , let V\ be the row
that the first quee n occupie s in the first column , and let V2 be the row occupie d by the secon d
quee n in the secon d column . The domain s of V\ and ¥2 are {1,2,3,4,5,6,7,8} . The no­attac k
constrain t linkin g V\ and lA can be represente d by a set of pair s of allowabl e value s for V\ and
V2: {{1,3} , (1,4),{1,5},.. . ,(2,4) , (2,5),... } and so on. Altogether , the no­attac k constrain t
betwee n V\ and Vi rule s out 22 of the 64 possibl e combinations . Usin g this idea of enumeration ,
any discret e CSP can be reduce d to a binar y CSP .
Constraint s involvin g continuou s variable s canno t be enumerate d in this way , and solvin g
continuou s CSP s involve s sophisticate d algebra . In this chapter , we will handl e only discrete ,
absolute , binar y (or unary ) constraints . Suc h constraint s are still sufficientl y expressiv e to handl e
a wid e variet y of problem s and to introduc e mos t of the interestin g solutio n methods .
Eet us first conside r how we migh t appl y a general­purpos e searc h algorith m to a CSP . The
initia l state will be the state in whic h all the variable s are unassigned . Operator s will assig n a
value to a variabl e from the set of possibl e values . The goal test will chec k if all variable s are
assigne d and all constraint s satisfied . Notic e that the maximu m dept h of the searc h tree is fixe d
84 Chapte r 3. Solvin g Problem s by Searchin g
BACKTRACKIN G
SEARC H
FORWAR D
CHECKIN G
ARC CONSISTENC Y
CONSTRAIN T
PROPAGATIO Nat n, the numbe r of variables , and that all solution s are at dept h n. We are therefor e safe in usin g
depth­firs t search , as ther e is no dange r of goin g too deep and no arbitrar y dept h limi t is needed .
In the most naiv e implementation , any unassigne d variabl e in a give n state can be assigne d
a valu e by an operator , in whic h case the branchin g facto r woul d be as high as ^ |£),|, or
64 in the 8­queen s problem . A bette r approac h is to take advantag e of the fact that the orde r
of variabl e assignment s make s no differenc e to the fina l solution . Almos t all CSP algorithm s
therefor e generat e successor s by choosin g value s for only a singl e variabl e at each node . For
example , in the 8­queen s problem , one can assig n a squar e for the first queen  at leve l 0, for the
secon d quee n at leve l 1, and so on. Thi s result s in a searc h spac e of size Y[, |A|> °r 88 in the
8­queen s problem . A straightforwar d depth­firs t searc h will examin e all of these possibilities .
Becaus e CSP s includ e as specia l case s som e well­know n NP­complet e problem s such as 3SA T
(see Exercis e 6.15 on page 182) , we canno t expec t to do bette r than exponentia l complexit y in
the wors t case . In mos t real problems , however , we can take advantag e of the proble m structur e
to eliminat e a large fractio n of the searc h space . The principa l sourc e of structur e in the proble m
space is that , in CSPs,  the goal  test is decomposed into  a set of constraints  on variables  rather
than being a  "black  box."
Depth­firs t searc h on a CSP waste s time searchin g whe n constraint s have alread y been
violated . Becaus e of the way that the operator s have been defined , an operato r can neve r redee m
a constrain t that has alread y been violated . Fo r example , suppos e that we put the first two
queen s in the top row. Depth­firs t searc h will examin e all 86 possibl e position s for the remainin g
six queen s befor e discoverin g that no solutio n exist s in that subtree . Ou r first improvemen t is
therefor e to inser t a test befor e the successo r generatio n step to chec k whethe r any constrain t has
been violate d by the variabl e assignment s mad e up to this point . The resultin g algorithm , calle d /
backtrackin g search , then backtrack s to try somethin g else.
Backtrackin g also has some obviou s failings . Suppos e that the square s chose n for the first I
six queen s mak e it impossibl e to plac e the eight h queen , becaus e they attac k all eigh t square s in]
the last column . Backtrackin g will try all possibl e placing s for the sevent h queen , even though !
the proble m is alread y rendere d unsolvable , give n the first six choices . Forwar d checking ]
avoid s this proble m by lookin g ahea d to detec t unsolvability . Each time a variabl e is instantiated, !
forwar d checkin g delete s from the domain s of the as­yet­uninstantiate d variable s all of those !
value s that conflic t with the variable s assigne d so far. If any of the domain s become s empty , then l
the searc h backtrack s immediately . Forwar d checkin g often runs far faste r than backtrackin g and j
is very simpl e to implemen t (see Exercis e 3.21) .
Forwar d checkin g is a specia l case of arc consistenc y checking . A state is arc­consisten t j
if ever y variabl e has a valu e in its domai n that is consisten t with each of the constraint s on that I
variable . Arc consistenc y can be achieve d by successiv e deletio n of value s that are inconsistent !
with some constraint . As value s are deleted , othe r value s may becom e inconsisten t becaus e they |
relied on the delete d values . Arc consistenc y therefor e exhibit s a form of constrain t propagation ,
as choice s are graduall y narrowe d down . In som e cases , achievin g arc consistenc y is enoug h to!
solve the proble m completel y becaus e the domain s of all variable s are reduce d to singletons . Arc |
consistenc y is ofte n used as a preprocessin g step , but can also be used durin g the search .
Much bette r result s can ofte n be obtaine d by carefu l choic e of whic h variabl e to instantiat e ]
and whic h valu e to try. We examin e such method s in the next chapter .
Sectio n 3.8^ Summar y 85
This chapte r has introduce d method s that an agen t can use whe n it is not clear whic h immediat e
actio n is best. In such cases , the agen t can conside r possibl e sequence s of actions ; this proces s is
called search .
• Befor e an agen t can start searchin g for solutions , it mus t formulat e a goal and then use the
goal to formulat e a problem .
• A proble m consist s of four parts : the initia l state , a set of operators , a goal test function ,
and a path cost function . The environmen t of the proble m is represente d by a state space .
A path throug h the state spac e from the initia l state to a goal state is a solution .
• In real life most problem s are ill­defined ; but with som e analysis , man y problem s can fit
into the state spac e model .
• A singl e genera l searc h algorith m can be used to solv e any problem ; specifi c variant s of
the algorith m embod y differen t strategies .
• Searc h algorithm s are judge d on the basis of completeness , optimality , time complexity ,
and spac e complexity.  Complexit y depend s on b, the branchin g facto r in the state space ,
and d, the dept h of the shallowes t solution .
• Breadth­firs t searc h expand s the shallowes t node in the searc h tree first . It is complete ,
optima l for unit­cos t operators , and has time and spac e complexit y of O(b'').  The spac e
complexit y make s it impractica l in most cases .
• Uniform­cos t searc h expand s the least­cos t leaf node first . It is complete , and unlik e
breadth­firs t searc h is optima l even whe n operator s have differin g costs . Its spac e and time
complexit y are the same as for breadth­firs t search .
• Depth­firs t searc h expand s the deepes t node in the searc h tree first . It is neithe r complet e
nor optimal , and has time complexit y of 0(bm) and spac e complexit y of O(bm),  wher e m is
the maximu m depth . In searc h trees of large or infinit e depth , the time complexit y make s
this impractical .
• Depth­limite d searc h place s a limit on how deep a depth­firs t searc h can go. If the limit
happen s to be equa l to the dept h of shallowes t goal state , then time and spac e complexit y
are minimized .
• Iterativ e deepenin g searc h calls depth­limite d searc h with increasin g limit s unti l a goal is
found . It is complet e and optimal , and has time complexit y of O(bd) and spac e complexit y
of O(bd).
• Bidirectiona l searc h can enormousl y reduc e time complexity , but is not alway s applicable .
Its memor y requirement s may be impractical .
86 Chapte r 3. Solvin g Problem s by Searchin g
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
Most of the state­spac e searc h problem s analyze d in this chapte r hav e a long histor y in the
literature , and are far less trivia l than they may seem . The missionarie s and cannibal s proble m
was analyze d in detai l from an artificia l intelligenc e perspectiv e by Amare l (1968) , althoug h
Amarel' s treatmen t was by no mean s the first ; it had been considere d earlie r in AI by Simo n
and Newel l (1961) , and elsewher e in compute r scienc e and operation s researc h by Bellma n and
Dreyfu s (1962) . Studie s such as thes e led to the establishmen t of searc h algorithm s as perhap s the
primar y tool s in the armor y of early AI researchers , and the establishmen t of proble m solvin g as
the canonica l AI task . (Of course , one migh t well claim that the latte r resulte d from the former. )
Amarel' s treatmen t of the missionarie s and cannibal s proble m is particularl y noteworth y
becaus e it is a classi c exampl e of forma l analysi s of a proble m state d informall y in natura l
language . Amare l give s carefu l attentio n to abstracting  from the informa l proble m statemen t
precisel y thos e feature s that are necessar y or usefu l in solvin g the problem , and selectin g a forma l
proble m representatio n that represent s only thos e features . A mor e recen t treatmen t of proble m
representatio n and abstraction , includin g AI program s that themselve s perfor m thes e task s (in
part) , is to be foun d in Knobloc k (1990) .
The 8­queen s proble m was firs t publishe d anonymousl y in the Germa n ches s magazin e
Schach  in 1848 ; it was later attribute d to one Max Bezzel . It was republishe d in 1850 , and at that
time drew the attentio n of the eminen t mathematicia n Carl Friedric h Gauss , who attempte d to
enumerat e all possibl e solutions . Even Gaus s was able to find only 72 of the 92 possibl e solution s
offhand , whic h give s som e indicatio n of the difficult y of this apparentl y simpl e problem . (Nauck ,
who had republishe d the puzzle , publishe d all 92 solution s later in 1850. ) Netto ( 1901 ) generalize d
the proble m to "n­queens " (on an n x n chessboard) .
The 8­puzzl e initiall y appeare d as the more comple x 4x4 version , calle d the 15­puzzle . It
was invente d by the famou s America n gam e designe r Sam Loy d (1959 ) in the 1870 s and quickl y
achieve d immens e popularit y in the Unite d States , comparabl e to the more recen t sensatio n cause d
by the introductio n of Rubik' s Cube . It also quickl y attracte d the attentio n of mathematician s
(Johnso n and Story , 1879 ; Tail , 1880) . The popula r receptio n of the puzzl e was so enthusiasti c
that the Johnso n and Stor y articl e was accompanie d by a note in whic h the editor s of the American
Journal  of Mathematics  felt it necessar y to state that "The '15' puzzl e for the last few week s
has been prominentl y befor e the America n public , and may safel y be said to have engage d the
attentio n of nine out of ten person s of both sexe s and all ages and condition s of the community .
But this woul d not have weighe d with the editor s to induc e them to inser t article s upo n such a
subjec t in the America n Journa l of Mathematics , but for the fact that ..." (ther e follow s a brief
summar y of the reason s for the mathematica l interes t of the 15­puzzle) . The 8­puzzl e has often
been used in AI researc h in plac e of the 15­puzzl e becaus e the searc h spac e is smalle r and thus
more easil y subjecte d to exhaustiv e analysi s or experimentation . An exhaustiv e analysi s was
carrie d out with compute r aid by P. D. A. Schofiel d (1967) . Althoug h very time­consuming ,
this analysi s allowe d other , faste r searc h method s to be compare d agains t theoretica l perfectio n
for the qualit y of the solution s found . An in­dept h analysi s of the 8­puzzle , usin g heuristi c
searc h method s of the kind describe d in Chapte r 4, was carrie d out by Dora n and Michi e (1966) .
The 15­puzzle , like the 8­queen s problem , has been generalize d to the n x n case . Ratne r and
Sectio n 3.8. Summar y 87
Warmut h (1986 ) showe d that findin g the shortes t solutio n in the generalize d n x n versio n belong s
to the class of NP­complet e problems .
"Uninformed " searc h algorithm s for findin g shortes t path s that rely on curren t path cost
alone , rathe r than an estimat e of the distanc e to the goal , are a centra l topic of classica l compute r
science , applie d mathematics , and a relate d field know n as operations  research.  Uniform­cos t
searc h as a way of findin g shortes t path s was invente d by Dijkstr a (1959) . A surve y of early
work in uninforme d searc h method s for shortes t path s can be foun d in Dreyfu s (1969) ; Deo
and Pang (1982 ) give a mor e recen t survey . For the varian t of the uninforme d shortest­path s
proble m that asks for shortes t path s betwee n all pair s of node s in a graph , the technique s of
dynamic  programming  and memoization  can be used . Fo r a proble m to be solve d by thes e
techniques , it mus t be capabl e of bein g divide d repeatedl y into subproblem s in such a way
that identica l subproblem s arise agai n and again . The n dynami c programmin g or memoizatio n
involve s systematicall y recordin g the solution s to subproblem s in a table so that they can be
looke d up whe n neede d and do not have to be recompute d repeatedl y durin g the proces s of
solvin g the problem . An efficien t dynami c programmin g algorith m for the all­pair s shortest­path s
proble m was foun d by Bob Floy d (1962a ; 1962b) , and improve d upon by Karge r et al. (1993) .
Bidirectiona l searc h was introduce d by Pohl (1969 ; 1971) ; it is often used with heuristi c guidanc e
technique s of the kind discusse d in Chapte r 4. Iterativ e deepenin g was first used by Slate and
Atkin (1977 ) in the CHES S 4.5 game­playin g program .
The textbook s by Nilsso n (1971 ; 1980 ) are goo d genera l source s of informatio n abou t
classica l searc h algorithms , althoug h they are now somewha t dated . A comprehensive , and muc h
more up­to­date , surve y can be foun d in (Korf , 1988) .
EXERCISE S
3.1 Explai n why proble m formulatio n mus t follo w goal formulation .
3.2 Conside r the accessible , two­locatio n vacuu m worl d unde r Murphy' s Law . Sho w that for
each initia l state , there is a sequenc e of action s that is guarantee d to reach a goal state .
3.3 Giv e the initia l state , goal test, operators , and path cost functio n for each of the following .
There are severa l possibl e formulation s for each problem , with varyin g level s of detail . The
main thin g is that your formulation s shoul d be precis e and "han g together " so that they coul d be
implemented .
a. You wan t to find the telephon e numbe r of Mr. Jimwil l Zollicoffer , who lives in Alameda ,
given a stack of directorie s alphabeticall y ordere d by city.
b. As for part (a), but you have forgotte n Jimwill' s last name .
c. You are lost in the Amazo n jungle , and have to reach the sea. Ther e is a strea m nearby .
d. You hav e to colo r a comple x plana r map usin g only four colors , with no two adjacen t
region s to have the sam e color .
Chapte r 3. Solvin g Problem s by Searchin g
e. A monke y is in a room with a crate , with banana s suspende d just out of reach on the ceiling .
He woul d like to get the bananas .
f. You are lost in a smal l countr y town , and mus t find a drug store befor e you r hay feve r
become s intolerable . Ther e are no maps , and the native s are all locke d indoors .
3.4 Implemen t the missionarie s and cannibal s proble m and use breadth­firs t searc h to find the
shortes t solution . Is it a good idea to chec k for repeate d states ? Draw a diagra m of the complet e
state spac e to help you decide .
3.5 On page 76, we said that we woul d not conside r problem s with negativ e path costs . In this
exercise , we explor e this in more depth .
a. Suppos e that a negativ e lowe r boun d c is place d on the cost of any give n step—tha t is,
negativ e cost s are allowed , but the cost of a step canno t be less than c. Doe s this allow
uniform­cos t searc h to avoi d searchin g the whol e tree?
b. Suppos e that there is a set of operator s that form a loop , so that executin g the set in some
order result s in no net chang e to the state . If all of these operator s have negativ e cost, wha t
does this impl y abou t the optima l behavio r for an agen t in such an environment ? :
c. One can easil y imagin e operator s with high negativ e cost, even in domain s such as route ­ !
finding . For example , som e stretche s of road migh t have such beautifu l scener y as to far j
outweig h the norma l costs in term s of time and fuel. Explain , in precis e terms , why human s
do not drive roun d sceni c loop s indefinitely , and explai n how to defin e the state spac e and I
operator s for route­findin g so that artificia l agent s can also avoi d looping .
d. Can you thin k of a real domai n in whic h step costs are such as to caus e looping ?
3.6 The GENERAL­SEARC H algorith m consist s of three steps : goal test, generate , and ordering !
function , in that order . It seem s a sham e to generat e a node that is in fact a solution , but to fail to|
recogniz e it becaus e the orderin g functio n fails to place it first .
a. Writ e a versio n of GENERAL­SEARC H that tests each node as soon as it is generate d and|
stops immediatel y if it has foun d a goal .
b. Sho w how the GENERAL­SEARC H algorith m can be used unchange d to do this by givin g it|
the prope r orderin g function .
3.7 Th e formulatio n of problem , solution , and searc h algorith m give n in this chapte r explicitly !
mention s the path to a goal state . This is becaus e the path is importan t in man y problems . For 1
other problems , the path is irrelevant , and only the goal state matters . Conside r the problem !
"Find the squar e root of 123454321. " A searc h throug h the spac e of number s may pass through ]
many states , but the only one that matter s is the goal state , the numbe r 11111 . Of course , from a 1
theoretica l poin t of view , it is easy to run the genera l searc h algorith m and then ignor e all of the f
path excep t the goal state . But as a programmer , you may realiz e an efficienc y gain by codin g a ]
versio n of the searc h algorith m that does not keep track of paths . Conside r a versio n of proble m I
solvin g wher e ther e are no path s and only the state s matter . Writ e definition s of proble m and •
solution , and the genera l searc h algorithm . Whic h of the problem s in Sectio n 3.3 woul d best use
this algorithm , and whic h shoul d use the versio n that keep s track of paths ?
Sectio n 3.8. Summar y 89
3.8 Give n a pathles s searc h algorith m such as the one calle d for in Exercis e 3.7, explai n how
you can modif y the operator s to keep track of the path s as part of the informatio n in a state . Sho w
the operator s neede d to solv e the route­findin g and tourin g problems .
3.9 Describ e a searc h spac e in whic h iterativ e deepenin g searc h perform s muc h wors e than
depth­firs t search .
3.10 Figur e 3.17 show s a schemati c view of bidirectiona l search . Wh y do you thin k we chose
to show trees growin g outwar d from the start and goal states , rathe r than two searc h trees growin g
horizontall y towar d each other ?
3.11 Writ e dow n the algorith m for bidirectiona l search , in pseudo­cod e or in a programmin g
language . Assum e that each searc h will be a breadth­firs t search , and that the forwar d and
backwar d searche s take turn s expandin g a node at a time . Be carefu l to avoi d checkin g each node
in the forwar d searc h agains t each node in the backwar d search !
3.12 Giv e the time complexit y of bidirectiona l searc h whe n the test for connectin g the two
searche s is done by comparin g a newl y generate d state in the forwar d directio n agains t all the
states generate d in the backwar d direction , one at a time .
3.13 W e said that at least one directio n of a bidirectiona l searc h mus t be a breadth­firs t search .
What woul d be a good choic e for the othe r direction ? Why ?
3.14 Conside r the followin g operato r for the 8­queen s problem : plac e a quee n in the colum n
with the fewes t unattacke d squares , in such a way that it does not attac k any othe r queens . How
many node s does this expan d befor e it find s a solution ? (Yo u may wish to have a progra m
calculat e this for you. )
3.15 Th e chai n proble m (Figur e 3.20 ) consist s of variou s length s of chain that mus t be recon ­
figure d into new arrangements . Operator s can open one link and close one link. In the standar d
form of the problem , the initia l state contain s four chains , each with three links . The goal state
consist s of a singl e chai n of 12 link s in a circle . Set this up as a forma l searc h proble m and find
the shortes t solution .
f­TT~ s —  ^ Li
f*"" ^"^ 0  ~* ^ U "
r~TT~TT~ ^ I
Start staten o
— S­iL ­
Goal stateh
=1
•1
»>
Figur e 3.20 Th e chai n problem . Operator s can open , remove , reattach , and close a singl e link
at a time .
90 Chapte r 3. Solvin g Problem s by Searchin g
3.16 Test s of huma n intelligenc e ofte n contai n sequenc e predictio n problems . The aim in
such problem s is to predic t the next membe r of a sequenc e of integers , assumin g that the numbe r
in positio n n of the sequenc e is generate d usin g som e sequenc e functio n s(n),  wher e the first
elemen t of the sequenc e correspond s to n = 0. For example , the functio n s(n) ­ 2" generate s the
sequenc e [1,2,4,8,16 , ...].
In this exercise , you wil l desig n a problem­solvin g syste m capabl e of solvin g such pre­
dictio n problems . The syste m will searc h the spac e of possibl e function s unti l it find s one that
matche s the observe d sequence . The spac e of sequenc e function s that we will conside r consist s
of all possibl e expression s buil t from the element s 1 and n, and the function s +, x, —, /, and
exponentiation . For example , the functio n 2" become s (1 + 1)" in this language . It will be usefu l
to thin k of functio n expression s as binar y trees , with operator s at the interna l node s and 1 's and
«'s at the leaves .
a. First , writ e the goal test function . Its argumen t will be a candidat e sequenc e functio n s. It
will contai n the observe d sequenc e of number s as loca l state .
b. Now writ e the successo r function . Give n a functio n expressio n s, it shoul d generat e all
expression s one step more comple x than s. This can be done by replacin g any leaf of the
expressio n with a two­lea f binar y tree.
c. Whic h of the algorithm s discusse d in this chapte r woul d be suitabl e for this problem ?
Implemen t it and use it to find sequenc e expression s for the sequence s [1,2,3,4,5] ,
[1,2,4,8,16 , ...],an d [0.5,2,4.5,8] .
d. If level d of the searc h spac e contain s all expression s of complexit y d +1, wher e complexit y
is measure d by the numbe r of leaf node s (e.g. , n + (1 x n) has complexit y 3), prov e by
inductio n that there are roughl y 20d(d +1)1  expression s at leve l d.
e. Commen t on the suitabilit y of uninforme d searc h algorithm s for solvin g this problem . Can
you sugges t othe r approaches ?
3.17 Th e full vacuu m worl d from the exercise s in Chapte r 2 can be viewe d as a searc h proble m
in the sens e we have defined , provide d we assum e that the initia l state is completel y known .
a. Defin e the initia l state , operators , goal test function , and path cost function .
b. Whic h of the algorithm s define d in this chapte r woul d be appropriat e for this problem ?
c. Appl y one of them to comput e an optima l sequenc e of action s for a 3 x 3 worl d with dirt in
the cente r and hom e squares .
d. Construc t a searc h agen t for the vacuu m world , and evaluat e its performanc e in a set of
3x3 world s with probabilit y 0.2 of dirt in each square . Includ e the searc h cost as well as
path cost in the performanc e measure , usin g a reasonabl e exchang e rate.
e. Compar e the performanc e of your searc h agen t with the performanc e of the agent s con­
structe d for the exercise s in Chapte r 2. Wha t happen s if you includ e computatio n time in
the performanc e measure , at variou s "exchang e rates " with respec t to the cost of takin g a
step in the environment ?
f. Conside r what woul d happe n if the worl d was enlarge d tonxn.  How does the performanc e
of the searc h agen t vary with «? Of the refle x agents ?
Sectio n 3.8. Summar y 91
:MAP­COLORIN G
FLOOR­PLANNIN G3.18 Th e searc h agent s we have discusse d mak e use of a complet e mode l of the worl d to
construc t a solutio n that they then execute . Modif y the depth­firs t searc h algorith m with repeate d
state checkin g so that an agen t can use it to explor e an arbitrar y vacuu m worl d even withou t a
mode l of the location s of wall s and dirt. It shoul d not get stuc k even with loop s or dead ends .
You may also wish to have your agen t construc t an environmen t descriptio n of the type used by
the standar d searc h algorithms .
3.19 In discussin g the cryptarithmeti c problem , we propose d that an operato r shoul d assig n
a valu e to whicheve r lette r has the leas t remainin g possibl e values . Is this rule guarantee d to
produc e the smalles t possibl e searc h space ? Wh y (not) ?
3.20 Defin e each of the followin g as constrain t satisfactio n problems :
a. The cryptarithmeti c problem .
b. The channel­routin g proble m in VLS I layout .
c. The map­colorin g problem . In map­coloring , the aim is to colo r countrie s on a map usin g
a give n set of colors , such that no two adjacen t countrie s are the same color .
d. The rectilinea r floor­plannin g problem , whic h involve s findin g nonoverlappin g place s in
a large rectangl e for a numbe r of smalle r rectangles .
3.21 Implemen t a constrain t satisfactio n syste m as follows :
a. Defin e a datatyp e for CSP s with finite , discret e domains . You will need to find a way to
represen t domain s and constraints .
b. Implemen t operator s that assig n value s to variables , wher e the variable s are assigne d in a
fixed orde r at each leve l of the tree.
c. Implemen t a goal test that check s a complet e state for satisfactio n of all the constraints .
d. Implemen t backtrackin g by modifyin g DEPTH­FIRST­SEARCH .
e. Add forwar d checkin g to your backtrackin g algorithm .
f. Run the three algorithm s on som e sampl e problem s and compar e their performance .
4INFORME D SEARC H
METHOD S
In which we  see how  information  about  the state  space  can prevent  algorithms  from
blundering  about in  the dark.
Chapte r 3 showe d that uninforme d searc h strategie s can find solution s to problem s by systemati ­
cally generatin g new state s and testin g them agains t the goal . Unfortunately , thes e strategie s are
incredibl y inefficien t in mos t cases . Thi s chapte r show s how an informe d searc h strategy—on e
that uses problem­specifi c knowledge—ca n find solution s mor e efficiently . It also show s how
optimizatio n problem s can be solved .
4.1 BEST­FIRS T SEARC H
EVALUATIO N
FUNCTIO N
BEST­FIRS T SEARC HIn Chapte r 3, we foun d severa l way s to appl y knowledg e to the proces s of formulatin g a proble m
in term s of state s and operators . Onc e we are give n a well­define d problem , however , our option s
are mor e limited . If we plan to use the GENERAL­SEARC H algorith m from Chapte r 3, then
the only plac e wher e knowledg e can be applie d is in the queuin g function , whic h determine s
the node to expan d next . Usually , the knowledg e to mak e this determinatio n is provide d by an
evaluatio n functio n that return s a numbe r purportin g to describ e the desirabilit y (or lack thereof )
of expandin g the node . Whe n the node s are ordere d so that the one with the best evaluatio n is
expande d first , the resultin g strateg y is calle d best­firs t search . It can be implemente d directl y
with GENERAL­SEARCH , as show n in Figur e 4.1.
The nam e "best­firs t search " is a venerabl e but inaccurat e one. Afte r all, if we coul d reall y
expan d the best node first , it woul d not be a searc h at all; it woul d be a straigh t marc h to the goal .
All we can do is choos e the nod e that appears  to be best accordin g to the evaluatio n function .
If the evaluatio n functio n is omniscient , then this will indee d be the best node ; in reality , the
evaluatio n functio n will sometime s be off, and can lead the searc h astray . Nevertheless , we will
stick with the nam e "best­firs t search, " becaus e "seemingly­best­firs t search " is a little awkward .
Just as ther e is a whol e famil y of GENERAL­SEARC H algorithm s with differen t orderin g
functions , there is also a whol e famil y of BEST­FIRST­SEARC H algorithm s with differen t evaluatio n
92
Sectio n 4.1. Best­Firs t Searc h 93
functio n BEST­FiRST­SEARCH(/7/­oWew? , EVAL­FN ) return s a solutio n sequenc e
inputs : problem,  a proble m
Eval­Fn,  an evaluatio n functio n
Queueing­Fn  <— a functio n that order s node s by EVAL­F N
retur n GENERAL­SEARCH(proW<?m , Queueing­Fn)
Figur e 4.1 A n implementatio n of best­firs t searc h usin g the genera l searc h algorithm .
functions . Becaus e they aim to find low­cos t solutions , thes e algorithm s typicall y use som e
estimate d measur e of the cost of the solutio n and try to minimiz e it. We have alread y seen one
such measure : the use of the path cost g to decid e whic h path to extend . This measure , however ,
does not direc t searc h toward  the goal.  In order  to focus  the search,  the measure must  incorporate
some  estimate  of the cost  of the path  from  a state  to the closest  goal state.  We look at two basi c
approaches . The first tries to expan d the node closes t to the goal . The secon d tries to expan d the
node on the least­cos t solutio n path .
Minimiz e estimate d cost to reac h a goal : Greed y searc h
HEURISTI C
FUNCTIO N
GREED Y SEARC H
STRAIGHT­LIN E
DISTANC EOne of the simplest  best­first  search  strategies  is to minimize the  estimated  cost to reach  the goal.
That is, the node whos e state is judge d to be closes t to the goal state is alway s expande d first .
For mos t problems , the cost of reachin g the goal from a particula r state can be estimate d but
canno t be determine d exactly . A functio n that calculate s such cost estimate s is calle d a heuristi c
function , and is usuall y denote d by the letter h:
h(n) = estimate d cost of the cheapes t path from the state at node n to a goal state .
A best­firs t searc h that uses h to selec t the next nod e to expan d is calle d greed y search , for
reason s that will becom e clear . Give n a heuristi c functio n h, the code for greed y searc h is just
the following :
functio n GREEDY­SEARCH(proWewi ) return s a solutio n or failur e
retur n BEST­FiRST­SEARCH(proWem , h)
Formall y speaking , h can be any functio n at all. We will requir e only that h(ri) = 0 if n is a goal .
To get an idea of wha t a heuristi c functio n look s like , we need to choos e a particula r
problem , becaus e heuristi c function s are problem­specific . Let us retur n to the route­findin g
proble m from Arad to Bucharest . The map for that proble m is repeate d in Figur e 4.2.
A good heuristi c functio n for route­findin g problem s like this is the straight­lin e distanc e
to the goal . Tha t is,
hsLo(n) = straight­lin e distanc e betwee n n and the goal location .
94 Chapte r 4. Informe d Searc h Method s
HISTOR Y OF "HEURISTIC "
By now the spac e alien s had mastere d my own language , but they still mad e
simpl e mistake s like usin g "hermeneutic " whe n they mean t "heuristic. "
— a Louisian a factor y worke r in Wood y Alien' s The UFO  Menace
The wor d "heuristic " is derive d from the Gree k verb heuriskein,  meanin g "to find "
or "to discover. " Archimede s is said to have run nake d dow n the stree t shoutin g
"Heureka"  (I have foun d it) afte r discoverin g the principl e of flotatio n in his bath .
Later generation s converte d this to Eureka .
The technica l meanin g of "heuristic " has undergon e severa l change s in the histor y
of AI. In 1957 , Georg e Poly a wrot e an influentia l book calle d How to  Solve It  that used
"heuristic " to refe r to the stud y of method s for discoverin g and inventin g problem ­
solvin g techniques , particularl y for the proble m of comin g up with mathematica l
proofs . Suc h method s had often been deeme d not amenabl e to explication .
Some peopl e use heuristi c as the opposit e of algorithmic . For example , Newell ,
Shaw , and Simo n state d in 1963 , "A proces s that may solve a give n problem , but offer s
no guarantee s of doin g so, is calle d a heuristi c for that problem. " But note that there
is nothin g rando m or nondeterministi c abou t a heuristi c searc h algorithm : it proceed s
by algorithmi c steps towar d its result . In som e cases , ther e is no guarante e how long
the searc h will take , and in som e cases , the qualit y of the solutio n is not guarantee d
either . Nonetheless , it is importan t to distinguis h betwee n "nonalgorithmic " and "not
precisel y characterizable. "
Heuristi c technique s dominate d early application s of artificia l intelligence . The
first "exper t systems " laboratory , starte d by Ed Feigenbaum , Brac e Buchanan , and
Joshu a Lederber g at Stanfor d University , was calle d the Heuristi c Programmin g
Projec t (HPP) . Heuristic s were viewe d as "rule s of thumb " that domai n experts  coul d
use to generat e good solution s withou t exhaustiv e search . Heuristic s were initiall y
incorporate d directl y into the structur e of programs , but this prove d too inflexibl e
when a large numbe r of heuristic s were needed . Gradually , system s were designe d
that coul d accep t heuristi c informatio n expresse d as "rules, " and rule­base d system s
were born .
Currently , heuristi c is mos t ofte n used as an adjective , referrin g to any techniqu e
that improve s the average­cas e performanc e on a problem­solvin g task , but does
not necessaril y improv e the worst­cas e performance . In the specifi c area of searc h
algorithms , it refer s to a functio n that provide s an estimat e of solutio n cost.
A good articl e on heuristic s (and one on hermeneutics! ) appear s in the Encyclo­
pedia  of AI (Shapiro , 1992) .
Sectio n 4.1. Best­Firs t Searc h 95
Neamt
lasi
GiurgiuStraight­lin e distan c
to Buchares t
Arad
Buchares t
Craiov a
Dobret a
Efori e
Fagara s
Giurgi u
Hirsov a
lasi
Lugo j
Mehadi a
Neam t
Orade a
] Hirsov a J"'.teSt!Rimnic u Vilce a
Sibiu
Timisoar a
Urzicen i
Efori e Vaslu i
Zerin d366
o
] 60
242
161
1 78
77
1 5 ]
226
244
24 1
234
380
98
1 93
253
329
80
199
374
Figur e 4.2 Ma p of Romani a with road  distance s in km, and straight­lin e distance s to Bucharest .
Notic e that we can only calculat e the value s of HSLD if we know the map coordinate s of the citie s
in Romania . Furthermore , hsio  is only usefu l becaus e a road from A to B usuall y tend s to head
in mor e or less the righ t direction . Thi s is the sort of extra informatio n that allow s heuristic s to
help in reducin g searc h cost .
Figur e 4.3 show s the progres s of a greed y searc h to find a path from Arad to Bucharest .
With the straight­line­distanc e heuristic , the first nod e to be expande d from Arad will be Sibiu ,
becaus e it is close r to Buchares t than eithe r Zerin d or Timisoara . The next node to be expande d
will be Fagaras , becaus e it is closest . Fagara s in turn generate s Bucharest , whic h is the goal . For
this particula r problem , the heuristi c lead s to minima l searc h cost : it find s a solutio n withou t ever
expandin g a node that is not on the solutio n path . However , it is not perfectl y optimal : the path
it foun d via Sibi u and Fagara s to Buchares t is 32 mile s longe r than the path throug h Rimnic u
Vilce a and Pitesti . This path was not foun d becaus e Fagara s is close r to Buchares t in straight­lin e
distanc e than Rimnic u Vilcea , so it was expande d first . The strateg y prefer s to take the bigges t
bite possibl e out of the remainin g cost to reac h the goal , withou t worryin g abou t whethe r this
will be best in the long run—henc e the nam e "greed y search. " Althoug h gree d is considere d
one of the seve n deadl y sins , it turn s out that greed y algorithm s ofte n perfor m quit e well . The y
tend to find solution s quickly , althoug h as show n in this example , they do not alway s find the
optima l solutions : that woul d take a more carefu l analysi s of the long­ter m options , not just the
immediat e best choice .
Greed y searc h is susceptibl e to false starts . Conside r the proble m of gettin g from lasi to
Fagaras . The heuristi c suggest s that Neam t be expande d first , but it is a dead end. The solutio n is
to go first to Vaslui— a step that is actuall y farthe r from the goal accordin g to the heuristic—an d
then to continu e to Urziceni , Bucharest , and Fagaras . Hence , in this case , the heuristi c cause s
unnecessar y node s to be expanded . Furthermore , if we are not carefu l to detec t repeate d states ,
the solutio n will neve r be found—th e searc h will oscillat e betwee n Neam t and lasi.
96 Chapte r 4. Informe d Searc h Method s
Aradj j
h=36 6
Arad
Sibiu^ r Timisoar a f§ ^^ Zerin d
h=25 3 h=32 9 h=37 4
Timisoar a j| \o Zerin d
h=32 9 h=37 4
Arad ^i Fagara s Jf Oradea  ® Rimnic i
h=36 6 h=17 8 h=38 0 h=19 3Arad i
Timisoara ^ ^^m Zerin d
h=32 9 h=37 4
Arad ^r Fagara s
h=36 6
Figur e 4.3 Stage s in a greed y searc h for Bucharest , usin g the straight­lin e distanc e to Buchares t
as the heuristi c functio n IISLD­  Node s are labelle d with thei r /j­values .
Greed y searc h resemble s depth­firs t searc h in the way it prefer s to follo w a singl e path all
the way to the goal , but will back up whe n it hits a dead end. It suffer s from the same defect s
as depth­firs t search—i t is not optimal , and it is incomplet e becaus e it can start dow n an infinit e
path and neve r retur n to try othe r possibilities . The worst­cas e time complexit y for greed y searc h
is O(bm~), wher e m is the maximu m dept h of the searc h space . Becaus e greed y searc h retain s
all node s in memory , its spac e complexit y is the sam e as its time complexity . Wit h a good
heuristi c function , the spac e and time complexit y can be reduce d substantially . The amoun t of
the reductio n depend s on the particula r proble m and qualit y of the h function .
Minimizin g the total path cost: A* searc h
Greed y searc h minimize s the estimate d cost to the goal , h(n),  and thereb y cuts the searc h cost
considerably . Unfortunately , it is neithe r optima l nor complete . Uniform­cos t search , on the
other hand , minimize s the cost of the path so far, g(ri);  it is optima l and complete , but can be
very inefficient . It woul d be nice if we coul d combin e thes e two strategie s to get the advantage s
of both . Fortunately , we can do exactl y that , combinin g the two evaluatio n function s simpl y by
summin g them :
f(n) = g(n)  + h(n).
Sectio n 4.1. Best­Firs t Searc h 97
ADMISSIBL E
HEURISTI C
A'SEARC HSince g(n) give s the path cost from the start node to node n, and h(ri) is the estimate d cost of the
cheapes t path from n to the goal , we have
/(«) = estimate d cost of the cheapes t solutio n throug h n
Thus , if we are tryin g to find the cheapes t solution , a reasonabl e thin g to try first is the node with
the lowes t valu e off. The pleasan t thing abou t this strateg y is that it is more than just reasonable .
We can actuall y prov e that it is complet e and optimal , give n a simpl e restrictio n on the h function .
The restrictio n is to choos e an h functio n that never  overestimates  the cost to reac h the
goal. Such an h is calle d an admissibl e heuristic . Admissibl e heuristic s are by natur e optimistic ,
becaus e they thin k the cost of solvin g the proble m is less than it actuall y is. Thi s optimis m
transfer s to the/ functio n as well : Ifh is admissible,  f(n) never overestimates the actual  cost of
the best solution through  n. Best­firs t searc h using / as the evaluatio n functio n and an admissibl e
h functio n is know n as A* searc h
functio n A*­SEARCH ( problem)  return s a solutio n or failur e
retur n BEST­FlRST­SEARCH(/?roWem, g + h)
Perhap s the mos t obviou s exampl e of an admissibl e heuristi c is the straight­lin e distanc e
that we used in gettin g to Bucharest . Straight­lin e distanc e is admissibl e becaus e the shortes t
path betwee n any two point s is a straigh t line. In Figur e 4.4, we show the first few steps of an A*
searc h for Buchares t usin g the hSLD heuristic . Notic e that the A* searc h prefer s to expan d from
Rimnic u Vilce a rathe r than from Fagaras . Eve n thoug h Fagara s is close r to Bucharest , the path
taken to get to Fagara s is not as efficient  in gettin g close to Buchares t as the path take n to get to
Rimnicu . The reade r may wish to continu e this exampl e to see wha t happen s next .
MONOTONICIT YThe behavio r of A* searc h
Befor e we prov e the completenes s and optimalit y of A*, it will be usefu l to presen t an intuitiv e
pictur e of how it works . A pictur e is not a substitut e for a proof , but it is often easie r to remembe r
and can be used to generat e the proo f on demand . First , a preliminar y observation : if you examin e
the searc h trees in Figur e 4.4, you will notic e an interestin g phenomenon . Alon g any path from
the root, the/­cos t neve r decreases . This is no accident . It hold s true for almos t all admissibl e
heuristics . A heuristi c for whic h it hold s is said to exhibi t monotonicity. '
If the heuristi c is one of thos e odd ones that is not monotonic , it turn s out we can mak e a
mino r correctio n that restore s monotonicity . Let us conside r two node s n and n', wher e n is the
paren t of n'. Now suppose , for example , thatg(« ) = 3 and/?(n ) = 4. Then/(n ) = g(ri)+h(ri)  ­ 1—
that is, we know that the true cost of a solutio n path throug h n is at leas t 7. Suppos e also that
g(n') = 4 and h(n')  ­ 2, so that/(n' ) = 6. Clearly , this is an exampl e of a nonmonotoni c heuristic .
Fortunately , from the fact that any path  through  n' is also a  path  through  n, we can see that the
value of 6 is meaningless , becaus e we alread y know the true cost is at least  7. Thus , we shoul d
1 It can be prove d (Pearl , 1984 ) that a heuristi c is monotoni c if and only if it obey s the triangl e inequality . The triangl e
inequalit y says that two side s of a triangl e canno t add up to less than the thir d side (see Exercis e 4.7) . Of course ,
straight­lin e distanc e obey s the triangl e inequalit y and is therefor e monotonic .
98 Chapte r 4. Informe d Searc h Method s
Arad
Zerin d
Arad
Zerin d
Arad Fagaras |
f=280+36 6 f=239+17 8 f=146+38 0
=646 =41 7 =52 6
Craiov a (
f=366+16 0
=526
Figur e 4.4 Stage s in an A* searc h for Bucharest . Node s are labelle d with / = g + h. The h
value s are the straight­lin e distance s to Buchares t taken from Figur e 4.1.
check , each time we generat e a new node , to see if its/­cos t is less than its parent's/­cost ; if it
is, we use the parent's/­cos t instead :
/(«') = max(f(n),  g(n')  + h(n'}).
In this way , we ignor e the misleadin g value s that may occu r with a nonmonotoni c heuristic . This
PATHMA X equatio n is calle d the pathma x equation . If we use it, then / will alway s be nondecreasin g alon g
any path from the root, provide d h is admissible .
The purpos e of makin g this observatio n is to legitimiz e a certai n pictur e of wha t A* does .
CONTOUR S If / neve r decrease s alon g any path out from the root , we can conceptuall y draw contour s in the
state space . Figur e 4.5 show s an example . Insid e the contou r labelle d 400, all node s have/(n )
less than or equa l to 400, and so on. Then , becaus e A* expand s the leaf node of lowest/ , we can
see that an A* searc h fans out from the start node , addin g node s in concentri c band s of increasin g
/­cost .
With uniform­cos t searc h (A* searc h usin g h = 0), the band s will be "circular " aroun d the
start state . Wit h more accurat e heuristics , the band s will stretc h towar d the goal state and becom e
more narrowl y focuse d aroun d the optima l path . If we define/ * to be the cost of the optimal
solutio n path , then we can say the following :
• A* expand s all node s with/(n ) </*.
• A* may then expan d som e of the node s righ t on the "goa l contour, " for which/(« ) =/*,
befor e selectin g a goal node .
Intuitively , it is obviou s that the first solutio n foun d mus t be the optima l one, becaus e node s in
all subsequen t contour s will have higher/­cost , and thus highe r g­cos t (becaus e all goal state s
have h(n)  = 0). Intuitively , it is also obviou s that A* searc h is complete . As we add band s of
Sectio n 4.1. Best­Firs t Searc h 99
OPTIMALL Y
EFFICIEN TFigur e 4.5 Ma p of Romani a showin g contour s at/ = 380. / = 400 and / = 420, with Arad as
the start state . Node s insid e a give n contou r have /­cost s lowe r than the contou r value .
increasing/ , we mus t eventuall y reac h a band where / is equa l to the cost of the path to a goal
state. We will turn these intuition s into proof s in the next subsection .
One fina l observatio n is that amon g optima l algorithm s of this type—algorithm s that exten d
searc h path s from the root—A * is optimall y efficien t for any give n heuristi c function . Tha t is,
no othe r optima l algorith m is guarantee d to expan d fewe r node s than A*. We can explai n this
as follows : any algorith m that does  not expan d all node s in the contour s betwee n the root and
the goal contou r runs the risk of missin g the optima l solution . A long and detaile d proo f of this
resul t appear s in Dechte r and Pear l (1985) .
Proof of the optimalit y of A*
Let G be an optima l goal state , with path cost/* . Let G2 be a suboptima l goal state , that is, a
goal state with path cost g(G 2) >/* . The situatio n we imagin e is that A* has selecte d G2 from
the queue . Becaus e G2 is a goal state , this woul d terminat e the searc h with a suboptima l solutio n
(Figur e 4.6). We will show that this is not possible .
Conside r a node « that is currentl y a leaf node on an optima l path to G (ther e mus t be som e
such node , unles s the path has been completel y expande d — in whic h case the algorith m woul d
have returne d G). Becaus e h is admissible , we mus t have
r >/(«) .
Furthermore , if n is not chose n for expansio n over G2, we mus t have
Combinin g these two together , we get
r >
100 Chapte r 4. Informe d Searc h Method s
Start
Figur e 4.6 Situatio n at the poin t wher e a suboptima l goal state 62 is abou t to be expanded .
Node n is a leaf node on an optima l path from the start node to the optima l goal state G.
But becaus e GI is a goal state , we have h(G2)  = 0; hence
from our assumptions , thata) = g(G2).  Thus , we have proved ,
This contradict s the assumptio n that G^_ is suboptimal , so it mus t be the case that A* neve r select s
a suboptima l goal for expansion . Hence , becaus e it only return s a solutio n after selectin g it for
expansion , A* is an optima l algorithm .
LOCALL Y FINIT E
GRAPH SProo f of the completenes s of A*
We said befor e that becaus e A* expand s node s in orde r of increasing/ , it must eventuall y expan d
to reach a goal state . This is true, of course,  unles s there are infinitel y many node s with/(« ) </*.
The only way ther e coul d be an infinit e numbe r of node s is eithe r (a) ther e is a node with an
infinit e branchin g factor , or (b) ther e is a path with a finit e path cost but an infinit e numbe r of
node s alon g it.2
Thus , the correc t statemen t is that A* is complet e on locall y finit e graph s (graph s with a
finite branchin g factor ) provide d there is som e positiv e constan t 6 such that ever y operato r costs
at least 6.
Complexit y of A*
That A* searc h is complete , optimal , and optimall y efficien t amon g all such algorithm s is rathe r
satisfying . Unfortunately , it does not mea n that A* is the answe r to all our searchin g needs . The
catch is that, for most problems , the numbe r of node s withi n the goal contou r searc h spac e is still
exponentia l in the lengt h of the solution . Althoug h the proo f of the resul t is beyon d the scop e of
this book , it has been show n that exponentia l growt h will occu r unles s the error in the heuristi c
2 Zeno' s paradox , whic h purport s to show that a rock throw n at a tree will neve r reac h it, provide s an exampl e that
violate s conditio n (b). The parado x is create d by imaginin g that the trajector y is divide d into a serie s of phases , each of
whic h cover s half the remainin g distanc e to the tree; this yield s an infinit e sequenc e of steps with a finit e total cost.
Sectio n 4.2. Heuristi c Function s 101
functio n grow s no faste r than the logarith m of the actua l path cost. In mathematica l notation , the
conditio n for subexponentia l growt h is that
\h(n) ­A*(n)|<0(log/.*(«)) ,
wher e h* (n) is the true cost of gettin g from n to the goal . For almos t all heuristic s in practica l use,
the error is at least  proportiona l to the path cost , and the resultin g exponentia l growt h eventuall y
overtake s any computer . Of course , the use of a good heuristi c still provide s enormou s saving s
compare d to an uninforme d search . In the next section , we will look at the questio n of designin g
good heuristics .
Computatio n time is not, however , A*'s main drawback . Becaus e it keep s all generate d
node s in memory , A* usuall y runs out of space long befor e it runs out of time . Recentl y develope d
algorithm s have overcom e the spac e proble m withou t sacrificin g optimalit y or completeness .
These are discusse d in Sectio n 4.3.
4.2 HEURISTI C FUNCTION S
So far we have seen just one exampl e of a heuristic : straight­lin e distanc e for route­findin g
problems . In this section , we will look at heuristic s for the 8­puzzle . This will shed ligh t on the
natur e of heuristic s in general .
The 8­puzzl e was one of the earlies t heuristi c searc h problems . As mentione d in Sectio n 3.3,
the objec t of the puzzl e is to slide the tiles horizontall y or verticall y into the empt y spac e unti l
the initia l configuratio n matche s the goal configuratio n (Figur e 4.7).
5 |
6 \
7 1i 4 ]
\ 1 I
\ 3 !ilPilK
I 8
! 2
Start State'I
8 I
7 i\ 2 ;
1 6 i; 3
: 4
5
Goal State
Figur e 4.7 A  typica l instanc e of the 8­puzzle .
The 8­puzzl e is just the right level of difficult y to be interesting . A typica l solutio n is abou t
20 steps , althoug h this of cours e varie s dependin g on the initia l state . The branchin g facto r is
abou t 3 (whe n the empt y tile is in the middle , there are four possibl e moves ; whe n it is in a corne r
there are two; and whe n it is alon g an edge there are three) . This mean s that an exhaustiv e searc h
to dept h 20 woul d look at abou t 320 = 3.5 x 109 states . By keepin g track of repeate d states , we
could cut this dow n drastically , becaus e ther e are only 9! = 362,88 0 differen t arrangement s of
9 squares . Thi s is still a larg e numbe r of states , so the next order  of busines s is to find a good
102 Chapte r 4. Informe d Searc h Method s
MANHATTA N
DISTANC Eheuristi c function . If we wan t to find the shortes t solutions , we need a heuristi c functio n that
never overestimate s the numbe r of steps to the goal . Her e are two candidates :
• h\ = the numbe r of tiles that are in the wron g position . For Figur e 4.7, non e of the 8 tiles
is in the goal position , so the start state woul d have h\ = 8. h\ is an admissibl e heuristic ,
becaus e it is clea r that any tile that is out of place mus t be move d at least once .
• h2 = the sum of the distance s of the tiles from thei r goal positions . Becaus e tiles canno t
move alon g diagonals , the distanc e we will coun t is the sum of the horizonta l and vertica l
distances . This is sometime s calle d the city bloc k distanc e or Manhatta n distance . h2 is
also admissible , becaus e any mov e can only mov e one tile one step close r to the goal . The
8 tiles in the start state give a Manhatta n distanc e of
h2 = 2 + 3 + 2 + 1+2 + 2+ 1+2 =15
EFFECTIV E
BRANCHIN G FACTO R
DOMINATE SThe effec t of heuristi c accurac y on performanc e
One way to characteriz e the qualit y of a heuristi c is the effectiv e branchin g facto r b*. If the \
total numbe r of node s expande d by A* for a particula r proble m is N, and the solutio n dept h is ;
d, then b* is the branchin g facto r that a unifor m tree of dept h d woul d have to have in orde r to ~>
contai n N nodes . Thus ,
N= l+b*  +(b*)2 + ­­­ + (b~)d.
For example , if A* find s a solutio n at dept h 5 usin g 52 nodes , then the effectiv e branchin g facto r is •
1.91. Usually , the effectiv e branchin g facto r exhibite d by a give n heuristi c is fairl y constan t over !
a large rang e of proble m instances , and therefor e experimenta l measurement s of b" on a smal l i
set of problem s can provid e a good guid e to the heuristic' s overall  usefulness . A well­designed !
heuristi c woul d have a valu e of b* clos e to 1, allowin g fairl y large problem s to be solved . To !
test the heuristi c function s h\ and h2, we randoml y generate d 100 problem s each with solutio n
length s 2,4,... , 20, and solve d them usin g A* searc h with h\ and h2, as well as with uninforme d ;
iterativ e deepenin g search . Figur e 4.8 give s the averag e numbe r of node s expande d by each
strategy , and the effectiv e branchin g factor . The result s show that h2 is bette r than h\, and that ,
uninforme d searc h is muc h worse .
One migh t ask if h2 is always  bette r than h\. The answe r is yes. It is easy to see from the j
definition s of the two heuristic s that for any node n, h2(n) > h\(n).  We say that h2 dominate s h\.  •
Dominatio n translate s directl y into efficiency : A* usin g h2 will expan d fewe r nodes , on average ,
than A* usin g h\. We can show this usin g the followin g simpl e argument . Recal l the observatio n !j
on page 98 that ever y node with/(w ) </* will be expanded . This is the same as sayin g that every ;
node with h(n) </* — g(n) will be expanded . But becaus e h2 is at least as big as h\ for all nodes ,
every node that is expande d by A* searc h with h2 will also be expande d with h\, and h\ may also I
cause othe r node s to be expande d as well . Hence,  it is always  better  to use a  heuristic  function  1
with higher values,  as long  as it does  not overestimate.
Sectio n 4.2. Heuristi c Function s 103
d
2
4
6
8
10
12
14
16
18
20
22
24Searc h Cost
IDS
10
112
680
6384
4712 7
36440 4
347394 1­
­
­
­
­A*(A, )
6
13
20 ,
39
93
227
539
1301
3056
7276
18094
3913 5A*(/z 2)
6
12
18
25
39
73
113
211
363
676
1219
1641Effectiv e Branchin g Facto r
IDS
2.45
2.87
2.73
2.80
2.79
2.78
2.83­
­
­
­
­A*(Ai )
1.79
1.48
1.34
1.33
1.38
1.42
1.44
1.45
1.46
1.47
1.48
1.48A*(/z 2)
1.79
1.45
1.30
1.24
1.22
1.24
1.23
1.25
1.26
1.27
1.28
1.26
Figur e 4.8 Compariso n of the searc h cost s and effectiv e branchin g factor s for the
ITERATIVE­DEEPENING­SEARC H and A* algorithm s with h\, hi. Dat a are average d over 100
instance s of the 8­puzzle , for variou s solutio n lengths .
Inventin g heuristi c function s
We have seen that both h\ and /z2 are fairl y good heuristic s for the 8­puzzle , and that hi is better .
But we do not know how to inven t a heuristi c function . How migh t one have com e up with /z2?
Is it possibl e for a compute r to mechanicall y inven t such a heuristic ?
h\ and /z2 are estimate s to the remainin g path lengt h for the 8­puzzle , but they can also
be considere d to be perfectl y accurat e path length s for simplifie d version s of the puzzle . If
the rule s of the puzzl e were change d so that a tile coul d mov e anywhere , instea d of just to the
adjacen t empt y square , then h \ woul d accuratel y give the numbe r of steps to the shortes t solution .
Similarly , if a tile coul d mov e one squar e in any direction , even onto an occupie d square , then hi
woul d give the exac t numbe r of steps in the shortes t solution . A proble m with less restriction s on
RELAXE D PROBLE M th e operator s is calle d a relaxe d problem . It is often  the case  that  the cost  of an exact  solution
igg^ to  a relaxed  problem  is a good  heuristic  for the original  problem.
*"• I f a proble m definitio n is writte n dow n in a forma l language , it is possibl e to construc t
relaxe d problem s automatically.3 For example , if the 8­puzzl e operator s are describe d as
A tile can mov e from squar e A to squar e B if A is adjacen t to B and B is blank ,
we can generat e three relaxe d problem s by removin g one or more of the conditions :
(a) A tile can mov e from squar e A to squar e B if A is adjacen t to B.
(b) A tile can mov e from squar e A to squar e B if B is blank .
(c) A tile can mov e from squar e A to squar e B.
Recently , a progra m calle d ABSOLVE R was writte n that can generat e heuristic s automaticall y from
proble m definitions , usin g the "relaxe d problem " metho d and variou s othe r technique s (Prieditis ,
1993) . ABSOLVE R generate d a new heuristi c for the 8­puzzl e bette r than any existin g heuristic ,
and foun d the first usefu l heuristi c for the famou s Rubik' s cube puzzle .
3 In Chapter s 7 and 11, we will describ e forma l language s suitabl e for this task . For now , we will use English .
104 Chapte r 4. Informe d Searc h Method s
One proble m with generatin g new heuristi c function s is that one ofte n fails to get one
"clearl y best " heuristic . If a collectio n of admissibl e heuristic s h\...h m is availabl e for a
problem , and none of them dominate s any of the others , whic h shoul d we choose ? As it turn s
out, we need not mak e a choice . We can have the best of all worlds , by definin g
h(n) = max(hi(n),...,h m(n)).,
This composit e heuristi c uses whicheve r functio n is mos t accurat e on the node in question .
Becaus e the componen t heuristic s are admissible , h is also admissible . Furthermore , h dominate s
all of the individua l heuristic s from whic h it is composed .
Anothe r way to inven t a good heuristi c is to use statistica l information . Thi s can be
gathere d by runnin g a searc h over a numbe r of trainin g problems , such as the 100 randoml y
chose n 8­puzzl e configurations , and gatherin g statistics . For example , we migh t find that whe n
h2(n) = 14, it turn s out that 90% of the time the real distanc e to the goal is 18. Then whe n faced
with the "real " problem , we can use 18 as the valu e wheneve r hi(n)  report s 14. Of course , if we
use probabilisti c informatio n like this, we are givin g up on the guarante e of admissibility , but we
are likel y to expan d fewe r node s on average .
FEATURE S Ofte n it is possibl e to pick out feature s of a state that contribut e to its heuristi c evaluatio n
function , even if it is hard to say exactl y wha t the contributio n shoul d be. For example , the goal
in ches s is to checkmat e the opponent , and relevan t feature s includ e the numbe r of piece s of each
kind belongin g to each side , the numbe r of piece s that are attacke d by opponen t pieces , and so
on. Usually , the evaluatio n functio n is assume d to be a linea r combinatio n of the featur e values .
Even if we have no idea how importan t each featur e is, or even if a featur e is good or bad, it
is still possibl e to use a learnin g algorith m to acquir e reasonabl e coefficient s for each feature ,
as demonstrate d in Chapte r 18. In chess , for example , a progra m coul d learn that one' s own
queen shoul d have a large positiv e coefficient , wherea s an opponent' s pawn shoul d have a smal l
negativ e coefficient .
Anothe r facto r that we have not considere d so far is the searc h cost of actuall y runnin g the
heuristi c functio n on a node . We have been assumin g that the cost of computin g the heuristi c
functio n is abou t the same as the cost of expandin g a node , so that minimizin g the numbe r of
nodes expande d is a good thing . But if the heuristi c functio n is so comple x that computin g its
value for one node take s as long as expandin g hundred s of nodes , then we need to reconsider .
After all, it is easy to have a heuristi c that is perfectl y accurate—i f we allow the heuristi c to do,
say, a full breadth­firs t searc h "on the sly." Tha t woul d minimiz e the numbe r of node s expande d
by the real search , but it woul d not minimiz e the overal l searc h cost. A good heuristi c functio n
must be efficien t as well as accurate .
Heuristic s for constrain t satisfactio n problem s
In Sectio n 3.7, we examine d a class of problem s calle d constrain t satisfactio n problem s (CSPs) .
A constrain t satisfactio n proble m consist s of a set of variable s that can take on value s from a given
domain , togethe r with a set of constraint s that specif y propertie s of the solution . Sectio n 3.7
examine d uninforme d searc h method s for CSPs , mostl y variant s of depth­firs t search . Here ,
we exten d the analysi s by considerin g heuristic s for selectin g a variabl e to instantiat e and for
choosin g a valu e for the variable .
Sectio n 4.2. Heuristi c Function s 105
MOST ­
CONSTRAINED ­
VARIABL E
LEAST ­
^STRAINING ­To illustrat e the basi c idea , we will use the map­colorin g proble m show n in Figur e 4.9.
(The idea of map colorin g is to avoi d colorin g adjacen t countrie s with the same color. ) Suppos e
that we can use at mos t three color s (red, green , and blue) , and that we have chose n gree n for
countr y A and red for countr y B. Intuitively , it seem s obviou s that we shoul d color E next, becaus e
the only possibl e colo r for E is blue . All the othe r countrie s have a choic e of colors , and we
migh t mak e the wron g choic e and have to backtrack . In fact, once we have colore d E blue , then
we are force d to colo r C red and F green . Afte r that, colorin g D eithe r blue or red result s in a
solution . In othe r words , we have solve d the proble m with no searc h at all.
GREE N
RED
Figur e 4.9 A  map­colorin g proble m after the first two variable s (A and B) have been selected .
Whic h countr y shoul d we colo r next ?
This intuitiv e idea is calle d the most­constrained­variabl e heuristic . It is used with
forwar d checkin g (see Sectio n 3.7), whic h keep s track of whic h value s are still allowe d for each
variable , give n the choice s mad e so far. At each poin t in the search , the variabl e with the fewest
possibl e value s is chose n to have a valu e assigned . In this way, the branchin g facto r in the searc h
tends to be minimized . For example , whe n this heuristi c is used in solvin g «­queen s problems ,
the feasible  proble m size is increase d from aroun d 30 for forwar d checkin g to approximatel y
100. The most­constraining­variabl e heuristi c is similarl y effective . It attempt s to reduc e the
branchin g facto r on futur e choice s by assignin g a valu e to the variabl e that is involve d in the
larges t numbe r of constraint s on othe r unassigne d variables .
Once a variabl e has been selected , we still need to choos e a valu e for it. Suppos e that
we decid e to assig n a valu e to countr y C afte r A and B. One' s intuitio n is that red is a bette r
choic e than blue , becaus e it leave s mor e freedo m for futur e choices . Thi s intuitio n is the
least­constraining­valu e heuristic—choos e a valu e that rules out the smalles t numbe r of value s
in variable s connecte d to the curren t variabl e by constraints . Whe n applie d to the «­queen s
problem , it allow s problem s up to n=100 0 to be solved .
106 Chapte r 4. Informe d Searc h Method s
4.3 MEMOR Y BOUNDE D SEARC H
Despit e all the cleve r searc h algorithm s that have been invented , the fact remain s that some prob ­
lems are intrinsicall y difficult , by the natur e of the problem . Whe n we run up agains t these expo ­
nentiall y comple x problems , somethin g has to give . Figur e 3.12 show s that the first thing to  give
is usually  the available  memory.  In this section , we investigat e two algorithm s that are designe d
to conserv e memory . The first, IDA* , is a logica l extensio n of ITERATIVE­DEEPENING­SEARC H
to use heuristi c information . The second , SMA* , is simila r to A*, but restrict s the queu e size to
fit into the availabl e memory .
IDA'
6­ADMISSIBL EIterativ e deepenin g A* searc h (IDA* )
In Chapte r 3, we showe d that iterativ e deepenin g is a usefu l techniqu e for reducin g memor y
requirements . We can try the same trick again , turnin g A* searc h into iterativ e deepenin g A*, or
IDA* (see Figur e 4.10) . In this algorithm , each iteratio n is a depth­firs t search , just as in regula r
iterativ e deepening . The depth­firs t searc h is modifie d to use an/­cos t limi t rathe r than a dept h
limit . Thus , each iteratio n expand s all node s insid e the contou r for the current/­cost , peepin g
over the contou r to find out wher e the next contou r lies. (Se e the DPS­CONTOU R functio n in
Figur e 4.10. ) Onc e the searc h insid e a give n contou r has been completed , a new iteratio n is
starte d usin g a new/­cos t for the next contour .
IDA* is complet e and optima l with the same caveat s as A* search , but becaus e it is depth ­
first, it only require s spac e proportiona l to the longes t path that it explores . If b is the smalles t
operato r cost and/ * the optima l solutio n cost, then in the wors t case , IDA * will requir e bf*!8
node s of storage . In mos t cases , bd is a good estimat e of the storag e requirements .
The time complexit y of IDA * depend s strongl y on the numbe r of differen t value s that the
heuristi c functio n can take on. The Manhatta n distanc e heuristi c used in the 8­puzzl e take s on
one of a smal l numbe r of intege r values . Typically, / only increase s two or three time s alon g any
solutio n path. Thus , IDA * only goes throug h two or three iterations , and its efficienc y is simila r
to that of A*—i n fact, the last iteratio n of IDA * usuall y expand s roughl y the same numbe r of
node s as A*. Furthermore , becaus e IDA * does not need to inser t and delet e node s on a priorit y
queue , its overhea d per node can be muc h less than that for A*. Optima l solution s for man y
practica l problem s wer e first foun d by IDA* , whic h for severa l year s was the only optimal ,
memory­bounded , heuristi c algorithm .
Unfortunately , IDA * has difficult y in more comple x domains . In the travellin g salesperso n
problem , for example , the heuristi c valu e is differen t for every state . This mean s that each contou r
only include s one more state than the previou s contour . If A* expand s N nodes , IDA * will have
to go throug h N iteration s and will therefor e expan d 1 +2  + • • ­ + N = O(N2) nodes . Now if N is
too large for the computer' s memory , then N2 is almos t certainl y too long to wait !
One way aroun d this is to increas e the/­cos t limit by a fixed amoun t e on each iteration , so
that the total numbe r of iteration s is proportiona l to 1/e. This can reduc e the searc h cost , at the
expens e of returnin g solution s that can be wors e than optimal  by at mos t e. Suc h an algorith m is
calle d f ­admissible .
Sectio n 4.3. Memor y Bounde d Searc h 107
SMA'functio n IDA*(pmblem)  return s a solutio n sequenc e
inputs : problem,  a proble m
static : f­limit,  the current/ ­ COS T limit
mot, a node
root  <— MAKE­NODE(lNlTIAL­STATE[prt>W<?m] )
f­limit  <—f­  CoSJ(mot)
loop do
solution,  f­limit  — DFS­CONTOUR(root,  f­limit)
if solution  is non­nul l then retur n solution
it f­limit  = oo then retur n failure ; end
functio n DFS­CoNTOUR(norfe , f­limit)  return s a solutio n sequenc e and a new/ ­ COS T limi t
inputs : rcorfe , a node
f­limit,  the current/ ­ COS T limit
static : nexf­/ , the/ ­ COS T limit for the next contour , initiall y oc
if/­ CoSTfnode ] > f­limit  then retur n null,/ ­ CoST[node]
if GoAL­TEST[proWem](STATE[node] ) then return  node,  f­limit
for each node j in SucCESSORS(norfe ) do
solution,  new­f—  DFS­CoNTOUR(s,  f­limit)
if solution  is non­nul l then retur n solution,  f­limit
next­f<—  MiN(next­f,  new­f);  end
retur n null , next­f
Figur e 4.10 Th e IDA * (Iterativ e Deepenin g A*) searc h algorithm .
SMA * searc h
IDA*' s difficultie s in certai n proble m space s can be trace d to usin g too little  memory . Betwee n
iterations , it retain s only a singl e number , the current/­cos t limit . Becaus e it canno t remembe r
its history , IDA * is doome d to repea t it. This is doubl y true in state space s that are graph s rathe r
than trees (see Sectio n 3.6). IDA * can be modifie d to chec k the curren t path for repeate d states ,
but is unabl e to avoi d repeate d state s generate d by alternativ e paths .
In this section , we describ e the SMA * (Simplifie d Memory­Bounde d A*) algorithm , whic h
can mak e use of all availabl e memor y to carry out the search . Usin g more memor y can only
improv e searc h efficiency—on e coul d alway s ignor e the additiona l space , but usuall y it is bette r to
remembe r a node than to have to regenerat e it whe n needed . SMA * has the followin g properties :
• It will utiliz e whateve r memor y is mad e availabl e to it.
• It avoid s repeate d state s as far as its memor y allows .
• It is complet e if the availabl e memor y is sufficien t to store the shallowest  solutio n path .
• It is optima l if enoug h memor y is availabl e to store the shallowes t optima l solutio n path .
Otherwise , it return s the best solutio n that can be reache d with the availabl e memory .
• Whe n enoug h memor y is availabl e for the entir e searc h tree, the searc h is optimall y efficient .
108 Chapte r 4. Informe d Searc h Method s
The one unresolve d questio n is whethe r SMA * is alway s optimall y efficien t amon g all algorithm s
given the same heuristi c informatio n and the same memor y allocation .
The desig n of SMA * is simple , at least in overview . Whe n it need s to generat e a successo r
but has no memor y left, it will need to mak e spac e on the queue . To do this, it drop s a node from
the queue . Node s that are droppe d from the queu e in this way are calle d forgotte n nodes . It
prefer s to drop node s that are unpromising—tha t is, node s with high/­cost . To avoi d reexplorin g
subtree s that it has droppe d from memory , it retain s in the ancesto r node s informatio n abou t the
qualit y of the best path in the forgotte n subtree . In this way , it only  regenerate s the subtre e whe n
all other  paths  have been show n to look wors e than the path it has forgotten . Anothe r way of
sayin g this is that if all the descendant s of a node n are forgotten , then we will not know whic h
way to go from «, but we will still have an idea of how worthwhil e it is to go anywher e from n.
SMA * is best explaine d by an example , whic h is illustrate d in Figur e 4.11 . The top of the
figur e show s the searc h space . Each node is labelle d with g + h — f values , and the goal node s (D,
F, I, J) are show n in squares . The aim is to find the lowest­cos t goal node with enoug h memor y
for only three  nodes . The stage s of the searc h are show n in order , left to right , with each stag e
numbere d accordin g to the explanatio n that follows . Each node is labelle d with its current/­cost ,
whic h is continuousl y maintaine d to reflec t the least/­cos t of any of its descendants.4 Value s in
parenthese s show the valu e of the best forgotte n descendant . The algorith m proceed s as follows :
1. At each stage , one successo r is adde d to the deepes t lowest­/­cos t node that has som e
successor s not currentl y in the tree. The left child B is adde d to the root A.
2. Now/(A ) is still 12, so we add the righ t child G (/' = 13). Now that we have seen all the
childre n of A, we can updat e its/­cos t to the minimu m of its children , that is, 13. The
memor y is now full.
3. G is now designate d for expansion , but we mus t first drop a node to mak e room . We drop
the shallowes t highest­/­cos t leaf, that is, B. Whe n we have done this, we note that A's best
forgotte n descendan t has/ = 15, as show n in parentheses . We then add H, with/(// ) =18 .
Unfortunately , H is not a goal node , but the path to H uses up all the availabl e memory .
Hence , there is no way to find a solutio n throug h H, so we set/'(// ) = oo.
4. G is expande d again . We drop H, and add I, with/(7 ) = 24. Now we have seen both
successor s of G, with value s of oo and 24, so/(G ) become s 24. f(A)  become s 15, the
minimu m of 15 (forgotte n successo r value ) and 24. Notic e that I is a goal node , but it
migh t not be the best solutio n becaus e A's/­cos t is only 15.
5. A is once agai n the mos t promisin g node , so B is generate d for the secon d time . We have
found that the path throug h G was not so grea t after all.
6. C, the first successo r of B, is a nongoa l node at the maximu m depth , so/(C ) = oo.
7. To look at the secon d successor , D, we first drop C. Then/(D ) = 20, and this valu e is
inherite d by B and A.
8. Now the deepest , lowest­/­cos t node is D. D is therefor e selected , and becaus e it is a goal
node , the searc h terminates .
4 Value s compute d in this way are calle d backed­u p values . Because/(n ) is suppose d to be an estimat e of the least­cos t
solutio n path throug h n, and a solutio n path throug h n is boun d to go throug h one of n's descendants , backin g up the least
/­cos t amon g the descendant s is a soun d policy .
Sectio n 4.3. Memor y Bounde d Searc h 109
0+12=1 2
24+0=2 4
24+5=2 9
12'
15A
/A
15 1 313(15 )
15(15 )
24(oo )A15 2 420(24 )
20(oo )
Figur e 4.11 Progres s of an SMA * searc h with a memor y size of three nodes , on the state spac e
show n at the top. Eac h node is labelle d with its current  f'­cost.  Value s in parenthese s show the
value of the best forgotte n descendant .
no Chapte r 4. Informe d Searc h Method s
In this case , ther e is enoug h memor y for the shallowes t optima l solutio n path . If J had had a cost
of 19 instea d of 24, however , SMA * still woul d not have been able to find it becaus e the solutio n
path contain s four nodes . In this case , SMA * woul d have returne d D, whic h woul d be the best
reachable  solution . It is a simpl e matte r to have the algorith m signa l that the solutio n foun d may
not be optimal .
A roug h sketc h of SMA * is show n in Figur e 4.12 . In the actua l program , som e gymnastic s
are necessar y to deal with the fact that node s sometime s end up with some successor s in memor y
and some forgotten . Whe n we need to chec k for repeate d nodes , thing s get even more complicated .
SMA * is the mos t complicate d searc h algorith m we have seen yet.
functio n SMA*(pmblem)  return s a solutio n sequenc e
inputs : problem,  a proble m
static : Queue,  a queu e of node s ordere d by/­cos t
Queu e — MAKE­QUEUE({MAKE­NODE(lNITIAL­STATE[proWem])} )
loop do
if Queue  is empt y then retur n failur e
n — deepes t least­f­cos t node in Queue
if GoAL­TEST(n ) then retur n succes s
S — NEXT­SUCCESSOR(rc )
if s is not a goal and is at maximu m dept h then
f(.v) ­ oc
else
f(s) <­ MAX(f(n) , g(i)+h(j) )
if all of ;j's successor s have been generate d then
updat e n's/­cos t and thos e of its ancestor s if necessar y
if SUCCESSORS(» ) all in memor y then remov e n from Queue
if memor y is full then
delete shallowest , highest­f­cos t node in Queue
remov e it from its parent' s successo r list
inser t its paren t on Queue  if necessar y
inser t s on Queue
end
Figur e 4.12  Sketc h of the SMA * algorithm . Not e that numerou s detail s have been omitte d in
the interest s of clarity .
Give n a reasonabl e amoun t of memory , SMA * can solv e significantl y more difficul t prob ­
lems than A* withou t incurrin g significan t overhea d in term s of extra node s generated . It perform s
well on problem s with highl y connecte d state space s and real­value d heuristics , on whic h IDA *
has difficulty . On very hard problems , however , it will ofte n be the case that SMA * is force d
to continuall y switc h back and fort h betwee n a set of candidat e solutio n paths . The n the extr a
time require d for repeate d regeneratio n of the sam e node s mean s that problem s that woul d be
practicall y solvabl e by A*, give n unlimite d memory , becom e intractabl e for SMA* . Tha t is to
Sectio n 4.4. Iterativ e Improvemen t Algorithm s 111
say, memor y limitation s can mak e a proble m intractabl e from the poin t of view of computatio n
time. Althoug h there is no theor y to explai n the trade­of f betwee n time and memory , it seem s
that this is an inescapabl e problem . The only way out is to drop the optimalit y requirement .
4. A ITERATIV E IMPROVEMEN T ALGORITHM S
ITERATIV E
IMPROVEMEN T
HILL­CLIMBIN G
GRADIEN T DESCEN T
SIMULATE D
ANNEALIN GWe saw in Chapte r 3 that severa l well­know n problem s (for example , 8­queen s and VLS I layout )
have the propert y that the state descriptio n itself contain s all the informatio n neede d for a solution .
The path by whic h the solutio n is reache d is irrelevant . In such cases , iterativ e improvemen t
algorithm s often provid e the most practica l approach . For example , we start with all 8 queen s on
the board , or all wire s route d throug h particula r channels . Then , we migh t mov e queen s aroun d
tryin g to reduc e the numbe r of attacks ; or we migh t mov e a wire from one channe l to anothe r
to reduc e congestion . The  general idea  is to start  with a  complete  configuration  and to make
modifications  to improve its  quality.
The best way to understan d iterativ e improvemen t algorithm s is to conside r all the state s
laid out on the surfac e of a landscape . The heigh t of any poin t on the landscap e correspond s to
the evaluatio n functio n of the state at that poin t (Figur e 4.13) . The idea of iterativ e improvemen t
is to move aroun d the landscap e tryin g to find the highes t peaks , whic h are the optima l solutions .
Iterativ e improvemen t algorithm s usuall y keep track of only the curren t state , and do not look
ahead beyon d the immediat e neighbor s of that state . Thi s resemble s tryin g to find the top of
Moun t Everes t in a thick fog whil e sufferin g from amnesia . Nonetheless , sometime s iterativ e
improvemen t is the metho d of choic e for hard , practica l problems . We will see severa l application s
in later chapters , particularl y to neura l networ k learnin g in Chapte r 19.
Iterativ e improvemen t algorithm s divid e into two majo r classes . Hill­climbin g (or, alter ­
natively , gradien t descen t if we view the evaluatio n functio n as a cost rathe r than a quality )
algorithm s alway s try to mak e change s that improv e the curren t state . Simulate d annealin g
algorithm s can sometime s mak e change s that mak e thing s worse , at least temporarily .
Hill­climbin g searc h
The hill­climbin g searc h algorith m is show n in Figur e 4.14 . It is simpl y a loop that continuall y
move s in the directio n of increasin g value . The algorith m does not maintai n a searc h tree, so the
node data structur e need only recor d the state and its evaluation , whic h we denot e by VALUE .
One importan t refinemen t is that whe n there is more than one best successo r to choos e from , the
algorith m can selec t amon g them at random . This simpl e polic y has three well­know n drawbacks :
<) Loca l maxima : a local maximum , as oppose d to a globa l maximum , is a peak that is lowe r
than the highes t peak in the state space . Onc e on a loca l maximum , the algorith m will halt
even thoug h the solutio n may be far from satisfactory .
<> Plateaux : a platea u is an area of the state spac e wher e the evaluatio n functio n is essentiall y
flat. The searc h will conduc t a rando m walk .
112 Chapte r 4. Informe d Searc h Method s
evaluation
Figur e 4.13 Iterativ e improvemen t algorithm s try to find peak s on a surfac e of state s wher e
heigh t is define d by the evaluatio n function .
functio n HILL­CLIMBING ( problem)  return s a solutio n state
inputs : problem,  a proble m
static : current,  a node
next, a node
current  <— MAKE­NODE(lNlTIAL­STATE[proW(?m] )
loop do
next'—  a highest­value d successo r of current
if VALUE[next ] < VALUE[current ] then retur n current
current  *—next
end
Figur e 4.14 Th e hill­climbin g searc h algorithm .
RANDOM­RESTAR T
HILL­CLIMBIN G<) Ridges : a ridg e may have steepl y slopin g sides , so that the searc h reache s the top of the
ridge with ease, but the top may slop e only very gentl y towar d a peak . Unles s there happe n
to be operator s that mov e directl y alon g the top of the ridge , the searc h may oscillat e from
side to side, makin g little progress .
In each case , the algorith m reache s a poin t at whic h no progres s is bein g made . If this happens , an
obviou s thin g to do is start agai n from a differen t startin g point . Random­restar t hill­climbin g
does just this : it conduct s a serie s of hill­climbin g searche s from randoml y generate d initia l
states , runnin g each unti l it halts or make s no discernibl e progress . It save s the best resul t foun d
so far from any of the searches . It can use a fixe d numbe r of iterations , or can continu e unti l the
best save d resul t has not been improve d for a certai n numbe r of iterations .
ISectio n 4.4. Iterativ e Improvemen t Algorithm s 113
Clearly , if enoug h iteration s are allowed , random­restar t hill­climbin g will eventuall y find
the optima l solution . The succes s of hill­climbin g depend s very muc h on the shap e of the state ­
space "surface" : if there are only a few loca l maxima , random­restar t hill­climbin g will find a
good solutio n very quickly . A realisti c problem s has surfac e that look s mor e like a porcupine .
If the proble m is NP­complete , then in all likelihoo d we canno t do bette r than exponentia l time .
It follow s that there mus t be an exponentia l numbe r of local maxim a to get stuc k on. Usually ,
however , a reasonabl y good solutio n can be foun d after a smal l numbe r of iterations .
Simulate d annealin g
Instea d of startin g agai n randoml y whe n stuc k on a local maximum , we coul d allow the searc h
to take some downhil l steps to escap e the local maximum . This is roughl y the idea of simulate d
annealin g (Figur e 4.15) . The innermos t loop of simulate d annealin g is quit e simila r to hill­
climbing . Instea d of pickin g the best  move , however , it pick s a random  move . If the mov e
actuall y improve s the situation , it is alway s executed . Otherwise , the algorith m make s the mov e
with som e probabilit y less than 1. The probabilit y decrease s exponentiall y with the "badness " of
the move—th e amoun t AE by whic h the evaluatio n is worsened .
A secon d paramete r 7is also used to determin e the probability . At highe r value s of T, "bad "
move s are mor e likel y to be allowed . As T tend s to zero , they becom e more and more unlikely ,
until the algorith m behave s mor e or less like hill­climbing . The schedule  inpu t determine s the
value of T as a functio n of how man y cycle s alread y have been completed .
The reade r by now may have guesse d that the name "simulate d annealing " and the paramete r
name s A£ and T were chose n for a good reason . The algorith m was develope d from an explici t
analog y with annealing —the proces s of graduall y coolin g a liqui d until it freezes . The VALU E
functio n correspond s to the total energ y of the atom s in the material , and T correspond s to the
functio n SIMULATED­ANNEALiNCXproWewi , schedule)  return s a solutio n state
inputs : problem,  a proble m
schedule,  a mappin g from time to "temperature "
static : current,  a node
next,  a node
T, a "temperature " controllin g the probabilit y of downwar d steps
current^  MAKE­NODE(lNITIAL­STATE[/>roWeTO] )
for t <— 1 to oo do
T'<— schedule[t]
if T=0 then retur n current
next  <— a randoml y selecte d successo r of current
A.E«­ VALUEfnexf ] ­ VALUE[current]
if A£ > 0 then current^  next
else current^­  nexton\y  with probabilit y e^17
Figur e 4.15 Th e simulate d annealin g searc h algorithm .
114 Chapte r 4. Informe d Searc h Method s
temperature . The schedule  determine s the rate at whic h the temperatur e is lowered . Individua l
move s in the state spac e correspon d to rando m fluctuation s due to therma l noise . One can prov e
that if the temperatur e is lowere d sufficientl y slowly , the materia l will attai n a lowest­energ y
(perfectl y ordered ) configuration . Thi s correspond s to the statemen t that if schedule  lower s T
slowl y enough , the algorith m will,fin d a globa l optimum .
Simulate d annealin g was firs t used extensivel y to solv e VLS I layou t problem s in the
early 1980s . Sinc e then , it has been applie d widel y to factor y schedulin g and othe r large­scal e
optimizatio n tasks . In Exercis e 4.12 , you are aske d to compar e its performanc e to that of
random­restar t hill­climbin g on the «­queen s puzzle .
HEURISTI C REPAI R
MIN­CONFLICT SApplication s in constrain t satisfactio n problem s
Constrain t satisfactio n problem s (CSPs ) can be solve d by iterativ e improvemen t method s by
first assignin g value s to all the variables , and then applyin g modificatio n operator s to mov e the
configuratio n towar d a solution . Modificatio n operator s simpl y assig n a differen t valu e to a
variable . For example , in the 8­queen s problem , an initia l state has all eigh t queen s on the board ,
and an operato r move s a quee n from one squar e to another .
Algorithm s that solv e CSP s in this fashio n are ofte n calle d heuristi c repai r methods ,
becaus e they repai r inconsistencie s in the curren t configuration . In choosin g a new valu e for a
variable , the most obviou s heuristi c is to selec t the valu e that result s in the minimu m numbe r of
conflict s with othe r variables—th e min­conflict s heuristic . This is illustrate d in Figur e 4.16 for
an 8­queen s problem , whic h it solve s in two steps .
Min­conflict s is surprisingl y effectiv e for man y CSPs , and is able to solv e the million ­
queen s proble m in an averag e of less than 50 steps . It has also been used to schedul e observation s
for the Hubbl e spac e telescope , reducin g the time taken to schedul e a week of observation s from
three week s (!) to aroun d ten minutes . Min­conflict s is closel y relate d to the GSA T algorith m
describe d on page 182, whic h solve s problem s in propositiona l logic .
Figur e 4.16 A  two­ste p solutio n for an 8­queen s proble m usin g min­conflicts . At each stage , a
quee n is chose n for reassignmen t in its column . The numbe r of conflict s (in this case , the numbe r
of attackin g queens ) is show n in each square . The algorith m move s the quee n to the min­conflic t
square , breakin g ties randomly .
Sectio n 4.5. Summar y
4 j_ SUMMAR Y115
This chapte r has examine d the applicatio n of heuristic s to reduc e searc h costs . We have looke d
at numbe r of algorithm s that use heuristics , and foun d that optimalit y come s at a stiff price in
terms of searc h cost, even with good heuristics .
• Best­firs t searc h is just GENERAL­SEARC H wher e the minimum­cos t node s (accordin g to
some measure ) are expande d first .
• If we minimiz e the estimate d cost to reac h the goal , h(n),  we get greed y search . The
searc h time is usuall y decrease d compare d to an uninforme d algorithm , but the algorith m
is neithe r optima l nor complete .
• Minimizing/(n ) = g(ri)  + h(n) combine s the the advantage s of uniform­cos t searc h and
greed y search . If we handl e repeate d state s and guarante e that h(n) neve r overestimates ,
we get A* search .
• A* is complete , optimal , and optimall y efficien t amon g all optima l searc h algorithms . Its
space complexit y is still prohibitive .
• The time complexit y of heuristi c algorithm s depend s on the qualit y of the heuristi c function .
Good heuristic s can sometime s be constructe d by examinin g the proble m definitio n or by
generalizin g from experienc e with the proble m class .
• We can reduc e the spac e requiremen t of A* with memory­bounde d algorithm s such as
IDA* (iterativ e deepenin g A*) and SMA * (simplifie d memory­bounde d A*).
• Iterativ e improvemen t algorithm s keep only a singl e state in memory , but can get stuc k
on loca l maxima . Simulate d annealin g provide s a way to escap e loca l maxima , and is
complet e and optima l give n a long enoug h coolin g schedule .
• For constrain t satisfactio n problems , variabl e and valu e orderin g heuristic s can provid e
huge performanc e gains . Curren t algorithm s often^solv e very larg e problem s very quickly .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
The sam e pape r that introduce d the phras e "heuristi c search " (Newel l and Ernst , 1965 ) also
introduce d the concep t of an evaluatio n function , understoo d as an estimat e of the distanc e to
the goal , to guid e search ; this concep t was also propose d in the same year by Lin (1965) . Dora n
and Michi e (1966 ) did extensiv e experimenta l studie s of heuristi c searc h as applie d to a numbe r
of problems , especiall y the 8­puzzl e and the 15­puzzle . Althoug h Dora n and Michi e carrie d out
theoretica l analyse s of path lengt h and "penetrance " (the ratio of path lengt h to the total numbe r
of node s examine d so far) in heuristi c search , they appea r to have used their heuristi c function s as
the sole guidin g elemen t in the search , ignorin g the informatio n provide d by curren t path lengt h
that is used by uniform­cos t searc h and by A*.
The A* algorithm , incorporatin g the curren t path lengt h into heuristi c search , was develope d
by Hart , Nilsson , and Raphae l (1968) . Certai n subtl e technica l error s in the origina l presentatio n
116 Chapte r 4. Informe d Searc h Method s
of A* were correcte d in a later pape r (Har t et al., 1972) . An excellen t summar y of early wor k in
searc h is provide d by Nilsso n (1971) .
The origina l A* pape r introduce d a propert y of heuristic s calle d "consistency. " Th e
monotonicit y propert y of heuristic s was introduce d by Poh l (1977 ) as a simple r replacemen t for
the consistenc y property . Pear l (19,84 ) showe d that consistenc y and monotonicit y were equivalen t
properties . The pathma x equatio n was first used in A* searc h by Mero (1984) .
Pohl (1970 ; 1977 ) pioneere d the stud y of the relationshi p betwee n the error in A*'s heuristi c
functio n and the time complexit y of A*. The proo f that A* runs in linea r time if the erro r in the
heuristi c functio n is bounde d by a constan t can be foun d in Pohl (1977 ) and in Gaschni g (1979) .
Pearl (1984 ) strengthene d this resul t to allow a logarithmi c growt h in the error . The "effectiv e
branchin g factor " measur e of the efficienc y of heuristi c searc h was propose d by Nilsso n (1971) .
A* and othe r state­spac e searc h algorithm s are closel y relate d to the branch­and­bound
technique s that are widel y used in operation s research . An earl y surve y of branch­and­boun d
technique s is give n by Lawle r and Woo d (1966) . The semina l pape r by Held and Karp (1970 )
consider s the use of the minimum­spanning­tre e heuristi c (see Exercis e 4.11 ) for the travellin g
salesperso n problem , showin g how such admissibl e heuristic s can be derive d by examinin g
relaxe d problems . Generatio n of effectiv e new heuristic s by proble m relaxatio n was successfull y
implemente d by Priediti s (1993) , buildin g on earlie r wor k with Jack Mosto w (Mosto w and
Prieditis , 1989) . Th e probabilisti c interpretatio n of heuristic s was investigate d in dept h by
Hansso n and Maye r (1989) .
The relationship s betwee n state­spac e searc h and branch­and­boun d have been investigate d
in depth by Dana Nau,Lavee n Kanal , andVipinKumar(KumarandKanal , 1983;Naue?a/. , 1984 ;
Kuma r et al., 1988) . Martell i and Montanar i (1978 ) demonstrat e a connectio n betwee n dynami c
programmin g (see Chapte r 17) and certai n type s of state­spac e search . Kuma r and Kana l (1988 )
attemp t a "gran d unification " of heuristi c search , dynami c programming , and branch­and­boun d
technique s unde r the nam e of CDP—th e "composit e decisio n process. " Mor e materia l alon g
these lines is foun d in Kuma r (1991) .
There are a large numbe r of mino r and majo r variation s on the A* algorithm . Pohl (1973 )
propose d the use of dynamic  weighting,  whic h uses a weighte d sum/ w,(n) = wgg(n) + wi,h(n)  of
the curren t path lengt h and the heuristi c functio n as an evaluatio n function , rathe r than the simpl e
sum/(n ) = g(ri)  + h(n) used in A*, and dynamicall y adjust s the weight s wg and w/, accordin g
to certai n criteri a as the searc h progresses . Dynami c weightin g usuall y canno t guarante e that
optima l solution s will be found , as A* can, but unde r certai n circumstance s dynami c weightin g
can find solution s muc h mor e efficientl y than A*.
The most­constrained­variabl e heuristi c was introduce d by Bitne r and Reingol d (1975) , and
furthe r investigate d by Purdo m (1983) . Empirica l result s on the n­queen s proble m were obtaine d
by Ston e and Ston e (1986) . Brela z (1979 ) used the most­constraining­variabl e heuristi c as a tie­
breake r after applyin g the most­constrained­variabl e heuristic . The resultin g algorithm , despit e
its simplicity , is still the best metho d for yt­colorin g arbitrar y graphs . The least­constraining­valu e
heuristi c was develope d and analyze d in Haralic k and Ellio t (1980) . The min­conflict s heuristi c
was first propose d by Gu (1989) , and was develope d independentl y by Stev e Minto n (Minto n et
al., 1992) . Minto n explain s the remarkabl e performanc e of min­conflict s by modellin g the searc h
proces s as a rando m walk that is biase d to mov e towar d solutions . The effectivenes s of algorithm s
such as min­conflict s and the relate d GSA T algorith m (see Exercis e 6.15 ) in solvin g randoml y
Sectio n 4.5. Summar y 117
TABU SEARC H
PARALLE L SEARC Hgenerate d problem s almos t "instantaneously, " despit e the NP­completenes s of the associate d
proble m classes , has prompte d an intensiv e investigation . It turn s out that almos t all randoml y
generate d problem s are eithe r triviall y easy or have no solutions . Onl y if the parameter s of the
proble m generato r are set in a certai n narro w range , withi n whic h roughl y half of the problem s
are solvable , do we find "hard " proble m instance s (Kirkpatric k and Selman , 1994) .
Becaus e computer s in the late 1950 s and early 1960 s had at mos t a few thousan d word s
of main memory , memory­bounde d heuristi c searc h was an early researc h topic . Dora n and
Michie' s (1966 ) Grap h Traverser , one of the earlies t searc h programs , commit s to an operato r
after searchin g best­firs t up to the memor y limit . As with othe r "stage d search " algorithms ,
optimalit y canno t be ensure d becaus e unti l the best path has been foun d the optimalit y of the
first step remain s in doubt . IDA * was the first widel y used optimal , memory­bounded , heuristi c
searc h algorithm , and a large numbe r of variant s have been developed . The first reasonabl y publi c
paper dealin g specificall y with IDA * was Korf' s (1985b) , althoug h Korf credit s the initia l idea
to a persona l communicatio n from Jude a Pear l and also mention s Berline r and Goetsch' s (1984 )
technica l repor t describin g their implementatio n of IDA * concurrentl y with Korf' s own work .
A mor e comprehensiv e expositio n of IDA * can be foun d in Korf (1985a) . A thoroug h analysi s
of the efficienc y of IDA* , and its difficultie s with real­value d heuristics , appear s in Patric k
et al. (1992) . The SMA * algorith m describe d in the text was base d on an earlie r algorith m
called MA * (Chakrabart i et al, 1989) , and first appeare d in Russel l (1992) . The latte r pape r
also introduce d the "contour " representatio n of searc h spaces . Kaind l and Khorsan d (1994 )
apply SMA * to produc e a bidirectiona l searc h algorith m that exhibit s significan t performanc e
improvement s over previou s algorithms .
Three othe r memory­bounde d algorithm s deserv e mention . RBF S (Korf , 1993 ) and
IE (Russell , 1992 ) are two very simila r algorithm s that use linea r spac e and a simpl e recur ­
sive formulation , like IDA* , but retai n informatio n from prune d branche s to improv e efficiency .
Particularl y in tree­structure d searc h space s with discrete­value d heuristics , they appea r to be
competitiv e with SMA * becaus e of their reduce d overhead . RBF S is also able to carry out a
best­firs t searc h whe n the heuristi c is inadmissible . Finally , tabu searc h algorithm s (Glover ,
1989) , whic h maintai n a bounde d list of state s that mus t not be revisited , have prove d effectiv e
for optimizatio n problem s in operation s research .
Simulate d annealin g was first describe d by Kirkpatrick , Gelatt , and Vecch i (1983) , who
borrowe d the algorith m directl y from the Metropoli s algorith m used to simulat e comple x
system s in statistica l physic s (Metropoli s et al., 1953) . Simulate d annealin g is now a subfiel d in
itself , with severa l hundre d paper s publishe d ever y year .
The topi c of paralle l searc h algorithm s was not covere d in the chapter , partl y becaus e it
require s a length y discussio n of paralle l architectures . As paralle l computer s are becomin g widel y
available , paralle l searc h is becomin g an importan t topi c in both Al and theoretica l compute r
science . A brief introductio n to the Al literatur e can be foun d in Mahant i and Daniel s (1993) .
By far the mos t comprehensiv e sourc e on heuristi c searc h algorithm s is Pearl' s (1984 )
Heuristics  text. This book provide s especiall y good coverag e of the wide variet y of offshoot s and
variation s of A*, includin g rigorou s proof s of their forma l properties . Kana l and Kuma r (1988 )
presen t an antholog y of substantiv e and importan t article s on heuristi c search . New result s on
searc h algorithm s appea r regularl y in the journa l Artificial  Intelligence.
118 Chapte r 4. Informe d Searc h Method s
EXERCISE S
4.1 Suppos e that we run a greed y searc h algorith m with h(ri) = —g(n).  Wha t sort of searc h will
the greed y searc h emulate ?
4.2 Com e up with heuristic s for the followin g problems . Explai n whethe r they are admissible ,
and whethe r the state space s contai n loca l maxim a with your heuristic :
a. The genera l case of the chai n proble m (i.e., with an arbitrar y goal state ) from Exercis e 3.15 .
b. Algebrai c equatio n solvin g (e.g. , "solv e jr>'3 = 3 — xy for*") .
c. Path plannin g in the plan e with rectangula r obstacle s (see also Exercis e 4.13) .
d. Maz e problems , as define d in Chapte r 3.
4.3 Conside r the proble m of constructin g crosswor d puzzles : fittin g word s into a grid of
intersectin g horizonta l and vertica l squares . Assum e that a list of word s (i.e. , a dictionary ) is
provided , and that the task is to fill in the square s usin g any subse t of this list. Go throug h a
complet e goal and proble m formulatio n for this domain , and choos e a searc h strateg y to solve it.
Specif y the heuristi c function , if you thin k one is needed .
4.4 Sometime s there is no good evaluatio n functio n for a problem , but there is a good compariso n
method : a way to tell if one node is bette r than another , withou t assignin g numerica l value s to
either . Sho w that this is enoug h to do a best­firs t search . Wha t propertie s of best­firs t searc h do
we give up if we only have a compariso n method ?
4.5 W e saw on page 95 that the straight­lin e distanc e heuristi c is misleadin g on the proble m of
going from lasi to Fagaras . However , the heuristi c is perfec t on the opposit e problem : goin g from
Fagara s to lasi. Are there problem s for whic h the heuristi c is misleadin g in both directions ?
4.6 Inven t a heuristi c functio n for the 8­puzzl e that sometime s overestimates , and show how it
can lead to a suboptima l solutio n on a particula r problem .
4.7 Prov e that if the heuristi c functio n h obey s the triangl e inequality , then the/­cos t alon g any
path in the searc h tree is nondecreasing . (Th e triangl e inequalit y says that the sum of the costs
from A to B and B to C mus t not be less than the cost from A to C directly. )
4.8 W e showe d in the chapte r that an admissibl e heuristi c heuristi c (whe n combine d with
pathmax ) lead s to monotonicall y nondecreasing / value s alon g any path (i.e.,f(successor(n))  >
/(«)) . Doe s the implicatio n go the othe r way? That is, does monotonicit y in/ impl y admissibilit y
of the associate d hi
4.9 We gave two simple  heuristic s for the 8­puzzle : Manhatta n distanc e and misplace d tiles .
Severa l heuristic s in the literatur e purpor t to be bette r than eithe r of these . (See , for example , Nils ­
son (1971 ) for additiona l improvement s on Manhatta n distance , and Mosto w and Priediti s (1989 )
for heuristic s derive d by semimechanica l methods. ) Tes t thes e claim s by implementin g the
heuristic s and comparin g the performanc e of the resultin g algorithms .
4.10 Woul d a bidirectiona l A* searc h be a good idea ? Unde r wha t condition s woul d it be
applicable ? Describ e how the algorith m woul d work .
Sectio n 4.5. Summar y 11 9
i­sss£j?%!£ 4.1 1 Th e travellin g salesperso n proble m (TSP ) can be solve d usin g the minimu m spannin g tree
55?=^­ " (MST ) heuristic , whic h is used to estimat e the cost of completin g a tour , give n that a partia l tour
has alread y been constructed . The MST cost of a set of citie s is the smalles t sum of the link costs
of any tree that connect s all the cities .
a. Sho w how this heuristi c can be derive d usin g a relaxe d versio n of the TSP.
b. Sho w that the MST heuristi c dominate s straight­lin e distance .
c. Writ e a proble m generato r for instance s of the TSP wher e citie s are represente d by rando m
point s in the unit square .
d. Find an efficien t algorith m in the literatur e for constructin g the MST , and use it with an
admissibl e searc h algorith m to solv e instance s of the TSP.
4.12 Implemen t the n­queen s proble m and solv e it usin g hill­climbing , hill­climbin g with
rando m restart , and simulate d annealing . Measur e the searc h cost and percentag e of solve d
problem s usin g randoml y generate d start states . Grap h these agains t the difficult y of the problem s
(as measure d by the optima l solutio n length) . Commen t on your results .
4.13 Conside r the proble m of findin g the shortes t path betwee n two point s on a plan e that has
conve x polygona l obstacle s as show n in Figur e 4.17 . This is an idealizatio n of the proble m that
a robo t has to solv e to navigat e its way aroun d a crowde d environment .
a. Suppos e the state spac e consist s of all position s (x,y ) in the plane . How man y state s are
there ? How man y path s are there to the goal ?
b. Explai n briefl y why the shortes t path from one polygo n verte x to any othe r in the scen e
must consis t of straight­lin e segment s joinin g some of the vertice s of the polygons . Defin e
a good state spac e now . How large is this state space ?
c. Defin e the necessar y function s to implemen t the searc h problem , includin g a successo r
functio n that take s a verte x as inpu t and return s the set of vertice s that can be reache d in
a straigh t line from the give n vertex . (Do not forge t the neighbor s on the same polygon. )
Use the straight­lin e distanc e for the heuristi c function .
d. Implemen t any of the admissibl e algorithm s discusse d in the chapter . Kee p the implemen ­
tation of the algorith m completely  independen t of the specifi c domain . The n appl y it to
solve variou s proble m instance s in the domain .
4.14 In this question , we will turn the geometri c scen e from a simpl e data structur e into a
complet e environment . The n we will put the agen t in the environmen t and have it find its way to
the goal.
a. Implemen t an evaluatio n environmen t as describe d in Chapte r 2. The environmen t shoul d
behav e as follows :
• The percep t at each cycl e provide s the agen t with a list of the place s that it can see
from its curren t location . Each place is a positio n vecto r (wit h an x and y component )
givin g the coordinate s of the plac e relative to  the agent.  Thus , if the agen t is at (1,1 )
and there is a visibl e verte x at (7,3) , the percep t will includ e the positio n vecto r (6,2) .
(It therefor e will be up to the agen t to find out wher e it is! It can assum e that all
location s have a differen t "view." )
120 Chapte r 4. Informe d Searc h Method s
Figur e 4.17 A  scen e with polygona l obstacles .
• The actio n returne d by the agen t will be the vecto r describin g the straight­lin e path
it wishe s to follo w (thus , the relativ e coordinate s of the plac e it wishe s to go). If the
move does not bum p into anything , the environmen t shoul d "move " the agen t and
give it the percep t appropriat e to the next place ; otherwis e it stay s put. If the agen t
want s to mov e (0,0 ) and is at the goal , then the environmen t shoul d mov e the agen t
to a random vertex  in the  scene.  (Firs t pick a rando m polygon , and then a rando m
verte x on that polygon. )
b. Implemen t an agen t functio n that operate s in this environmen t as follows :
• If it does not know wher e it is, it will need to calculat e that from the percept .
• If it know s wher e it is and does not have a plan , it mus t calculat e a plan to get hom e
to the goal , usin g a searc h algorithm .
• Onc e it know s wher e it is and has a plan , it shoul d outpu t the appropriat e actio n from
the plan . It shoul d say (0,0) once it gets to the goal .
c. Sho w the environmen t and agen t operatin g together . The environmen t shoul d prin t out
some usefu l message s for the user showin g wha t is goin g on.
d. Modif y the environmen t so that 30% of the time the agen t end s up at an unintende d
destinatio n (chose n randoml y from the othe r visibl e vertice s if any, otherwis e no mov e at
all). This is a crud e mode l of the actua l motio n error s from whic h both human s and robot s
suffer . Modif y the agen t so that it alway s tries to get back on trac k whe n this happens .
What it shoul d do is this: if such an erro r is detected , first find out wher e it is and then
modif y its plan to first go back to wher e it was and resum e the old plan . Remembe r that
sometime s gettin g back to wher e it was may fail also ! Sho w an exampl e of the agen t
successfull y overcomin g two successiv e motio n error s and still reachin g the goal .
e. No w try two differen t recover y scheme s afte r an error : hea d for the closes t verte x on
the origina l route ; and repla n a rout e to the goa l from the new location . Compar e the
Sectio n 4.5. Summar y 12 1
performanc e of the thre e recover y scheme s usin g a variet y of exchang e rate s betwee n
searc h cost and path cost.
4.15 In this exercise , we will examin e hill­climbin g in the contex t of robo t navigation , usin g
the environmen t in Figur e 4.17 as an example .
a. Repea t Exercis e 4.14 usin g hill­climbing . Doe s you r agen t ever get stuc k on a loca l
maximum ? Is it possible  for it to get stuc k with conve x obstacles ?
b. Construc t a nonconve x polygona l environmen t in whic h the agen t gets stuck .
c. Modif y the hill­climbin g algorith m so that instea d of doin g a depth­ 1 searc h to decid e
wher e to go next , it does a depth­ & search . It shoul d find the best &­ste p path and do one
step alon g it, and then repea t the process .
d. Is there some k for whic h the new algorith m is guarantee d to escap e from local maxima ?
4.16 Prov e that IDA * return s optima l solution s wheneve r ther e is sufficien t memor y for the
longes t path with cost < /*. Coul d it be modifie d alon g the lines of SMA * to succee d even with
enoug h memor y for only the shortes t solutio n path ?
4.17 Compar e the performanc e of A*, SMA* , and IDA * on a set of randoml y generate d
problem s in the 8­puzzl e (wit h Manhatta n distance ) and TSP (wit h minimu m spannin g tree )
domains . Discus s your results . Wha t happen s to the performanc e of IDA * whe n a smal l rando m
numbe r is adde d to the heuristi c value s in the 8­puzzl e domain ?
4.18 Proof s of propertie s of SMA* :
a. SMA * abandon s path s that fill up memor y by themselve s but do not contai n a solution .
Show that withou t this check , SMA " will get stuc k in an infinit e loop wheneve r it does not
have enoug h memor y for the shortes t solutio n path .
b. Prov e that SMA * terminate s in a finit e spac e or if there is a finit e path to a goal. The proo f
will work by showin g that it can neve r generat e the same tree twice . This follow s from the
fact that betwee n any two expansion s of the same node , the node' s paren t mus t increas e its
/­cost . We will prov e this fact by a downwar d inductio n on the dept h of the node .
(i) Sho w that the propert y hold s for any node at dept h d = MAX.
(ii) Sho w that if it hold s for all node s at dept h d + 1 or more , it mus t also hold for all
node s at dept h d.
5GAM E PLAYIN G
In which  we examine  the problems  that arise  when  we try to plan  ahead in  a world
that includes  a hostile  agent.
5.1 INTRODUCTION : GAME S AS SEARC H PROBLEM S
Game s have engage d the intellectua l facultie s of humans—sometime s to an alarmin g degree—fo r
as long as civilizatio n has existed . Boar d game s such as ches s and Go are interestin g in part
becaus e they offe r pure , abstrac t competition , withou t the fuss and bothe r of musterin g up two
armie s and goin g to war. It is this abstractio n that make s gam e playin g an appealin g targe t of AI
research . The state of a gam e is easy to represent , and agents  are usuall y restricte d to a fairl y
small numbe r of well­define d actions . That  makes  game  playing  an idealization  of worlds  in
which  hostile agents act so  as to diminish  one's  well­being.  Less abstrac t games , such as croque t
or football , have not attracte d muc h interes t in the AI community .
Game playin g is also one of the oldes t areas of endeavo r in artificia l intelligence . In 1950 ,
almos t as soon as computer s becam e programmable , the first ches s program s were writte n by
Claud e Shanno n (the invento r of informatio n theory ) and by Alan Turing . Sinc e then , there has
been stead y progres s in the standar d of play, to the poin t wher e curren t system s can challeng e
the huma n worl d champio n withou t fear of gross embarrassment .
Early researcher s chos e ches s for severa l reasons . A chess­playin g compute r woul d be an
existenc e proo f of a machin e doin g somethin g though t to requir e intelligence . Furthermore , the
simplicit y of the rules , and the fact that the worl d state is fully accessibl e to the program1 mean s
that it is easy to represen t the gam e as a searc h throug h a spac e of possibl e gam e positions . The
compute r representatio n of the gam e actuall y can be correc t in ever y relevan t detail—unlik e the
representatio n of the proble m of fightin g a war, for example .
' Recal l from Chapte r 2 that accessibl e mean s that the agen t can perceiv e everythin g ther e is to kno w abou t the
environment . In gam e theory , ches s is calle d a gam e of perfec t information .
122
Section  5.2. Perfec t Decision s in Two­Perso n Game s 12 3
The presenc e of an opponen t make s the decisio n proble m somewha t mor e complicate d
than the searc h problem s discusse d in Chapte r 3. The opponen t introduce s uncertainty,  becaus e
one neve r know s wha t he or she is goin g to do. In essence , all game­playin g program s mus t deal
with the contingenc y proble m denne d in Chapte r 3. The uncertaint y is not like that introduced ,
say, by throwin g dice or by the weather . The opponen t will try as far as possibl e to mak e the least
benig n move , wherea s the dice arid the weathe r are assume d (perhap s wrongly! ) to be indifferen t
to the goal s of the agent . This complicatio n is discusse d in Sectio n 5.2.
But wha t make s game s really  differen t is that they are usuall y muc h too hard to solve .
Chess , for example , has an averag e branchin g facto r of abou t 35, and game s ofte n go to 50
move s by each player , so the searc h tree has abou t 35100 node s (althoug h there are "only " abou t
1040 different  lega l positions) . Tic­Tac­To e (nought s and crosses ) is borin g for adult s precisel y
becaus e it is easy to determin e the righ t move . The complexit y of game s introduce s a completel y
new kind of uncertaint y that we have not seen so far; the uncertaint y arise s not becaus e there is
missin g information , but becaus e one does not have time to calculat e the exac t consequence s of
any move . Instead , one has to mak e one' s best gues s base d on past experience , and act befor e
one is sure of what  actio n to take . In this respect , games  are much more  like the real world  than
I ;~ the  standard  search  problems  we have  looked  at so far.
Becaus e they usuall y have time limits , game s also penaliz e inefficienc y very severely .
Wherea s an implementatio n of A* searc h that is 10% less efficien t will simpl y cost a little bit
extra to run to completion , a ches s progra m that is 10% less effectiv e in usin g its availabl e time
probabl y will be beate n into the ground , othe r thing s bein g equal . Game­playin g researc h has
therefor e spawne d a numbe r of interestin g idea s on how to mak e the best use of time to reac h
good decisions , whe n reachin g optima l decision s is impossible . Thes e idea s shoul d be kept in
mind throughou t the rest of the book , becaus e the problem s of complexit y arise in ever y area of
AI. We will retur n to them in mor e detai l in Chapte r 16.
We begi n our discussio n by analyzin g how to find the theoreticall y best move . We then
look at technique s for choosin g a good mov e whe n time is limited . Prunin g allow s us to ignor e
portion s of the searc h tree that mak e no differenc e to the fina l choice , and heuristi c evaluatio n
function s allo w us to approximat e the true utilit y of a state withou t doin g a complet e search .
Sectio n 5.5 discusse s game s such as backgammo n that includ e an elemen t of chance . Finally , we
look at how state­of­the­ar t game­playin g program s succee d agains t stron g huma n opposition .
5^2PERFEC T DECISION S IN TWO­PERSO N GAME S
We will conside r the genera l case of a gam e with two players , who m we will call MA X and M1N ,
for reason s that will soon  becom e obvious . MA X move s first , and then they take turn s movin g
until the gam e is over . At the end of the game , point s are awarde d to the winnin g playe r (or
sometime s penaltie s are give n to the loser) . A gam e can be formall y define d as a kind of searc h
proble m with the followin g components :
• The initia l state , whic h include s the boar d positio n and an indicatio n of whos e mov e it is.
• A set of operators , whic h defin e the lega l move s that a playe r can make .
124 Chapte r 5. Gam e Playin g
TERMINA L TEST •  A termina l test, whic h determine s whe n the gam e is over . State s wher e the gam e has
ended are calle d termina l states .
PAYOF F FUNCTIO N •  A utilit y functio n (als o calle d a payof f function) , whic h give s a numeri c valu e for the
outcom e of a game . In chess , the outcom e is a win, loss, or draw , whic h we can represen t
by the value s +1, —1 , or 0. Som e game s hav e a wide r variet y of possibl e outcomes ; for
example , the payoff s in backgammo n rang e from +192 to —192 .
If this were a norma l searc h problem , then all MA X woul d have to do is searc h for a sequenc e
of move s that lead s to a termina l state that is a winne r (accordin g to the utilit y function) , and
then go ahea d and mak e the first mov e in the sequence . Unfortunately , MIN has somethin g to say
STRATEG Y abou t it. MA X therefor e mus t find a strateg y that will lead to a winnin g termina l state regardles s
of what MIN does , wher e the strateg y include s the correc t mov e for MAX for each possibl e mov e
by MIN . We will begi n by showin g how to find the optima l (or rational ) strategy , even thoug h
normall y we will not have enoug h time to comput e it.
Figur e 5.1 show s part of the searc h tree for the gam e of Tic­Tac­Toe . From the initia l state ,
MAX has a choic e of nine possibl e moves . Play alternate s betwee n MA X placin g x's and MIN
placin g o's unti l we reac h leaf node s correspondin g to termina l states : state s wher e one playe r
has three in a row or all the square s are filled . The numbe r on each leaf node indicate s the utilit y
value of the termina l state from the poin t of view of MAX ; high value s are assume d to be good
for MA X and bad for MIN (whic h is how the player s get their names) . It is MAX' S job to use the
searc h tree (particularl y the utilit y of termina l states ) to determin e the best move .
Even a simpl e gam e like Tic­Tac­To e is too comple x to show the whol e searc h tree, so we
will switc h to the absolutel y trivia l gam e in Figur e 5.2. The possibl e move s for MAX are labelle d
AI, AT, and AS. The possibl e replie s loA\  for MIN are AH , A\2, AH,  and so on. Thi s particula r
game ends after one mov e each by MA X and MIN . (In gam e parlance , we say this tree is one mov e
deep, consistin g of two half­move s or two ply. ) The utilitie s of the termina l state s in this gam e
range from 2 to 14.
The mininia x algorith m is designe d to determin e the optima l strateg y for MAX , and thus
to decid e wha t the best first mov e is. The algorith m consist s of five steps :
• Generat e the whol e gam e tree, all the way dow n to the termina l states .
• Appl y the utilit y functio n to each termina l state to get its value .
• Use the utilit y of the termina l state s to determin e the utilit y of the node s one leve l highe r
up in the searc h tree. Conside r the leftmos t three leaf node s in Figur e 5.2. In the V node
above it, MIN has the optio n to move , and the best MIN can do is choos e AI i, whic h leads
to the minima l outcome , 3. Thus , even thoug h the utilit y functio n is not immediatel y
applicabl e to this V node , we can assig n it the utilit y valu e 3, unde r the assumptio n that
MIN will do the righ t thing . By simila r reasoning , the othe r two V node s are assigne d the
utilit y valu e 2.
• Continu e backin g up the value s from the leaf node s towar d the root, one laye r at a time .
• Eventually , the backed­u p value s reac h the top of the tree; at that point , MA X choose s the
move that leads to the highes t value . In the topmos t A node of Figur e 5.2, MA X has a choic e
of three move s that will lead to state s with utilit y 3, 2, and 2, respectively . Thus , MAX' s
MINIMAXDECISIO N bes t openin g mov e is A] \. Thi s is calle d the mininia x decision , becaus e it maximize s the
utilit y unde r the assumptio n that the opponen t will play perfectl y to minimiz e it.PLY
MINIMA X
Sectio n 5.2. Perfec t Decision s in Two­Perso n Game s 125
MAX (X)
MIN (O)
MAX (X)
MIN (O)
TERMINA L
Utilit yX
Xo
Xo
1
Xoooy
^?
X
^
X
X
­1——
=\
==\
^=^^^
X
^^
Xo
^=
X
Xo
==;
X
o
Xoo
X~~~~^
X
X
0X
o
_
^^^
V:^
X
X
0
=^^^ / \^^^^^r^ ^
X
XX X
XX X
r~^ ^
r^^­ —5E .. .
ym^­—^
0 X
X
0 O
+1
Figur e 5.1 A  (partial ) searc h tree for the gam e of Tic­Tac­Toe . The top node is the initia l
state, and MA X move s first , placin g an x in som e square . We show part of the searc h tree, givin g
alternatin g move s by MIN (o) and MAX , unti l we eventuall y reac h termina l states , whic h can be
assigne d utilitie s accordin g to the rule s of the game .
MAX
MIN
Figur e 5.2 A  two­pl y gam e tree as generate d by the minima x algorithm . The A node s ars
move s by MA X and the V node s UK move s by MIN . The termina l node s show the utilit y valu e for
MAX compute d by the utilit y functio n (i.e. , by the rules of the game) , wherea s the utilitie s of the
other node s are compute d by the minima x algorith m from the utilitie s of their successors . MAX' S
best mov e is A\, andMlN' s best reply is A\\.
126 Chapte r 5. Gam e Playin g
Figur e 5.3 show s a mor e forma l descriptio n of the minima x algorithm . Th e top leve l func ­
tion, MINIMAX­DECISION , select s from the availabl e moves , whic h are evaluate d in turn by the
MINIMAX­VALU E function .
If the maximu m dept h of the tree is m, and there are b lega l move s at each point , then
the time complexit y of the minirna x algorith m is O(bm). The algorith m is a depth­firs t searc h
(althoug h here the implementatio n is throug h recursio n rathe r than usin g a queu e of nodes) , so
its spac e requirement s are only linea r in m and b. For real games , of course , the time cost is
totall y impractical , but this algorith m serve s as the basi s for more realisti c method s and for the
mathematica l analysi s of games .
functio n MiNlMAX­DECisiON(gam<? ) return s an operator
for each op in OPERATORSfgame ] do
VALUE[op ] — MINIMAX­VALUE(APPLY(op , game),  game)
end
retur n the op with the highes t VALUE[op ]
functio n MiNlMAX­VALUE(5fafe , game)  return s a utility value
if TERMiNAL­TEST[gamel(.stofc ) then
retur n VT]UT^[game](state)
else if MA X is to mov e in state  then
retur n the highes t MIN I MAX­VALU E of SuccESSORS(.sto/e )
else
retur n the lowes t MINIMAX­VALU E of SucCESSORS(.sfate )
Figur e 5.3 A n algorith m for calculatin g minima x decisions . It return s the operato r that
correspondin g to the best possibl e move , that is, the mov e that lead s to the outcom e with the
best utility , unde r the assumptio n that the opponen t play s to minimiz e utility . Th e functio n
MINIMAX­VALU E goes throug h the whol e gam e tree, all the way to the leaves , to determin e the
backed­u p valu e of a state .
5.3 IMPERFEC T DECISION S
The minima x algorith m assume s that the progra m has time to searc h all the way to termina l
states , whic h is usuall y not practical . Shannon' s origina l pape r on ches s propose d that instea d of
going all the way to termina l state s and usin g the utilit y function , the progra m shoul d cut off the
searc h earlie r and appl y a heuristi c evaluatio n functio n to the leave s of the tree. In othe r words ,
the suggestio n is to alter minima x in two ways : the utilit y functio n is replace d by an evaluatio n
functio n EVAL , and the termina l test is replace d by a cutof f test CUTOFF­TEST .
Sectio n 5.3. Imperfec t Decision s 127
Evaluatio n function s
An evaluatio n functio n return s an estimate  of the expecte d utilit y of the gam e from a give n
position . The idea was not new whe n Shanno n propose d it. For centuries , ches s player s (and , of
course , aficionado s of other games ) have develope d way s of judgin g the winnin g chance s of each
side base d on easil y calculate d feature s of a position . For example , introductor y ches s book s
MATERIA L VALU E giv e an approximat e materia l valu e for each piece : each paw n is wort h 1, a knigh t or bisho p
is wort h 3, a rook 5, and the queen  9. Othe r feature s such as "goo d paw n structure " and "kin g
safety " migh t be wort h half a pawn , say. All othe r thing s bein g equal , a side that has a secur e
materia l advantag e of a paw n or more will probabl y win the game , and a 3­poin t advantag e is
sufficien t for near­certai n victory . Figur e 5.4 show s four position s with their evaluations .
It shoul d be clear that the performanc e of a game­playin g progra m is extremel y dependen t
on the qualit y of its evaluatio n function . If it is inaccurate , then it will guid e the progra m towar d
position s that are apparentl y "good, " but in fact disastrous . How exactl y do we measur e quality ?
First, the evaluatio n functio n must agree with the utilit y functio n on termina l states . Second ,
it mus t not take too long ! (As mentione d in Chapte r 4, if we did not limi t its complexity , then it
could call minima x as a subroutin e and calculat e the exac t valu e of the position. ) Hence , there is
a trade­of f betwee n the accurac y of the evaluatio n functio n and its time cost. Third , an evaluatio n
functio n shoul d accuratel y reflec t the actua l chance s of winning .
One migh t well wonde r abou t the phras e "chance s of winning. " Afte r all, ches s is not a
game of chance . But if we have cut off the searc h at a particula r nontermina l state , then we do not
know wha t will happe n in subsequen t moves . For concreteness , assum e the evaluatio n functio n
count s only materia l value . Then , in the openin g position , the evaluatio n is 0, becaus e both sides
have the same material . All the position s up to the first captur e will also have an evaluatio n of
0. If MAX manage s to captur e a bisho p withou t losin g a piece , then the resultin g positio n will
have an evaluatio n valu e of 3. The importan t poin t is that a give n evaluatio n valu e cover s man y
differen t positions—al l the position s wher e MA X is up by a bisho p are groupe d togethe r into
a category  that is give n the labe l "3." Now we can see how the word "chance " make s sense :
the evaluatio n functio n shoul d reflec t the chanc e that a positio n chose n at rando m from such a
categor y lead s to a win (or draw or loss) , base d on previou s experience.2
This suggest s that the evaluatio n functio n shoul d be specifie d by the rules of probability :
if positio n A has a 100% chanc e of winning , it shoul d have the evaluatio n 1.00, and if position s
has a 50% chanc e of winning , 25% of losing , and 25% of bein g a draw , its evaluatio n shoul d be
+1 x .50 + ­1 x .25 + 0 x .25 = .25. But in fact, we need not be this precise ; the actua l numeri c
value s of the evaluatio n functio n are not important , as long as A is rated highe r than B.
The materia l advantag e evaluatio n functio n assume s that the valu e of a piece can be judge d
independentl y of the othe r piece s presen t on the board . This kind of evaluatio n functio n is calle d
a weighte d linea r function , becaus e it can be expresse d as
W]/i + W2/ 2 + • • •  + Wnf n
wher e the w's are the weights , and the/' s are the feature s of the particula r position . The w's
woul d be the value s of the piece s (1 for pawn , 3 for bishop , etc.) , and the/' s woul d be the number s
2 Technique s for automaticall y constructin g evaluatio n function s with this propert y are discusse d in Chapte r 18. In
assessin g the valu e of a category , more  normall y occurrin g position s shoul d be give n mor e weight .
128 Chapte r 5. Gam e Playin g
(a) Whit e to mov e
Fairl y even(b) Blac k to mov e
Whit e slightl y bette r
(c) Whit e to mov e
Black winnin g(d) Blac k to mov e
Whit e abou t to lose
Figur e 5.4 Som e ches s position s and their evaluations .
of each kind of piece on the board . Now we can see wher e the particula r piece value s com e from :
they give the best approximatio n to the likelihoo d of winnin g in the individua l categories .
Most game­playin g program s use a  linea r evaluatio n function , althoug h recentl y nonlinea r
function s have had a good deal of success . (Chapte r 19 give s an exampl e of a neura l networ k that
is traine d to learn a nonlinea r evaluatio n functio n for backgammon. ) In constructin g the linea r
formula , one has to first pick the features , and then adjus t the weight s unti l the progra m play s
well. The latte r task can be automate d by havin g the progra m play lots of game s agains t itself ,
but at the moment , no one has a good idea of how to pick good feature s automatically .
Sectio n 5.4. Alpha­Bet a Prunin g 129
QUIESCEN T
QUIESCENC E
SEARC H
HORIZO N PROBLE MCuttin g off searc h
The mos t straightforwar d approac h to controllin g the amoun t of searc h is to set a fixe d dept h
limit , so that the cutof f test succeed s for all node s at or belo w dept h d. The dept h is chose n so
that the amoun t of time used will not excee d wha t the rule s of the gam e allow . A slightl y more
robus t approac h is to appl y iterativ e deepening , as define d in Chapte r 3. Whe n time runs out, the
progra m return s the mov e selecte d by the deepes t complete d search .
These approache s can have som e disastrou s consequence s becaus e of the approximat e
natur e of the evaluatio n function . Conside r agai n the simpl e evaluatio n functio n for ches s base d
on materia l advantage . Suppos e the progra m searche s to the dept h limit , reachin g the positio n
show n in Figur e 5.4(d) . Accordin g to the materia l function , whit e is ahea d by a knigh t and
therefor e almos t certai n to win . However , becaus e it is black' s move , white' s quee n is lost
becaus e the blac k knigh t can captur e it withou t any recompens e for white . Thus , in realit y the
positio n is won for black , but this can only be seen by lookin g ahea d one more ply.
Obviously , a more sophisticate d cutof f test is needed . The evaluatio n functio n shoul d only
be applie d to position s that are quiescent , that is, unlikel y to exhibi t wild swing s in valu e in the
near future . In chess , for example , position s in whic h favorabl e capture s can be mad e are not
quiescen t for an evaluatio n functio n that just count s material . Nonquiescen t position s can be
expande d furthe r unti l quiescen t position s are reached . Thi s extr a searc h is calle d a quiescenc e
search ; sometime s it is restricte d to conside r only certai n type s of moves , such as captur e moves ,
that will quickl y resolv e the uncertaintie s in the position .
The horizo n proble m is mor e difficul t to eliminate . It arise s whe n the progra m is facin g
a mov e by the opponen t that cause s seriou s damag e and is ultimatel y unavoidable . Conside r the
chess gam e in Figur e 5.5. Blac k is slightl y ahea d in material , but if whit e can advanc e its paw n
from the sevent h row to the eighth , it will becom e a quee n and be an easy win for white . Blac k can
forestal l this for a doze n or so ply by checkin g whit e with the rook , but inevitabl y the paw n will
becom e a queen . The proble m with fixed­dept h searc h is that it believe s that these stallin g move s
have avoide d the queenin g move—w e say that the stallin g move s push the inevitabl e queenin g
move "ove r the horizon " to a plac e wher e it canno t be detected . At present , no genera l solutio n
has been foun d for the horizo n problem .
54ALPHA­BET A PRUNIN G
Let us assum e we have implemente d a minima x searc h with a reasonabl e evaluatio n functio n for
chess , and a reasonabl e cutof f test with a quiescenc e search . Wit h a well­writte n progra m on an
ordinar y computer , one can probabl y searc h abou t 1000 position s a second . How well will our
progra m play ? In tournamen t chess , one gets abou t 150 second s per move , so we can look at
150,00 0 positions . In chess , the branchin g facto r is abou t 35, so our progra m will be able to look
ahead only three or four ply, and will play at the level of a complet e novice ! Eve n averag e huma n
player s can mak e plan s six or eigh t ply ahead , so our progra m will be easil y fooled .
Fortunately , it is possibl e to comput e the correc t minima x decisio n withou t lookin g at ever y
node in the searc h tree. The proces s of eliminatin g a branc h of the searc h tree from consideratio n
130 Chapte r 5. Gam e Playin g
PRUNIN G
ALPHA­BET A
PRUNIN GBlack to mov e
Figur e 5.5 Th e horizo n problem . A serie s of check s by the blac k rook force s the inevitabl e
queenin g mov e by whit e "ove r the horizon " and make s this positio n look like a sligh t advantag e
for black , whe n it is reall y a sure win for white .
withou t examinin g it is calle d prunin g the searc h tree. The particula r techniqu e we will examin e
is calle d alpha­bet a pruning . Whe n applie d to a standar d minima x tree, it return s the same mov e
as minima x would , but prune s away branche s that canno t possibl y influenc e the final decision .
Conside r the two­pl y gam e tree from Figur e 5.2, show n agai n in Figur e 5.6. The searc h
proceed s as before : A\, then A\\, A\2,  A[$,  and the node unde r AI gets minima x valu e 3. Now
we follo w A2, and AI\, whic h has valu e 2. At this point , we realiz e that if MAX play s AI, MlNha s
the optio n of reachin g a positio n wort h 2, and some othe r option s besides . Therefore , we can say
alread y that mov e A2 is wort h at most  2 to MAX . Becaus e we alread y know that mov e A \ is wort h
3, there is no poin t at lookin g furthe r unde r AI­ In othe r words , we can prun e the searc h tree at
this poin t and be confiden t that the prunin g will have no effec t on the outcome .
The genera l principl e is this. Conside r a nod e n somewher e in the tree (see Figur e 5.7),
such that Playe r has a choic e of movin g to that node . If Playe r has a bette r choic e ra eithe r at the
paren t node of n, or at any choic e poin t furthe r up, then n will never  be reached in  actual  play.
So once we have foun d out enoug h abou t n (by examinin g some of its descendants ) to reach this
conclusion , we can prun e it.
Remembe r that minima x searc h is depth­first , so at any one time we just have to conside r
the node s alon g a singl e path in the tree. Let a be the valu e of the best choic e we have foun d so
far at any choic e poin t alon g the path for MAX , and /? be the valu e of the best (i.e., lowest­value )
choic e we have foun d so far at any choic e poin t alon g the path for MTN . Alpha­bet a searc h update s
the valu e of a and 8 as it goes along , and prune s a subtre e (i.e., terminate s the recursiv e call) as
soon as it is know n to be wors e than the curren t a or 13 value .
The algorith m descriptio n in Figur e 5.8 is divide d into a MAX­VALU E functio n and a
MlN­VALU E function . Thes e appl y to MA X node s and MIN nodes , respectively , but each does the
same thing : retur n the minima x valu e of the node , excep t for node s that are to be prune d (in
Sectio n 5.4. Alpha­Bet a Prunin g 131
MAX
MIN
12 8  2 14 5
Figur e 5.6 Th e two­pl y gam e tree as generate d by alpha­beta .
Playe r
Opponen t \m
Playe r
Opponen t
Figur e 5.7 Alpha­bet a pruning : the genera l case . If m is bette r than n for Player , we will neve r
get to n in play .
whic h case the returne d valu e is ignore d anyway) . The alpha­bet a searc h functio n itsel f is just a
copy of the MAX­VALU E functio n with extra code to remembe r and retur n the best mov e found .
Effectivenes s of alpha­bet a prunin g
The effectivenes s of alpha­bet a depend s on the orderin g in whic h the successor s are examined .
This is clear from Figur e 5.6, wher e we coul d not prun e A 3 at all becaus e AJI and A^ (the wors t
move s from the poin t of view of MIN ) were generate d first . This suggest s it migh t be worthwhil e
to try to examin e first the successor s that are likel y to be best.
If we assum e that this can be done,3 then it turn s out that alpha­bet a only need s to examin e
O(bd/2) node s to pick the best move , instea d of O(b'')  with minimax . This mean s that the effectiv e
branchin g facto r is \/b instea d of b—for chess , 6 instea d of 35. Put anothe r way , this mean s
Obviously , it canno t be done perfectly , otherwis e the orderin g functio n coul d be used to play a perfec t game !
132 Chapte r 5. Gam e Playin g
functio n MAX­VA.LVE(state,  game,  a, ft) return s the minima x valu e of state
inputs : state,  curren t state in gam e
game,  gam e descriptio n
a, the best scor e for MA X alon g the path to state
ft, the best scor e for MIN alon g the path to state
if CuTOFF­TEST(state ) then retur n EvAL(state )
for each s in SuccESSORsOtafe ) do
a <— MAX( Q , MlN­VALUE(.« , game,  a, ft))
if a > ft then retur n ft
end
retur n a
functio n MiN­VALUE(.vfate , game,  a, ft) return s the minima x valu e of state
if CuTOFF­TEST(stafe ) then retur n Ev&L(state)
for each s in SucCESSORS(.sta?e ) do
ft — MlN ( ft, MAX ­VALUER , game,  a, ft))
if ft < a then retur n a
end
retur n ft
Figur e 5.8 Th e alpha­bet a searc h algorithm ,
minimax , but prune s the searc h tree.It does the sam e computatio n as a norma l
TREE MODE Lthat alpha­bet a can look ahea d twic e as far as minima x for the same cost. Thus , by generatin g
150,00 0 node s in the time allotment , a progra m can look ahea d eigh t ply instea d of four . By
thinkin g carefull y abou t which  computations  actually  affect  the decision,  we are able to transfor m
a progra m from a novic e into an expert .
The effectivenes s of alpha­bet a prunin g was first analyze d in dept h by Knut h and Moor e
(1975) . As wel l as the best case describe d in the previou s paragraph , they analyze d the
case in whic h successor s are ordere d randomly . It turn s out that the asymptoti c complexit y
is O((b/\ogb)d), whic h seem s rathe r disma l becaus e the effectiv e branchin g facto r b/logb  is not
much less than b itself . On the othe r hand , the asymptoti c formul a is only accurat e for b > 1000
or so—i n othe r words , not for any game s we can reasonabl y play usin g thes e techniques . For
reasonabl e b, the total numbe r of node s examine d will be roughl y O(b3dl4). In practice , a fairly
simpl e orderin g functio n (suc h as tryin g capture s first , then threats , then forwar d moves , then
backwar d moves ) gets you fairl y clos e to the best­cas e resul t rathe r than the rando m result .
Anothe r popula r approac h is to do an iterativ e deepenin g search , and use the backed­u p value s
from one iteratio n to determin e the orderin g of successor s in the next iteration .
It is also wort h notin g that all complexit y result s on game s (and , in fact, on searc h problem s
in general ) have to assum e an idealize d tree mode l in orde r to obtai n their results . For example ,
the mode l used for the alpha­bet a resul t in the previou s paragrap h assume s that all node s have the
same branchin g facto r b; that all path s reac h the fixe d dept h limit d; and that the leaf evaluation s
Sectio n 5.5. Game s That Includ e an Elemen t of Chanc e 133
are randoml y distribute d acros s the last laye r of the tree. This last assumptio n is seriousl y flawed :
for example , if a mov e highe r up the tree is a disastrou s blunder , then mos t of its descendant s
will look bad for the playe r who mad e the blunder . The valu e of a node is therefor e likel y to
be highl y correlate d with the value s of its siblings . The amoun t of correlatio n depend s very
much on the particula r gam e and indee d the particula r positio n at the root . Hence , ther e is
an unavoidabl e componen t of empirical  science  involve d in game­playin g research , eludin g the
powe r of mathematica l analysis .
5.5 GAME S THA T INCLUD E AN ELEMEN T OF CHANC E
In real life, unlik e chess , there are man y unpredictabl e externa l event s that put us into unforesee n
situations . Man y game s mirro r this unpredictabilit y by includin g a rando m elemen t such as
throwin g dice. In this way, they take us a step neare r reality , and it is worthwhil e to see how this
affect s the decision­makin g process .
Backgammo n is a typica l gam e that combine s luck and skill . Dic e are rolle d at the
beginnin g of a player' s turn to determin e the set of legal move s that is availabl e to the player . In
the backgammo n positio n of Figur e 5.9, whit e has rolle d a 6­5, and has four possibl e moves .
Althoug h whit e know s wha t his or her own lega l move s are, whit e does not know wha t
black is goin g to roll, and thus does not know wha t black' s legal move s will be. Tha t mean s whit e
canno t construc t a complet e gam e tree of the sort we saw in ches s and Tic­Tac­Toe . A gam e tree
CHANC E NODE S i n backgammo n mus t includ e chanc e node s in additio n to MAX and MIN nodes . Chanc e node s
are show n as circle s in Figur e 5.10 . The branche s leadin g from each chanc e node denot e the
possibl e dice rolls , and each is labelle d with the roll and the chanc e that it will occur . Ther e are
36 way s to roll two dice, each equall y likely ; but becaus e a 6­5 is the same as a 5­6, there are
only 21 distinc t rolls . The six double s (1­1 throug h 6­6) have a 1/36 chanc e of comin g up, the
other fiftee n distinc t rolls a 1/18 chance .
The next step is to understan d how to mak e correc t decisions . Obviously , we still wan t
to pick the mov e from A\,... , An that lead s to the best position . However , each of the possibl e
position s no longe r has a definit e minima x valu e (whic h in deterministi c game s was the utilit y
EXPECTE D VALU E o f the leaf reache d by best play) . Instead , we can only calculat e an averag e or expecte d value ,
wher e the averag e is taken over all the possibl e dice rolls that coul d occur .
It is straightforwar d to calculat e expecte d value s of nodes . For termina l nodes , we use the
utilit y function , just like in deterministi c games . Goin g one step up in the searc h tree, we hit a
chanc e node . In Figur e 5.10 , the chanc e node s are circles;  we will conside r the one labelle d C.
Let di be a possibl e dice roll, and P(d t) be the chanc e or probabilit y of obtainin g that roll. For
each dice roll, we calculat e the utilit y of the best mov e for MIN , and then add up the utilities ,
weighte d by the chanc e that the particula r dice roll is obtained . If we let S(C,  di) denot e the set
of position s generate d by applyin g the lega l move s for dice roll P(d t) to the positio n at C, then
EXPECTMA X VALU E w e can calculat e the so­calle d expectima x valu e of C usin g the formul a
expectimax(C)  =
134 Chapte r 5. Gam e Playin g
8 9 10 11 12
2524 23 22 21 20 1918 17 16 15 14 13
Figur e 5.9 A  typica l backgammo n position . The aim of the gam e is to move  all one' s piece s
off the board . Whit e move s clockwis e towar d 25, and black move s counterclockwis e towar d 0. A
piece can mov e to any positio n excep t one wher e there are two or more of the opponent' s pieces .
If it move s to a positio n with one opponen t piece , that piece is capture d and has to start its journe y
again from the beginning . In this position , whit e has just rolle d 6­5 and has four legal moves :
(5­10,5­11) , (5­11,19­24) , (5­10,10­16) , and (5­11,11­16) .
MAX
DICE
MIN
DICE
MAX
TERMINA L 2 ­1 1 ­1 1
Figur e 5.10 Schemati c gam e tree for a backgammo n position .
Sectio n Game s That Includ e an Elemen t of Chanc e 135
This give s us the expecte d utilit y of the positio n at C assumin g best play . Goin g up one mor e
level to the MIN node s (y in Figur e 5.10) , we can now appl y the norma l minimax­valu e formula ,
becaus e we have assigne d utilit y value s to all the chanc e nodes . We then mov e up to chanc e node
EXPECTIMI N VALU E B,  wher e we can comput e the expectimi n valu e usin g a formul a that is analogou s to expectimax .
This proces s can be applie d recursivel y all the way up the tree, excep t at the top leve l
wher e the dice roll is alread y known . To calculat e the best move , then , we simpl y replac e
MINIMAX­VALU E in Figur e 5.3 by EXPECTIMINIMAX­VALUE , the implementatio n of whic h we
leave as an exercise .
Positio n evaluatio n in game s with chanc e node s
As with minimax , the obviou s approximatio n to mak e with expectiminima x is to cut off searc h
at som e poin t and appl y an evaluatio n functio n to the leaves . On e migh t thin k that evaluatio n
function s for game s such as backgammo n are no different , in principle , from evaluatio n function s
for chess—the y shoul d just give highe r score s to bette r positions .
In fact , the presenc e of chanc e node s mean s one has to be mor e carefu l abou t wha t the
evaluatio n value s mean . Remembe r that for minimax , any order­preservin g transformatio n of
the leaf value s does not affec t the choic e of move . Thus , we can use eithe r the value s 1, 2, 3, 4
or the value s 1, 20, 30, 400, and get the same decision . This gives us a good deal of freedo m in
designin g the evaluatio n function : it will wor k fine as long as position s with highe r evaluation s
lead to wins more often , on average .
With chanc e nodes , we lose this freedom . Figur e 5.11 show s wha t happens : wit h leaf
value s 1, 2, 3, 4, mov e A\ is best ; with leaf value s 1, 20, 30, 400, mov e AI is best . Hence ,
the program  behave s totall y differentl y if we mak e a chang e in the scale of evaluatio n values !
It turn s out that to avoi d this sensitivity , the evaluatio n functio n can be only a positive  linear
transformatio n of the likelihoo d of winnin g from a positio n (or, mor e generally , of the expecte d
utilit y of the position) . This is an importan t and genera l propert y of situation s in whic h uncertaint y
is involved , and we discus s it furthe r in Chapte r 16.
Complexit y of expectiminima x
If the progra m knew in advanc e all the dice rolls that woul d occu r for the rest of the game , solvin g
a gam e with dice woul d be just like solvin g a gam e withou t dice, whic h minima x does in O(bm)
time. Becaus e expectiminima x is also considerin g all the possibl e dice­rol l sequences , it will
take O(bmnm), wher e n is the numbe r of distinc t rolls .
Even if the dept h of the tree is limite d to som e smal l dept h d, the extra cost compare d to
minima x make s it unrealisti c to conside r lookin g ahea d very far in game s such as backgammon ,
wher e « is 21 and b is usuall y aroun d 20, but in som e situation s can be as high as 4000 . Two ply
is probabl y all we coul d manage .
Anothe r way to thin k abou t the proble m is this: the advantag e of alpha­bet a is that it ignore s
futur e development s that just are not goin g to happen , give n best play.  Thus , it concentrate s on
likely occurrences . In game s with dice, there are no likel y sequence s of moves , becaus e for thos e
move s to take place , the dice woul d first have to com e out the righ t way to mak e them legal .
136 Chapte r 5. Gam e Playin g
MAX
MIN
22 321
20 2 0 30 30 1  1  40 0 40 0
Figur e 5.11 A n order­preservin g transformatio n on leaf value s change s the best move .
This is a genera l proble m wheneve r uncertaint y enter s the picture : the possibilitie s are multiplie d
enormously , and formin g detaile d plan s of actio n become s pointles s becaus e the worl d probabl y
will not play along .
No doub t it will have occurre d to the reade r that perhap s somethin g like alpha­bet a prunin g
could be applie d to gam e trees with chanc e nodes . It turn s out that it can, with a bit of ingenuity .
Conside r the chanc e node C in Figur e 5.10 , and wha t happen s to its valu e as we examin e and
evaluat e its children ; the questio n is, is it possibl e to find an uppe r boun d on the valu e of C
befor e we have looke d at all its children ? (Recal l that this is wha t alpha­bet a need s in orde r to
prune a node and its subtree. ) At first sight , it migh t seem impossible , becaus e the valu e of C is
the average  of its children' s values , and unti l we have looke d at all the dice rolls , this averag e
could be anything , becaus e the unexamine d childre n migh t have any valu e at all. But if we put
boundarie s on the possibl e value s of the utilit y function , then we can arriv e at boundarie s for the
average . For example , if we say that all utilit y value s are betwee n +1 and ­1, then the valu e of
leaf node s is bounded , and in turn we can plac e an uppe r boun d on the valu e of a chanc e node
withou t lookin g at all its children . Designin g the prunin g proces s is a little bit more complicate d
than for alpha­beta , and we leav e it as an exercise .
5.6 STATE­OF­THE­AR T GAM E PROGRAM S
Designin g game­playin g program s has a dual purpose : both to bette r understan d how to choos e
action s in comple x domain s with uncertai n outcome s and to develop  high­performanc e system s
for the particula r gam e studied . In this section , we examin e progres s towar d the latte r goal .
Sectio n 5.6. State­of­the­Ar t Gam e Program s 137
Ches s
Ches s has receive d by far the larges t shar e of attentio n in gam e playing . Althoug h not meetin g
the promis e made by Simo n in 1957 that withi n 10 years , computer s woul d beat the huma n worl d
champion , they are now withi n reac h of that goal . In spee d chess , computer s have defeate d the
world champion , Gary Kasparov , In both 5­minut e and 25­minut e games , but in full tournamen t
game s are only ranke d amon g the top 100 player s worldwid e at the time of writing . Figure5.1 2
show s the rating s of huma n and compute r champion s over the years . It is temptin g to try to
extrapolat e and see wher e the lines will cross .
Progres s beyond  a mediocr e leve l was initiall y very slow : som e program s in the early
1970s becam e extremel y complicated , with variou s kind s of trick s for eliminatin g some branche s
of search , generatin g plausibl e moves , and so on, but the program s that won the ACM Nort h
America n Compute r Ches s Championship s (initiate d in 1970 ) tende d to use straightforwar d alpha ­
beta search , augmente d with book opening s and infallibl e endgam e algorithms . (Thi s offer s an
interestin g exampl e of how high performanc e require s a hybri d decision­makin g architectur e to
implemen t the agen t function. )
The first real jump in performanc e came not from bette r algorithm s or evaluatio n functions ,
but from hardware . Belle , the first special­purpos e ches s compute r (Condo n and Thompson ,
1982) , used custo m integrate d circuit s to implemen t mov e generatio n and positio n evaluation ,
enablin g it to searc h severa l millio n position s to mak e a singl e move . Belle' s ratin g was aroun d
2250, on a scale wher e beginnin g human s are 1000 and the worl d champio n aroun d 2750 ; it
becam e the first master­leve l program .
The HITEC H system , also a special­purpos e computer , was designe d by forme r worl d
correspondenc e champio n Han s Berline r and his studen t Carl Ebelin g to allow rapid calculatio n
of very sophisticate d evaluatio n functions . Generatin g abou t 10 millio n position s per mov e
and usin g probabl y the mos t accurat e evaluatio n of position s yet developed , HITEC H becam e
3000— ,
_
­
­
2000 —
­^
1000 —
0
19I
Ol
03
60>
I
O
^
CL.
1I '
€
a
I
195 < jChes s 3.0 (1500 )
1 1
65P
3
CN
Q.I
>
Fischcr(2785 )
1 '
1970> •
€ Ches s 4.6 (1900 )
1 1
19)
§
75>l Korchno i (2645 )1 *
9
<N
JJ
03
1 '  1 '
19803
I
1 ' 1
1985» '
0Q Hitec h (2400 )
Deep Though t (255 1
o
0I
S3
1 1 '
199010
KDeep Though t 2 (app
11
Figur e 5.12 Rating s of huma n and machin e ches s champions .
138 Chapte r 5. Gam e Playin g
compute r worl d champio n in 1985 , and was the first progra m to defea t a huma n grandmaster ,
Arnol d Denker , in 1987 . At the time it ranke d amon g the top 800 huma n player s in the world .
The best curren t syste m is Deep Though t 2. It is sponsore d by IBM , whic h hired part of the
team that built the Deep Though t syste m at Carnegi e Mello n University . Althoug h Deep Though t
2 uses a simpl e evaluatio n function , it examine s abou t half a billio n position s per move , allowin g
it to reac h dept h 10 or 11, with a specia l provisio n to follo w lines of force d move s still furthe r
(it once foun d a 37­mov e checkmate) . In Februar y 1993 , Deep Though t 2 compete d agains t the
Danis h Olympi c team and won , 3­1, beatin g one grandmaste r and drawin g agains t another . Its
FIDE ratin g is aroun d 2600 , placin g it amon g the top 100 huma n players .
The next versio n of the system , Deep Blue , will use a paralle l array of 1024 custo m VLS I
chips . Thi s will enabl e it to searc h the equivalen t of one billio n position s per secon d (100­20 0
billio n per move ) and to reach dept h 14. A 10­processo r versio n is due to play the Israel i nationa l
team (one of the stronges t in the world ) in May 1995 , and the full­scal e syste m will challeng e
the worl d champio n shortl y thereafter .
Checker s or Draught s
Beginnin g in 1952 , Arthu r Samue l of IBM , workin g in his spar e time , develope d a checker s
progra m that learne d its own evaluatio n functio n by playin g itsel f thousand s of times . We
describ e this idea in more detai l in Chapte r 20. Samuel' s progra m bega n as a novice , but after
only a few days ' self­pla y was able to compet e on equa l term s in som e very stron g huma n
tournaments . Whe n one consider s that Samuel' s computin g equipmen t (an IBM 704) had 10,00 0
word s of mai n memory , magneti c tape for long­ter m storage , and a cycl e time of almos t a
millisecond , this remain s one of the grea t feats of AI.
Few othe r peopl e attempted  to do bette r unti l Jonatha n Schaeffe r and colleague s develope d
Chinook , whic h runs on ordinar y computer s usin g alpha­bet a search , but uses severa l techniques ,
includin g perfec t solutio n database s for all six­piec e positions , that mak e its endgam e play
devastating . Chinoo k won the 199 2 U.S . Open , and becam e the first progra m to officiall y
challeng e for a real worl d championship . It then ran up agains t a problem , in the form of Mario n
Tinsley . Dr. Tinsle y had been worl d champio n for over 40 years , losin g only three game s in all that
time. In the first matc h agains t Chinook , Tinsle y suffere d his fourt h and fifth losses , but won the
matc h 21.5­18.5 . Mor e recently , the worl d championshi p matc h in Augus t 1994 betwee n Tinsle y
and Chinoo k ende d prematurel y whe n Tinsle y had to withdra w for healt h reasons . Chinoo k
becam e the officia l worl d champion .
Othell o
Othello , also calle d Reversi , is probabl y more popula r as a compute r gam e than as a boar d game .
It has a smalle r searc h spac e than chess , usuall y 5 to 15 lega l moves , but evaluatio n expertis e
had to be develope d from scratch . Eve n so, Othell o program s on norma l computer s are far bette r
than humans , who generall y refus e direc t challenge s in tournaments .
ISectio n 5.7. Discussio n 139
Backgammo n
As mentione d before , the inclusio n of uncertaint y from dice rolls make s searc h an expensiv e
luxur y in backgammon . The first progra m to mak e a seriou s impact , BKG , used only a one­pl y
searc h but a very complicate d evaluatio n function . In an informa l matc h in 1980 , it defeate d the
huma n worl d champio n 5­1, but was quit e luck y with the dice . Generally , it play s at a stron g
amateu r level .
More recently , Gerr y Tesaur o (1992 ) combine d Samuel' s learnin g metho d with neura l
networ k technique s (Chapte r 19) to develo p a new evaluatio n function . His progra m is reliabl y
ranke d amon g the top three player s in the world .
Go
Go is the mos t popula r boar d gam e in Japan , requirin g at leas t as muc h disciplin e from its
professional s as chess . The branchin g facto r approache s 360, so that regula r searc h method s are
totall y lost. System s base d on large knowledg e base s of rules for suggestin g plausibl e move s
seem to have some hope , but still play very poorly . Particularl y give n the $2,000,00 0 prize for the
first progra m to defea t a top­leve l player , Go seem s like an area likel y to benefi t from intensiv e
investigatio n usin g more sophisticate d reasonin g methods .
5.7 DISCUSSIO N
Becaus e calculatin g optima l decision s in game s is intractabl e in mos t cases , all algorithm s
must mak e som e assumption s and approximations . The standar d approach , base d on minimax ,
evaluatio n functions , and alpha­beta , is just one way to do this. Probabl y becaus e it was propose d
so earl y on, it has been develope d intensivel y and dominate s othe r method s in tournamen t
play. Som e in the field believ e that this has cause d gam e playin g to becom e divorce d from the
mainstrea m of AI research , becaus e the standar d approac h no longe r provide s muc h room for
new insigh t into genera l question s of decisio n making . In this section , we look at the alternatives ,
considerin g how to relax the assumption s and perhap s deriv e new insights .
First, let us conside r minimax . Minima x is an optima l metho d for selectin g a mov e from
a give n searc h tree provided  the leaf node  evaluations are  exactly  correct.  In reality , evaluation s
are usuall y crud e estimate s of the valu e of a position , and can be considere d to have larg e
errors associate d with them . Figur e 5.13 show s a two­pl y gam e tree for whic h minima x seem s
inappropriate . Minima x suggest s takin g the right­han d branch , wherea s it is quit e likel y that
true valu e of the left­han d branc h is higher . The minima x choic e relie s on the assumptio n that
all of the node s labelle d with value s 100, 101 , 102 , and 100 are actually  bette r than the node
labelle d with valu e 99. One way to deal with this proble m is to have an evaluatio n that return s a
probability  distribution  over possibl e values . The n one can calculat e the probabilit y distributio n
for the parent' s valu e usin g standar d statistica l techniques . Unfortunately , the value s of siblin g
node s are usuall y highl y correlated , so this can be an expensiv e calculatio n and may requir e
detaile d correlatio n informatio n that is hard to obtain .
140 Chapte r 5. Gam e Playin g
MAX
MIN 100
1000 100 0 100 0 100 10 1 102 100
Figur e 5.13 A  two­pl y gam e tree for whic h minima x may be inappropriate .
METAREASONIN GNext , we conside r the searc h algorith m that generate s the tree. The aim of an algorith m
designe r is to specif y a computatio n that complete s in a timel y manne r and result s in a good move
choice . The mos t obviou s proble m with the alpha­bet a algorith m is that it is designe d not just to
select a good move , but also to calculat e the value s of all the legal moves . To see why this extra
informatio n is unnecessary,  conside r a positio n in whic h there is only one legal move . Alpha­bet a
searc h still will generat e and evaluat e a large , and totall y useless , searc h tree. Of course , we
can inser t a test into the algorithm , but this merel y hide s the underlyin g problem—man y of the
calculation s done by alpha­bet a are largel y irrelevant . Havin g only one legal mov e is not much
differen t from havin g severa l lega l moves , one of whic h is fine and the rest of whic h are obviousl y
disastrous . In a "clear­favorite " situatio n like this, it woul d be bette r to reac h a quic k decisio n
after a smal l amoun t of searc h than to wast e time that coul d be bette r used late r for a mor e
problemati c position . Thi s lead s to the idea of the utility  of a node  expansion.  A good searc h
algorith m shoul d selec t node expansion s of high utility—tha t is, ones that are likel y to lead to the
discover y of a significantl y bette r move . If there are no node expansion s whos e utilit y is highe r
than their cost (in term s of time) , then the algorith m shoul d stop searchin g and mak e a move .
Notic e that this work s not only for clear­favorit e situations , but also for the case of symmetrical
moves , wher e no amoun t of searc h will show that one mov e is bette r than another .
This kind of reasonin g abou t wha t computation s to do is calle d metareasonin g (reasonin g
abou t reasoning) . It applie s not just to gam e playing , but to any kind of reasonin g at all. All
computation s are don e in the servic e of tryin g to reac h bette r decisions , all have costs , and
all have som e likelihoo d of resultin g in a certai n improvemen t in decisio n quality . Alpha­bet a
incorporate s the simples t kind of metareasoning , namely , a theore m to the effec t that certai n
branche s of the tree can be ignore d withou t loss. It is possibl e to do muc h better . In Chapte r 16,
we will see how these idea s can be mad e precis e and implementable .
Finally , let us reexamin e the natur e of searc h itself . Algorithm s for heuristi c searc h and
for gam e playin g work by generatin g sequence s of concret e state s startin g from the initia l state
and then applyin g an evaluatio n function . Clearly , this is not how human s play games . In
chess , one often has a particula r goal in mind—fo r example , trappin g the opponent' s queen—an d
can use this to selectively  generat e plausibl e plan s for achievin g it. Thi s kind of goal­directe d
reasonin g or plannin g sometime s eliminate s combinatoria l searc h altogethe r (see Part IV). Davi d
Wilkins ' (1980 ) PARADIS E is the only progra m to have used goal­directe d reasonin g successfull y
ISectio n 5.8. Summar y 14 1
in chess : it was capabl e of solvin g some ches s problem s requirin g an 18­mov e combination . As
yet, however , ther e is no good understandin g of how to combine  the two kind s of algorith m into
a robus t and efficien t system . Suc h a syste m woul d be a significan t achievemen t not just for
game­playin g research , but also for AI researc h in general , becaus e it woul d be muc h more likel y
to appl y to the proble m faced by a genera l intelligen t agent.
5.8 SUMMARY_____________________________ _
Game s are fascinating , and writin g game­playin g program s perhap s even mor e so. We migh t
say that gam e playin g is to AI as Gran d Prix moto r racin g is to the car industry : althoug h the
specialize d task and extreme  competitiv e pressur e lead one to desig n system s that do not look
much like your garden­variety , general­purpos e intelligen t system , a lot of leading­edg e concept s
and engineerin g idea s com e out of it. On the othe r hand , just as you woul d not expec t a Gran d
Prix racin g car to perfor m wel l on a bump y dirt road , you shoul d not expec t advance s in gam e
playin g to translat e immediatel y into advance s in less abstrac t domains .
The mos t importan t ideas are as follows :
• A gam e can be define d by the initia l state (how the boar d is set up), the operator s (whic h
defin e the lega l moves) , a termina l test (whic h says whe n the gam e is over) , and a utilit y
or payof f functio n (whic h says who won , and by how much) .
• In two­playe r game s with perfec t information , the minima x algorith m can determin e the
best mov e for a playe r (assumin g the opponen t play s perfectly ) by enumeratin g the entir e
game tree.
• The alpha­bet a algorith m does the sam e calculatio n as minimax , but is more efficien t
becaus e it prune s awa y branche s of the searc h tree that it can prov e are irrelevan t to the
final outcome .
• Usually , it is not feasibl e to conside r the whol e gam e tree (eve n with alpha­beta) , so we
need to cut off the searc h at som e poin t and appl y an evaluatio n functio n that give s an
estimat e of the utilit y of a state .
• Game s of chanc e can be handle d by an extensio n to the minima x algorith m that evaluate s
chanc e node s by takin g the averag e utilit y of all its childre n nodes , weighte d by the
probabilit y of the child .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
The earl y histor y of mechanica l gam e playin g was marre d by numerou s frauds . Th e mos t
notoriou s of these was Baro n Wolfgan g von Kempelen' s "Turk, " exhibite d in 1769 , a suppose d
chess­playin g automato n whos e cabine t actuall y conceale d a diminutiv e huma n ches s exper t
durin g play.  The Turk is describe d in Harknes s and Battel l (1947) . In 1846 , Charle s Babbag e
appear s to have contribute d the first seriou s discussio n of the feasibilit y of compute r gam e playin g
142 Chapte r 5. Gam e Playin g
(Morriso n and Morrison , 1961) . He believe d that if his mos t ambitiou s desig n for a mechanica l
digita l computer , the Analytica l Engine , were ever completed , it coul d be programme d to play
checker s and chess . He also designed , but did not build , a special­purpos e machin e for playin g
Tic­Tac­Toe . Erns t Zermelo , the designe r of moder n axiomati c set theory , later speculate d on
the rathe r quixoti c possibilit y of searchin g the entir e gam e tree for ches s in orde r to determin e
a perfec t strateg y (Zermelo , 1976) . Th e first functionin g (and nonfraudulent ) game­playin g
machin e was designe d and built aroun d 1890 by the Spanis h enginee r Leonard o Torre s y Quevedo .
It specialize d in the "KRK " ches s endgam e (kin g and rook vs. king) , playin g the side with the
king and rook agains t a huma n opponen t attemptin g to defen d with the lone king . Its play was
correc t and it was capabl e of forcin g mate from any startin g positio n (wit h the machin e movin g
first) . The "Nimotron " (Condo n et al., 1940 ) demonstrate d perfec t play for the very simpl e
game of Nim . Significantly , a completel y optima l strateg y for Nim and an adequat e strateg y for
the KRK ches s endgam e (i.e., one whic h will alway s win whe n give n the first move , althoug h
not necessaril y in the minima l numbe r of moves ) are both simpl e enoug h to be memorize d and
execute d algorithmicall y by humans .
Torre s y Quevedo' s achievement , and even Babbage' s and Zermelo' s speculations , re­
maine d relativel y isolate d until the mid­1940s—th e era whe n programmabl e electroni c digita l
computer s were first being developed . The comprehensiv e theoretica l analysi s of game strateg y
in Theory of  Games  and  Economic  Behavior  (Vo n Neuman n and Morgenstern , 1944 ) place d
emphasi s on minimaxin g (withou t any dept h cutoff ) as a way to defin e mathematicall y the
game­theoreti c valu e of a positio n in a game . Konra d Zuse (1945) , the first perso n to desig n
a programmabl e computer , develope d idea s as to how mechanica l ches s play migh t be accom ­
plished . Adriaa n de Groo t (1946 ) carrie d out in­dept h psychologica l analysi s of huma n ches s
strategy , whic h was usefu l to designer s of compute r ches s programs . Norber t Wiener' s (1948 )
book Cybernetics  include d a brief sketc h of the functionin g of a possibl e compute r chess­playin g
program , includin g the idea of usin g minima x searc h with a dept h cutof f and an evaluatio n func ­
tion to selec t a move . Claud e Shanno n (1950 ) wrot e a highl y influentia l articl e that laid out the
basic principle s underlyin g moder n compute r game­playin g programs , althoug h the articl e did
not actuall y includ e a progra m of his own . Shanno n describe d minimaxin g with a dept h cutof f
and evaluatio n functio n mor e clearl y and in mor e detai l than had Wiener , and introduce d the
notio n of quiescenc e of a position . Shanno n also describe d the possibilit y of using nonexhaustiv e
("typ e B") as oppose d to exhaustiv e ("typ e A") minimaxing . Slate r (1950 ) and the commentator s
on his articl e in the sam e volum e also explore d the possibilitie s for compute r ches s play . In
particular , Goo d (1950 ) develope d the notio n of quiescenc e independentl y of Shannon .
In 1951 , Alan Turin g wrot e the first actua l compute r progra m capabl e of playin g a full
game of chess . (Th e progra m was publishe d in Turin g (1953). ) But Turing' s progra m neve r
actuall y ran on a computer ; it was teste d by hand simulatio n agains t a very weak huma n player ,
who defeate d it. Meanwhil e D. G. Prin z (1952 ) had written , and actuall y run, a progra m that
solve d ches s problems , althoug h it did not play a full game .
Checkers , rathe r than chess , was the first of the classi c game s for whic h a progra m actuall y
runnin g on a compute r was capabl e of playin g out a full game . Christophe r Strache y (1952 ) was
the first to publis h such research , althoug h Slagl e (1971 ) mention s a checker s progra m writte n
by Arthu r Samue l as early as 1947 . Chinook , the checker s progra m that recentl y took over the
world title from Mario n Tinsley , is describe d by Schaeffe r et al. (1992) .
Sectio n 5.8. Summary_______ _ 14 3
A grou p workin g at Los Alamo s (Kiste r et ai, 1957 ) designe d and ran a progra m that
playe d a full gam e of a varian t of ches s usin g a 6 x 6 board . Ale x Bernstei n wrot e the first
progra m to play a full gam e of standar d ches s (Bernstei n and Roberts , 1958 ; Bernstei n et al.,
1958) , unles s possibl y this feat was accomplishe d by the Russia n BES M progra m mentione d in
Newel l et al. (1958) , abou t whic h little informatio n is available .
John McCarth y conceive d the idea of alpha­bet a searc h in 1956 , althoug h he did not publis h
it. The NSS ches s progra m (Newel l et al., 1958 ) used a simplifie d versio n of alpha­beta ; it was
the first chess progra m to do so. Accordin g to Nilsso n (1971) , Arthu r Samuel' s checker s progra m
(Samuel , 1959 ; Samuel , 1967 ) also used alpha­beta , althoug h Samue l did not mentio n it in the
publishe d report s on the system . Paper s describin g alpha­bet a were publishe d in the early 1960 s
(Hart and Edwards , 1961 ; Brudno , 1963 ; Slagle , 1963b) . An implementatio n of full alpha­bet a
is describe d by Slagl e and Dixo n (1969 ) in a progra m for playin g the gam e of kalah . Alpha­bet a
was also used by the "Kotok­McCarthy " ches s progra m writte n by a studen t of John McCarth y
(Kotok , 1962 ) and by the MacHac k 6 ches s progra m (Greenblat t et al., 1967) . MacHac k 6 was
the first ches s progra m to compet e successfull y with humans , althoug h it fell considerabl y shor t
of Herb Simon' s predictio n in 1957 that a compute r progra m woul d be worl d ches s champio n
withi n 10 year s (Simo n and Newell , 1958) . Knut h and Moor e (1975 ) provid e a histor y of alpha ­
beta, alon g with a proo f of its correctnes s and a time complexit y analysis . Furthe r analysi s of
the effectiv e branchin g facto r and time complexit y of alpha­bet a is give n by Pear l (1982b) . Pear l
show s alpha­bet a to be asymptoticall y optima l amon g all game­searchin g algorithms .
It woul d be a mistak e to infe r that alpha­beta' s asymptoti c optimalit y has completel y
suppresse d interes t in other game­searchin g algorithms . The best­know n alternative s are probabl y
the B* algorith m (Berliner , 1979) , whic h attempt s to maintai n interva l bounds  on the possibl e
value of a node in the gam e tree, rathe r than givin g it a singl e point­value d estimat e as minima x
and alpha­bet a do, and SSS * (Stockman , 1979) , whic h dominate s alpha­bet a in the sens e that
the set of node s in the tree that it examine s is a (sometime s proper ) subse t of those examine d by
alpha­beta . Pala y (1985 ) uses probabilit y distribution s in plac e of the poin t value s of alpha­bet a
or the interval s of B*. Davi d McAllester' s (1988 ) conspirac y numbe r searc h is an interestin g
generalizatio n of alpha­beta . MGSS * (Russel l and Wefald , 1989 ) uses the advanced  decision ­
theoreti c technique s of Chapte r 16 to decid e whic h node s to examin e next , and was able to
outpla y an alpha­bet a algorith m at Othell o despit e searchin g an orde r of magnitud e fewe r nodes .
Individua l game s are subjec t to ad hoc mathematica l analysis ; a fascinatin g stud y of a hug e
numbe r of game s is give n by Berlekam p et al. (1982) .
D. F. Beal (1980 ) and Dan a Nau (1980 ; 1983 ) independentl y and simultaneousl y showe d
that unde r certai n assumption s abou t the game bein g analyzed , any form of minimaxing , includin g
alpha­beta , usin g an evaluatio n function , yield s estimate s that are actuall y less reliabl e than the
direc t use of the evaluatio n function , withou t any searc h at all! Heuristics  (Pearl , 1984 ) give s
a thoroug h analysi s of alpha­bet a and describe s B*, SSS* , and othe r alternativ e gam e searc h
algorithms . It also explore s the reason s for the Beal/Na u paradox , and why it does not appl y
to ches s and othe r game s commonl y approache d via automate d game­tre e search . Pear l also
describe s AND/O R graph s (Slagle , 1963a) , whic h generaliz e game­tre e searc h but can be applie d
to othe r type s of problem s as well , and the AO* algorith m (Martell i and Montanari , 1973 ;
Martell i and Montanari , 1978 ) for searchin g them . Kaind l (1990 ) give s anothe r surve y of
sophisticate d searc h algorithms .
144 Chapte r 5. Gam e Playin g
The first two compute r ches s program s to play a matc h agains t each othe r were the Kotok ­
McCarth y progra m and the "ITEP " progra m writte n at Moscow' s Institut e of Theoretica l and
Experimenta l Physic s (Adelson­Velsk y et ai, 1970) . This intercontinenta l matc h was playe d by
telegraph . It ende d in 196 7 with a 3­1 victor y for the ITE P program . The first ACM Nort h
America n Compute r Ches s Championshi p tournamen t was held in New York City in 1970 . The
first Worl d Compute r Ches s Championshi p was held in 1974 in Stockhol m (Haye s and Levy ,
1976) . It was won by Kaiss a (Adelson­Velsk y et al., 1975) , anothe r progra m from ITEP .
A later versio n of Greenblatt' s MacHac k 6 was the first ches s progra m to run on custo m
hardwar e designe d specificall y for ches s (Moussouri s etal.,1919),  but the first progra m to achiev e
notabl e succes s throug h the use of custo m hardwar e was Bell e (Condo n and Thompson , 1982) .
Most of the stronges t recen t programs , such as HITEC H (Ebeling , 1987 ; Berline r and Ebeling ,
1989) and Dee p Though t (Hsu et al., 1990 ) have run on custo m hardware . Majo r exception s
are Cray Blit z (Hyat t et al., 1986) , whic h runs on a general­purpos e Cray supercomputer , and
Socrate s II, winne r of the 23rd ACM Nort h America n Compute r Ches s Championshi p in 1993 ,
whic h runs on an Intel 486­base d microcomputer . It shoul d be note d that Deep Though t was not
there to defen d its title . Dee p Though t 2 regaine d the championshi p in 1994 . It shoul d also be
noted that even custom­hardwar e machine s can benefi t greatl y from improvement s purel y at the
softwar e level (Berliner , 1989) .
The Fredki n Prize , establishe d in 1980 , offere d $500 0 to the first progra m to achiev e a
Maste r rating , $10,00 0 to the first progra m to achiev e a USC F (Unite d State s Ches s Federation )
rating of 2500 (nea r the grandmaste r level) , and $100,00 0 for the first progra m to defea t the
huma n worl d champion . The $500 0 prize was claime d by Bell e in 1983 , and the $10,00 0 prize
by Deep Though t in 1989 . The $100,00 0 priz e remain s unclaimed , in view of convincin g wins
in extende d play by worl d champio n Gary Kasparo v over Deep Though t (Hsu et al., 1990) .
The literatur e for compute r ches s is far bette r develope d than for any othe r gam e playe d
by compute r programs . Asid e from the tournament s alread y mentioned , the rathe r misleadingl y
name d conferenc e proceeding s Heuristic  Programming in  Artificial  Intelligence  repor t on the
Compute r Ches s Olympiads . The Internationa l Compute r Ches s Associatio n (ICCA) , founde d
in 1977 , publishe s the quarterl y ICCA  Journal.  Importan t paper s have been publishe d in the
numbere d seria l antholog y Advances  in Computer  Chess,  startin g with (Clarke , 1977) . Som e
early genera l AI textbook s (Nilsson , 1971 ; Slagle , 1971 ) includ e extensiv e materia l on game ­
playin g programs , includin g ches s programs . Davi d Levy' s Computer  Chess  Compendium  (Levy ,
1988a ) anthologize s man y of the mos t importan t historica l paper s in the field , togethe r with the
score s of importan t game s playe d by compute r programs . The edite d volum e by Marslan d and
Schaeffe r (1990 ) contain s interestin g historica l and theoretica l paper s on ches s and Go alon g
with description s of Cray Blitz , HlTECH , and Dee p Thought . Severa l importan t paper s on chess ,
along with materia l on almos t all game s for whic h compute r game­playin g program s have been
writte n (includin g checkers , backgammon , Go, Othello , and severa l card games ) can be foun d in
Levy (1988b) . Ther e is even a textboo k on how to writ e a compute r game­playin g program , by
one of the majo r figure s in compute r ches s (Levy , 1983) .
The expectima x algorith m describe d in the text was propose d by Donal d Michi e (1966) ,
althoug h of cours e it follow s directl y from the principle s of game­tre e evaluatio n due to Von
Neuman n and Morgenstern . Bruc e Ballar d (1983 ) extende d alpha­bet a prunin g to cove r tree s
with chanc e nodes . Th e backgammo n program  BK G (Berliner , 1977 ; Berliner , 1980b ) was
Sectio n 5.8. Summar y 145
the first progra m to defea t a huma n worl d champio n at a majo r classi c gam e (Berliner , 1980a) ,
althoug h Berline r was the first to acknowledg e that this was a very shor t exhibitio n matc h (not a
world championshi p match ) and that BKG was very luck y with the dice .
The first Go­playin g program s were develope d somewha t later than thos e for checker s and
chess (Lefkovitz , 1960 ; Remus , 1962 ) and have progresse d more slowly . Ryde r (1971 ) used a
search­base d approac h simila r to that take n by mos t ches s program s but with more selectivit y to
overcom e the enormou s branchin g factor . Zobris t (1970 ) used a pattern­recognitio n approach .
Reitma n and Wilco x (1979 ) used condition­actio n rules base d on comple x patterns , combine d
with highl y selectiv e localize d search . The Go Explore r and its successor s (Kierul f et al., 1990 )
continu e to evolv e alon g thes e lines . YUG O (Shirayanagi , 1990 ) place s heav y emphasi s on
knowledg e representatio n and patter n knowledge . The Computer  Go Newsletter,  publishe d by
the Compute r Go Association , describe s curren t developments .
EXERCISE S
5.1 Thi s proble m exercise s the basic concept s of game­playin g usin g Tic­Tac­To e (nought s and
crosses ) as an example . We defin e Xn as the numbe r of rows , columns , or diagonal s with exactl y
nX'sandn o O's. Similarly , On is the numbe r of rows , columns , or diagonal s with just n O's. The
utilit y functio n thus assign s +1 to any positio n with XT, = 1 and —1 to any positio n with OT, ­ 1.
All othe r termina l position s have utilit y 0. We will use a linea r evaluatio n functio n define d as
a. Approximatel y how man y possibl e game s of Tic­Tac­To e are there ?
b. Sho w the whol e gam e tree startin g from an empt y boar d dow n to dept h 2, (i.e., one X and
one O on the board) , takin g symmetr y into account . You shoul d have 3 position s at level
1 and 12 at level 2.
c. Mar k on your tree the evaluation s of all the position s at level 2.
d. Mar k on your tree the backed­u p value s for the position s at level s 1 and 0, usin g the
minima x algorithm , and use them to choos e the best startin g move .
e. Circl e the node s at level 2 that woul d not be evaluate d if alpha­bet a prunin g were applied ,
assumin g the node s are generate d in the optimal  order  for alpha­beta  pruning.
5.2 Implemen t a genera l game­playin g agen t for two­playe r deterministi c games , usin g alpha ­
beta search . Yo u can assum e the gam e is accessible , so the inpu t to the agen t is a complet e
descriptio n of the state .
5.3 Implemen t mov e generator s and evaluatio n function s for one or mor e of the followin g
games : kalah , Othello , checkers , chess . Exercis e your game­playin g agen t usin g the implemen ­
tation . Compar e the effec t of increasin g searc h depth , improvin g mov e ordering , and improvin g
the evaluatio n function . How clos e does you r effectiv e branchin g facto r com e to the idea l case
of perfec t mov e ordering ?
146 Chapte r 5. Gam e Playin g
5.4 Th e algorithm s describe d in this chapte r construc t a searc h tree for each mov e from scratch .
Discus s the advantage s and disadvantage s of retainin g the searc h tree from one mov e to the next
and extendin g the appropriat e portion . How woul d tree retentio n interac t with the use of selectiv e
searc h to examin e "useful " branche s of the tree?
5.5 Develo p a forma l proo f of'correctnes s of alpha­bet a pruning . To do this , conside r the
situatio n show n in Figur e 5.14 . The questio n is whethe r to prun e node «/, whic h is a max­nod e
and a descendan t of node n\. The basi c idea is to prun e it if and only if the minima x valu e of n\
can be show n to be independen t of the valu e of «,.
a. The valu e of n\ is give n by
MI = min(«2,«2i, ­ • •,«26i )
By writin g a simila r expressio n for the valu e of «2, find an expressio n for n\ in term s of rij.
b. Let /, be the minimu m (or maximum ) of the node value s to the left of node «, at dept h i.
Thes e are the node s whos e minima x valu e is alread y known . Similarly , let r, be the
minimu m (or maximum ) of the node value s to the righ t of n, at dept h i. Thes e node s have
not yet been explored . Rewrit e your expressio n for n\ in term s of the /, and r, values .
c. Now reformulat e the expressio n to show that in orde r to affec t n\, «/ mus t not excee d a
certai n boun d derive d from the /, values .
d. Repea t the proces s for the case wher e «/ is a min­node .
You migh t wan t to consul t Wand  (1980) , who show s how the alpha­bet a algorith m can be auto ­
maticall y synthesize d from the minima x algorithm , usin g som e genera l program­transformatio n
techniques .
Figur e 5.14 Situatio n whe n considerin g whethe r to prun e nod e n/.
Sectio n 5.8. Summar y 147
5.6 Prov e that with a positiv e linea r transformatio n of leaf values , the mov e choic e remain s
unchange d in a gam e tree with chanc e nodes .
5.7 Conside r the followin g procedur e for choosin g move s in game s with chanc e nodes :
• Generat e a suitabl e numbe r (say, 50) dice­rol l sequence s dow n to a suitabl e dept h (say, 8).
• Wit h know n dice rolls , the gam e tree become s deterministic . For each dice­rol l sequence ,
solve the resultin g deterministi c gam e tree usin g alpha­beta .
• Use the result s to estimat e the valu e of each mov e and choos e the best.
Will this procedur e work correctly ? Why (not) ?
5.8 Le t us conside r the proble m of searc h in a three­player  game . (Yo u can assum e no alliance s
are allowe d for now. ) We will call the player s 0, 1, and 2 for convenience . The first chang e is
that the evaluatio n functio n will retur n a list of three values , indicatin g (say ) the likelihoo d of
winnin g for player s 0, 1, and 2, respectively .
a. Complet e the followin g game tree by fillin g in the backed­u p valu e triple s for all remainin g
nodes , includin g the root:
to mov e
0
(123 ) (421 ) (612 ) (74­1 ) (5­1­1 ) (­1 5 2) (77­1 ) (545 )
Figur e 5.15 Th e first three ply of a gam e tree with  three player s (0, 1, and 2).
b. Rewrit e MlNMAX­DECiSiO N and MINIMAX­VALU E so that they wor k correctl y for the
three­playe r game .
c. Discus s the problem s that migh t arise if player s coul d form and terminat e alliance s as well
as mak e move s "on the board. " Indicat e briefl y how these problem s migh t be addressed .
5.9 Describ e and implemen t a genera l game­playin g environmen t for an arbitrar y numbe r of
players . Remembe r that time is part of the environmen t state , as well as the boar d position .
5.10 Suppos e we play a varian t of Tic­Tac­To e in whic h each playe r sees only his or her own
moves . If the playe r make s a mov e on a squar e occupie d by an opponent , the boar d "beeps " and
the playe r gets anothe r try. Woul d the backgammo n mode l suffic e for this game , or woul d we
need somethin g mor e sophisticated ? Why ?
148 Chapte r 5. Gam e Playin g
5.11 Describ e and/o r implemen t state descriptions , mov e generators , and evaluatio n function s
for one or more of the followin g games : backgammon , Monopoly , Scrabble , bridg e (declare r
play is easiest) .
5.12 Conside r carefull y the interpla y of chanc e event s and partia l informatio n in each of the
game s in Exercis e 5.11 .
a. For whic h is the standar d expectiminima x mode l appropriate ? Implemen t the algorith m
and run it in your game­playin g agent , with appropriat e modification s to the game­playin g
environment .
b. For whic h woul d the schem e describe d in Exercis e 5.7 be appropriate ?
c. Discus s how you migh t deal with the fact that in som e of the games , the player s do not
have the same knowledg e of the curren t state .
5.13 Th e Chinoo k checker s progra m make s extensiv e use of endgam e databases , whic h provid e
exact value s for ever y positio n withi n 6 move s of the end of the game . How migh t such database s
be generate d efficiently ?
5.14 Discus s how well the standar d approac h to gam e playin g woul d appl y to game s such as
tennis , pool , and croquet , whic h take plac e in a continuous , physica l state space .
5.15 Fo r a gam e with whic h you are familiar , describ e how an agen t coul d be denne d with
condition­actio n rules , subgoal s (and thei r condition s for generation) , and action­utilit y rules ,
instea d of by minima x search .
5.16 Th e minima x algorith m return s the best mov e for MA X unde r the assumptio n that MIN
plays optimally . Wha t happen s whe n MIN play s suboptimally ?
5.17 W e have assume d that the rules of each gam e defin e a utilit y functio n that is used by both
players , and that a utilit y of x for MAX mean s a utilit y of — x for MIN . Game s with this propert y
are calle d zero­su m games . Describ e how the minima x and alpha­bet a algorithm s chang e whe n
we have nonzero­su m games—tha t is, whe n each playe r has his or her own utilit y function . You
may assum e that each playe r know s the other' s utilit y function .
Part III
KNOWLEDG E AND REASONIN G
In Part II, we showe d that an agen t that has goal s and searche s for solution s to
the goal s can do bette r than one that just react s to its environment . We focuse d
mainl y on the questio n of how to carry out the search , leavin g asid e the questio n
of genera l method s for describin g state s and actions .
In this part, we exten d the capabilitie s of our agent s by endowin g them with
the capacit y for genera l logica l reasoning . A logical , knowledge­base d agen t
begin s with som e knowledg e of the worl d and of its own actions . It uses logica l
reasonin g to maintai n a descriptio n of the worl d as new percept s arrive , and to
deduc e a cours e of actio n that will achiev e its goals .
In Chapte r 6, we introduc e the basi c desig n for a knowledge­base d agent .
We then presen t a simpl e logica l languag e for expressin g knowledge , and show
how it can be used to draw conclusion s abou t the worl d and to decid e wha t to
do. In Chapte r 7, we augmen t the languag e to mak e it capabl e of expressin g a
wide variet y of knowledg e abou t comple x worlds . In Chapte r 8, we exercis e this
capabilit y by expressin g a significan t fragmen t of commonsens e knowledg e abou t
the real world , includin g time , change , objects , categories , events , substances ,
and of cours e money . In Chapter s 9 and 10 we discus s the theor y and practic e of
compute r system s for logica l reasoning .
6AGENT S THA T REASO N
LOGICALL Y
In which  we design  agents  that can form  representations  of the world,  use a process
of inference  to derive  new representations  about  the world,  and  use these  new
representations  to deduce  what  to do.
LOGICIn this chapter , we introduc e the basi c desig n for a knowledge­base d agent . As we discusse d
in Part I (see , for example , the statemen t by Crai k on page 13), the knowledge­base d approac h
is a particularl y powerfu l way of constructin g an agen t program . It aims to implemen t a view
of agent s in whic h they can be seen as knowing  abou t thei r world , and reasoning  abou t thei r
possibl e course s of action . Knowledge­base d agent s are able to accep t new task s in the form of
explicitl y describe d goals ; they can achiev e competenc e quickl y by bein g told or learnin g new
knowledg e abou t the environment ; and they can adap t to change s in the environmen t by updatin g
the relevan t knowledge . A knowledge­base d agen t need s to know man y things : the curren t state
of the world ; how to infe r unsee n propertie s of the worl d from percepts ; how the worl d evolve s
over time ; wha t it want s to achieve ; and wha t its own action s do in variou s circumstances .
We begi n in Sectio n 6.1 with the overal l agen t design . Sectio n 6.2 introduce s a simpl e new
environment , the wumpu s world , wher e a knowledge­base d agen t can easil y attai n a competenc e
that woul d be extremel y difficul t to obtai n by othe r means . Sectio n 6.3 discusse s the basi c
element s of the agen t design : a forma l languag e in whic h knowledg e can be expressed , and a
mean s of carryin g out reasonin g in such a language . Thes e two element s constitut e wha t we
call a logic . Sectio n 6.4 give s an exampl e of how thes e basi c element s wor k in a logi c calle d
propositiona l logic , and Sectio n 6.5 illustrate s the use of propositiona l logi c to buil d a logica l
agent for the wumpu s world .
A KNOWLEDGE­BASE D AGEN T
KNOWLEDG E BASE Th e centra l componen t of a knowledge­base d agen t is.its knowledg e base , or KB. Informally , a
knowledg e base is a set of representation s of facts abou t the world . Eac h individua l representatio n
SENTENC E "  is calle d a sentence . (Her e "sentence " is used as a technica l term . It is relate d to the sentence s
151
152 Chapte r 6. Agent s that Reaso n Logicall y
KNOWLEDG E
REPRESENTATIO N
LANGUAG E
INFERENC E
BACKGROUN D
KNOWLEDG Eof Englis h and othe r natura l languages , but is not identical. ) The sentence s are expresse d in a
languag e calle d a knowledg e representatio n language .
There mus t be a way to add new sentence s to the knowledg e base , and a way to quer y wha t
is known . The standar d name s for these task s are TEL L and ASK , respectively . The fundamenta l
requiremen t that we will impos e on TEL L and ASK is that whe n one ASK S a questio n of the
knowledg e base , the answe r shoul d follo w from wha t has been told (or rather , TELLed ) to the
knowledg e base previously . Late r in the chapter , we will be more precis e abou t the crucia l word
"follow. " For now , take it to mea n that the knowledg e base shoul d not just mak e up thing s as
it goes along . Determinin g wha t follow s from wha t the KB has been TELLe d is the job of the
inferenc e mechanism , the othe r main componen t of a knowledge­base d agent .
Figur e 6.1 show s the outlin e of a knowledge­base d agen t program . Lik e all our agents , it
takes a percep t as inpu t and return s an action . The agen t maintain s a knowledg e base , KB, whic h
may initiall y contai n som e backgroun d knowledge . Eac h time the agen t progra m is called , it
does two things . First , it TELL S the knowledg e base wha t it perceives.1 Second , it ASK S the
knowledg e base wha t actio n it shoul d perform . In the proces s of answerin g this query , logica l
reasonin g is used to prov e whic h actio n is bette r than all others , give n wha t the agen t know s and
what its goal s are. The agen t then perform s the chose n action .
functio n KB­AGEN1(  percept)  return s an action
static : KB, a knowledg e base
t, a counter , initiall y 0, indicatin g time
TELL(A'S , MAKE­PERCEPT­SENTENCE(percepf , t))
action  — ASK(KB,  MAKE­ACTION­QUERY(f) )
TELL(A"B , MAKE­ACTION­SENTENCE(flCf(0« , t))
t^t+  1
retur n action
Figur e 6.1 A  generi c knowledge­base d agent .
The detail s of the representatio n languag e are hidde n insid e two function s that implemen t
the interfac e betwee n the agen t progra m "shell " and the core representatio n and reasonin g system .
MAKE­PERCEPT­SENTENC E takes a percep t and a time and return s a sentenc e representin g the fact
that the agen t perceive d the percep t at the give n time , and MAKE­ACTION­QUER Y take s a time as
input and return s a sentenc e that is suitabl e for askin g wha t actio n shoul d be performe d at that
time . The detail s of the inferenc e mechanis m are hidde n insid e TEL L and ASK . Late r section s
will revea l thes e details .
A carefu l examinatio n of Figur e 6.1 reveal s that it is quit e simila r to the desig n of agents
with interna l state describe d in Chapte r 2. Becaus e of the definition s of TEL L and ASK , however ,
the knowledge­base d agen t is not an arbitrar y progra m for calculatin g action s base d on the interna l
state variable . At any point , we can describ e a knowledge­base d agen t at three levels :
1 Yo u migh t thin k of TEL L and ASK as procedure s that human s can use to communicat e with knowledg e bases . Don' t
be confuse d by the fact that here it is the agen t that is TELLin g thing s to its own knowledg e base .
Sectio n The Wumpu s Worl d Environmen t 153
KNOWLEDG E LEVE L
cpiSTEMOLOGICA L
LEVE L
LOGICA L LEVE L
IMPLEMENTATIO N
LEVE L
DECLARATIV E• The knowledg e leve l or epistemologica l leve l is the mos t abstract ; we can describ e the
agent by sayin g wha t it knows . For example , an automate d taxi migh t be said to know that
the Golde n Gate Bridg e link s San Francisc o and Mari n County . If TEL L and ASK wor k
correctly , then mos t of the time we can wor k at the knowledg e leve l and not worr y abou t
lowe r levels .
• The logica l level is the level at whic h the knowledg e is encode d into sentences . For example ,
the taxi migh t be describe d as havin g the logica l sentenc e Links(GGBridge,  SF, Marin)  in
its knowledg e base .
• The implementatio n leve l is the leve l that runs on the agen t architecture . It is the
level at whic h ther e are physica l representation s of the sentence s at the logica l level . A
sentenc e such as Links(GGBridge,  SF, Marin)  coul d be represente d in the KB by the strin g
"Link s (GGBridge , SF,Marin ) " containe d in a list of strings ; or by a "1" entr y in
a three­dimensiona l table indexe d by road link s and locatio n pairs ; or by a comple x set
of pointer s connectin g machin e addresse s correspondin g to the individua l symbols . The
choic e of implementatio n is very importan t to the efficien t performanc e of the agent , but it
is irrelevan t to the logica l level and the knowledg e level .
We said that it is possibl e to understan d the operatio n of a knowledge­base d agen t in term s of
what it knows . It is possible  to construct  a knowledge­based  agent  by TELLing  it what  it needs
to know.  Th e agent' s initia l program , befor e it start s to receiv e percepts , is buil t by addin g one
by one the sentence s that represen t the designer' s knowledg e of the environment . Provide d that
the representatio n languag e make s it easy to expres s this knowledg e in the form of sentences ,
this simplifie s the constructio n proble m enormously . Thi s is calle d the declarativ e approac h
to syste m building . Also , one can desig n learnin g mechanism s that outpu t genera l knowledg e
abou t the environmen t give n a serie s of percepts . By hookin g up a learnin g mechanis m to a
knowledge­base d agent , one can mak e the agen t fully autonomous .
6,2 TH E WUMPU S WORL D ENVIRONMEN T
Befor e launchin g into a full expositio n of knowledg e representatio n and reasoning , we will
WUMPU S WORL D describ e a simpl e environmen t class—th e wumpu s world —tha t provide s plent y of motivatio n
for logica l reasoning . Wumpu s was an early compute r game , base d on an agen t who explore s
a cave consistin g of room s connecte d by passageways . Lurkin g somewher e in the cave is the
wumpus , a beas t that eats anyon e who enter s its room . To mak e matter s worse , som e room s
contai n bottomles s pits that will trap anyon e who wander s into thes e room s (excep t for the
wumpus , who is too big to fall in). The only mitigatin g featur e of livin g in this environmen t is
the occasiona l heap of gold .
It turn s out that the wumpu s gam e is rathe r tame  by moder n compute r gam e standards .
However , it make s an excellen t testbe d environmen t for intelligen t agents . Michae l Geneseret h
was the first to sugges t this .
154 Chapte r 6. Agent s that Reaso n Logicall y
Specifyin g the environmen t
Like the vacuu m world , the wumpu s worl d is a grid of square s surrounde d by walls , wher e each
squar e can contai n agent s and objects . The agen t alway s start s in the lowe r left corner , a squar e
that we will labe l [1,1] . The agent' s task is to find the gold , retur n to [1,1 ] and clim b out of the
cave. An exampl e wumpu s worl d is, show n in Figur e 6.2.
?<r T?? 5S stenc h ^>
STAR T' Breeze  ­­ Breeze  ­
' Breeze  ­' Breeze  ­
' Breeze  ­
Figur e 6.2 A  typica l wumpu s world .
To specif y the agent' s task , we specif y its percepts , actions , and goals . In the wumpu s
world , these are as follows :
• In the square  containin g the wumpu s and in the directl y (not diagonally ) adjacen t square s
the agen t will perceiv e a stench .
• In the square s directl y adjacen t to a pit, the agen t will perceiv e a breeze .
• In the squar e wher e the gold is, the agen t will perceiv e a glitter .
• Whe n an agen t walk s into a wall , it will perceiv e a bump .
• Whe n the wumpu s is killed , it give s out a woefu l screa m that can be perceive d anywher e
in the cave .
• The percept s will be give n to the agen t in the form of a list of five symbols ; for example ,
if there is a stench , a breeze , and a glitte r but no bum p and no scream , the agen t will
receiv e the percep t [Stench,Breeze,  Glitter,None,None].  The agen t cannot  perceiv e its
own location .
• Just as in the vacuu m world , there are action s to go forward , turn right by 90°, and turn left
by 90°. In addition , the actio n Grab  can be used to pick up an objec t that is in the same
squar e as the agent . The actio n Shoot  can be used to fire an arro w in a straigh t line in the
directio n the agen t is facing . The arro w continue s unti l it eithe r hits and kills the wumpu s
or hits the wall . The agen t only has one arrow , so only the first Shoot  actio n has any effect .
Sectio n 6.2. Th e Wumpu s Worl d Environmen t 155
Finally , the actio n Climb  is used to leav e the cave ; it is effectiv e only whe n the agen t is in
the start square .
• The agen t dies a miserabl e deat h if it enter s a squar e containin g a pit or a live wumpus . It
is safe (but smelly ) to ente r a squar e with a dead wumpus .
• The agent' s goal is to find the gold and brin g it back to the start as quickl y as possible ,
withou t gettin g killed . To be precise , 100 0 point s are awarde d for climbin g out of the
cave whil e carryin g the gold , but there is a 1­poin t penalt y for each actio n taken , and a
10,000­poin t penalt y for gettin g killed .
As we emphasize d in Chapte r 2, an agen t can do well in a singl e environmen t merel y by
memorizin g the sequenc e of action s that happen s to work in that environment . To provid e a real
test, we need to specif y a complet e class of environments , and insis t that the agen t do well , on
average , over the whol e class . We will assum e a 4 x 4 grid surrounde d by walls . The agen t
alway s start s in the squar e labele d (1,1) , facin g towar d the right . The location s of the gold and
the wumpu s are chose n randomly , with a unifor m distribution , from the square s othe r than the
start square . In addition , each squar e othe r than the start can be a pit, with probabilit y 0.2.
In mos t of the environment s in this class , there is a way for the agen t to safel y retriev e the
gold. In som e environments , the agen t mus t choos e betwee n goin g hom e empty­hande d or takin g
a chanc e that coul d lead eithe r to deat h or to the gold . And in abou t 21% of the environment s
(the ones wher e the gold is in a pit or surrounde d by pits) , ther e is no way the agen t can get a
positiv e score . Sometime s life is just unfair .
After gainin g experienc e with this clas s of environments , we can experimen t with othe r
classes . In Chapte r 22 we conside r world s wher e two agent s explor e togethe r and can commu ­
nicat e with each other . We coul d also conside r world s wher e the wumpu s can move , or wher e
there are multipl e trove s of gold , or multipl e wumpuses.2
Actin g and reasonin g in the wumpu s worl d
We now know the rules of the wumpu s world , but we do not yet have an idea of how a wumpu s
world agen t shoul d act. An exampl e will clea r this up and will show why a successfu l agen t
will need to have som e kind of logica l reasonin g ability . Figur e 6.3(a ) show s an agent' s state of
knowledg e at the start of an exploratio n of the cave in Figur e 6.2, afte r it has receive d its initia l
percept . To emphasiz e that this is only a representation , we use letter s such as A and OK to
represen t sentences , in contras t to Figur e 6.2, whic h used (admittedl y primitive ) picture s of the
wumpu s and pits.
From the fact that there was no stenc h or breez e in [1,1] , the agen t can infe r that [1,2] and
[2,1] are free of dangers . The y are marke d with an OK to indicat e this. Fro m the fact that the
agen t is still alive , it can infe r that [1,1 ] is also OK.  A cautiou s agen t will only mov e into a squar e
that it know s is OK.  Let us suppos e the agen t decide s to mov e forwar d to [2,1] , givin g the scen e
in Figur e 6.3(b) .
The agen t detects  a breez e in [2,1] , so ther e mus t be a pit in a neighborin g square , eithe r
[2,2] or [3,1] . The notatio n PI indicate s a possibl e pit. The pit canno t be in [1,1] , becaus e the
2 Or is it wumpi ?
156 Chapte r 6. Agent s that Reaso n Logicall y
1,4
1,3
1,2
OK
1,1
OK2,4
2,3
2,2
2,1
OK3,4
3,3
3,2
3,14,4
4,3
4,2
4,1[X] = Agent
B = Breeze
G = Glitter,  Gold
OK = Safe square
P = Pit
S = Stench
V = Visited
W = Wumpus1,4
1,3
1,2
OK
1,1
V
OK2,4
2,3
2,2
P?
2,1 ,— |
* *
B
OK3,4
3,3
3,2
3,1 p?4,4
4,3
4,2
4,1
(a) (b )
Figur e 6.3 Th e firs t step take n by the agen t in the wumpu s world , (a ) The initia l sit­
uation , afte r percep t [None,  None,  None,  None,  None].  (b ) Afte r one move , wit h percep t
[None,  Breeze,  None,  None,  None].
1,4
1'3w<
120
S
OK
1,1
V
OK2,4
2,3
2,2
OK
2,1 „o
V
OK3,4
3,3
3,2
3 1P!4,4
4,3
4,2
4,1[A | = Agent
B = Breeze
G = Glitter,  Gold
OK = Safe square
P = Pit
S = Stench
V = Visited
W = Wumpus1,4
1,3 w.
1<2s
V
OK
1,1
V
OK2,4
P?
2'30
S G
B
2,2
V
OK
2'1 RK
V
OK3,4
3,3 p?
3,2
3 1' P!4,4
4,3
4,2
4,1
(a) (b )
Figur e 6.4 Tw o late r stage s in the progres s of the agent , (a ) Afte r the thir d move ,
with percep t [Stench,  None,  None,  None,  None].  (b ) Afte r the fift h move , wit h percep t
[Stench,  Breeze, Glitter,  None,  None].
agent was alread y ther e and did not fall in. At this point , there is only one know n squar e that is
OK and has not been visite d yet. So the pruden t agen t will turn around , go back to [1,1] , and
then procee d to [1,2] , givin g the state of knowledg e in Figur e 6.4(a) .
The agen t detect s a stenc h in [1,2] , whic h mean s that there mus t be a wumpu s nearby . But
the wumpu s canno t be in [1,1 ] (or it woul d have eaten the agen t at the start) , and it canno t be in
Sectio n 6.3. Representation , Reasoning , and Logi c 157
[2,2] (or the agen t woul d have detecte d a stenc h whe n it was in [2,1]) . Therefore , the agen t can
infer that the wumpu s is in [1,3] . The notatio n Wl indicate s this. Mor e interestin g is that the
lack of a Breeze  percep t in [1,2] mean s that there mus t be a pit in [3,1] . The reasonin g is that no
breez e in [1,2 ] mean s ther e can be no pit in [2,2] . But we alread y inferre d that ther e mus t be a
pit in eithe r [2,2 ] or [3,1] , so this mean s it mus t be in [3,1] . Thi s is a fairl y difficul t inference ,
becaus e it combine s knowledg e gaine d at differen t time s in differen t places , and relie s on the
lack of a percep t to mak e one crucia l step. The inferenc e is beyon d the abilitie s of most animals ,
but it is typica l of the kind of reasonin g that a logica l agen t does .
After these impressiv e deductions , there is only one know n unvisite d OK squar e left, [2,2] ,
so the agen t will mov e there . We will not show the agent' s state of knowledg e at [2,2] ; we just
assum e the agen t turn s and move s to [2,3] , givin g us Figur e 6.4(b) . In [2,3] , the agen t detect s a
glitter , so it shoul d grab the gold and head for home , makin g sure its retur n trip only goes throug h
square s that are know n to be OK.
In the rest of this chapter , we describ e how to build a logica l agen t that can represen t belief s
such as "ther e is a pit in [2,2] or [3,1] " and "ther e is no wumpu s in [2,2], " and that can mak e all
the inference s that were describe d in the precedin g paragraphs .
6.3 REPRESENTATION . REASONING . AND LOGI C
KNOWLEDG E
REPRESENTATIO N
SYNTA X
SEMANTIC SIn this section , we will discus s the natur e of representatio n languages , and of logica l language s
in particular , and explai n in detai l the connectio n betwee n the languag e and the reasonin g
mechanis m that goes with it. Together , representatio n and reasonin g suppor t the operatio n of a
knowledge­base d agent .
The objec t of knowledg e representatio n is to expres s knowledg e in computer­tractabl e
form , such that it caiTb e used to help agent s perfor m well . A knowledg e representatio n languag e
is define d by two aspects :
• The synta x of a languag e describe s the possibl e configuration s that can constitut e sentences .
Usually , we describ e synta x in term s of how sentence s are represente d on the printe d page ,
but the real representatio n is insid e the computer : eac h sentenc e is implemente d by a
physica l configuratio n or physica l propert y of some part of the agent . For now , thin k of
this as bein g a physica l patter n of electron s in the computer' s memory .
• The semantic s determine s the facts in the worl d to whic h the sentence s refer . Withou t
semantics , a sentenc e is just an arrangemen t of electron s or a collectio n of mark s on a page .
With semantics , each sentenc e make s a claim abou t the world . And with semantics , we
can say that whe n a particula r configuratio n exist s withi n an agent , the agen t believe s the
correspondin g sentence .
For example , the synta x of the languag e of arithmeti c expression s say s that if x and y are
expression s denotin g numbers , then x > y is a sentenc e abou t numbers . The semantic s of the
languag e says that x > y is false whe n y is a bigge r numbe r than x, and true otherwise .
158 Chapte r 6. Agent s that Reaso n Logicall y
Provide d the synta x and semantic s are define d precisely , we can call the languag e a logic.3
From the synta x and semantics , we can deriv e an inferenc e mechanis m for an agen t that uses the
language . We now explai n how this come s about .
First, recal l that the semantic s of the languag e determin e the fact to whic h a give n sentenc e
refer s (see Figur e 6.5) . It is impprtan t to distinguis h betwee n fact s and thei r representations .
Facts are part of the world,4 wherea s their representation s mus t be encode d in som e way that can
be physicall y store d withi n an agent . We canno t put the worl d insid e a compute r (nor can we put
it insid e a human) , so all reasonin g mechanism s mus t operat e on representation s of facts , rathe r
than on the facts themselves . Because sentences are physical configurations of parts of the agent,
reasoning  must  be a process  of constructing  new  physical  configurations  from  old ones.  Proper
reasoning  should ensure that the new configurations represent facts that actually follow from the
facts that the  old configurations  represent.
Sentence s
Representation
WorldEntail s­•­ Sentenc e
FactsFollow sFact
Figur e 6.5 Th e connectio n betwee n sentence s and fact s is provide d by the semantic s of the
language . The propert y of one fact followin g from som e othe r facts is mirrore d by the propert y of
one sentenc e bein g entaile d by som e othe r sentences . Logica l inferenc e generate s new sentence s
that are entaile d by existin g sentences .
ENTAILMEN TConside r the followin g example . Fro m the fact that the sola r syste m obey s the laws of
gravitation , and the fact of the curren t arrangemen t of the sun, planets , and other bodies , it follow s
(so the astronomer s tell us) thatplut o will eventuall y spin off into the interstella r void . But if our
agent reason s improperly , it migh t start with representation s of the first two facts and end with a
representatio n that mean s that Plut o will shortl y arriv e in the vicinit y of Bucharest . Or we migh t
end up with "logical " reasonin g like that in Figur e 6.6. Y
We wan t to generat e new sentence s that are necessaril y true, given that the old sentence s
are true . Thi s relatio n betwee n sentence s is calle d entailment , and mirror s the relatio n of on'e
fact followin g from anothe r (Figur e 6.5) . In mathematica l notation , the relatio n of entailmen t
betwee n a knowledg e base KB and a sentenc e a is pronounce d "KB entail s a" and writte n as
An inferenc e procedur e can do one of two things : give n a knowledg e base KB, it can generat e
3 Thi s is perhap s a rathe r broa d interpretatio n of the term "logic, " one that make s "representatio n language " and "logic "
synonymous . However , mos t of the principle s of logi c appl y at this genera l level , rathe r than just at the leve l of the
particula r language s mos t ofte n associate d with the term .
4 As Wittgenstei n (1922 ) put it in his famou s Tractatus  Logico­Philosophicus:  "Th e worl d is everythin g that is the
case." We are usin g the word "fact " in this sense : as an "arrangement " of the worl d that may or may not be the case .
Sectio n 6.3. Representation , Reasoning , and Logi c 159
FIRS T VILLAGER : We have foun d a witch . May we burn her?
ALL: A witch ! Burn her!
BEDEVERE : Why do you think she is a witch ?
SECON D VILLAGER : She turne d me into a newt .
BEDEVERE : A newt ?
SECON D VILLAGE R (after  looking at himself  for some  time):  I got better .
ALL: Bur n her anyway .
BEDEVERE : Quiet ! Quiet ! Ther e are way s of tellin g whethe r she is a witch .
BEDEVERE : Tell me ... wha t do you do with witches ?
ALL : Burn them .
BEDEVERE : And wha t do you burn , apar t from witches ?
FOURT H VILLAGER : ... Wood ?
BEDEVERE : So why do witche s burn ?
SECON D VILLAGER : (pianissimo)  Becaus e they'r e made of wood ?
BEDEVERE : Good .
ALL : I see. Yes, of course .
BEDEVERE : So how can we tell if she is mad e of wood ?
FIRS T VILLAGER : Mak e a bridg e out of her.
BEDEVERE : Ah ... but can you not also mak e bridge s out of stone ?
ALL : Yes , of cours e ... um ... er ...
BEDEVERE : Does wood sink in water ?
ALL: No, no, it floats.  Thro w her in the pond .
BEDEVERE : Wait . Wait.. . tell me, what  also float s on water ?
ALL : Bread ? No, no no. Apple s ... grav y ... very smal l rock s ...
BEDEVERE : No, no no,
KING ARTHUR : A duck !
(They  all turn  and look  at ARTHUR.  BEDEVERE  looks  up very  impressed.)
BEDEVERE : Exactly . So ... logicall y ...
FIRS T VILLAGE R (beginning  to pick up the thread):  If she ... weigh s the same as
a duck ... she's mad e of wood .
BEDEVERE : And therefore ?
ALL: A witch !
Figur e 6.6 A n exampl e of "logical " reasonin g gone wrong . (Excerpte d with permissio n from
Monty  Python  and the Holy  Grail,  © 1977 , Ree d Consume r Books. )
SOUN D
TRUTH­PRESERVIN Gnew sentence s a that purpor t to be entaile d by KB. Or, give n a knowledg e base KB and anothe r
sentenc e a, it can repor t whethe r or not a is entaile d by KB. An inferenc e procedur e that generate s
only entaile d sentence s is calle d soun d or truth­preserving .
An inferenc e procedur e z can be describe d by the sentence s that it can derive . If z can
deriv e a from KB, a logicia n woul d writ e
KB
160 Chapte r 6. Agent s that Reaso n Logicall y
PROO F
COMPLET E
PROO F THEOR Ywhic h is pronounce d "Alph a is derive d from KB by /" or "/' derive s alph a from KBT  Sometime s
the inferenc e procedur e is implici t and the i is omitted . The recor d of operatio n of a soun d
inferenc e procedur e is calle d a proof .
In understandin g entailmen t and proof , it may help to thin k of the set of all consequence s
of KB as a haystac k and a as a ne,edle . Entailmen t is like the needl e bein g in the haystack ; proo f
is like findin g it. For real haystacks , whic h are finit e in extent , it seem s obviou s that a systemati c
examinatio n can alway s decid e whethe r the needl e is in the haystack . Thi s is the questio n
of completeness : an inferenc e procedur e is complet e if it can find a proo f for any sentenc e
that is entailed . But for man y knowledg e bases , the haystac k of consequence s is infinite , and
completenes s become s an importan t issue.5
We have said that soun d inferenc e is desirable . Ho w is it achieved ? The  key to  sound
inference  is to have the inference steps  respect  the semantics of  the sentences  they  operate  upon.
That is, given  a knowledge  base,  KB,  the inference  steps  should  only  derive  new sentences  that
represent  facts  that  follow from  the facts  represented  by KB.  B y examinin g the semantic s of
logica l languages , we can extrac t wha t is calle d the proo f theor y of the language , whic h specifie s
the reasonin g steps that are sound . Conside r the followin g familia r exampl e from mathematics ,
whic h illustrate s syntax , semantics , and proo f theory . Suppos e we have the followin g sentence :
E = me2
The synta x of the "equatio n language " allow s two expression s to be connecte d by an "=" sign .
An expressio n can be a simpl e symbo l or number , a concatenatio n of two expressions , two
expression s joine d by a "+" sign , and so on. Th e semantic s of the languag e says that the
two expression s on each side of "=" refe r to the same quantity ; that the concatenatio n of two
expression s refer s to the quantit y that is the produc t of the quantitie s referre d to by each of the
expressions ; and so on. From the semantics , we can show that a new sentenc e can be generate d
by, for example , concatenatin g the same expressio n to both sides of the equation :
ET = mc2T
Most reader s will have plent y of experienc e with inferenc e of this sort. Logica l language s are like
this simpl e equatio n language , but rathe r than dealin g with algebrai c propertie s and numerica l
quantities , they mus t deal with more or less everythin g we migh t wan t to represen t and abou t
whic h we migh t wan t to reason .
Representatio n
We will now look a littl e mor e deepl y into the natur e of knowledg e representatio n languages ,
with the aim of designin g an appropriat e synta x and semantics . We will begi n with two familia r
classe s of languages , programmin g language s and natura l languages , to see wha t they are good
at representin g and wher e they have problems .
Programmin g language s (suc h as C or Pasca l or Lisp ) are good for describin g algorithm s
and concret e data structures . We coul d certainl y imagin e usin g an 4 x 4 arra y to represen t
the content s of the wumpu s world , for example . Thus , the programmin g languag e statemen t
World[2,2]  «—Pit  is a fairl y natura l way to say that ther e is a pit in squar e [2,2] . However , mos t
5 Compar e with the case of infinit e searc h spaces  in Chapte r 3, wher e depth­firs t searc h is not complete .
Sectio n 6.3. Representation , Reasoning , and Logi c 161
programmin g language s do not offe r any easy way to say "ther e is a pit in [2,2 ] or [3,1] " or
"there is a wumpu s in some  square. " The proble m is that programmin g language s are designe d to
completel y describ e the state of the compute r and how it change s as the progra m executes . But
we woul d like our knowledg e representatio n languag e to suppor t the case wher e we do not have
complet e information—wher e we do not know for certai n how thing s are, but only kno w som e
possibilitie s for how they migh t or migh t not be. A languag e that does not let us do this is not
expressive  enough .
Natura l language s (suc h as Englis h or Spanish ) are certainl y expressive—w e manage d to
write this whol e book usin g natura l languag e with only occasiona l lapse s into othe r language s
(includin g logic , mathematics , and the languag e of diagrams) . But natura l language s have evolve d
more to mee t the need s of communicatio n rathe r than representation . Whe n a speake r point s
and says , "Look! " the listene r come s to know that, say, Superma n has finall y appeare d over the
rooftops . But we woul d not wan t to say that the sentenc e "Look! " encode d that fact. Rather ,
the meanin g of the sentenc e depend s both on the sentenc e itsel f and on the contex t in whic h the
sentenc e was spoken . A natura l languag e is a good way for a speake r to get a listene r to com e
to know something , but ofte n this sharin g of knowledg e is don e withou t explici t representatio n
of the knowledg e itself . Natura l language s also suffe r from ambiguity—i n a phras e such as
"smal l dogs and cats, " it is not clear whethe r the cats are small . Contras t this to the programmin g
languag e construc t "­d  + c," wher e the precedenc e rule s for the languag e tell us that the minu s
sign applie s to d, not to d + c.
A goo d knowledg e representatio n languag e shoul d combin e the advantage s of natura l
language s and forma l languages . It shoul d be expressiv e and concis e so that we can say everythin g
we need to say succinctly . It shoul d be unambiguou s and independen t of context , so that wha t we
say toda y will still be interpretabl e tomorrow . And it shoul d be effectiv e in the sens e that there
shoul d be an inferenc e procedur e that can mak e new inference s from sentence s in our language .
Many representatio n language s hav e been designe d to try to achiev e thes e criteria . In
this book , we concentrat e on first­orde r logi c as our representatio n languag e becaus e it form s
the basi s of mos t representatio n scheme s in AI. Just as it woul d be overoptimisti c to believ e
that one can mak e real progres s in physic s withou t understandin g and usin g equations , it is
importan t to develo p a talen t for workin g with logica l notatio n if one is to mak e progres s in
artificia l intelligence . However , it is also importan t not to get too concerne d with the specifics
of logica l notation—afte r all, there are literall y dozen s of differen t versions , som e with x's and
>''s and exoti c mathematica l symbols , and som e with rathe r visuall y appealin g diagram s with
arrow s and bubbles . Th e mai n thin g to keep hold of is how a precise , forma l languag e can
represen t knowledge , and how mechanica l procedure s can operat e on expression s in the languag e
to perfor m reasoning . The fundamenta l concept s remai n the sam e no matte r wha t languag e is
being used to represen t the knowledge .
'INTERPRETATIO NSemantic s
In logic , the meanin g of a sentenc e is wha t it state s abou t the world , that the worl d is this way and
not that way. So how does a sentenc e get its meaning ? How do we establis h the correspondenc e
betwee n sentence s and facts ? Essentially , this is up to the perso n who wrot e the sentence . In
order  to say wha t it means , the write r has to provid e an interpretatio n for it; to say wha t fact
162 Chapte r 6. Agent s that Reaso n Logicall y
THE LANGUAG E OF THOUGH T
Philosopher s and psychologist s have long pondere d how it is that human s and othe r
animal s represen t knowledge . It is clea r that the evolutio n of natura l languag e has
playe d an importan t role in developin g this abilit y in humans . But it is also true that
human s seem to represen t muc h of their knowledg e in a nonverba l form . Psychologist s
have done studie s to confir m that human s remembe r the "gist " of somethin g they have
read rathe r than the exac t words . Yo u coul d look at Anderson' s (1980 , page 96)
descriptio n of an experimen t by Wanner , or you coul d perfor m your own experimen t
by decidin g whic h of the followin g two phrase s forme d the openin g of Sectio n 6.3:
"In this section , we will discus s the natur e of representatio n languages... "
"This sectio n cover s the topic of knowledg e representatio n languages... "
In Wanner' s experiment , subject s mad e the righ t choic e at chanc e level—abou t 50%
of the time—bu t remembere d the overal l idea of wha t they read with bette r than
90% accuracy . Thi s indicate s that the exac t word s are not part of the representation s
they formed . A simila r experimen t (Sachs , 1967 ) showe d that subject s remembe r the
word s for a shor t time (second s to tens of seconds) , but eventuall y forge t the word s
and remembe r only the meaning . This suggest s that peopl e proces s the word s to form
some kind of nonverba l representatio n whic h they maintai n as memories .
The exac t mechanis m by whic h languag e enable s and shape s the representatio n of
ideas in human s remain s a fascinatin g question . The famou s Sapir­Whor f hypothesi s
claim s that the languag e we spea k profoundl y influence s the way in whic h we thin k
and mak e decisions , in particula r by settin g up the categor y structur e by whic h we
divid e up the worl d into differen t sorts of objects . Whor f (1956 ) claime d that Eskimo s
have man y word s for snow , and thus experienc e snow in a differen t way from speaker s
of othe r languages . His analysi s has sinc e been discredite d (Pullum , 1991) ; Inuit ,
Yupik , and othe r relate d language s seem to have abou t the sam e numbe r of word s
for snow­relate d concept s as Englis h (conside r blizzard , sprinkling , flurries , powder ,
slush , snowbank , snowdrift , etc.) . Of course , differen t language s do carv e up the
world differently . Spanis h has two word s for "fish, " one for the live anima l and one
for the food . Englis h does not mak e this distinction , but it does have the cow/bee f
distinction . Ther e is no evidenc e that this mean s that Englis h and Spanis h speaker s
think abou t the worl d in fundamentall y differen t ways .
For our purposes , it is importan t to remembe r that the languag e used to represen t
an agent' s interna l knowledg e is quit e differen t from the externa l languag e used to
communicat e with othe r agents . (See Chapte r 22 for the stud y of communication. )
Sectio n 6.3. Representation , Reasoning , and Logi c 163
it correspond s to. A sentenc e does not mea n somethin g by itself.  Thi s is a difficul t concep t to
accept , becaus e we are used to language s like Englis h wher e the interpretatio n of most thing s
was fixed a long time ago.
The idea of interpretatio n is easie r to see in made­u p languages . Imagin e that one spy
want s to pass a messag e to another , but worrie s that the messag e may be intercepted . The two
spies coul d agre e in advanc e on a "nonstandar d interpretatio n in which , say, the interpretatio n of
"Pope " is a particula r piece of microfil m and the interpretatio n of "Denver " is the pumpki n left
on the porch , and so forth . Then , whe n the first spy send s a newspaper  clippin g with the headlin e
"The Pope is in Denver, " the secon d spy will know that the microfil m is in the pumpkin .
It is possible , in principle , to defin e a languag e in whic h ever y sentenc e has a completel y
arbitrar y interpretation . But in practice , all representatio n language s impos e a systematic  relation ­
COMPOSITIONA L shi p betwee n sentence s and facts . The language s we will deal with are all compositional —the
meanin g of a sentenc e is a functio n of the meanin g of its parts . Just as the meanin g of the math ­
ematica l expressio n jc2 + y2 is relate d to the meanings  of r2 and y2, we woul d like the meanin g
of the sentenc e "S\^  and £1,2 " to be relate d to the meaning s of "£1.4 " and "Si,2­ " It woul d be
very strang e if "51,4 " mean t there is a stenc h in squar e [1,4] and "$1,2 " mean t there is a stenc h in
squar e [1,2] , but "Si ,4 and £1,2" mean t that Franc e and Polan d drew 1­1 in last week' s ice­hocke y
qualifyin g match . In Sectio n 6.4, we describ e the semantic s of a simpl e language , the languag e
of prepositiona l logic , that obey s constraint s like these . Such constraint s mak e it easy to specif y
a proo f theor y that respect s the semantics .
Once a sentenc e is give n an interpretatio n by the semantics , the sentenc e says that the
" worl d is this way and not that way . Hence , it can be true or false . A sentence  is true under  a
particular  interpretation  if the state of  affairs  it represents  is the case.  Not e that truth depend s
both on the interpretatio n of the sentenc e and on the actua l state of the world . For example , the
sentenc e "5],2 " woul d be true unde r the interpretatio n in whic h it mean s that there is a stenc h in
[1,2], in the worl d describe d in Figur e 6.2. But it woul d be false in world s that do not have a
stenc h in [1,2] , and it woul d be false in Figur e 6.2 unde r the interpretatio n in whic h it mean s that
there is a breez e in [1,2] .
LOGICA L INFERENC E
DEDUCTIO NInferenc e
The term s "reasoning " and "inference " are generall y used to cove r any proces s by whic h con­
clusion s are reached . In this chapter , we are mainl y concerne d with soun d reasoning , whic h
we will call logica l inferenc e or deduction . Logica l inferenc e is a proces s that implement s the
entailmen t relatio n betwee n sentences . Ther e are a numbe r of way s to approac h the desig n of
logica l inferenc e systems . We will begi n with the idea of a necessaril y true sentence .
VALIDValidit y and satisfiabilit y
A sentenc e is vali d or necessaril y true if and only if it is true unde r all possibl e interpretation s in
all possibl e worlds , that is, regardles s of wha t it is suppose d to mea n and regardles s of the state
of affair s in the univers e bein g described . For example , the sentenc e
"Ther e is a stenc h at [1,1] or there is not a stenc h at [1,1]. "
164 Chapte r 6. Agent s that Reaso n Logicall y
SATISFIABL E
UNSATISFIABL Eis valid , becaus e it is true whethe r or not "ther e is a stenc h in [1,1] " is true, and it is true regardles s
of the interpretatio n of "ther e is a stenc h in [ 1,1]. " In contrast ,
"Ther e is an open area in the squar e in fron t of me or there is a wall in the squar e in
front of me."
is not vali d by itself . It is only valid unde r the assumptio n that ever y squar e has eithe r a wall or
an open area in it. So the sentenc e
"If ever y squar e has eithe r a wall or an open area in it, then there is an open area in
the square  in fron t of me, or there is a wall in the squar e in fron t of me."
is valid.6 Ther e are severa l synonym s for valid sentences . Som e author s use the term s analytic ^
sentence s or tautologie s for valid sentences .
A sentenc e is satisfiabl e if and only if ther e is som e interpretatio n in som e worl d for
whic h it is true . The sentenc e "ther e is a wumpu s at [1,2] " is satisfiabl e becaus e there migh t
well be a wumpu s in that square , even thoug h there does not happe n to be one in Figur e 6.2. A
sentenc e that is not satisfiabl e is unsatisfiable . Self­contradictor y sentence s are unsatisfiable , if
the contradictorines s does not depen d on the meanfiTg s of the symbols . For example , the sentenc e
"Ther e is a wall in fron t of me and there is no wall in fron t of me"
is unsatisfiable .
Inferenc e in computer s
It migh t seem that vali d and unsatisfiabl e sentence s are useless , becaus e they can only expres s
thing s that are obviousl y true or false . In fact, we will see that validit y and unsatisfiabilit y are
crucia l to the abilit y of a compute r to reason . " "
The compute r suffer s from two handicaps : it does not necessaril y know the interpretatio n
you are usin g for the sentence s in the knowledg e base , and it know s nothin g at all abou t the worl d
excep t wha t appear s in the knowledg e base . Suppos e we ask the compute r if it is OK to mov e to
squar e [2,2] . The compute r does not know wha t OK means , nor does it know wha t a wumpu s or
a pit is. So it canno t reaso n informall y as we did on page 155. All it can do is see if its knowledg e
base entail s the sentenc e "[2,2 ] is OK. " In othe r words , the inferenc e procedur e has to show that
the sentenc e "If KB is true then [2,2 ] is OK" is a valid sentence . If it is valid , then it does not
matte r that the compute r does not know the interpretatio n you are usin g or that it does not know
much abou t the world—th e conclusio n is guarantee d to be correc t unde r all interpretation s in all
world s in whic h the origina l KB is true . In Sectio n 6.4, we will give an exampl e of a forma l
procedur e for decidin g if a sentenc e is valid .
Wha t make s forma l inferenc e powerfu l is that ther e is no limi t to the complexit y of the
sentence s it can handle . Whe n we thin k of valid sentences , we usuall y thin k of simpl e example s
like "The wumpu s is dead or the wumpu s is not dead. " But the forma l inferenc e mechanis m can
just as wel l deal with valid sentence s of the form "If KB then P" wher e KB is a conjunctio n of
thousand s of sentence s describin g the laws of gravit y and the curren t state of the sola r system ,
and Pis a long descriptio n of the eventua l departur e of Plut o from the system .
6 In thes e examples , we are assumin g that word s like "if." "then, " "every, " "or" and "not " are part of the standar d synta x
of the language , and thus are not open to varyin g interpretation .
Sectio n 6.3. Representation , Reasoning , and Logi c 165
To reiterate , the grea t thin g abou t forma l inferenc e is that it can be used to deriv e vali d
conclusion s even whe n the compute r does not kno w the interpretatio n you are using . Th e
compute r only report s valid conclusions , whic h mus t be true regardles s of your interpretation .
Becaus e you know the interpretation , the conclusion s will be meaningfu l to you, and they are
guarantee d to follo w from you r premises . The  word  "you"  in this  paragraph  can be applied
equally  to human  and computer  agents.
PROPOSITIONA L
LOGI C
BOOLEA N
CONNECTIVE S
ONTOLOGICA L
COMMITMENT S
TEMPORA L LOGI C
S'STEMOLOGICA LCOMMITMENT SLogic s
To summarize , we can say that a logic consist s of the following :
1. A forma l syste m for describin g state s of affairs , consistin g of
(a) the synta x of the language , whic h describe s how to mak e sentences , and
(b) the semantic s of the language , whic h state s the systemati c constraint s on how sen­
tence s relat e to state s of affairs .
2. The proo f theory —a set of rules for deducing  the entailment s of a set of sentences .
We will concentrat e on two kind s of logic : propositiona l or Boolea n logic , and first­orde r logic
(more precisely , first­orde r predicat e calculu s with equality) .
In propositiona l logic , symbol s represen t whol e proposition s (facts) ; for example , D
migh t have the interpretatio n "the wumpu s is dead. " whic h may or may not be a true proposition .
Propositio n symbol s can be combine d usin g Boolea n connective s to generat e sentence s with more
comple x meanings . Suc h a logic make s very little commitmen t to how thing s are represented , so
it is not surprisin g that it does not give us muc h mileag e as a representatio n language .
First­orde r logic commit s to the representatio n of world s in term s of object s and predicate s
on object s (i.e., propertie s of object s or relation s betwee n objects) , as well as usin g connective s
and quantifiers , whic h allo w sentence s to be writte n abou t everythin g in the univers e at once .
First­orde r logic seem s to be able to captur e a good deal of wha t we know abou t the world , and
has been studie d for abou t a hundre d years . We will spen d therefor e a good deal of time lookin g
at how to do representatio n and deductio n usin g it.
It is illuminatin g to conside r logic s in the ligh t of thei r ontologica l and epistemologica l
commitments . Ontologica l commitment s have to do with the natur e of reality.  For example ,
propositiona l logic assume s that there are facts that eithe r hold or do not in the world . Each fact
can be in one of two states : true or false . First­orde r logic assume s more : namely , that the worl d
consist s of object s with certai n relation s betwee n them that do or do not hold . Special­purpos e
logic s mak e still furthe r ontologica l commitments ; for example , tempora l logi c assume s that
the worl d is ordere d by a set of time point s or intervals , and include s built­i n mechanism s for
reasonin g abou t time .
Epistemologica l commitment s have to do with the possibl e state s of knowledge  an agen t
can have usin g variou s type s of logic . In both propositiona l and first­orde r logic , a sentenc e
represent s a fact and the agen t eithe r believe s the sentenc e to be true , believe s it to be false ,
or is unabl e to conclud e eithe r way . Thes e logic s therefor e have thre e possibl e state s of belie f
regardin g any sentence . System s usin g probabilit y theory , on the othe r hand , can hav e any
degree  of belief , ranging  from 0 (tota l disbelief ) to 1 (tota l belief) . For example , a probabilisti c
166 Chapte r 6. Agent s that Reaso n Logicall y
wumpus­worl d agen t migh t believ e that the wumpu s is in [1,3 ] with probabilit y 0.75 . System s
FUZZY LOGI C base d on fuzz y logi c can have degree s of belie f in a sentence , and also allow degrees  of truth:
a fact need not be true or false in the world , but can be true  to a certai n degree . For example ,
"Vienn a is a large city" migh t be true only to degre e 0.6. The ontologica l and epistemologica l
commitment s of variou s logic s are summarize d in Figur e 6.7. '/­ ;
Languag e
Prepositiona l logic
First­orde r logic
Tempora l logi c
Probabilit y theor y
Fuzz y logi cOntologica l Commitmen t
(Wha t exist s in the world )
facts
facts, objects , relation s
facts , objects , relations , time s
facts
degre e of truthEpistemologica l Commitmen t
(Wha t an agen t believe s abou t facts )
true/false/unknow n
true/false/unknow n
true/false/unknow n
degre e of belie f 0...1
degre e of belie f 0...1
Figur e 6.7 Forma l language s and their ontologica l and epistemologica l commitments .
6.4 PROPOSITIONA L LOGIC:  A VER Y SIMPL E LOGI C
Despit e its limite d expressiveness , prepositiona l logi c serve s to illustrat e man y of the concept s
of logic just as well as first­orde r logic . We will describ e its syntax , semantics , and associate d
inferenc e procedures .
CONJUNCTIO N
(LOGIC )
DISJUNCTIO NSynta x
The synta x of prepositiona l logic is simple . The symbol s of prepositiona l logic are the logica l
constant s True  and False,  propositio n symbol s such as P and Q, the logica l connective s A, V, •&,
=>, and ­i, and parentheses , (). All sentence s are mad e by puttin g these symbol s togethe r using
the followin g rules :
• The logica l constant s True  and False  are sentence s by themselves .
• A prepositiona l symbo l such as P or Q is a sentenc e by itself.
• Wrappin g parenthese s aroun d a sentenc e yield s a sentence , for example , (P A Q).
• A sentenc e can be forme d by combinin g simple r sentence s with one of the five logica l
connectives :
A (and) . A sentenc e whos e main connectiv e is A, such as P A (Q V R), is calle d a
conjunctio n (logic) ; itspart s are the conjuncts . (The A look s like an "A" for "And." )
V (or) . A sentenc e usin g V, such as A V (P A Q), is a disjunctio n of the disjunct s A
and (P A Q). (Historically , the V come s from the Latin "vel, " whic h mean s "or." For
most people , it is easie r to remembe r as an upside­dow n and. )
Sectio n 6.4. Prepositiona l Logic : A Very Simpl e Logi c 167
^PLICATIO N => • (implies) . A sentenc e such as (PA Q) =>• R is calle d ani implicatio n (or conditional) .
PREMIS E It s premis e or anteceden t is P A Q, and its conclusio n or consequen t is R. Impli ­
CONCLUSIO N cation s are also know n as rule s or if­the n statements . The implicatio n symbo l is
sometime s writte n in othe r book s as D or —.
EQUIVALENC E <£ > (equivalent) . The sentenc e (P A Q) &  (Q A P) is an equivalenc e (also calle d a
biconditional) .
­NEGATIO N ­1 (not) . A sentenc e such as ­<P is calle d the negatio n of P. All the othe r connective s
combin e two sentence s into one; ­> is the only connectiv e that operate s on a singl e
sentence .
Figur e 6.8 give s a forma l gramma r of propositiona l logic ; see page 854 if you are not fa­
ATOMI C SENTENCE S milia r with the BNF notation . The gramma r introduce s atomi c sentences , whic h in propositiona l
SENTENCE S logi c consis t of a singl e symbo l (e.g. , P), and comple x sentences , whic h contai n connective s or
LITERA L parenthese s (e.g. , P A Q). The term litera l is also used , meanin g eithe r an atomi c sentence s or a
negate d atomi c sentence .
Sentence  — AtomicSentence  ComplexSentence
AtomicSentence
ComplexSentence  —True | False
P Q R .. .
( Sentence  )
Sentence  Connective  Sentence
­^Sentence
Connective  — A | V
Figur e 6.8 A  BNF (Backus­Nau r Form ) gramma r of sentence s in propositiona l logic .
Strictl y speaking , the gramma r is ambiguou s — a sentenc e such as P A Q V R coul d be
parse d as eithe r (P A Q) V R or as P A (Q V R). Thi s is simila r to the ambiguit y of arithmeti c
expression s such as P + Q x R, and the way to resolv e the ambiguit y is also similar : we pick an
order of precedenc e for the operators , but use parenthese s wheneve r ther e migh t be confusion .
The orde r of precedenc e in propositiona l logic is (fro m highes t to lowest) : ­>, A, V, =>•, and ­O.
Hence , the sentenc e
is equivalen t to the sentenc e
((­iP ) V (Q A R)) => S.
168 Chapte r 6. Agent s that Reaso n Logicall y
TRUT H TABL ESemantic s
The semantic s of prepositiona l logic is also quite straightforward . We defin e it by specifyin g
the interpretatio n of the propositio n symbol s and constants , and specifyin g the meaning s of the
logica l connectives .
A propositio n symbo l cari mea n whateve r you want . Tha t is, its interpretatio n can be any
arbitrar y fact. The interpretatio n of P migh t be the fact that Paris is the capita l of Franc e or that
the wumpu s is dead . A sentenc e containin g just a propositio n symbo l is satisiiabl e but not valid :
it is true just when the fact that it refer s to is the case.
With logica l constants , you have no choice ; the sentenc e True  alway s has as its interpretatio n
the way the worl d actuall y is—th e true fact. The sentenc e False  alway s has as its interpretatio n
the way the worl d is not.
A comple x sentenc e has a meanin g derive d from the meanin g of its parts . Each connective _
can be though t of as a function . Just as additio n is a functio n that take s two number s as inpu t
and return s a number , so and is a functio n that take s two truth value s as inpu t and return s a truth
value . We know that one way to defin e a functio n is to mak e a table that gives the outpu t value
for every possibl e inpu t value . For most function s (such as addition) , this is impractica l becaus e
of the size of the table , but there are only two possibl e truth values , so a logica l functio n with
two argument s need s a table with only four entries . Suc h a table is calle d a truth table . We give
truth table s for the logica l connective s in Figur e 6.9. To use the table to determine , for example ,
the valu e of True  V False,  first look on the left for the row wher e P is true and Q is false  (the third
row) . Then look in that row unde r the P V Q colum n to see the result : True.
P
False
False
True
TrueQ
False
True
False  '
True^P
True
True
False
FalseP f\Q
False
False
False
TrueP\/Q
False
True
True
TrueP => Q
True
True
'False  "
TrueP ^ Q
True
False
False
True
Figur e 6.9 Trut h table s for the five logica l connectives .
Truth table s defin e the semantic s of sentence s such as True  A True.  Comple x sentence s
such as (P V 0 A ­iS1 are define d by a proces s of decomposition : first , determin e the meanin g of
(P A Q) and of ­iS, and then combin e them usin g the definitio n of the A function . This is exactl y
analogou s to the way a comple x arithmeti c expressio n such as (p x q) + —s is evaluated .
The truth table s for "and, " "or," and "not " are in close accor d with our intuition s abou t the
Englis h words . The main poin t of possibl e confusio n is that P V Q is true whe n eithe r or both  P
and Q are true. Ther e is a differen t connectiv e calle d "exclusiv e or" ("xor " for short ) that give s
false whe n both disjunct s are true.7 Ther e is no consensu s on the symbo l for exclusiv e or; two
choice s are V and ­£.
In some ways , the implicatio n connectiv e =>• is the most important , and its truth table migh t
seem puzzlin g at first , becaus e it does not quit e fit our intuitiv e understandin g of "P implie s <2"
Latin has a separat e word , out,  for exclusiv e or.
ISectio n 6.4. Prepositiona l Logic : A Very Simpl e Logi c 169
or "if P then Q." For one thing , propositiona l logic does not requir e any relatio n of causatio n or
relevanc e betwee n P andQ. The sentenc e "5 is odd implie s Toky o is the capita l of Japan " is a true
sentenc e of propositiona l logic (unde r the norma l interpretation) , even thoug h it is a decidedl y
odd sentenc e of English . Anothe r poin t of confusio n is that any implicatio n is true wheneve r its
anteceden t is false . For example , "5 is even implie s Sam is smart " is true , regardles s of whethe r
Sam is smart . This seem s bizarre , but it make s sens e if you thin k of "P => Q" as saying , "If P
is true, then I am claimin g that Q is true. Otherwis e I am makin g no claim. "
Validit y and inferenc e
Truth table s can be used not only to defin e the connectives , but also to test for valid sentences .
Given a sentence , we mak e a truth table with one row for each of the possibl e combination s of
truth value s for the propositio n symbol s in the sentence . For each row, we can calculat e the truth
value of the entir e sentence . If the sentenc e is true in ever y row, then the sentenc e is valid . For
example , the sentenc e
((P V H) A ­.#) => P
is valid , as can be seen in Figur e 6.10 . We includ e som e intermediat e column s to mak e it clear
how the final colum n is derived , but it is not importan t that the intermediat e column s are there , as
long as the entrie s in the final colum n follo w the definition s of the connectives . Suppos e P mean s
that there is a wumpu s in [1,3] and H mean s there is a wumpu s in [2,2] . If at some poin t we learn
(P V H) and then we also learn ­>//, then we can use the valid sentenc e abov e to conclud e that P
is true—tha t the wumpu s is in [1,3J .
P
False
False
True
TrueH
False
True
False
TruePVH
False
True
True
True(PVH)/\  ­.//
False
False
True
False((PVtf)A­iH ) => P
True
True
True
True
Figur e 6.10 Trut h table showin g validit y of a comple x sentence .
This is important . It says that if a machin e has som e premise s and a possibl e conclusion ,
it can determin e if the conclusio n is true . It can do this by buildin g a truth table for the sentenc e
Premises  => Conclusion  and checkin g all the rows . If ever y row is true , then the conclusio n is
entaile d by the premises , whic h mean s that the fact represente d by the conclusio n follow s from
the state of affair s represente d by the premises . Eve n thoug h the machin e has no idea wha t the
conclusio n means , the user coul d read the conclusion s and use his or her interpretatio n of the
propositio n symbol s to see wha t the conclusion s mean—i n this case , that the wumpu s is in [1,3] .
Thus , we have fulfille d the promis e mad e in Sectio n 6.3.
It will often be the case that the sentence s inpu t into the knowledg e base by the user refer
to a worl d to whic h the compute r has no independen t access , as in Figur e 6.11 , wher e it is the
user who observe s the worl d and type s sentence s into the computer . It is therefor e essentia l
170 Chapte r 6. Agent s that Reaso n Logicall y
that a reasonin g syste m be able to draw conclusion s that follo w from the premises , regardles s
of the worl d to whic h the sentence s are intende d to refer . But it is a good idea for a reasonin g
syste m to follo w this principl e in any case. Suppos e we replac e the "user " in Figur e 6.11 with
a camera­base d visua l processin g syste m that send s inpu t sentence s to the reasonin g system . It
make s no difference ! Eve n thoug h the compute r now has "direc t access " to the world , inferenc e
can still take plac e throug h direc t operation s on the synta x of sentences , withou t any additiona l
informatio n as to their intende d meaning .
input sentence s
Figur e 6.11 Sentence s often refer to a worl d to whic h the agen t has no independen t access .
MODE LModel s
Any worl d in whic h a sentenc e is true unde r a particula r interpretatio n is calle d a mode l of that
sentenc e unde r that interpretation . Thus , the worl d show n in Figur e 6.2 is a mode l of the sentenc e
"Si >2" unde r the interpretatio n in whic h it mean s that there is a stenc h in [1,2] . Ther e are man y
other model s of this sentence—yo u can mak e up a worl d that does or does not have pits and gold
in variou s locations , and as long as the worl d has a stenc h in [1,2] , it is a mode l of the sentence .
The reaso n there are so man y model s is becaus e "£1,2 " make s a very weak claim abou t the world .
The more we claim (i.e., the more conjunction s we add into the knowledg e base) , the fewe r the
model s there will be.
Model s are very importan t in logic , because , to restat e the definitio n of entailment , a
sentence  a is entailed  by a knowledge  base  KB if the models of  KB are all models  of a. If this is
the case , then wheneve r KB is true, a mus t also be true.
In fact, we coul d defin e the meanin g of a sentenc e by mean s of set operation s on sets of
models . For example , the set of model s of P A Q is the intersectio n of the model s of P and the
model s of Q. Figur e 6.12 diagram s the set relationship s for the four binar y connectives .
We have said that model s are worlds . One migh t feel that real world s are rathe r mess y
thing s on whic h to base a forma l system . Som e author s prefe r to think of model s as mathematical
objects . In this view , a mode l in prepositiona l logic is simpl y a mappin g from propositio n symbol s
Sectio n 6.4. Propositiona l Logic : A Very Simpl e Logi c 171
PvQ PAQ
Figur e 6.12 Model s of comple x sentence s in term s of the model s of their components . In each
diagram , the shade d parts correspon d to the model s of the comple x sentence .
directl y to truth and falsehood , that is, the labe l for a row in a truth table . The n the model s of
a sentenc e are just thos e mapping s that mak e the sentenc e true . The two view s can easil y be
reconcile d becaus e each possibl e assignmen t of true and false to a set of propositio n symbol s
can be viewe d as an equivalenc e class of world s that, unde r a give n interpretation , have thos e
truth value s for thos e symbols . Ther e may of cours e be man y differen t "real worlds " that have
the sam e truth value s for thos e symbols . The only requiremen t to complet e the reconciliatio n is
that each propositio n symbo l be either  true  or false  in each world . Thi s is, of course , the basi c
ontologica l assumptio n of propositiona l logic , and is wha t allow s us to expec t that manipulation s
of symbol s lead to conclusion s with reliabl e counterpart s in the actua l world .
INFERENC E RULERule s of inferenc e for propositiona l logic
The proces s by whic h the soundnes s of an inferenc e is establishe d throug h truth table s can be
extende d to entir e classes  of inferences . Ther e are certai n pattern s of inference s that occu r over
and over again , and their soundnes s can be show n once and for all. The n the patter n can be
capture d in wha t is calle d an inferenc e rule . Onc e a rule is established , it can be used to mak e
inference s withou t goin g throug h the tediou s proces s of buildin g truth tables .
We have alread y seen the notatio n a h /? to say that /3 can be derive d from a by inference .
There is an alternativ e notation ,
a
whic h emphasize s that this is not a sentence , but rathe r an inferenc e rule. Wheneve r somethin g in
the knowledg e base matche s the patter n abov e the line , the inferenc e rule conclude s the premis e
172 Chapte r 6. Agent s that Reaso n Logicall y
below the line . The letter s a, f3, etc., are intende d to matc h any sentence , not just individua l
propositio n symbols . If there are severa l sentences , in eithe r the premis e or the conclusion , they
are separate d by commas . Figur e 6.13 give s a list of seven commonl y used inferenc e rules .
An inferenc e rule is soun d if the conclusio n is true in all case s wher e the premise s are true .
To verif y soundness , we therefor e construc t a truth table with one line for each possibl e mode l
of the propositio n symbol s in the premise , and show that in all model s wher e the premis e is true ,
the conclusio n is also true. Figur e 6.14 show s the truth table for the resolutio n rule.
<C> Modu s Ponen s or Implication­Elimination : (Fro m an implicatio n and the
premis e of the implication , you can infe r the conclusion. )
a =/•/? , a
~3
0 And­Elimination : (Fro m a conjunction , you can infer any of the conjuncts. )
a\ A 02 A ... A an
«/
<) And­Introduction : (Fro m a list of sentences , you can infe r their conjunction. )
Ql, Q2. ••• , un
QI A 02 A ... A an
0 Or­Introduction : (Fro m a sentence , you can infe r its disjunctio n with anythin g
else at all.)
a i
«, V a­2 V ... V a,,
<0> Double­Negatio n Elimination : (Fro m a doubl y negate d sentence , you can infe r
a positiv e sentence. )
­i­i a
<) Uni t Resolution : (Fro m a disjunction , if one of the disjunct s is false,  then you
can infe r the othe r one is true. )
a V /?, ­n/ j
a
<) Resolution : (Thi s is the mos t difficult . Becaus e 0 canno t be both true and false ,
one of the othe r disjunct s mus t be true in one of the premises . Or equivalently ,
implicatio n is transitive. )
a V/3 , ­v/i V 7
a V­vor equivalentl y­1Q => /?,
Figur e 6.13 Seve n inferenc e rules for prepositiona l logic . The unit resolutio n rule is a specia l
case of the resolutLoJX­Cule, _ whic h in turn is a specia l case of the full resolutio n rule for first­orde r
logic discusse d in Chapte r 9.
Sectio n 6.4. Prepositiona l Logic : A Very Simpl e Logi c 173
ft
False
False
False
False
True
True
True
True13
False
False
True
True
False
False
True
True7
False
True
False
­ True
False
True
False
TrueaV 13
False
False
True
True
True
True
True
True^0 V 7
True
True
False
True
True
True
False
Truea V 7
False
True
False
True
True
True
True
True
Figur e 6.14 A  truth table demonstratin g the soundnes s of the resolutio n inferenc e rule . We
have underline d the rows wher e both premise s are true.
As we mentione d above , a logica l proo f consist s of a sequenc e of application s of inferenc e
rules , startin g with sentence s initiall y in the KB, and culminatin g in the generatio n of the sentenc e
whos e proo f is desired . To prov e that P follow s from (P V H) and ~^H,  for example , we simpl y
requir e one applicatio n of the resolutio n rule , with a as P, i3 as H, and 7 empty . The job of an
inferenc e procedure , then , is to construc t proof s by findin g appropriat e sequence s of application s
of inferenc e rules .
MONOTONICIT YComplexit y of prepositiona l inferenc e
The truth­tabl e metho d of inferenc e describe d on page 169 is complete , becaus e it is alway s
possibl e to enumerat e the 2" row s of the table for any proo f involvin g n propositio n symbols .
On the othe r hand , the computatio n time is exponentia l in n, and therefor e impractical . One
migh t wonde r whethe r there is a polynomial­tim e proo f procedur e for prepositiona l logic base d
on usin g the inferenc e rules from Sectio n 6.4.
In fact, a versio n of this very proble m was the first addresse d by Coo k (1971 ) in his theor y
of NP­completeness . (See also the appendi x on complexity. ) Coo k showe d that checkin g a set of
sentence s for satisfiabilit y is NP­complete , and therefor e unlikel y to yield to a polynomial­tim e
algorithm . However , this does not mean that all instance s of prepositiona l inferenc e are goin g to
take time proportiona l to 2". In man y cases , the proo f of a give n sentenc e refer s only to a smal l
subse t of the KB and can be foun d fairl y quickly . In fact, as Exercis e 6.15 shows , reall y hard
problem s are quit e rare.
The use of inferenc e rule s to draw conclusion s from a knowledg e base relie s implicitl y
on a genera l propert y of certai n logic s (includin g prepositiona l and first­orde r logic ) calle d
monotonicity . Suppos e that a knowledg e base KB entail s som e set of sentences . A logic is
rnooQtoni c if when we add some new sentence s to the knowledg e base , all the sentence s entaile d
by the origina l KB are still entaile d by the new large r knowledg e base . Formally , we can state
the propert y of monotonicit y of a logic as follows :
if KB\  \= a then (KB t U KB 2) \= a
This is true regardles s of the content s of KB^— it can be irrelevan t or even contradictor y to KB\.
174 Chapte r 6. Agent s that Reaso n Logicall y
It is fairl y easy to show that prepositiona l and first­orde r logic are monotoni c in this sense ;
one can also show that probabilit y theor y is not monotoni c (see Chapte r 14). An inferenc e rule"
LOCA L suc h as Modu s Ponen s is loca l becaus e its premis e need only be compare d with a smal l portio n
of the KB (two sentences , in fact) . Wer e it not for monotonicity , we coul d not have any loca l
inferenc e rule s becaus e the rest of the KB migh t affec t the soundnes s of the inference . Thi s
woul d potentiall y crippl e any inferenc e procedure .
There is also a usefu l clas s of sentence s for whic h a polynomial­tim e inferenc e procedur e
HORN SENTENCE S exists . This is the class calle d Hor n sentences.8 A Horn sentenc e has the form :
PI A P2 A ... A Pn => Q
wher e the P, and Q are nonnegate d atoms . Ther e are two importan t specia l cases : ­First , whe n
Q is the constan t False,  we get a sentenc e that is equivalen t to ­>P] V ... V ­>P n. Second , whe n
n = 1 and PI = True,  we get True  => Q, whic h is equivalen t to the atomi c sentenc e Q. Not ever y
knowledg e base can be writte n as a collectio n of Horn sentences , but for those that can, we can use
a simpl e inferenc e procedure : appl y Modu s Ponen s whereve r possibl e unti l no new inference s
remai n to be made . We discus s Horn sentence s and their associate d inferenc e procedure s in more
detai l in the contex t of first­orde r logic (Sectio n 9.4).
6.5 AN AGEN T FOR THE WUMPU S WORL D
In this section , we show a snapsho t of a propositiona l logi c agen t reasonin g abou t the wumpu s
world . We assum e that the agen t has reache d the poin t show n in Figur e 6.4(a) , repeate d here as
Figur e 6.15 , and show how the agen t can conclud e that the wumpu s is in [1,3] .
The knowledg e base
On each turn , the agent' s percept s are converte d into sentence s and entere d into the knowledg e
base, alon g with som e valid sentence s that are entaile d by the percept  sentences . Let us assum e
that the symbol9 £1,2 , for example , mean s "Ther e is a stenc h in [1,2]. " Similarly , BI,\ mean s
"Ther e is a breez e in [2,1]. " At this point , then , the knowledg e base contains , amon g others , the
percep t sentence s
#2,
In addition , the agen t mus t start out with som e knowledg e of the environment . For example , the
agen t know s that if a square  has no smell , then neithe r the squar e nor any of its adjacen t square s
8 Als o know n as Hor n clauses . The nam e honor s the logicia n Alfre d Horn .
9 Th e subscript s mak e thes e symbol s look like they have som e kind of interna l structure , but do not let that mislea d you.
We coul d have used Q or StenchOneTwo  instea d of S1­2, but we wante d symbol s that are both mnemoni c and succinct .
Sectio n 6.5. A n Agen t for the Wumpu s Worl d 175
1,4
1,3W!
1,2,— |
S
OK
1,1
V
OK2,4
2,3
2,2
OK
2,1B
V
OK3,4
3,3
3,2
3,1P!4,4
4,3
4,2
4,1[T] = Agent
B = Breeze
G = Glitter,  Gold
OK = Safe square
P = Pit
S = Stench
V = Visited
W = Wumpus
Figur e 6.15 Th e agent' s knowledg e afte r the thir d move . Th e curren t percep t is
[Stench,  None,  None, None,  None].
can hous e a wumpus . The agen t need s to know this for each squar e in the world , but here we just
show sentence s for three relevan t squares , labelin g each sentenc e with a rule number :
RI : ­,5], i => ­iWi. i A­iWi. 2 A­iW 2,i
^2: ­^2, 1 => ­1^1, 1 A­iW 2,i A­1^2, 2 A­iW 3,i
fl3; ­,5, >2 => ­>W M A ­nWi, 2 A ­.W 2>2 A ­Wi, 3
Anothe r usefu l fact is that if there is a stenc h in [1,2] , then there mus t be a wumpu s in [1,2] or in
one or more of the neighborin g squares . This fact can be represente d by the sentenc e
R4: Si, 2 => W\, 3 V W,, 2 V W2,2 V Wu
Findin g the wumpu s
Give n these sentences , we will now show how an agen t can mechanicall y conclud e Wi, 3. All the
agent has to do is construc t the truth table for KB => W\^  to show that this sentenc e is valid .
There are 12 prepositiona l symbols,10 so the truth table will have 212 = 4096 rows , and ever y
row in whic h the sentenc e KB is true also has W\$  true . Rathe r than show all 4096 rows , we use
inferenc e rule s instead , but it is importan t to recogniz e that we coul d have done it in one (long )
step just by followin g the truth­tabl e algorithm .
First, we will show that the wumpu s is not in one of the othe r squares , and then conclud e
by eliminatio n that it mus t be in [ 1 ,3] :
1. Applyin g Modu s Ponen s with ­>Si, i and the sentenc e labelle d R\, we obtai n
A A
2. Applyin g And­Eliminatio n to this, we obtai n the three separat e sentence s
10 The 12 symbol s are S,,,,  52,i,5i, 2, W u, W u, W 2,i, W2,2, Wj.i , W,, 3,BM,52,i,Bi,2 .
176 Chapte r 6. Agent s that Reaso n Logicall y
3. Applyin g Modu s Ponen s to ­182,1  and the sentenc e labelle d R2, and then applyin g And ­
Eliminatio n to the result , we obtai n the three sentence s
4. Applyin g Modu s Ponen s to £1,2 and the sentenc e labelle d /?4, we obtai n
W|,3 V W\,2  V W 2,2 VWi. i
5. Now we appl y the unit resolutio n rule , wher e a is Vt^ j V W\^  V ^2,2 and J3 is W^i . (We
derive d ­*W\,\  in step 2.) Uni t resolutio n yield s
Wl,3 V Wi. 2 V W 2,2
6. Applyin g unit resolutio n agai n with W\j V W|,2 as a and W 2,2 as /? (­1^2, 2 was derive d in
step 3), we obtai n
Wi, 3 V Wi,2
1 . Finally , one mor e resolutio n with W[j as Q and W\^  as /3 (we derive d ­W\,2  in step 2)
gives us the answe r we want , namely , that the wumpu s is in [1,3] :
Translatin g knowledg e into actio n
We have show n how prepositiona l logic can be used to infe r knowledg e such as the whereabout s
of the wumpus . But the knowledg e is only usefu l if it help s the agen t take action . To do that,
we will need additiona l rule s that relat e the curren t state of the worl d to the action s the agen t
shoul d take . For example , if the wumpu s is in the squar e straigh t ahead , then it is a bad idea to
execut e the actio n Forward.  We can represen t this with a serie s of rules , one for each locatio n
and orientatio n in whic h the agen t migh t be. Her e is the rule for the case wher e the agen t is in
[1,1] facin g east:
A\t\ A East^  A W2,\  => ­^Forward
Once we have thes e rules , we need a way to ASK the knowledg e base wha t actio n to take .
Unfortunately , propositiona l logi c is not powerfu l enoug h to represen t or answe r the questio n
"wha t actio n shoul d I take?, " but it is able to answe r a serie s of question s such as "shoul d I go
forward? " or "shoul d I turn right? " Tha t mean s that the algorith m for a knowledge­base d agen t
using propositiona l logic woul d be as in Figur e 6.16 .
Problem s with the propositiona l agen t
Propositiona l logic allow s us to get acros s all the importan t point s abou t wha t a logic is and how
it can be used to perfor m inferenc e that eventuall y result s in action . But propositiona l logic is so
weak that it reall y canno t handl e even a domai n as simpl e as the wumpu s world .
The mai n proble m is that there are just too man y proposition s to handle . The simpl e rule
"don' t go forwar d if the wumpu s is in fron t of you" can only be state d in propositiona l logic by
a set of 64 rule s (16 square s x 4 orientation s for the agent) . Naturally , it just gets wors e if the
Sectio n 6.5. A n Agen t for the Wumpu s Worl d 177
functio n PROPOSITIONAL­KB­AGENltpercepf ) return s an action
static : KB,  a knowledg e base
t, a counter , initiall y 0, indicatin g time
TELL(A'B , MAKE­PERCEPT­SENTENCE(perce/7/ , /))
for each action  in the list of possibl e action s do
if ASK(KB,  MAKE­ACTION­QUERY(f , action))  then
t — t + 1
retur n action
end
Figur e 6.16 A  knowledge­base d agen t usin g propositiona l logic .
world is large r than a 4 x 4 grid . Give n this multiplicatio n of rules , it will take thousand s of rules
to defin e a competen t agent . The proble m is not just that it is taxin g to writ e the rules down , but
also that havin g so man y of them slow s dow n the inferenc e procedure . Remembe r that the size
of a truth table is 2", wher e n is the numbe r of propositiona l symbol s in the knowledg e base .
Anothe r proble m is dealin g with change . We showe d a snapsho t of the agen t reasonin g at
a particula r poin t in time , and all the proposition s in the knowledg e base were true at that time .
But in genera l the worl d change s over time . Whe n the agen t make s its first move , the propositio n
A],i become s false and A2,i become s true . But it may be importan t for the agen t to remembe r
wher e it was in the past , so it canno t just forge t AII . To avoi d confusion , we will need differen t
propositiona l symbol s for the agent' s locatio n at each time step . This cause s difficultie s in two
ways . First , we do not know how long the gam e will go on, so we do not know how man y of these
time­dependen t proposition s we will need . Second , we will now have to go back and rewrit e
time­dependen t version s of each rule . For example , we will need
=>• ­^Forward01??1 A East° A A
\\{ f\East\f\
\\ , A East  % A­i Forward1
^Fonvard2
A° , A North° A A W? 2
A]^ A North\  A W\ 2
A{, f\North2
Af\W'l 2•^Forward0
^Forward1
^Forward2
wher e the superscript s indicat e times . If we wan t the agen t to run for 100 time steps , we will
need 6400 of these rules , just to say one shoul d not go forwar d whe n the wumpu s is there .
In summary , the proble m with propositiona l logic is that it only has one representationa l
device : the proposition . In the nex t chapter , we will introduc e first­orde r logic , whic h can
represen t objects  and relations  betwee n object s in additio n to propositions . In first­orde r logic ,
the 6400 propositiona l rules can be reduce d to one.
178 Chapte r 6. Agent s that Reaso n Logicall y
6.6 SUMMAR Y
We have introduce d the idea of a knowledge­base d agent , and showe d how we can defin e a logic
with whic h the agen t can reason , abou t the worl d and be guarantee d to draw correc t conclusions ,
given correc t premises . We have also showe d how an agen t can turn this knowledg e into action .
The main point s are as follows :
• Intelligen t agent s need knowledg e abou t the worl d in orde r to reac h good decisions .
• Knowledg e is containe d in agent s in the form of sentence s in a knowledg e representatio n
languag e that are store d in a knowledg e base .
• A knowledge­base d agen t is compose d of a knowledg e base and an inferenc e mechanism .
It operate s by storin g sentence s abou t the worl d in its knowledg e base , usin g the inferenc e
mechanis m to infer new sentences , and usin g them to decid e wha t actio n to take .
• A representatio n languag e is define d by its synta x and semantics , whic h specif y the
structur e of sentence s and how they relat e to facts in the world .
• The interpretatio n of a sentenc e is the fact to whic h it refers . If it refer s to a fact that is
part of the actua l world , then it is true .
• Inferenc e is the proces s of derivin g new sentence s from old ones . We try to desig n soun d
inferenc e processe s that deriv e true conclusion s give n true premises . An inferenc e proces s
is complet e if it can deriv e all true conclusion s from a set of premises .
• A sentenc e that is true in all world s unde r all interpretation s is calle d valid . If an implicatio n
sentenc e can be show n to be valid , then we can deriv e its consequen t if we know its premise .
The abilit y to show validit y independen t of meanin g is essential .
• Differen t logic s mak e differen t commitment s abou t wha t the worl d is mad e of and wha t
kinds of belief s we can have regardin g facts .
• Logic s are usefu l for the commitment s they do not make , becaus e the lack of commitmen t
gives the knowledg e base write r more freedom .
• Prepositiona l logic commit s only to the existenc e of facts that may or may not be the case in
the worl d bein g represented . It has a simpl e synta x and semantics , but suffice s to illustrat e
the proces s of inference .
• Prepositiona l logi c can accommodat e certai n inference s neede d by a logica l agent , but
quickl y become s impractica l for even very smal l worlds .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
Logic had its origin s in ancien t Gree k philosoph y and mathematics . Logica l principles —
principle s connectin g the syntacti c structur e of sentence s with thei r trut h and falsity , thei r
meaning , or the validit y of argument s in whic h they figure—ca n be foun d in scattere d loca ­
tions in the work s of Plato (428­34 8 B.C.) . The first know n systemati c stud y of logic was carrie d
Sectio n 6.6. Summar y 179
out by Aristotle , whos e wor k was assemble d by his student s after his deat h in 322 B.C . as a
treatis e calle d the Organon,  the first systemati c treatis e on logic . However , Aristotle' s logic
was very wea k by moder n standards ; excep t in a few isolate d instances , he did not take accoun t
of logica l principle s that depen d essentiall y on embeddin g one entir e syntacti c structur e withi n
anothe r structur e of the same type , in the way that sentence s are embedde d withi n othe r sentence s
in moder n prepositiona l logic . Becaus e of this limitation , there was a fixed limi t on the amoun t
of interna l complexit y withi n a sentenc e that coul d be analyze d usin g Aristotelia n logic .
The closel y relate d Megaria n and Stoi c school s (originatin g in the fifth centur y B.C . and
continuin g for severa l centurie s thereafter ) introduce d the systemati c stud y of implicatio n and
other basic construct s still used in moder n prepositiona l logic . The Stoic s claime d that their logic
was complet e in the sens e of capturin g all valid inferences , but wha t remain s is too fragmentar y
to tell. A good accoun t of the histor y of Megaria n and Stoic logic , as far as it is known , is give n
by Benso n Mate s (1953) .
The idea s of creatin g an artificia l forma l languag e patterne d on mathematica l notatio n
in orde r to clarif y logica l relationships , and of reducin g logica l inferenc e to a purel y forma l
and mechanica l process , were due to Leibni z (1646­1716) . Leibniz' s own mathematica l logic ,
however , was severel y defective , and he is bette r remembere d simpl y for introducin g these idea s
as goal s to be attaine d than for his attempt s at realizin g them .
Georg e Bool e (1847 ) introduce d the first reasonabl y comprehensiv e and approximatel y
correc t syste m of logic base d on an artificia l forma l languag e with his book The Mathematical
Analysis  of Logic.  Boole' s logic was closel y modele d on the ordinar y algebr a of real numbers .
Boole' s syste m subsume d the main parts of Aristotelia n logic and also containe d a close analogu e
to moder n prepositiona l logic . Althoug h Boole' s syste m still fell shor t of full prepositiona l logic ,
it was close enoug h that othe r 19th­centur y writer s followin g Bool e coul d quickl y fill in the gaps .
The first comprehensiv e expositio n of moder n prepositiona l logic (and first­orde r logic ) is foun d
in Gottlo b Frege' s (1879 ) Begriffschrift  ("Concep t Writing " or "Conceptua l Notation") .
Truth table s as a metho d of testin g the validit y or unsatisfiabilit y of sentence s in the lan­
guag e of prepositiona l logic were independentl y introduce d simultaneousl y by Ludwi g Wittgen ­
stein (1922 ) and by Emi l Post (1921) . (As a metho d of explainin g the meaning s of propositiona l
connectives , truth table s go back to Philo of Megara. )
Quin e (1982 ) describe s "truth­valu e analysis, " a proo f metho d closel y resemblin g truth
tables but mor e efficien t because , in effect , it can handl e multipl e line s of the truth table si­
multaneously . Wan g (1960 ) take s a genera l proo f metho d for first­orde r logi c designe d by
Gentze n (1934 ) and select s a convenien t and efficien t subse t of inferenc e rule s for use in a
tree­base d procedur e for decidin g validit y in propositiona l logic .
John McCarthy' s (1968 ) pape r "Program s with Commo n Sense " promulgate d the notio n
of agent s that use logica l reasonin g to mediat e betwee n percept s and actions . This pape r was the
first to mak e this conceptio n widel y known , althoug h it draw s on muc h earlie r work (McCarthy ,
1958) . The 1968 pape r is also calle d the "Advic e Taker " pape r becaus e it introduce s a hypothetica l
progra m by that nam e whic h uses logic to enabl e its designer s to communicat e usefu l knowledg e to
it withou t havin g to write furthe r directl y executabl e compute r code . Alie n Newell' s (1982 ) articl e
"The Knowledg e Level " focuse s on the use of logic by agen t designer s to describ e the knowledg e
that is, in effect , bein g used by the agents  they are designing , whethe r or not the agent s themselve s
use explici t logica l formula s to represen t this knowledg e internally . This them e of Newell' s was
180 Chapte r 6. Agent s that Reaso n Logicall y
hinte d at in 194 3 by the psychologis t Kennet h Craik , who writes , "My hypothesi s then is that
though t models , or parallels , reality—tha t its essentia l featur e is not 'the mind, ' 'the self, ' 'sense ­
data, ' nor proposition s but symbolism , and that this symbolis m is largel y of the same kind as that
whic h is familia r to us in mechanica l device s whic h aid though t and calculation... " (Craik , 1943) .
Furthe r work alon g these lines has been done by Rosenschei n and Kaelblin g (Rosenschein , 1985 ;
Kaelblin g and Rosenschein , 1990) . Rosenschei n and Geneseret h (1987 ) hav e researche d the
proble m of cooperativ e actio n amon g agent s usin g propositiona l logi c internall y to represen t
the world . Gabba y (1991 ) has explore d extension s to standar d logi c to enhanc e guidanc e of
reasonin g and retrieva l from large knowledg e bases .
EXERCISE S
6.1 W e said that truth table s can be used to establis h the validit y of a comple x sentence . Sho w
how they can be used to decid e if a give n sentenc e is valid , satisfiable , or unsatisfiable .
6.2 Us e truth table s to show that the followin g sentence s are valid , and thus that the equivalence s
hold. Som e of these equivalenc e rules have standar d names , whic h are give n in the righ t column .
P A (Q A R)
P V (Q V R)
P/\Q
PV Q
Pf\(Q\/K )
P V (£> A ft)
­CPA 0
­>(P V 0
P => Q
­.­.pp ^ e
­ ~P <=> 2p o g
PA­ P
PV^ P<=>
<=>
o­(P A 2) A ft
(P V 0 V ft
GAP
2vP
(P A 0 V (P A ft)
(P V 0 A (P V ft)
­.PV­i g
­.PA­. Q
­2 => ­P
P
^PV Q
(P => 0 A (Q =>
(P A 0 V (­iP A ­
False
TrueAssociativit y of conjunctio n
Associativit y of disjunctio n
Commutativit y of conjunctio n
Commutativit y of disjunctio n
Distributivit y of A over V
Distributivit y of V over A
de Morgan' s Law
de Morgan' s Law
Contrapositio n
Doubl e Negatio n
P)
Q)
6.3 Loo k at the followin g sentence s and decid e for each if it is valid , unsatisfiable , or neither .
Verif y your decision s usin g truth tables , or by usin g the equivalenc e rule s of Exercis e 6.2. Wer e
there any that initiall y confuse d you?
a. Smoke  => Smoke
b. Smoke  => Fire
c. (Smoke  => Fire}  => (^Smoke  => ­'Fire)
d. Smoke  V Fire  V —iFire
Sectio n 6.6. Summar y 181
e. ((Smoke  A Heat)  => Fire)  O ((Smoke  => Fire)  V (Heat
f. (Smoke  => Fire)  => ((Smoke  A /fetf ) =^ Fire)
g. Big  V Dumb V  (Big  => Dumb)
h. (Big  f\ Dumb)  \lFire))
6.4 Is the sentenc e "Eithe r 2 + 2 = 4 and it is raining , or 2 + 2 = 4 and it is not raining " makin g
a claim abou t arithmetic , weather , or neither ? Explain .
6.5 (Adapte d from (Barwis e and Etchemendy , 1993). ) Give n the following , can you prov e that
the unicor n is mythical ? How abou t magical ? Horned ?
* ' I f the unicor n is mythical , then it is immortal , but if it is not mythical , then it is a
morta l mammal . If the unicor n is eithe r immorta l or a mammal , then it is horned .
The unicor n is magica l if it is horned .
6.6 Wha t ontologica l and epistemologica l commitment s are mad e by the languag e of real ­
numbe r arithmetic ?
6.7 Conside r a worl d in whic h there are only four propositions , A, B, C, and D. Ho w man y
model s are there for the followin g sentences ?
a. A f\B
b. A V B
C. A A B A C
6.8 W e have define d four differen t binar y logica l connectives .
a. Are there any other s that migh t be useful ?
b. How man y binar y connective s can there possibl y be?
c. Wh y are som e of them not very useful ?
6.9 Som e agent s mak e inference s as soon as they are told a new sentence , whil e other s wait until
they are aske d befor e they do any inferencing . Wha t differenc e does this mak e at the knowledg e
level , the logica l level , and the implementatio n level ?
6.10 We said it woul d take 64 prepositiona l logic sentence s to expres s the simpl e rule "don' t
go forwar d if the wumpu s is in fron t of you. " Wha t if we represente d this fact with the singl e rule
WumpmAhead  => ­^Forward
Is this feasible ? Wha t effect s does it have on the rest of the knowledg e base ?
6.11 Provid e a forma l syntax , semantics , and proo f theor y for algebrai c equation s includin g
variables , numbers , +, —, x, and ­K Yo u shoul d be able to provid e inferenc e step s for mos t
standar d equatio n manipulatio n techniques .
182 Chapte r 6. Agent s that Reaso n Logicall y
6.12 (Adapte d from (Davis , 1990). ) Jones , Smith , and Clar k hold the jobs of programmer ,
knowledg e engineer , and manage r (not necessaril y in that order) . Jone s owe s the programme r
$10. The manager' s spous e prohibit s borrowin g money . Smit h is not married . You r task is to
figur e out whic h perso n has whic h job.
Represen t the facts in prepositiona l logic . You shoul d have nine propositiona l symbol s to
represen t the possibl e person/jo b assignments . For example , you migh t use the symbo l SM to
indicat e that Smit h is the manager . You do not need to represen t the relatio n betwee n owin g and
borrowing , or bein g marrie d and havin g a spouse ; you can just use these to draw conclusion s (e.g,
from "Smit h is not married " and "the manager' s spouse " we know that Smit h can't be the manager ,
whic h you can represen t as ­>5M) . The conjunctio n of all the relevan t facts form s a sentenc e
whic h you can call KB. The possibl e answer s to the proble m are sentence s like JP A SK A CM.
There are six such combination s of person/jo b assignments . Solv e the proble m by showin g that
only one of them is implie d by KB, and by sayin g wha t its interpretatio n is.
6.13 Wha t is the performanc e scor e that one coul d expec t from the optima l wumpu s worl d
agent ? Desig n an experimen t wherei n you look at all possibl e 4x4 wumpu s world s (or a rando m
sampl e of them if there are too many) , and for each one determin e the shortes t safe path to pick
up the gold and retur n to start , and thus the best score . Thi s give s you an idea of the expecte d
performanc e scor e for an idea l omniscien t agent . How coul d you determin e the expecte d scor e
for an optima l non­omniscien t agent ?
(=^sf^  6.1 4 Implemen t a functio n VALIDIT Y that take s a sentenc e as inpu t and return s eithe r valid ,
—^ " satisfiable , or unsatisfiable . Us e it to answe r the question s in Exercis e 6.12 . Yo u will need
to defin e an implementation­leve l representatio n of sentences . The cleanes t way to do this is
to defin e an abstrac t data type for compoun d sentences . Begi n by writin g EVAL­TRUT H as a
recursiv e functio n that takes a sentenc e and an assignmen t of truth value s to propositio n symbols ,
and return s true or false . The n call EVAL­TRUT H for all possibl e assignment s of truth value s to
the propositio n symbols .
hgvj­ = ­ v 6.1 5 SA T is the abbreviatio n for the satisfiabilit y problem : give n a propositiona l sentence ,
=4^=— determin e if it is satisfiable , and if it is, show whic h proposition s have to be true to mak e the
sentenc e true . 3SA T is the proble m of findin g a satisfyin g truth assignmen t for a sentenc e in a
specia l forma t calle d 3­CNF , whic h is define d as follows :
• A litera l is a propositio n symbo l or its negatio n (e.g. , P or ­if).
• A claus e is a disjunctio n of literals ; a 3­claus e is a disjunctio n of exactl y 3 literal s (e.g. ,
P V Q V ­./?).
• A sentenc e in CNF or conjunctiv e norma l form is a conjunctio n of clauses ; a 3­CN F
sentenc e is a conjunctio n of 3­clauses .
For example ,
(P V Q V ­iS) A (­.P V 2 V R) A (­>P  V ­i/? V ­>S) A (P V ­>S V T)
is a 3­CN F sentenc e with four clause s and five propositio n symbols .
In this exercise , you will implemen t and test GSAT , an algorith m for solvin g SAT problem s
that has been used to investigat e how hard 3SA T problem s are. GSA T is a random­restart , hill­
ISectio n 6.6. Summar y 183
climbin g searc h algorithm . The initia l state is a rando m assignmen t of true  and false  to the
propositio n symbols . For example , for the precedin g 3­CN F sentence , we migh t start with P and
Q false and R, S, and T true .
The evaluatio n functio n measure s the numbe r of satisfie d clauses , that is, clause s with at
least one true  disjunct . Thus , the initia l state gets an evaluatio n of 3, becaus e the second , third ,
and fourt h clause s are true . If there are n propositio n symbols , then ther e are n operators , wher e
each operato r is to chang e the truth assignmen t for one of the symbols . As a hill­climbin g search ,
we alway s use the operato r that yield s the best evaluatio n (randoml y choosin g one if there are
severa l equall y good operators) . Ou r exampl e 3­CN F sentenc e is solve d in one step , becaus e
changin g S from true  to false  yield s a solution . As with a random­restar t algorithm , unles s we
find a solutio n afte r a certai n amoun t of hill­climbing , we give up and start over from a new
rando m truth assignment . Afte r a certai n numbe r of restarts , we give up entirely . The complet e
algorith m is show n in Figur e 6.17 .
functio n GSAT(sentence,  max­restarts,  max­climbs)  return s a truth assignmen t or failur e
for i — 1 to max­restarts  do
A — A randoml y generate d truth assignmen t
for j — 1 to max­climbs  do
if A satisfie s sentence  then retur n A
A — a rando m choic e of one of the best successor s of A
end
end
retur n failur e
Figur e 6.17 Th e GsA T algorith m for satisfiabilit y testing . The successor s of an assignmen t A
are truth assignmen t with one symbo l flipped . A "bes t assignment " is one that make s the mos t
clause s true.
Answe r the followin g question s abou t the algorithm :
a. Is the GSA T algorith m sound ?
b. Is it complete ?
c. Implemen t GSA T and use it to solv e the problem s in Exercis e 6.3.
d. Use GSA T to solv e randoml y generate d 3SA T problem s of differen t sizes . Ther e are two
key parameters : N, the numbe r of propositiona l symbols , and C, the numbe r of clauses .
We will investigat e the effect s of the ratio C/N  on the executio n time of GSAT . Wit h ,/V
fixed at 20, mak e a grap h of the media n executio n time versu s C/N  for C/N  from 1 to 10.
(The media n is a bette r statisti c than the mea n becaus e one or two outlier s can reall y thro w
off the mean. ) Use N as the valu e of max­restarts  and 5N as the valu e of max­climbs.
e. Repea t for othe r value s of N as time permits .
f. Wha t can you conclud e abou t the difficult y of 3SA T problem s for differen t value s of C, N,
and the ratio C/N?
184 Chapte r Agent s that Reaso n Logicall y
See Selma n et al. (\ 992) for mor e on GSAT . The y presen t an implementatio n of GSA T that solve s
even the hardes t 3SA T problem s with N = 70 in unde r a second . GSA T can be used to solv e a
wide variet y of problem s by constructin g a reductio n from each clas s of problem s to 3SAT . It
is so efficien t that it ofte n outperform s special­purpos e algorithm s that are expertl y designe d for
specifi c problems .
6.16 Conside r the proble m of designin g a logica l agen t for the wumpu s worl d usin g a Boolea n
circuit—tha t is, a collectio n of logi c gate s connectin g the input s (percep t values ) to output s
(actio n values) .
a. Explai n why you woul d need flip­flops .
b. Giv e an order­of­magnitud e estimat e of how man y gate s and flip­flop s woul d you need .
7FIRST­ORDE R LOGI C
In which we  introduce  a logic  that is  sufficient  for building knowledge­based  agents.
In Chapte r 6, we showe d how a knowledge­base d agen t coul d represen t the worl d in whic h it
operate s and use thos e representation s to deduc e wha t action s to take . We used propositiona l
logic as our representatio n languag e becaus e it is one of the simples t language s that demonstrate s
all the importan t points . Unfortunately Lpjopqsitiona l logic has a very limite d ontology , makin g
only the commitmen t that .the worl d consist s of facts . Thi s mad e it difficul t to represen t even
somethin g as simpl e as the wumpu s world .
FIRST­ORDE R LOGI C I n this chapter , we examin e first­orde r logic,1 whic h make s a stronge r set of ontologica l
OBJECT S commitments . The mai n one is that the worl d consist s of objects , that is, thing s with individua l
PROPERTIE S identitie s and propertie s that distinguis h them from othe r objects .
RELATION S Amon g thes e objects , variou s relation s hold . Sqme..Qfjhes e relation s are functions —
FUNCTION S relation s in whic h there is only one "value " for a give n "input. " It is easy to start listin g example s
of objects , properties , relations , and functions :
• Objects : people , houses , numbers , theories , Ronal d McDonald , colors , basebal l games ,
wars, centurie s ...
• Relations : brothe r of, bigge r than , inside , part of, has color , occurre d after , own s ...
• Properties : red, round , bogus , prime , multistoried.. .
.­j» Functions : fathe r of, best friend , third innin g of, one mor e than ...
Indeed , almos t any fact can be though t of as referrin g to object s and propertie s or relations . Som e
example s follow :
• "On e plus two equal s three "
Objects : one, two, three , one plus two; Relation : equals ; Function : plus . (On e plus two is
a nam e for the object  that is obtaine d by applyin g the functio n plus to the object s one and
two. Thre e is anothe r nam e for this object. )
• "Square s neighborin g the wumpu s are smelly. "
Objects : wumpus , square ; Property : smelly ; Relation : neighboring .
Also calle d first­orde r predicat e calculus , and sometime s abbreviate d as FOL or FOPC .
185
186 Chapte r 7. First­Orde r Logi c
• "Evi l King John ruled Englan d in 1200. "
Objects : John , England , 1200 ; Relation : ruled ; Properties : evil, king .
First­orde r logic has been so importan t to mathematics , philosophy , and artificia l intelligenc e
precisel y becaus e thos e fields—an d indeed , muc h of everyda y huma n existence—ca n be usefull y
though t of as dealin g with object s and the relation s betwee n them . ,We_ar e not claimin g that the
world reall y is made up of object s and relations , just that dividin g up the worl d that way help s us
reaso n abou t it. First­orde r logic can also expres s fact s abou t all of the object s in the! universe. ^ .
This, togethe r with the implicatio n connectiv e from propositiona l logic , enable s one to represent ^ :
genera l laws or rules , such as the statemen t "Square s neighborin g the wumpu s are smelly. "
Althoug h first­orde r logic commit s to the existenc e of object s and relations , it does not
make an ontologica l commitmen t to such thing s as categories , time , and events , whic h also seem
to show up in mos t facts abou t the world . Strangel y enough , this reluctanc e to tackl e categories ,
time, and event s has not hurt the popularit y of first­orde r logic ; in fact it has'contributed ^ its
success . Importan t as thes e thing s are, ther e are just too man y differen t way s to deal with them ,
and a logic that committe d to a singl e treatmen t woul d only have limite d appeal . By remainin g
neutral , first­orde r logi c give s its user s the freedo m to describ e thes e thing s in a way that is
appropriat e for the domain . This freedo m of choic e is a genera l characteristi c of first­orde r logic .
In the previou s exampl e we liste d King  as a propert y of people , but we coul d just as well have
made King­a .relation  betwee n peopl e and countries , or a functio n from countrie s to peopl e (in a
world in whic h each countr y has only one king) .
There are man y differen t representatio n scheme s in use in AI, som e of whic h we will
discus s in later chapters . Som e are theoreticall y equivalen t to first­orde r logic and some are not.
B.ut_fiistorde r logic is universa l in the sens e that it can expres s anythin g that can be programmed ­
We choose to study  knowledge  representation  and  reasoning  using  first­order  logic  because  it
is by far the most studied  and best  understood  scheme  yet devised.  Generall y speaking , othe r
proposal s involvin g additiona l capabilitie s are still hotl y debate d and only partiall y understood .
Othe r proposal s that are a subse t of first­orde r logic are usefu l only in limite d domains . Despit e
its limitations , first­orde r logic will be aroun d for a long time .
7.1 SYNTA X AND SEMANTIC S
TERM S
CONSTAN T SYMBOL SIn propositiona l logi c ever y expressio n is a sentence , whic h represent s a fact. First­orde r logic
has sentences , but it also has terms , whic h represen t objects . Constan t symbols , variables­,­ari d
functio n symbol s are used to build terms , and quantifier s and predicat e symbol s are used to build
sentences . Figur e 7.1 give s a complet e gramma r of first­orde r logic , usin g Backus­Nau r form
(see page 854 if you are not familia r with this notation) . A mor e detaile d explanatio n of each
element , describin g both synta x and semantics , follows :
<C> Constan t symbols : A, B, C, John ...
An interpretatio n mus t specif y whic h objec t in the worl d is referre d to by each constan t
symbol . Eac h constan t symbo l name s exactl y one object , but not all object s need to
have names , and som e can have severa l names . Thus , the symbo l John,  in one particula r
ISectio n 7. Synta x and Semantic s 187
PREDICAT E
SYMBOL S
TUPLE SSentence  — AtomicSentence
Sentence  Connective  Sentence
Quantifier  Variable,  . . . Sentence
­i Sentence
(Sentence)
AtomicSentence  — Predicate(Term,  . . .) Term  = Term
Term  — » Function(Term,  . . .)
| Constant
\ Variable
Connective
Quantifier
Constant
Variable
Predicate
FunctionA V |
V | 3
A \ X\ \ John
a | x s • • •
Before  \ HasColor  \ Raining  \
Mother  \ LeftLegOf  \
Figur e 7.1 Th e synta x of first­orde r logic (wit h equality ) in BNF (Backus­Nau r Form) .
interpretation , migh t refe r to the evil Kin g John , king of Englan d from 119 9 to 121 6
and younge r brothe r of Richar d the Lionheart . The symbo l King  coul d refer to the same
object/perso n in the same interpretation .
<> Predicat e symbols : Round,  Brother,...
An interpretatio n specifie s that a predicat e symbo l refer s to a particula r relatio n in
the model . For example , the Brother  symbo l migh t refe r to the relatio n of brotherhood .
Brother  is a binar y predicat e symbol , and accordingl y brotherhoo d is a relatio n that hold s
(or fails to hold ) betwee n pair s of objects . In any give n model , the relatio n is define d by
the set of tuple s of object s that satisf y it. A tupl e is a collectio n of object s arrange d in
a fixe d order . The y are writte n with angl e bracket s surroundin g the objects . In a mode l
containin g three objects , Kin g John , Robi n Hood , and Richar d the Lionheart , the relatio n
of brotherhoo d is define d by the set of tuple s
{ (Kin g John , Richar d the Lionheart} ,
(Richar d the Lionheart , King John} }
Thus , formall y speaking , Brother  refer s to this set of tuple s unde r the interpretatio n we
have chosen .
188 Chapte r 7. First­Orde r Logi c
FUNCTIO N SYMBOL S 0  Functio n symbols : Cosine.  FatherOf,  LeftLegOf...
Some relation s are functional— that is, any give n objec t is relate d to exactl y one othe r
objec t by the relation . For example , any angl e has only one numbe r that is its cosine ; any
perso n has only one perso n that is his or her father . In such cases , it is often more convenien t
to defin e a functio n symbo l (e.g. , Cosine)  that refer s to the appropriat e relatio n betwee n
angle s and numbers . In the model , the mappin g is just a set of n + 1­tuple s with a specia l
property , namely , that the last elemen t of each tupl e is the valu e of the functio n for the
first n elements , and each combinatio n of the first n element s appear s in exactl y one tuple .
A tabl e of cosine s is just such a set of tuples—fo r each possibl e angl e of interest , it give s
the cosin e of the angle . Unlik e predicat e symbols , whic h are. used to state that relation s
hold amon g certai n objects , functio n symbol s are used to refer to particula r object s withou t
using their_narnes T­as­wejyil l see In the next section .
The choic e of constant , predicate , and functio n symbol s is entirel y up to the user. A mathematicia n
migh t wan t to use + and Cosine,  a compose r Crescendo  and F­sharp.  The name s do not matte r
from a forma l poin t of view , but it enhance s readabilit y if the intende d interpretatio n of the
symbol s is clear . We retur n to this poin t in Sectio n 8.1.
Term s
A term is a logica l expressio n that refer s to an object . Constan t symbol s are therefor e terms .
Sometimes , it is mor e convenien t to use an expressio n to refe r to an object . Fo r example ,
in Englis h we migh t use the expressio n "Kin g John' s left leg" rathe r than givin g a nam e to
his leg. Thi s is wha t functio n symbol s are for: instea d of usin g a constan t symbol , we use
LeftLegOf  (John).  In the genera l case , a comple x term is forme d by a functio n symbo l followe d
by a parenthesize d list of term s as argument s to the functio n symbol . It is importan t to remember ^
that a comple x term is just a complicate d kind of name . It is not a "subroutin e call" that "return s
a value. " Ther e is no LeftLegOf  subroutin e that take s a perso n as inpu t and return s a leg. We can
reaso n abou t left legs (e.g. , statin g the genera l rule that everyon e has one and then deducin g that
John mus t have one) withou t ever providin g a definitio n of LeftLegOf.  Thi s is somethin g that
canno t be done with subroutine s in programmin g languages .
The forma l semantic s of term s is straightforward . An interpretatio n specifie s a functiona l
relatio n referre d to by the functio n symbol , and object s referre d to by the term s that are its
arguments . Thus , the whol e term refer s to the objec t that appear s as the («+l)­t h entr y in that
tuple in the relatio n whos e first n element s are the object s referre d to by the arguments . Thus ,
the LeftLegOf  functio n symbo l migh t refer to the followin g functiona l relation :
{ (Kin g John , King John' s left leg) ,
(Richar d the Lionheart , Richard' s left leg) }
and if King  John  refer s to Kin g John , then LeftLegOf  (King  John)  refers , accordin g to the relation ,
to Kin g John' s left leg.
Sectio n 7.1. Synta x and Semantic s 189
Atomi c sentence s
Now that we have^term s for referrin g to objects , and predicat e symbol s for referrin g to relations ,
we can put them togetHe f to mak e atomi c sentenctssTfia T state " facts . An atomi c sentenc e is
forme d from a predicat e symbo l followe d by a parenthesize d list of terms . For example ,
Brother(Richard,  John)
states , unde r the interpretatio n give n before , that Richar d the Lionhear t is the brothe r of Kin g
John.2 Atomi c sentence s can have argument s that are comple x terms :
Married(FatherOf  (Richard),MotherOf  (John))
states that Richar d the Lionheart' s fathe r is marrie d to King John' s mothe r (again , unde r a suitabl e
interpretation) . An atomic  sentence  is true  if the relatiqn  referred  to by the predicate  symbol
holds  between  the objects  referred  to by the  argumentsi^he  relatio n hold s just in case the tuple
of object s is in the relation ! The truth of a sentenc e therefor e depend s on both the interpretatio n
and the world .
Comple x sentence s
We can use logica l connective s to construc t mor e comple x sentences , just as in propositiona l
calculus . The semantic s of sentence s forme d usin g logica l connective s is identica l to that in the
propositiona l case. For example :
• Brother(Richard,John)  A Brother(John,  Richard)  is true just whe n John is the brothe r of
Richar d and Richar d is the brothe r of John .
• Older(John,  30) V Younger(John,  30) is true just whe n John is olde r than 30 or John is
younge r than 30.
• Older(John,  30) => ­^Younger(John,  30) state s that if John is olde r than 30, then he is not
younge r than 30.3
• ­^Bmther(Robin,  John)  is true just whe n Robi n is not the brothe r of John .
QUANTIFIER SQuantifier s
Once we have a logic that allow s objects , it is only natura l to wan t to expres s propertie s of entir e
collection s of objects , rathe r than havin g to enumerat e the object s by name . Quantifier s let us
do this. First­orde r logi c contain s two standar d quantifiers , calle d universal  and existential.
Universa l quantificatio n (V)
Recal l the difficult y we had in Chapte r 6 with the proble m of expressin g genera l rule s in propo ­
sitiona l logic . Rule s such as "All cats are mammals " are the brea d and butte r of first­orde r logic .
2 We will usuall y follo w the argumen t orderin g conventio n that P(x,  y) is interprete d as "jc is a P of y."
3 Althoug h thes e last two sentence s may seem like tautologies , they are not. Ther e are interpretation s of Younger  and
Older  in whic h they are false .
190 Chapte r 7. First­Orde r Logi c
VARIABL ETo expres s this particula r rule , we will use unar y predicate s Cat and Mammal,  thus , "Spo t is a
cat" is represente d by Cat(Spof),  and "Spo t is a mammal " by Mammal(Spot).  In English , wha t
we wan t to say is that for any objec t x, if x is a cat then x is a mammal . First­orde r logic lets us
do this as follows :
V;c Cat(x)  => Mammal(x)
V is usuall y pronounce d "For all..." . Remembe r that the upside­dow n A stand s for "all." You
can thin k of a sentenc e MX P, wher e P is any logica l expression , as bein g equivalen t to the
conjunctio n (i.e., the A) of all the sentence s obtaine d by substitutin g the nam e of an objec t for
the variabl e x whereve r it appear s in P. The precedin g sentenc e is therefor e equivalen t to
Cat(Spot)  => Mammal(Spot)  A
Cat(Rebecca)  =>• Mammal(Rebecca)  A
Cat(Felix)  => Mammal(Felix)  A
Cat(Richard)  => Mammal(Richard)  A
Cat(John)  => Mammal(John)  A
Thus , it is true if and only if all these sentence s are true, that is, if P is true for all object s x in the
universe . Henc e V is calle d a universa l quantifier .
We use the conventio n that all variable s start with a lowercas e letter , and that all constant ,
predicate , and functio n symbol s are capitalized . A variabl e is a term all by itself , and as such can
also serv e as the argumen t of a function , for example , ChildOf(x).  A term with no variable s is
GROUN D TERM calle d a groun d term .
It is wort h lookin g carefull y at the conjunctio n of sentence s give n before . If Spot , Rebecca ,
and Felix are know n to be cats, then the first three conjunct s allow us to conclud e that they are
mammals . But what  abou t the next two conjuncts , whic h appea r to mak e claim s abou t King John
and Richar d the Lionheart ? Is that part of the meanin g of "all cats are mammals" ? In fact, these
conjunct s are true , but mak e no claim whatsoeve r abou t the mammalia n qualification s of John
and Richard . Thi s is becaus e Cat(Richard)  and Cat(John)  are (presumably ) false . Lookin g at
the truth table for => (Figur e 6.9), we see that the whol e sentenc e is true wheneve r the left­han d
side of the implicatio n is false— regardles s of the truth of the right­han d side. Thus , by assertin g
the universall y quantifie d sentence , whic h is equivalen t to assertin g a whol e list of individua l
implicatio n sentences , we end up assertin g the right­han d side of the rule just for those individual s
for who m the left­han d side is true , and sayin g nothin g at all abou t thos e individual s for who m
the left­han d side is false . Thus , the truth­tabl e entrie s for =>• turn out to be perfec t for writin g
genera l rules with universa l quantifiers .
It is temptin g to thin k that the presenc e of the conditio n Cat(x)  on the left­han d side of the
implicatio n mean s that someho w the universa l quantifie r range s only over cats. This is perhap s
helpfu l but not technicall y correct . Again , the universa l quantificatio n make s a statemen t abou t
everything,  but it does not mak e any claim abou t whethe r non­cat s are mammal s or not. On the
other hand , if we tried to expres s "all cats are mammals " usin g the sentenc e
MX Cat(x)  A Mammal(x)
Sectio n 7.1. Synta x and Semantic s 191
this woul d be equivalen t to
Cat(Spot)  A Mammal(Spot)  A
Cat(Rebecca)  A Mammal(Rebecca)  A
Cat(Felix)  A Mammal(Felix)  A
Cat(Richard)  A Mammal  (Richard)  A
Cat(John)  A Mammal(John)  A
Obviously , this does not captur e wha t we want , becaus e it says that Richar d the Lionhear t is both
a cat and a mammal .
Existentia l quantificatio n (3)
Universa l quantificatio n make s statement s abou t every object . Similarly , we can mak e a statemen t
abou t some  objec t in the univers e withou t namin g it, by usin g an existentia l quantifier . To say,
for example , that Spot has a siste r who is a cat, we writ e
3 x Sister(x,  Spot)  A Cat(x)
3 is pronounce d "Ther e exist s ...". In general , 3 x P is true if P is true for some  objec t in the
universe . It therefor e can be though t of as equivalen t to the disjunctio n (i.e. , the V) of all the
sentence s obtaine d by substitutin g the nam e of an objec t for the variable* . Doin g the substitutio n
for the abov e sentence , we woul d get
(Sister(Spot,  Spot)  A Cat(Spot))  V
(Sister(Rebecca,  Spot)  A Cat(Rebecca))  V
(Sister(Felix,  Spot)  A Cat(Fdix))  V
(Sister(Richard,  Spot)  A Cat(Richard))  V
(Sister(John,  Spot)  A Cat(John))  V
The existentiall y quantifie d sentenc e is true just in case at least one of these disjunct s is true. If
Spot had two sister s who were cats , then two of the disjunct s woul d be true , makin g the whol e
disjunctio n true also. This is entirel y consisten t with the origina l sentenc e "Spo t has a siste r who
is a cat."4
Just as => appear s to be the natura l connectiv e to use with V, A is the natura l connectiv e
to use with 3. Risin g A as the main connectiv e with V ledJ o .an .overl y stron g statemen t in the
exampl e in the previou s section ; visin g =^ with 3 usuall y lead s to a very wea k statemen t indeed .
Conside r the followin g representation :
3x Sister(x,Spot)  => Cat(x)
4 Ther e is a varian t of the existentia l quantifier , usuall y writte n 3' or 3!, that mean s "Ther e exist s exactl y one ...". The
same meanin g can be expresse d usin g equalit y statements , as we show in Sectio n 7.1.
192 Chapte r 7. First­Orde r Logi c
On the surface , this migh t look like a reasonabl e renditio n of our sentence . But expande d out
into a disjunction , it become s
(Sister(Spot,  Spot)  => Cat(Spot))  V
(Sister(Rebecca,  Spot}  =>• Cat(Rebecca))  V
(Sister(Felix,  Spot)  => Cat(Felix))  V
(Sister(Richard,  Spot)  => Cat(Richard))  V
(Sister(John,  Spot)  => Cat(John))  V
Now, this disjunctio n will be satisfie d if any of its disjunct s are true . An implicatio n is true
if both premis e and conclusio n are true , or if its premise  is false.  So if Richar d the Lionhear t
is not Spot' s sister , then the implicatio n Sister(Spot,  Richard)  => Cat(Richard)  is true and the
entire disjunctio n is therefor e true. So, an existentiall y quantifie d implicatio n sentenc e is true in
a univers e containin g any objec t for whic h the premis e of the implicatio n is false ; henc e
sentence s reall y do not say muc h at all.
Neste d quantifier s
We will often wan t to expres s mor e comple x sentence s usin g multipl e quantifiers . The simples t
case is wher e the quantifier s are of the sam e type . For example , "For all x and all y, if x is the
paren t of y then y is the child of x" become s
Child(y,x)
V y. Similarly , the fact that a person' s brothe r has that perso n as aMx,y Parent(x,y)
Mx,y is equivalen t to V x
siblin g is expresse d by
V x,y Brother(x,y)  => Sibling(y,x~)
In othe r cases we will have mixtures . "Everybod y love s somebody " mean s that for ever y person ,
there is someon e that perso n loves :
MX 3y Loves(x,y)
On the othe r hand , to say "Ther e is someon e who is loved by everyone " we writ e
3>' V x Loves(x,y)
The order of quantificatio n is therefor e very important . It become s cleare r if we put in parentheses .
In general , V* (3 v P(x,yj),  wher e P(x,y)  is som e arbitrar y sentenc e involvin g * and y, says
that every  object  in the univers e has a particula r property , namely , the propert y that it is related_tp _
some objec t by the jelatiQn_P._O n the othe r hand , 3x (V> > P(x,y)  says that there is some  objec t
in the worl d that has a particula r property , namel y the propert y of bein g relate d by P to ever y
objec t in the world .
A mino r difficult y arise s whe n two quantifier s are used with the sam e variabl e name .
Conside r the sentenc e
\tx [Cat(x)V(3x  Bmther(Richard,xy)}
Here the x in Brother(Richard,x)  is existentially  quantified . The rule is that the variabl e belong s
\ to the innermos t quantifie r that mention s it; then it will not be subjec t to any othe r quantification .
Sectio n 7.1. Synta x and Semantic s 193
WFFThis is just like variabl e scopin g in block­structure d programmin g language s like Pasca l and Lisp ,
wher e an occurrenc e of a variabl e nam e refer s to the innermos t bloc k that declare d the variable .
Anothe r way to thin k of it is this: 3x Brother(Richard,x)  is a sentenc e abou t Richar d (that he
has a brother) , not abou t x; so puttin g a Vx outsid e it has no effect . It coul d equall y well have
been writte n 3 z Brother(Richard,  z). Becaus e this can be a sourc e of confusion , we will alway s
use differen t variables .
Every variabl e mus t be introduce d by a quantifie r befor e it is used . A sentenc e like MX P(y),
in whic h y does not have a quantifier , is incorrect.5 The term well­forme d formul a or wff is
sometime s used for sentence s that have all their variable s properl y introduced .
Connection s betwee n V and 3
The two quantifier s are actuall y intimatel y connecte d with each other , throug h negation . Whe n
one says that everyon e dislike s parsnips , one is also sayin g that there does not exist someon e who
likes them ; and vice versa :
Vx ~^Likes(x,  Parsnips)  is equivalen t to ­*3x  Likes(x,  Parsnips)
We can go one step further . "Everyon e likes ice cream " mean s that there is no one who does not
like ice cream :
Vjc Likes(x,  IceCream)  is equivalen t to ^3 x ^Likes(xJceCream)  \,
­A".'w .  ­^Becaus e V is reall y a conjunctio n over the univers e of object s and 3 is a disjunction,^ ! shoul d
not be surprisin g that they obey De Morgan' s rules . The De Morga n rule s for quantifie d and
unqualifie d sentence s are as follows :
V* ­iP = ­>3x  P ­.PA­"( 2 = ­>(PV 0
­A/x P = 3x ­.P ­>(PA 0 = ­.PV­. g
MX P =  ­>3x  ­.P Pf\Q  =  ­.(­.PV­. 0
3* P =  ­A/ x ­.P PVQ  =  ­n(^P A ­­0
Thus , we do not reall y need both V and 3, just as we do not reall y need both A and V. Som e
forma l logician s adop t a principa l of parsimony , throwin g out any syntacti c item that is not strictl y
necessary . For the purpose s of AI, the content , and henc e the readability , of the sentence s are
important . Therefore , we will keep both of the quantifiers .
Equalit y
First­orde r logic include s one mor e way to mak e atomi c sentences , othe r than usin g a predicat e
EQUALIT Y SYMBO L an d term s as describe d earlier . We can use the equalit y symbo l to mak e statement s to the effec t
that two term s refe r to the same object . For example ,
Father(John)  = Henry
says that the objec t referre d to by Father(John)  and the objec t referre d to by Henry  are the same .
5 Sometime s there is an assumptio n that all unquantifle d variable s are introduce d with an implici t V. This is the case in
most logic programmin g languages .
194 Chapte r 7. First­Orde r Logi c
Equalitj^ca n be .viewe d as a predicatejymbo l with a predefine d meaning , jiamely , that it
IDENTIT Y RELATIO N i s fixecTt o refer to the identit y relation . The identit y relatio n is the set of all pairs of object s in
~'~' whic h both element s of each pair are the sam e object :
{ (Spot,Spot) ,
(Rebecca,Rebecca) , ,
(Felix , Felix) ,
(Richar d the Lionheart , Richar d the Lionheart) ,
(Kin g John , King John) ,
(Henr y II, Henr y II),
Thus , to see if Father(John)=  Henry  is true in a particula r interpretation , we first look in the
functiona l relatio n for Father  and find the entr y
(Kin g John , Henr y II),
Then , becaus e Henry  refer s to Henr y II, the equalit y statemen t is true becaus e (Henr y II, Henr y II)
is in the equalit y relation .
The equalit y symbo l can be used to describ e the propertie s of a give n function , as we did
above for the Father  symbol . It can also be used with negatio n to insis t that two term s are not
the same  object . To say that Spot has at least two sisters , we woul d writ e
3 A, y Sister(Spot,  x) A Sister(Spot,  y) A ­i(jt = y)
If we simpl y wrot e
3 x, y Sister(Spot,  x) A Sister(Spot,  y)
that woul d not asser t the existenc e of two distinc t sisters , becaus e nothin g says that A­ and y have
to be different . Conside r the expansio n of the existentia l statemen t into a disjunction : it will
includ e as a disjunc t
... V (Sister(Spot,  Rebecca)  A Si.ster(Spot,  Rebecca))  V ...
whic h occur s whe n Rebecca  is substitute d for both x and y. If Rebecc a is indee d Spot' s sister ,
then the existentia l will be satisfie d becaus e this disjunc t will be true . The additio n of ­*(x = y)
make s the disjunc t false , becaus e Rebecca­Rebecca  is necessaril y true . The notatio n x^y is
sometime s used as an abbreviatio n for ­i(x = y).
7.2 EXTENSION S AND NOTATIONA L VARIATION S
In this sectio n we look at thre e type s of alternative s to first­orde r logic . First , we look at an
extensio n calle d higher­orde r logic . Second , we conside r som e abbreviation s that add no new
powe r to first­orde r logi c but do mak e the resultin g sentence s mor e concise . Third , we look at
variation s on our notatio n for first­orde r logic .
Sectio n7.2. Extension s and Notationa l Variation s 195
HIGHER­ORDE R
LOGIC
A­EXPRESSIO NHigher­orde r logi c
First­orde r logic gets its nam e from the fact that one can quantif y over objects  (the first­orde r
entitie s that actuall y exis t in the world ) but not over relation s or function s on thos e objects .
Higher­orde r logic allow s us to quantif y over relation s and function s as well as over objects . For
example , in higher­orde r logic we,ca n say that two object s are equa l if and only if all propertie s
applie d to them are equivalent :
Vx,y (x = y) & (Vp  p(x) O p(y))
Or we coul d say that two function s are equa l if and only if they have the sam e valu e for all
arguments :
V/,£ (f = g) «• (V */(* ) = £(*))
Higher­orde r logic s have strictl y mor e expressiv e powe r than first­orde r logic . As yet, however ,
logician s have little understandin g of how to reaso n effectivel y with sentence s in higher­orde r
logic, and the genera l proble m is know n to be undecidable . In this book , we will stick to first­orde r
logic , whic h is muc h bette r understoo d and still very expressive .
Functiona l and predicat e expression s usin g the A operato r
It is ofte n usefu l to be able to construc t comple x predicate s and function s from simple r compo ­
nents , just as we can construc t comple x sentence s from simple r component s (e.g. , P A Q), or
comple x term s from simple r ones (e.g. , x2 +y3). To turn the term x2 ­ y2 into a function , we need
to say wha t its argument s are: is it the functio n wher e you squar e the first argumen t and subtrac t
the squar e of the secon d argument , or vice versa ? The operato r A (the Gree k lette r lambda ) is
traditionall y used for this purpose . The functio n that take s the differenc e of the square s of its first
and secon d argumen t is writte n as
Xx,yx2­y2
This A­expression6 can then be applie d to argument s to yield a logica l term in the same way that
an ordinary , name d functio n can:
(\x,y x2 ­ y2)(25,24) =252 ­ 242 = 49
We will also find it usefu l (in Chapte r 22) to generat e A­expression s for predicates . For example ,
the two­plac e predicat e "are of differin g gende r and of the same address " can be writte n
\x,y Gender(x)^Gender(y)  f\Address(x)  = Address(y)
As one woul d expect , the applicatio n of a predicat e A­expressio n to an appropriat e numbe r of
argument s yield s a logica l sentence . Notic e that the use of A in this way does not increas e the
forma l expressiv e powe r of first­orde r logic , becaus e any sentenc e that include s a A­expressio n
can be rewritte n by "pluggin g in" its argument s to yield a standar d term or sentence .
The sam e terminolog y is used in Lisp , wher e lambd a play s exactl y the sam e role as the A operator .
196 Chapte r 7. First­Orde r Logi c
The uniquenes s quantifie r 3!
We have seen how to use 3 to say that some  object s exist , but there is no concis e way to say that
a unique  objec t satisfyin g some predicat e exists . Som e author s use the notatio n
31 x King(x)
to mea n "ther e exist s a uniqu e objec t x satisfyin g King(x)"  or mor e informally , "there' s exactl y
one King. " Yo u shoul d thin k of this not as addin g a new quantifier , 3!, but rathe r as bein g a
convenien t abbreviatio n for the longe r sentenc e
3x King(x)  f\M y King(y)  => x = y
Of course , if we knew from the start there was only one King , we probabl y woul d have used the
constan t King  rathe r than the predicat e King(x).  A more comple x exampl e is "Ever y countr y has
exactl y one ruler" :
V c Country(c)  =>• 3! r Ruler(r,  c)
The uniquenes s operato r t.
It is convenien t to use 3! to state uniqueness , but sometime s it is even mor e convenien t to have
a term representin g the uniqu e objec t directly . The notatio n / xP(x)  is commonl y used for this.
(The symbo l L is the Gree k lette r iota. ) To say that "the uniqu e rule r of Freedoni a is dead " or
equivalentl y "the r that is the rule r of Freedoni a is dead, " we woul d write :
Dead(i  r Ruler(r,  Freedonia))
This is just an abbreviatio n for the followin g sentence :
3! r Ruler(r,  Freedonia)  A Dead(r)
Notationa l variation s
The first­orde r logic notatio n used in this book is the de facto  standar d for artificia l intelligence ;
one can safel y use the notatio n in a journa l articl e withou t definin g it, becaus e it is recognizabl e
to mos t readers . Severa l othe r notation s have been developed , both withi n AI and especiall y in
other field s that use logic , includin g mathematics , compute r science , and philosophy . Her e are
some of the variations :
Synta x item
Negatio n (not )
Conjunctio n (and )
Disjunctio n (or)
Implicatio n (if)
Equivalenc e (iff)
Universa l (all)
Existentia l (exists )
Relatio nThis book
­.f
P f\Q
P\IQ
P => Q
P o Q
MX P(x)
3x P(x)
R(x,y)Other s
~P P
P&Q  P  Q PQ  P,Q
P\Q P;Q  P + Q
P^Q  P^Q
p=Q  p^Q
(Mx)P(x)  f\xP(x)  P(x)
(3x)P(x)  V x pW P(Skolemi)
(R x y) Rxy  xRy
Sectio n 7.3. Usin g First­Orde r Logi c 197
Two othe r commo n notation s deriv e from implementation s of logic in compute r systems . The
logic programmin g languag e Prolo g (whic h we discus s furthe r in Chapte r 10) has two main
differences . It uses uppercas e letter s for variable s and lowercas e for constants , wherea s mos t
other notation s do the reverse . Prolo g also reverse s the order  of implications , writin g Q :­ P
instea d of P => Q. A comm a is used both to separat e argument s and for conjunction , and a
perio d mark s the end of a sentence :
cat(X) :- furry(X) , meows(X) , has(X,claws) .
In reasonin g system s implemente d in Lisp , a consisten t prefi x notatio n is common . Each sentenc e
and nonconstan t term is surrounde d by parentheses , and the connective s com e first , just like the
predicat e and functio n symbols . Becaus e Lisp does not distinguis h betwee n uppercas e and
lowercas e symbols , variable s are usuall y distinguishe d by an initia l ? or $ character , as in this
example :
(forall ?x
(implies(and
(cat(furr y ?x) (meow s ?x) (ha s ?x claws) )
7.3 USIN G FIRST­ORDE R LOGI C
DOMAI N In knowledg e representation , a domai n is a sectio n of the worl d abou t whic h we wish to expres s
some knowledge . In this chapter , we start with som e simpl e domains , usin g first­orde r logic to
represen t famil y relationship s and mathematica l sets. We then mov e on to show the knowledg e
that an agen t in the wumpu s worl d woul d need . Chapte r 8 goes into mor e dept h on the use of
first­orde r logic for knowledg e representation .
The kinshi p domai n
The first exampl e we conside r is the domai n of famil y relationships , or kinship . Thi s domai n
include s facts such as "Elizabet h is the mothe r of Charles " and "Charle s is the fathe r of William, "
and rules such as "If x is the mothe r of y and y is a paren t of z, then x is a grandmothe r of z."
Clearly , the object s in our domai n are people . The propertie s they have includ e gender ,
and they are relate d by relation s such as parenthood , brotherhood , marriage , and so on. Thus ,
we will are have two unar y predicates , Male  and Female.  Mos t of the kinshi p relation s will be
binar y predicates : Parent,  Sibling,  Brother,  Sister,  Child,  Daughter,  Son,  Spouse,  Wife,  Husband,
Grandparent,  Grandchild, Cousin,  Aunt,  Uncle.  We will use function s for Mother  and Father,
becaus e ever y perso n has exactl y one of each of these (at least accordin g to nature' s design) .
We can go throug h each functio n and predicate,  writin g dow n wha t we kno w in term s of
the othe r symbols . For example , one' s mothe r is one' s femal e parent :
V m, c Mother(c)  = m O Female(m)  A Parent(m,  c)
198 Chapte r 7. First­Orde r Logi c
One' s husban d is one' s male spouse :
Vw,A Husband(h,w)  O­ Male(h)  A Spouse(h,  w)
Male and femal e are disjoin t categories :
V* Male(x)  <=> ­^Female(x)
Paren t and chil d are invers e relations :
Vp,c Parent(p,c)  O Child(c,p)
A grandparen t is a paren t of one's parent :
Vg, c Grandparent(g,c)  <^> 3/7 Parent(g,p)  A Parent(p,  c)
A siblin g is anothe r child of one' s parents :
V;t,y Sibling(x,y)  <$•  xfyf\3 p Parent  (p, x)/\  Parent  (p,y)
We coul d go on for severa l mor e page s like this, and in Exercis e 7.6 we ask you to do just that.
Axioms , definitions , and theorem s
AXIOM S Mathematician s writ e axiom s to captur e the basi c facts abou t a domain , defin e othe r concept s
THEOREM S i n term s of thos e basi c facts , and then use the axiom s and definition s to prov e theorems . In
AI, we rarel y use the term "theorem, " but the sentence s that are in the knowledg e base initiall y
are sometime s calle d "axioms, " and it is commo n to talk of "definitions. " Thi s bring s up an
importan t question : how do we know whe n we have writte n dow n enoug h axiom s to fully specif y
a domain ? One way to approac h this is to decid e on a set of basic predicate s in term s of whic h
all the othe r predicate s can be defined . In the kinshi p domain , for example , Child,  Spouse, Male,
and Female  woul d be reasonabl e candidate s for basi c predicates . In othe r domains , as we will
see, there is no clearl y identifiabl e basic set.7
The convers e proble m also arises : do we have too man y sentences ? For example , do we
need the followin g sentence , specifyin g that siblinghoodi s a symmetri c relation ?
\fx,y Sibling(x,y)  <= > Sibling(y,x)
In this case , we do not. From Sibling(John,  Richard),  we can infe r that
3 p Parent(p,  John)  A Parent(p,  Richard),
and from that we can infe r Sibling(Richard,  John).  In mathematics , an independen t axio m is one
that canno t be derive d from all the othe r axioms . Mathematician s striv e to produc e a minima l set
of axiom s that are all independent . In AI it is commo n to includ e redundan t axioms , not becaus e
they can have any effec t on wha t can be proved , but becaus e they can mak e the proces s of findin g
a proo f more efficient .
DEFINITIO N A n axio m of the form V x,y P(x,y)  = ... is ofte n calle d a definitio n of P, becaus e it
serve s to defin e exactl y for wha t object s P does and does not hold . It is possibl e to have severa l
definition s for the sam e predicate ; for example , a triangl e coul d be define d both as a polygo n
7 In all cases , the set of sentences.,wil l have model s othe r than the intende d model ; this follow s from a theore m of
Lowenheim' s statin g that al{ consisten t axio m sets have a mode l whos e domai n is the integers .INDEPENDEN T
AXIO M
ISectio n 7.3. Usin g First­Orde r Logi c 199
with thre e side s and as a polygo n with thre e angles . Man y predicate s will have no complet e
definitio n of this kind , becaus e we do not know enoug h to fully characteriz e them . For example ,
how woul d you complet e the sentence :
\/x Person(x)  <=> ...
Fortunately , first­orde r logi c allow s us to mak e use of the Person  predicat e withou t completel y
definin g it. Instea d we can writ e partia l specification s of propertie s that ever y perso n has and
propertie s that mak e somethin g a person :
V* Person(x)  =>• ...
V* ... => Person(x)
The domai n of sets
The domai n of mathematica l sets is somewha t mor e abstrac t than cats or kings , but nevertheles s
form s a coheren t body of knowledg e that we woul d like to be able to represent . We wan t to be
able to represen t individua l sets, includin g the empt y set. We need a way to buil d up sets by
addin g an elemen t to a set or takin g the unio n or intersectio n of two sets. We will want  to know
if an elemen t is a membe r of a set, and to be able to distinguis h sets from object s that are not sets.
We will use the norma l vocabular y of set theory : EmptySet  is a constant ; Member  and
Subset  are predicates ; and Intersection,  Union,  and Adjoin  are functions . (Adjoin  is the functio n
that adds one elemen t to a set.) Set is a predicat e that is true only of sets. The followin g eigh t
axiom s provid e this:
1. The only sets are the empt y set and thos e mad e by adjoinin g somethin g to a set.
Vs Set(s)  •» (s = EmptySet)  V (3x,s 2 Set(s 2) A s = Adjoin(x,s 2))
2. The empt y set has no element s adjoine d into it. (In othe r words , ther e is no way to
decompos e EmptySet  into a smalle r set and an element. )
­*3x,s  Adjoin(x,  s) ­ EmptySet
3. Adjoinin g an elemen t alread y in the set has no effect :
V*, s Member(x,s)  o­ s=Adjoin(x,s)
4. The only member s of a set are the element s that were adjoine d into it. We expres s this
recursively , sayin g that x is a membe r of s if and only if 5 is equa l to som e set s2 adjoine d
with som e elemen t y, wher e eithe r y is the same as x or x is a membe r of s2.
\/x,s Member(x,s)  <^>
3 y, s2 (s = Adjoin(y,  s2) A (x = y V Member(x,  s2)))
5. A set is a subse t of anothe r if and only if all of the first set's member s are member s of the
secon d set.
Vs\,s 2 Subset(si,s 2) •<= > (V x Member(x,si)  => Member(x,s 2))
6. Two sets are equa l if and only if each is a subse t of the other .
\/s\,s 2 (s\=s 2) •£> (Subset(s\,S2)  /\ Subset(s 2,s\))
200 Chapte r 7. First­Orde r Logi c
7. An objec t is a membe r of the intersectio n of two sets if and only if it is a membe r of each
of the sets.
Vx,si,$2  Member(x,Intersection(s\,s 2)) O­
Member(x,  s\) A Member(x,  $2)
8. An objec t is a membe r of the unio n of two sets if and only if it is a membe r of eithe r set.
Vx,si,S2  Member(x,Union(S[,S2))  ­O ­
Member(x,  s\) V Member(x,  .$2)
The domai n of lists is very simila r to the domai n of sets. The differenc e is that lists are ordered ,
and the same elemen t can appea r more than once in a list. We can use the vocabular y of Lisp for
lists: Nil is the constan t list with no elements ; Cons,  Append,  First,  and Rest  are functions ; and
Find is the predicat e that does for lists wha t Member  does for sets. List!  is a predicat e that is true
only of lists . Exercis e 7.8 asks you to writ e axiom s for the list domain .
Specia l notation s for sets, lists and arithmeti c
Becaus e sets and lists are so common , and becaus e there is a well­define d mathematica l notatio n
for them , it is temptin g to exten d the synta x of first­orde r logi c to includ e this mathematica l
notation . The importan t thin g to remembe r is that whe n this is done , it is only a chang e to the
synta x of the language ; it does not chang e the semantics . The notatio n is just an abbreviatio n
for the norma l first­orde r logic notation . Suc h notationa l extension s are ofte n calle d syntacti c
SYNTACTI C SUGA R sugar . We will use standar d mathematica l notatio n for arithmeti c and set theor y from here on,
and will also use the nonstandar d notatio n {a s} as an abbreviatio n forAdjoin(a,  s). Ther e is less
consensu s amon g mathematician s abou t the notatio n for lists ; we have chose n the notatio n used
by Prolog :
0 = EmptySet  []
{x} = Adjoin(x,  EmptySet)  [x]
{x,y}  = Adjoin(x,Adjoin(y,  EmptySet))  [x,y]
{x,y s} = Adjoin(x,Adjoin(y,sJ)  [x,y\l]
r(J s — Union(r,  s)
r n s = lntersection(r,  s)
x£s = Member(x,  s)
r C s = Subset(r,  s)
We will also use standar d arithmeti c notatio n in logica l sentences , saying , for example , x > 0
instea d of >(.v, Zero)  and 1+2 instea d of +(1,2) . It shoul d be understoo d that each use of an infix
mathematica l operato r is just an abbreviatio n for a standar d prefi x predicat e or function . The
integer s are used as constan t symbols , with their usua l interpretation .Nil
Cons(x,  Nil)
Cons(x,  Cons(y,  Nil))
Cons(x,  Cons(y,  /))
Askin g question s and gettin g answer s
Althoug h we will dela y discussio n of the internal s of TEL L and ASK unti l chapter s 9 and 10, it
is importan t to understan d how they are used in first­orde r logic . If we wan t to add the kinshi p
Sectio n 7.4. Logica l Agent s for the Wumpu s Worl d 201
sentence s to a knowledg e base KB, we woul d call
JELL(KB,  (V m, c Mother(c)  = m •& Female(m)  A Parent(m,  c)))
and so on. Now if we tell it
TELL(KB,  (Female(Maxi)  A Parent(Maxi,  Spot)  A Parent(Spot,  Boots)))
then we can
ASK(KB,  Gmndparent(Maxi,  Boots))
ASSERTION S an d receiv e an affirmativ e answer . The sentence s adde d usin g TEL L are often calle d assertions ,
QUERIE S an d the question s aske d usin g ASK are calle d querie s or goal (not to be confuse d with goal s as
GOAL use d to describ e an agent' s desire d states) .
Certai n peopl e thin k it is funn y to answe r question s such as "Can you tell me the time? "
with "Yes. " A knowledg e base shoul d be mor e cooperative . Whe n we ask a questio n that is
existentiall y quantified , such as
ASK(KB,  3x  Child(x,Spot))
we shoul d receiv e an answe r indicatin g that Boot s is a child of Spot . Thus , a quer y with existentia l
variable s is askin g "Is ther e an x such that ...," and we solv e it by providin g such an x. The
SUBSTITUTIO N standar d form for an answe r of this sort is a substitutio n or bindin g list, whic h is a set of
BINDIN G LIST variable/ter m pairs . In this particula r case, the answe r woul d be {x/Boots}.  (If there is more than
one possibl e answer , a list of substitution s can be returned . Differen t implementation s of ASK do
differen t things. )
7.4 LOGICA L AGENT S FOR THE WUMPU S WORL D
MODEL­BASE D
AGENT S
GOAL­BASE D
AGENT SIn Chapte r 6 we showe d the outlin e of a knowledge­base d agent , repeate d here in slightl y modifie d
form as Figur e 7.2. We also hinte d at how a knowledge­base d agen t coul d be constructe d for
the wumpu s world , but the limitation s of propositiona l logi c kept us from completin g the agent .
With first­orde r logic , we have all the representationa l powe r we need , and we can turn to the
more interestin g questio n of how an agen t shoul d organiz e wha t it know s in orde r to take the
right actions . We will conside r three agen t architectures : refle x agents8 that merel y classif y their
percept s and act accordingly ; model­base d agents9 that construc t an interna l representatio n of
the worl d and use it to act; and goal­base d agent s that form goal s and try to achiev e them .
(Goal­base d agent s are usuall y also mo3el­base d agents. )
The first step in constructin g an agen t for the wumpu s worl d (or for any world ) is to defin e
the interfac e betwee n the environmen t and the agent . The percep t sentenc e mus t includ e both
the percep t and the time at whic h it occurred ; otherwis e the agen t will get confuse d abou t whe n
it saw what . We will use integer s for time steps . A typica l percep t sentenc e woul d be
Percept([Stench,  Breeze,  Glitter,  None,  None},  5)
8 Refle x agent s are also know n as tropisti c agents . The biologica l term tropism  mean s havin g a tendenc y to reac t in a
definit e manne r to stimuli . A heliotropi c plant , for example , is one that turn s towar d the sun.
9 Not e that this usag e of "model " is relate d but not identica l to its meanin g as a worl d referre d to by a sentence .
202 Chapte r 7. First­Orde r Logi c
functio n KB­AGENT ( percept)  return s an action
static : KB,  a knowledg e base
t, a counter , initiall y 0, indicatin g time
, MAKE­PERCEPT­SENTENCE(percepf , t))
action  — ASK(KB,  MAKE­ACTION­QUERY(O )
KB, MAKE­ACTION­SENTENCE(ac­rion , t))
retur n action
Figur e 7.2 A  generi c knowledge­base d agent .
The agent' s actio n mus t be one of the following :
Turn(Right),  Turn(Left),  Forward,  Shoot,  Grab,  Release,  Climb
To determin e whic h one is best , the functio n MAKE­ACTION­QUER Y create s a quer y such as
3 a Action(a,5)
with the intentio n that ASK return s a bindin g list such as {a/Grab}  and Grab  is assigne d as the
value of the variabl e action.  The agen t progra m then call s TEL L once agai n to recor d whic h
actio n was taken . Becaus e the agen t does not perceiv e its own actions , this is the only way the
KB coul d know wha t the agen t has done .
7.5 A SIMPL E REFLE X AGEN T
The simples t possibl e kind of agen t has rule s directl y connectin g percept s to actions . Thes e rules
resembl e reflexe s or instincts . For example , if the agen t sees a glitter , it shoul d do a grab in orde r
to pick up the gold :
Vs,b,u,c,t  Percept([s,b,  Glitter,  u, c],t)  =>• Action(Grab,i)
The connectio n betwee n percep t and actio n can be mediate d by rule s for perception , whic h
abstrac t the immediat e perceptua l inpu t into mor e usefu l forms :
Vb,g,u,c,t  Percept([Stench,b,g,u,c],t)  •=> Stench(t)
Vs,g,u,c,t  Percept([s,  Breeze,  g,u,c],t ) => Breeze(t)
\/s,b,u,c,t  Percept([s,b,  Glitter,  u,c],t ) =>• AtGold(t)
Then a connectio n can be mad e from thes e predicate s to actio n choices :
Vr AtGold(t)  => Action(Grab,t)
This rule is mor e flexibl e than the direc t connection , becaus e it coul d be used with othe r mean s
for deducin g AtGold— for example , if one stub s one' s toe on the gold .
Sectio n 7.6. Representin g Chang e in the Worl d 203
In a more comple x environment , the percep t migh t be an entir e array of gray­scal e or colo r
value s (a camer a image) , and the perceptua l rules woul d have to infe r such thing s as "There' s a
large truck right behin d me flashin g its lights. " Chapte r 24 cover s the topic of perceptio n in more
detail . Of course , compute r visio n is a very difficul t task, wherea s these rule s are trivial , but the
idea is the same .
Limitation s of simpl e refle x agent s
Simpl e refle x agent s will have a hard time in the wumpu s world . The easies t way to see this
is to conside r the Climb  action . The optimal  agen t shoul d eithe r retriev e the gold or determin e
that it is too dangerou s to get the gold , and then retur n to the start squar e and clim b out of the
cave. A pure refle x agen t canno t know for sure whe n to Climb,  becaus e neithe r havin g the gold
nor bein g in the start squar e is part of the percept ; they are thing s the agen t know s by formin g a
representatio n of the world .
Refle x agent s are also unabl e to avoi d infinit e loops . Suppos e that such an agen t has picke d
up the gold and is heade d for home . It is likel y that the agen t will ente r one of the square s it was
in before , and receiv e the same percept . In that case, it must,  by definition , do exactl y wha t it did
before . Lik e the dung beetl e of Chapte r 2, it does not know whethe r it is carryin g the gold , and
thus canno t mak e its action s conditiona l on that. Randomizatio n provide s som e relief , but only
at the expens e of riskin g man y fruitles s actions .
7.6 REPRESENTIN G CHANG E IN THE WORL D
In our agen t design , all percept s are adde d into the knowledg e base , and in principl e the percep t
histor y is all there is to know abou t the world . If we allow rules that refer to past percept s as well
as the curren t percept , then we can, in principle , exten d the capabilitie s of an agen t to the poin t
wher e the agen t is actin g optimally .
Writin g such rules , however , is remarkabl y tediou s unles s we adop t certai n pattern s of
INTERNA L MODE L reasonin g that correspon d to maintainin g an interna l mode l of the world , or at leas t of the
relevan t aspect s thereof . Conside r an exampl e from everyda y life: findin g one's keys . An agen t
that has take n the time to build a representatio n of the fact "my keys are in my pocket " need only
recal l that fact to find the keys . In contrast , an agen t that just store d the complet e percep t sequenc e
woul d have to do a lot of reasonin g to discove r wher e the keys are. It woul d be theoreticall y
possible  for the agen t to in effec t rewin d the vide o tape of its life and repla y it in fast­forward ,
using inferenc e rules to keep track of wher e the keys are, but it certainl y woul d not be convenient.
ISrtfef1 I t can t*e show n that any system  that  makes  decisions  on the basis  of past percepts  can
wr be  rewritten  to use instead  a set of sentences  about  the current world  state,  provide d that thes e
sentence s are update d as each new percept  arrive s and as each actio n is done .
Rule s describin g the way in whic h the worl d change s (or does not change ) are calle d
DIACHRONI C diachroni c rules , from the Gree k for "acros s time. " Representin g chang e is one of the mos t
importan t area s in knowledg e representation . The real world , as well as the wumpu s world , is
204 Chapte r 7. First­Orde r Logi c
characterize d by chang e rathe r than stati c truth . Richar d has no brothe r unti l John is born ; Spot
is a kitte n for a whil e and then become s a full­grow n tomcat ; the agen t move s on.
The easies t way to deal with chang e is simpl y to chang e the knowledg e base ; to erase
the sentenc e that says the agen t is at [1,11 , and replac e it with one that says it is at [1,2] . Thi s
approac h can mitigat e som e of the difficultie s we saw with the propositiona l agent . If we only
want the knowledg e base to answe r question s abou t the lates t situation , this will work fine . But
it mean s that all knowledg e abou t the past is lost, and it prohibit s speculatio n abou t differen t
possibl e futures .
A secon d possibilit y was hinte d at in Chapter s 3 and 4: an agen t can searc h throug h a
space of past and possibl e futur e states , wher e each state is represente d by a differen t knowledg e
base. The agen t can explor e hypothetica l situations—wha t woul d happe n if I wen t two square s
forward ? This approac h allow s an agen t to switc h its attentio n betwee n severa l knowledg e bases ,
but it does not allow the agen t to reaso n abou t more than one situatio n simultaneously .
To answe r question s like "was ther e a stenc h in both [1,2 ] and [2,3]? " or "did Richar d
go to Palestin e befor e he becam e king? " require s representin g differen t situation s and action s
in the sam e knowledg e base . In principle , representing  situations  and actions  is no  different
from representing more  concrete objects  such  as cats  and kings,  or concrete  relations  such  as
brotherhood.  We need to decid e on the appropriat e object s and relations , and then writ e axiom s
abou t them . In this chapter , we will use the simplest , and oldest , solutio n to the problem . In
Chapte r 8, we will explor e more comple x approaches .
SITUATIO N
CALCULU SSituatio n calculu s
Situatio n calculu s is the nam e for a particula r way of describin g chang e in first­orde r logic . It
conceive s of the worl d as consistin g of a sequenc e of situations , each of whic h is a "snapshot "
of the state of the world . Situation s are generate d from previou s situation s by actions , as show n
in Figur e 7.3.
Every relatio n or propert y that can chang e over time is handle d by givin g an extra situatio n
argumen t to the correspondin g predicate . We use the conventio n that the situatio n argumen t is
alway s the last, and situatio n constant s are of the form 5,. Thus , instea d of At(Agent,  location),
we migh t have
•~­At(Agent,[\,  1],5 0) A At(Agent,  [1,2],S, )
> tcujescribe.th e locatio n of the agen t in the first two situation^jn_Figijre_l,3 ; Relation s or properties ^
that are not subjec t to chang e do not need the extra situatio n argument . For example , we can just
say Even(8)  instea d of Even(8 , So). In the wumpu s world , wher e wall s canno t move , we can just
use Wall([0, 1]) to say that there is a wall at [0,1] .
The nex t step is to represen t how the worl d change s from one situatio n to the next .
Situatio n calculu s uses the functio n Remlt(action,  situation)  to denot e the situatio n that result s
from performin g an actio n in som e initia l situation . Hence , in the sequenc e show n in Figur e 7.3,
We have the following :
Result(Forward,So)  = S\
Result(Turn(Right),  SO = S2
Result(Forward,  82) = S^
Sectio n 7.6. Representin g Chang e in the Worl d 205
EFFECT AXIOM SForward
Turn (Right)
Forward
Figur e 7.3 In situatio n calculus , the worl d is a sequenc e of situation s linke d by actions .
Action s are describe d by statin g their effects . Tha t is, we specif y the propertie s of the situatio n
that result s from doin g the action . Suppose , for example , that the agen t want s to keep track of
whethe r it is holdin g the gold . The descriptio n shoul d state that, in any situation , if gold is presen t
and the agen t does a grab , then it will be holdin g the gold in the resultin g situation . We writ e this
in such a way as to appl y to any portabl e object :
Portable(Gold)
V.v AtGold(s)  => Present(Gold,s)
\/x,s Present(x,  s) A Portable(x)  => Holding(x,Result(Grab,s))
A simila r axio m says that the agen t is not holdin g anythin g after a Release  action :
V x, s ­iffolding(x,  Result(Release,  s))
These axiom s are calle d effec t axioms . Unfortunately , they are not sufficien t to keep track of
whethe r the agen t is holdin g the gold . We also need to say that if the agen t is holdin g somethin g
206 Chapte r 7. First­Orde r Logi c
FRAM E AXIOM S
SUCCESSOR­STAT E
AXIO Mand does not releas e it, it will be holdin g it in the next state . Similarly , if the agen t is not holdin g
somethin g and does not or canno t grab it, it will not be holdin g it in the next state :
Va,x,s  Holding(x,  s) A (a^Release)  =>• Holding(x,Result(a,s))
V a,x,  s ­iHolding(x,  s) A (a^Grab  V ­^(Present(x,  s) A Portable(x))
=> ­iHolding(x,Result(a,s))
Axiom s like these that describ e how the worl d stays the same (as oppose d to how it changes ) are
called fram e axioms.10 Together/effec t axiom s and fram e axiomsprovid e a complet e descriptio n
of how the worl d evolve s in respons e to the agent' s actions .
We can obtai n a mor e elegan t representatio n by combinin g the­effec t axiom s and fra
axiom s into a singl e axio m that describe s how to comput e the Holding  predicat e for the nexttim e
step, give n its valu e for the curren t time step . The axio m has the followin g structure :
true afterward s o­ [a n actio n mad e it true
V tru e alread y and no actio n mad e it false ]
Notic e the use of " <=> " here . It says that the predicat e will be true after the actio n if it is mad e
true or if it stay s true ; and that it will be fals e afterward s in othe r cases . In the case of Holding,
the axio m is the following :
V a, x, s Holding(x,  Result(a,  s))
V[(a = Grab  A Present(x,  s) A Portable(x))
(Holding(x,  s) A a ^Release)}
This axio m is calle d a successor­stat e axiom . One such axio m is neede d for each predicat e that
may chang e its valu e over time . A successor­stat e axio m mus t list alLth e way s in whic h the
predicat e can becom e true, and all the way s in whic h it can becom e false .
Keepin g trac k of locatio n
In the wumpu s world , locatio n is probabl y the mos t importan t thing to worr y about ; it canno t be
perceive d directly , but the agen t need s to remembe r wher e it has been and wha t it saw there in
order to deduc e wher e pits and wumpuse s are and to mak e sure it does a complet e exploratio n
for the gold . We have alread y seen that the initia l locatio n can be describe d by the sentenc e
At(Agent,  [1,1] , So). The agen t also need s to know the following :
• Wha t directio n it is facin g (an angl e in degrees , wher e 0 degree s is out alon g the X­axis ,
90 degree s is up alon g the Y­axis , and so on):
Orientation(Agent,  So) =  0
• How location s are arrange d (a simpl e "map") ­ The map consist s of value s of the functio n
LocalionToward,  whic h take s a locatio n and a directio n and give s the locatio n that is one
10 The nam e derive s from film animation , wher e the backgroun d imag e usuall y remain s constan t as the character s mov e
aroun d from fram e to frame .
Sectio n 7.6. Representin g Chang e in the Worl d 207
THE FRAM E PROBLE M AND ITS RELATIVE S
Man y AI texts refer to somethin g calle d the fram e problem . The proble m was notice d
soon afte r situatio n calculu s was applie d to reasonin g abou t actions . Originally , it
centere d on the apparentl y unavoidabl e need for a large numbe r of fram e axiom s that
made for a very inelegan t and inefficien t descriptio n of actions . Man y researcher s
considere d the proble m insolubl e withi n first­orde r logic . Ther e are at leas t thre e
entire volume s of collecte d paper s on the problem , and at leas t one autho r (Crockett,
1994) cites the proble m as one sympto m of the inevitabl e failur e of the AI enterprise .
The proliferatio n of fram e axiom s is now calle d the representationa l fram e
problem , and as we have shown , is easil y solve d usin g successor­stat e axioms . The
inferentia l fram e proble m concern s the way we mak e inference s abou t the world .
Whe n reasonin g abou t the resul t of a long sequenc e of action s in situatio n calculus ,
one has to carr y each propert y throug h all intervenin g situation s one step at a time ,
even if the property  remains  unchanged  throughout.  Thi s is true whethe r one uses
fram e axiom s or successor­stat e axioms . On e woul d like to be able to mak e only
the change s require d by the actio n descriptions , and have the rest availabl e withou t
furthe r effort . Thi s seem s like the natura l and efficien t thin g to do becaus e we do
not expec t mor e than a smal l fractio n of the worl d to chang e at any give n moment .
Of course , we canno t expec t a general­purpose  representatio n languag e such as first ­
order logic (and associate d general­purpos e reasonin g systems ) to have this bias. It is
possible , however , to build special­purpose  reasonin g system s that do work efficientl y
for reasonin g abou t action s that chang e only a smal l fractio n of the world . Thes e are
calle d plannin g systems , and are the subjec t of Part IV.
Other problem s beside s the fram e proble m have surface d in the stud y of reasonin g
abou t actions . The qualificatio n proble m arise s becaus e it is difficult , in the real
world , to defin e the circumstance s unde r whic h a give n actio n is guaranteed  to work .
In the real world , grabbin g a gold brick may not work if the brick is wet and slippery ,
or if it is electrifie d or screwe d to the table , or if your back give s out whe n you bend
over, or if the guar d shoot s you, and so on. If some of these condition s are left out of
the successor­stat e axiom , then the agen t is in dange r of generatin g false beliefs .
Finally , the ramificatio n proble m concern s the proliferatio n of implicit  conse ­
quence s of actions . For example , if a gold bric k is covere d in dust , then whe n the
agent pick s up the brick it also pick s up each particl e of dust that adhere s to the brick .
One prefer s to avoi d describin g the motio n of the dust (if any) as part of the descriptio n
of pickin g up thing s in general . It is bette r to state separatel y that the dust is stuc k to
the brick , and to infer  the new locatio n of the dust as necessary . Doin g this efficientl y
require s a special­purpos e reasonin g system , just as with the fram e problem .
208 Chapte r 7. First­Orde r Logi c
step forwar d in the give n direction .
\/x,\  LocationToward([x,y],Q)  = [x+ l,y]
V.r,y LocationToward([x,y],9Q)  = [x,y+  1]
V x, y LocationToward([x,  y], 180) = [x ­ 1, y]
Vx,y  LocationToward([x,y],27Q)  = [x,y  — 1]
From the map , it is possibl e to tell whic h squar e is directl y ahea d of the agent , or indee d
of any agen t p at any locatio n /:
Vp,l,s  At(p,l,s)  => •
LocationAhead(p,  s) =LocationToward(l,  Orientation(p,  s))
It is also usefu l to defin e adjacency :
V/i,/ ? Adjacent(l\,l­i)  o­ 3rf l\=LocationToward(l2,d)
• Whateve r is know n abou t the content s of the location s (geographica l detail s on the map) .
In wha t follow s we assum e that the location s surroundin g the 4x4 cave contai n walls , and
other location s do not. Sometime s the agen t know s this kind of informatio n to start , 'and
sometime s it does not.
Mx,y  Wall([x,y\)  O (x = 0 V* = 5 Vy = 0 Vy = 5)
• Wha t the action s do to location . Onl y goin g forwar d change s location , and then only if
there is no wall ahead . The successor­stat e axio m for locatio n is
Va,d,p,s  At(p,l,Result(a,s))  <^ >
[ (a = Fonvard  A / = LocationAhead(p,  s)f\­*  Wall(l))
V (At(p,  I,s) A a^Fonvard)  ]
• Wha t the action s do to orientation . Turnin g is the only actio n that change s orientation . The
successor­stat e axio m for orientatio n is
\/a,d,p,s  Onentation(p,Result(a,s))=d  <= >
[ (a = Turn(Right)  A d = Mod(Orientation(p,  s) ­ 90,360) )
V (a = Turn(Left)  f\d = Mod(Orientation(p,  s) + 90,360) )
V (Onentation(p,  s) = d A ­i(a = Tum(Right)  f\a = Turn(Left)))  ]
In additio n to keepin g track of locatio n and the gold , the agen t shoul d also keep track of whethe r
the wumpu s is alive or dead . We leave the proble m of describin g Shoot  as an exercise .
7.7 DEDUCIN G HIDDE N PROPERTIE S OF THE WORL D _______ .
Once the agen t know s wher e it is, it can associat e qualitie s with the places  rathe r than just the
situations , so, for example , one migh t say that if the agen t is at a plac e and perceives  a breeze ,
then that plac e is breezy , and if the agen t perceive s a stench , then that plac e is smelly :
V /, s At(Agent,  I, s) A Breeze(s)  =>• Breezy(l)
V /, s At(Agent,  /, s) A Stenches)  => Smelly(l)
Sectio n 7.7. Deducin g Hidde n Propertie s of the Worl d 209
SYNCHRONI C
CAUSA L RULE S
MODEL­BASE D
REASONING
DIAGNOSTI C RULE SIt is usefu l to know if a place  is breez y or smell y becaus e we know that the wumpu s and the pits
canno t mov e about . Notic e that neithe r Breezy  nor Smelly  need s a situatio n argument .
Havin g discovere d whic h place s are breez y or smell y (and , very importantly , not smell y or
not breezy) , the agen t can deduc e wher e the pits are, and wher e the wumpu s is. Furthermore , it
can deduc e whic h square s are safe to mov e to (we use the predicat e OK to represen t this) , and
can use this informatio n to hun t for the gold .
The axiom s we will writ e to captur e the necessar y informatio n for these deduction s are
called synchroni c ("sam e time" ) rules , becaus e they relat e propertie s of a worl d state to othe r
propertie s of the same worl d state . Ther e are two main kind s of synchroni c rules :
<C> Causa l rules :
Causa l rule s reflec t the assume d directio n of causalit y in the world : som e hidde n propert y
of the worl d cause s certai n percept s to be generated . For example , we migh t have rule s
statin g that square s adjacen t to wumpuse s are smell y and square s adjacen t to pits are
breezy :
V/i,/2, 5 At(Wumpus,l },s)f\Adjacent(li,l 2) => • Smelly(l 2)
V/i,/2,, s At(Pit,li,s)/\Adjacent(li,l 2) =>  Breezy(l 2)
System s that reaso n with causa l rule s are calle d model­base d reasonin g systems .
0 Diagnosti c rules :
Diagnosti c rules infe r the presenc e of hidde n propertie s directl y from the percept­derive d
information . We have alread y seen two diagnosti c rules :
V /, s At(Agent,  I, s) A Breeze(s)  => Breezy(l)
V/,5 At(Agent,l,s)/\Stench(s)  => Smelly(l)
For deducin g the presenc e of wumpuses , a diagnosti c rule can only draw a wea k conclusion ,
namely , that if a locatio n is smelly , then the wumpu s mus t eithe r be in that locatio n or in an
adjacen t location :
V/i, 5 Smelly(li)  =>
(3 /2 At(Wumpus,  h, s) A (12 = l\ V Adjacent(h,  /2))
Althoug h diagnosti c rules seem to provid e the desire d informatio n more directly , it is very trick y
to ensur e that they deriv e the stronges t possibl e conclusion s from the availabl e information . For
example , the absenc e of stenc h or breez e implie s that adjacen t square s are OK:
V.x,y, g, u, c,s Percept([None,None,g,u,c],  ?)A
At(Agent,  x, s) A Adjacent(x,  y) => OK(y)
But sometime s a squar e can be OK even whe n smell s and breeze s abound . The model­base d rule
\/x,t (­^At(Wumpus,x,t)f\^Pit(x))  O  OK(x)
is probabl y the best way to represen t safety .
The distinctio n betwee n model­base d and diagnosti c reasonin g is importan t in man y area s
of AI. Medica l diagnosi s in particula r has been an activ e area of research , wher e approache s base d
on direc t association s betwee n symptom s and disease s (a diagnosti c approach ) have graduall y
been replace d by approache s usin g an explici t mode l of the diseas e proces s and how it manifest s
itself in symptoms . The issue s com e up agai n in Chapte r 14.
210 Chapte r 7. First­Orde r Logi c
The importan t thin g to remembe r is that if the axioms  correctly  and completely  describe  the
way the world  works  and the way  that percepts  are produced,  then  the inference  procedure  will
correctly  infer  the strongest  possible  description  of the world  state  given  the available  percepts.
A complet e specificatio n of the wumpu s worl d axiom s is left as an exercise .
7.8 PREFERENCE S AMON G ACTION S
ACTION­VALU ESo far, the only way we have to decid e on action s is to writ e rules recommendin g them on the
basis of certai n condition s in the world . This can get very tedious . For example , generall y it is a
good idea to explor e by movin g to OK squares , but not whe n there is a glitte r afoot . Henc e our
rules for explorin g woul d also have to mentio n glitter . This seem s arbitrar y and mean s the rules
are not modular : changes  in the agent's  beliefs about  some  aspects of  the world  would, require
changes  in rules  dealing  with other  aspects  also.  It is mor e modula r to separat e fact s abou t
action s from facts abou t goals , whic h mean s our agen t can be reprogramme d simpl y by askin g it
to achiev e somethin g different . Goal s describ e the desirabilit y of outcom e states , regardles s of
how achieved . We discus s goal s furthe r in Sectio n 7.9.
A first step is to describ e the desirabilit y of action s themselves , and leav e the inferenc e
engin e to choos e whichever  is the actio n that has the highes t desirability . We will use a simpl e
scale : action s can be Great,  Good,  Medium,  Risky,  or Deadly.  The agen t shoul d alway s do a
great actio n if it can find one; otherwise , a good one; otherwise , an OK action ; and a risky one if
all else fails .
Ma,s  Great(a,s)  ­^ Action(a,s)
\/a,s  Good(a,  s~) A (­G b Great(b,s))  => Action(a,s)
V a, s Medium(a,  s) A (­d b Great(b,  s) V Good(b,  s)) => Action(a,  s)
\/a,s  Risky(a,  s) A (­.3 b Great(b,  s) V Good(b,  s) V OK(b,  s)) => Action(a,s)
A syste m containin g rules of this type is calle d an action­valu e system . Notic e that the rules do
not refer to wha t the action s actuall y do, just how desirabl e they are.
Up to the poin t wher e it find s the gold , the basi c strateg y for our agen t will be as follows :
• Grea t action s includ e pickin g up the gold whe n foun d and climbin g out of the cave with
the gold .
• Goo d action s includ e movin g to a square  that' s OK and has not yet been visited .
• Mediu m action s includ e movin g to a square  that' s OK and has been visite d already .
• Risk y action s includ e movin g to a squar e that' s not know n to be deadly , but is not know n
to be OK either .
• Deadl y action s are movin g into a square  that is know n to contai n a pit or a live wumpus .
Again , we leav e the specificatio n of the actio n value s as an exercise .
Sectio n 7.9. Towar d a Goal­Base d Agen t 21
7 Q_ TOWAR D A GOAL­BASE D AGEN T
The precedin g set of actio n valu e statement s is sufficien t to prescrib e a reasonabl y intelligen t
exploratio n policy . It can be show, n (Exercis e 7.15 ) that the agen t usin g these axiom s will alway s
succee d in findin g the gold safel y wheneve r there is a safe sequenc e of action s that does so. This
is abou t as muc h as we can ask from a logica l agent .
Once the gold is found , the policie s need to chang e radically . The aim now is to retur n to
the start squar e as quickl y as possible . Wha t we woul d like to do is infe r that the agen t now has
the goal of bein g at locatio n [1,1] :
V.5 Holding(Gold,s)  => GoalLocation([\,\],s)
The presenc e of an explici t goal allow s the agen t to wor k out a sequenc e of action s that will
achiev e the goal . Ther e are at leas t three way s to find such a sequence :
0 Inference : It is not hard to writ e axiom s that will allow us to ASK the KB for a sequenc e
of action s that is guarantee d to achiev e the goal safely . For the 4x4 wumpu s world , this
is feasible , but for large r worlds , the computationa l demand s are too high . In any case , we
have the proble m of distinguishin g good solution s from wastefu l solution s (e.g. , ones that
make a long serie s of wanderin g move s befor e gettin g on the righ t track) .
0 Search : We can use a best­firs t searc h procedur e (see Chapte r 4) to find a path to the goal .
This require s the agen t to translat e its knowledg e into a set of operators , and accompanyin g
state representation , so that the searc h algorith m can be applied .
0 Planning : This involve s the use of special­purpos e reasonin g system s designe d to reaso n
about actions . Chapte r 11 describe s thes e system s in detail , explainin g their advantage s
over searc h algorithms .
HO SUMMAR Y
This chapte r has show n how first­orde r logic can be used as the representatio n languag e for a
knowledge­base d agent . The importan t point s are as follows :
• First­orde r logic is a general­purpos e representatio n languag e that is base d on an onto ­
logica l commitmen t to the existenc e of object s and relation s in the world .
• Constan t symbol s and predicat e symbol s nam e object s and relations , respectively . Com ­
plex term s nam e object s usin g functio n symbols . The interpretatio n specifie s wha t the
symbol s refe r to.
• An atomi c sentenc e consist s of a predicat e applie d to one or mor e terms ; it is true just
when the relatio n name d by the predicat e hold s betwee n the object s name d by the terms .
Comple x sentence s use connective s just like propositiona l logic , and quantifie d sentence s
allow the expressio n of genera l rules .
• It is possibl e to defin e an agen t that reasons  usin g first­orde r logic . Suc h an agen t need s to
212 Chapte r 7. First­Orde r Logi c
1. reac t to wha t it perceives ;
2. extrac t abstrac t description s of the curren t state from percepts ;
3. maintai n an interna l mode l of relevan t aspect s of the worl d that are not directl y
availabl e from percepts ;
4. expres s and use informatio n abou t the desirabilit y of action s in variou s circumstances ;
5. use goal s in conjunctio n with knowledg e abou t action s to construc t plans .
Knowledg e abou t action s and thei r effect s can be represente d usin g the convention s of
situatio n calculus . Thi s knowledg e enable s the agen t to keep track of the worl d and to
deduc e the effect s of plan s of action .
We have a choic e of writin g diagnosti c rule s that reaso n from percept s to proposition s
abou t the worl d or causa l rules that describ e how condition s in the worl d caus e percept s to
come about . Causa l rules are often more flexibl e and entai l a wide r rang e of consequences ,
but can be more expensiv e to use in inference .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
Althoug h even Aristotle' s logic deals with generalization s over objects , true first­orde r logic dates
from the introductio n of quantifier s in Gottlo b Frege' s (1879 ) Begriffschrift  ("Concep t Writing "
or "Conceptua l Notation") . Frege' s abilit y to nest quantifier s was a big step forward , but he used
an awkwar d notation . (An exampl e appear s on the fron t cove r of this book. ) The presen t notatio n
for first­orde r logi c is substantiall y due to Giusepp e Pean o (1889) , but the semantic s is virtuall y
identica l to Frege's .
A majo r barrie r to the developmen t of first­orde r logic had been the concentratio n on one­
place predicate s to the exclusio n of many­plac e relationa l predicates . This fixatio n on one­plac e
predicate s had been nearl y universa l in logica l system s from Aristotl e up to and includin g Boole .
The first systemati c treatmen t of the logic of relation s was give n by Augustu s De Morga n (1864) .
De Morga n cited the followin g exampl e to show the sort s of inference s that Aristotle' s logi c
could not handle : "All horse s are animals ; therefore , the head of a hors e is the head of an animal. "
This inferenc e is inaccessibl e to Aristotl e becaus e any valid rule that can suppor t this inferenc e
must first analyz e the sentenc e usin g the two­plac e predicat e "x is the head of y." The logi c of
relation s was studie d in dept h by Charle s Sander s Peirc e (1870) , who also develope d first­orde r
logic independentl y of Frege , althoug h slightl y later (Peirce , 1883) .
Leopol d Lowenhei m (1915 ) gave a systemati c treatmen t of mode l theor y in 1915 . Thi s
paper also treate d the equalit y symbo l as an integra l part of logic . Lowenheim' s result s were
furthe r extende d by Thoral f Skole m (1920) . Tarsk i (1935 ) gave an explici t definitio n of truth and
model­theoreti c satisfactio n in first­orde r logic , usin g set theory . (An Englis h translatio n of this
Germa n articl e is give n in (Tarski , 1956). )
McCarth y (1958 ) was primaril y responsibl e for the introductio n of first­orde r logic as a tool
for buildin g AI systems , and later (1963 ) propose d the use of state s of the world , or situations ,
as object s to be reasone d abou t usin g first­orde r logic . The first AI syste m to mak e substantia l
use of general­purpos e reasonin g abou t action s in first­orde r logi c was QA3 (Green , 1969b) .
Sectio n 7.10 . Summar y 213
Kowalsk i (1979b ) furthe r advance d situatio n calculu s by introducin g proposition s as objects .
The fram e proble m was pointe d out as a majo r proble m for the use of logic in AI by McCarth y
and Haye s (1969) . The axiomatizatio n we use in the chapte r to avoi d the representationa l fram e
proble m was propose d by Charle s Elka n (1992 ) and independentl y by Ray Reite r (1991) . The
qualificatio n proble m was also pointe d out by McCarth y (1977) . The inferentia l fram e proble m
is discusse d at lengt h by Shoha m and McDermot t (1988) .
There are a numbe r of good moder n introductor y texts on first­orde r logic . Quin e (1982 )
is one of the mos t readable . Enderto n (1972 ) give s a more mathematicall y oriente d perspective .
A highl y forma l treatmen t of first­orde r logic , alon g with man y mor e advance d topic s in logic , is
provide d by Bell and Machove r (1977) . Mann a and Waldinge r (1985 ) give a readabl e introductio n
to logi c from a compute r scienc e perspective . Gallie r (1986 ) provide s an extremel y rigorou s
mathematica l expositio n of first­orde r logic , alon g with a grea t deal of materia l on its use in
automate d reasoning . Logical  Foundations  of Artificial  Intelligence  (Geneseret h and Nilsson ,
1987) provide s both a solid introductio n to logic and the first systemati c treatmen t of logica l
agent s with percept s and actions . Barwis e and Etchemend y (1993 ) give a moder n overview  of
logic that include s an interactiv e graphica l logic gam e calle d Tarski's  World.
EXERCISE S
7.1 A logica l knowledg e base represent s the worl d usin g a set of sentence s with no explici t
structure . An analogica l representation , on the othe r hand , is one in whic h the representatio n has
structur e that correspond s directl y to the structur e of the thing represented . Conside r a road map
of your countr y as an analogica l representatio n of facts abou t the country . The two­dimensiona l
structur e of the map correspond s to the two­dimensiona l surfac e of the area .
a. Giv e five example s of symbols  in the map language .
b. An explicit  sentenc e is one that the creato r of the representatio n actuall y write s down . An
implicit  sentenc e is one that result s from explici t sentence s becaus e of propertie s of the
analogica l representation . Giv e three example s each of implicit  and explicit  sentence s in
the map language .
c. Giv e thre e example s of facts abou t the physica l structur e of your countr y that canno t be
represente d in the map language .
d. Giv e two example s of facts that are muc h easie r to expres s in the map languag e than in
first­orde r logic ,
e. Giv e two othe r example s of usefu l analogica l representations . Wha t are the advantage s
and disadvantage s of each of these languages ?
7.2 Represen t the followin g sentence s in first­orde r logic , usin g a consisten t vocabular y (whic h
you mus t define) :
a. Not all student s take both Histor y and Biology .
b. Onl y one studen t faile d History .
214 Chapte r 7. First­Orde r Logi c
c. Onl y one studen t faile d both Histor y and Biology .
d. The best scor e in Histor y was bette r than the best scor e in Biology .
e. Ever y perso n who dislike s all vegetarian s is smart .
f. No perso n likes a smar t vegetarian .
g. Ther e is a woma n who likes all men who are not vegetarians .
h. Ther e is a barbe r who shave s all men in town who do not shav e themselves .
i. No perso n likes a professo r unles s the professo r is smart .
j. Politician s can fool som e of the peopl e all of the time , and they can fool all of the peopl e
some of the time , but they can' t fool all of the peopl e all of the time .
7.3 W e note d that ther e is ofte n confusio n becaus e the =>• connectiv e does not correspon d
directl y to the Englis h "if ... then " construction . The followin g Englis h sentence s use "and, "
"or," and "if" in way s that are quit e differen t from first­orde r logic . For each sentence , give
both a translatio n into first­orde r logi c that preserve s the intende d meanin g in English , and
a straightforwar d translatio n (as if the logica l connective s had thei r regula r first­orde r logi c
meaning) . Sho w an unintuitiv e consequenc e of the latte r translation , and say whethe r each
translatio n is valid , satisfiabl e or invalid .
a. One mor e outburs t like that and you'l l be in contemp t of court .
b. Annie  Hall  is on TV tonight , if you'r e interested .
c. Eithe r the Red Sox win or I'm out ten dollars .
d. The specia l this mornin g is ham and eggs .
e. Mayb e I'll com e to the party and mayb e I won't .
f. Well , I like Sand y and I don' t like Sandy .
g. I don' t jump off the Empir e State Buildin g implie s if I jum p off the Empir e State Buildin g
then I floa t safel y to the ground .
h. It is not the case that if you attemp t this exercis e you will get an F. Therefore , you will
attemp t this exercise .
i. If you lived here you woul d be hom e now . If you were hom e now , you woul d not be here.
Therefore , if you lived here you woul d not be here .
7.4 Giv e a predicat e calculu s sentenc e such that ever y worl d in whic h it is true contain s exactl y
one object .
7.5 Represen t the sentenc e "All German s spea k the same languages " in predicat e calculus . Use
Speaks(x,  /), meanin g that perso n x speak s languag e /.
7.6 Writ e axiom s describin g the predicate s Grandchild  GreatGrandparent,  Brother,  Sister,
Daughter,  Son,  Aunt,  Uncle,  BrotherlnLaw, SisterlnLaw,  and FirstCousin.  Fin d out the prope r
definitio n of mth cousi n n time s removed , and writ e it in first­orde r logic .
Writ e dow n the basi c facts depicte d in the famil y tree in Figur e 7.4. Usin g the logica l rea­
sonin g syste m in the code repository , TEL L it all the sentence s you have writte n down , and ASK it
who are Elizabeth' s grandchildren , Diana' s brothers­in­law , and Zara' s great­grandparents .
Sectio n 7.10 . Summar y 215
Georg e =  Mum
Spence r = Kydd Elizabet h = Phili p Margare t
Diana = Charle s Ann e =  Mark Andre w =  Sara h Edwar d
Willia m Harr y Pete r Zar a Beatric e Eugeni e
Figur e 7.4 A  typica l famil y tree.
7.7 Explai n wha t is wron g wit h the followin g propose d definitio n of the set membershi p
predicat e G :
\/x,s x£  {x\s}
\/x,s xes  => Vy x<E{y\s}
7.8 Usin g the set axiom s as examples , writ e axiom s for the list domain , includin g all the
constants , functions , and predicate s mentione d in the chapter .
7.9 Thi s exercis e can be done withou t the computer , althoug h you may find it usefu l to use a
backwar d chaine r to chec k your proo f for the last part. The idea is to formaliz e the block s worl d
domai n usin g the situatio n calculus . The object s in this domai n are blocks , tables , and situations .
The predicate s are
On(x,y,s)  ClearTop(x,s)  Block(x)  Table(x)
The only actio n is PutOn(x,y),  wher e x mus t be a bloc k whos e top is clea r of any othe r blocks ,
and y can be eithe r the table or a differen t bloc k with a clea r top. The initia l situatio n So has A
on B on C on the table .
a. Writ e an axio m or axiom s describin g PutOn.
b. Describ e the initia l state , SQ, in whic h there is a stack of three blocks , A on B on C, wher e
C is on the table , T.
c. Giv e the appropriat e quer y that a theore m prove r can solv e to generat e a plan to buil d a
stack wher e C is on top of B and B is on top of A. Writ e dow n the solutio n that the theore m
prove r shoul d return . (Hint:  The solutio n will be a situatio n describe d as the resul t of
doing some action s to SQ.)
d. Sho w formall y that the solutio n fact follow s from your descriptio n of the situatio n and the
axiom s for Put On.
7.10 Writ e sentence s to defin e the effect s of the Shoot  actio n in the wumpu s world . As well as
describin g its effect s on the wumpus , remembe r that shootin g uses the agent' s arrow .
7.11 In this exercise , we will conside r the proble m of plannin g a rout e from one city to another .
The basi c actio n take n by the robo t is Go(x,  y), whic h take s it from city x to city y provide d there
216 Chapte r 7. First­Orde r Logi c
is a direc t route . DirectRoute(x,  >•) is true if and only if there is a direc t rout e from x to y\ you can
assum e that all such fact s are alread y in the KB (see the map on page 62). The robo t begin s in
Arad and mus t reach Bucharest .
a. Writ e a suitabl e logica l descriptio n of the initia l situatio n of the robot .
b. Writ e a suitabl e logica l queV y whos e solution s will provid e possibl e path s to the goal .
c. Writ e a sentenc e describin g the Go action .
d. Now suppos e that followin g the direc t rout e betwee n two citie s consume s an amoun t of
fuel equa l to the distanc e betwee n the cities . The robo t start s with fuel at full capacity .
Augmen t you r representatio n to includ e thes e considerations . You r actio n descriptio n
shoul d be such that the quer y you specifie d earlie r will still resul t in feasibl e plans .
e. Describ e the initia l situation , and writ e a new rule or rules describin g the Go action .
f. Now suppos e some of the vertice s are also gas stations , at whic h the robo t can fill its tank
using the Fillup  action . Exten d your representatio n to includ e gas station s and writ e all the
rules neede d to completel y describ e the Fillup  action .
7.12 In this exercise , you will exten d situatio n calculu s to allo w for action s that take plac e
simultaneously . Yo u will use a functio n calle d Simultaneously,  whic h take s two action s as
argument s and denote s the combine d action . Conside r a grid worl d containin g two agents . Writ e
axiom s describin g the effect s of simultaneou s Forward  actions :
a. Whe n two agent s mov e at once , unles s they are both tryin g to mov e to the same location ,
the resul t is the same as if one had move d and then the othe r had moved .
b. If the agent s are tryin g to mov e to the same location , they remai n in place .
7.13 Usin g the wumpu s worl d simulato r and the logica l reasonin g syste m in the code repository ,
implemen t a workin g agen t for the wumpu s world . You will need all of the wumpus­relate d
axiom s in the chapter , and perhap s more besides . Evaluat e the performanc e of your agent .
7.14 Ho w hard woul d it be to buil d a successfu l wumpu s worl d agen t by writin g a progra m in
your favorit e programmin g language ? Compar e this to the logica l reasonin g agent .
7.15 Sketc h an argumen t to the effec t that a logica l agen t usin g the axiom s and actio n preference s
given in the chapte r will alway s succee d in findin g the gold safel y wheneve r ther e is a safe
sequenc e of action s that does so.
7.16 A refle x agen t is one whos e actio n is alway s a functio n of its percept s in the curren t time
step. Tha t is, the agent' s actio n canno t be base d on anythin g it learne d in the past , and it canno t
carry over any interna l state informatio n from one time step to the next . In the wumpu s world ,
there are 32 differen t possibl e percept s and 6 differen t actions .
a. How man y differen t refle x agent s can ther e be in the wumpu s world ?
b. How man y differen t 4 x 4 wumpu s worl d are there ? How man y 10x1 0 worlds ?
c. Wha t do you thin k the chance s are that a refle x agen t can be successfu l in a majorit y of
wumpu s worlds ? Why ?
8BUILDIN G A
KNOWLEDG E BAS E
In which  we develop  a methodology  for building  knowledge  bases  for particular
domains,  sketch  a representation  for the world  in general,  and go shopping.
KNOWLEDG E
ENGINEERIN G
KNOWLEDG E
ACQUISITIO N
ONTOLOGICA L
ENGINEERIN GThe previou s chapte r showe d that first­orde r logic is a powerfu l tool for knowledg e representatio n
and reasoning . However , a logic by itsel f consist s of only the syntax , semantics , and proo f theory .
A logi c does not offe r any guidanc e as to wha t facts shoul d be expressed , nor wha t vocabular y
shoul d be used to expres s them .
The proces s of buildin g a knowledg e base is calle d knowledg e engineering . A knowledg e
enginee r is someon e who investigate s a particula r domain , determine s wha t concept s are importan t
in that domain , and create s a forma l representatio n of the object s and relation s in the domain .
Often , the knowledg e enginee r is traine d in representatio n but is not an exper t in the domai n
at hand , be it circui t design , spac e statio n missio n scheduling , or whatever . The knowledg e
enginee r will usuall y intervie w the real expert s to becom e educate d abou t the domai n and to
elicit the require d knowledge , in a proces s calle d knowledg e acquisition . This occur s prio r to,
or interleave d with , the proces s of creatin g forma l representations . In this chapter , we will use
domain s that shoul d alread y be fairl y familiar , so that we can concentrat e on the representationa l
issue s involved .
One does not becom e a proficien t knowledg e enginee r just by studyin g the synta x and
semantic s of a representatio n language . It take s practic e and exposur e to lots of example s befor e
one can develo p a good style in any language , be it a languag e for programming , reasoning , or
communicating . Section s 8.1 and 8.2 discus s the principle s and pitfall s of knowledg e engineering .
We then show how to represen t knowledg e in the fairl y narro w domai n of electroni c circuit s in
Sectio n 8.3. A numbe r of narro w domain s can be tackle d by simila r techniques , but domain s
such as shoppin g in a supermarke t seem to requir e muc h mor e genera l representations . In
Sectio n 8.4, we discus s way s to represen t time , change , objects , substances , events , actions ,
money , measures , and so on. Thes e are importan t becaus e they show up in one form or anothe r
in ever y domain . Representin g thes e very genera l concept s is sometime s calle d ontologica l
engineering . Sectio n 8.5 describe s in detai l a simplifie d shoppin g environment , and uses the
genera l ontolog y to develo p representation s capabl e of sustainin g rationa l actio n in the domain .
217
218 Chapte r Buildin g a Knowledg e Base
8.1 PROPERTIE S OF GOO D AND BAD KNOWLEDG E BASE S
In Chapte r 6, we said that a good knowledg e representatio n languag e shoul d be expressive ,
concise , unambiguous , context­insensitive , and effective . A knowledg e base should , in addition ,
be clear  and correct.  Th e relation s that matte r shoul d be denned , and the irrelevan t detail s
shoul d be suppressed . Of course , ther e will be trade­off s betwee n properties : we can mak e
simplification s that sacrific e som e correctnes s to gain clarit y and brevity .
The questio n of efficienc y is a little mor e difficul t to deal with . Ideally , the separatio n
betwee n the knowledg e base and the inferenc e procedur e shoul d be maintained . Thi s allow s
the creato r of the knowledg e base to worr y only abou t the content  of the knowledge , and not
abou t how it will be used by the inferenc e procedure . The same answer s shoul d be obtainabl e by
the inferenc e procedure , no matte r how the knowledg e is encoded . As far as possible , ensurin g
efficien t inferenc e is the task of the designe r of the inferenc e procedur e and shoul d not distor t the
representation .
In practice , som e consideration s of efficienc y are unavoidable . Automati c method s exis t
that can eliminat e the most obviou s source s of inefficienc y in a give n encoding , in muc h the same
way that optimizin g compiler s can spee d up the executio n of a program , but at presen t these
method s are too wea k to overcom e the determine d effort s of a profligat e knowledg e enginee r
who has no concer n for efficiency . Eve n in the best case , then , the knowledg e enginee r shoul d
have som e understandin g of how inferenc e is done , so that the representatio n can be designe d for
maximu m efficiency . In the wors t case, the representatio n languag e is used primaril y as a way
of "programming " the inferenc e procedure .
As we will see throughou t this chapter , you cannot  do, or understand,  knowledge  engi­
neering  by just talking  about  it. To explai n the genera l principle s of good design , we need to
have an example . We will start by doin g the exampl e incorrectly , and then fix it.
Every knowledg e base has two potentia l consumers : huma n reader s and inferenc e proce ­
dures . A commo n mistak e is to choos e predicat e name s that are meaningfu l to the huma n reader ,
and then be lulle d into assumin g that the nam e is someho w meaningfu l to the inferenc e procedur e
as well . The sentenc e BearOJVerySmallBrain(Pooh)  migh t be appropriat e in certai n domains,1
but from this sentenc e alone , the inferenc e procedur e will not be able to infer eithe r that Pooh is a
bear or that he has a very smal l brain ; that he has a brain at all; that very smal l brain s are smalle r
than smal l brains ; or that this fact implie s somethin g abou t Pooh' s behavior . The hard part is for
the huma n reade r to resis t the temptatio n to mak e the inference s that seem to be implie d by long
predicat e names . A knowledg e enginee r will often notic e this kind of mistak e whe n the inferenc e
procedur e fails to conclude , for example , Silly(Pooh).  It is compoundin g the mistak e to writ e
Mb BearOfVerySmallBrain(b)  => Silly(b)
becaus e this expresse s the relevan t knowledg e at too specific  a level . Althoug h such VeryLong­
Names  can be mad e to wor k for simpl e example s coverin g a small , spars e portio n of a large r
domain , they do not scal e up well . Addin g AnotherVeryLongName  take s just as muc h wor k as
1 Winni e the Pooh is a toy bear belongin g to Christophe r Robi n in the well­know n serie s of children's ' book s (Milne ,
1926) . The style of our introductor y sentenc e in each chapte r is borrowe d from thes e works .
Section . Propertie s of Goo d and Bad Knowledg e Base s 219
KNOWLEDG E ENGINEERIN G vs. PROGRAMMIN G
A usefu l analog y can be mad e betwee n knowledg e engineerin g and programming .
Both activitie s can be seen as consistin g of four steps :
Knowledge  Engineering  Programming
(1) Choosin g a logi c Choosin g a programmin g languag e
(2) Buildin g a knowledg e base Writin g a progra m
(3) Implementin g the proo f theor y Choosin g or writin g a compile r
(4) Inferrin g new facts Runnin g a progra m
In both activities , one write s dow n a descriptio n of a proble m or state of affairs ,
and then uses the definitio n of the languag e to deriv e new consequences . In the
case of a program , the outpu t is derive d from the inpu t and the program ; in the case
of a knowledg e base , answer s are derive d from description s of problem s and the
knowledg e base .
Give n thes e similarities , wha t is the poin t of doin g "knowledg e engineering " at
all? Wh y not just admi t that the fina l resul t will be a program , and set abou t to writ e
that progra m from the start , usin g a traditiona l programmin g language ?
The mai n advantag e of knowledg e engineerin g is that it require s less commit ­
ment , and thus less work . A knowledg e enginee r only has to decid e wha t object s
and relation s are wort h representing , and whic h relation s hold amon g whic h objects .
A programme r has to do all that , and in additio n mus t decid e how to comput e the
relation s betwee n objects , give n som e initia l input . The knowledg e enginee r specifie s
what  is true, and the inferenc e procedur e figure s out how  to turn the facts into a solu ­
tion to the problem . Furthermore , becaus e a fact is true regardles s of wha t task one is
tryin g to solve , knowledg e base s can, in principle , be reuse d for a variet y of differen t
tasks withou t modification . Finally , debuggin g a knowledg e base is mad e easie r by
the fact that any give n sentenc e is true or false by itself,  wherea s the correctnes s of a
progra m statemen t depend s very strongl y on its context .
The advantage s of this declarativ e approac h to syste m buildin g have not been
lost on othe r subfield s of compute r science . Databas e system s deriv e mos t of their
usefulnes s from the fact that they provid e a metho d to store and retriev e informatio n
in a way that is independen t of the particula r application . Databas e system s have
also starte d to add the capabilit y to do logica l inference , thereb y movin g mor e of
the functionalit y from the applicatio n progra m into the databas e syste m (Stonebraker ,
1992) . The field of agent­base d softwar e engineerin g (Geneseret h and Ketchpel ,
1994 ) attempt s to mak e all sorts of system s and resource s interoperabl e by providin g
a declarativ e interfac e base d on first­orde r logic .
220 Chapte r 8. Buildin g a Knowledg e Base
addin g the first one. For example , to deriv e Silly(Piglet)  from ShyBabyPigOfSmallBrain(Piglet),
we woul d have to writ e
Mb ShyBabyPigOfSmallBrain(b)  => Silly(b)
This is a sign that somethin g is wrong . The first fact abou t sillines s is of no help in a simila r
situation . In a properl y designe d knowledg e base , facts that were entere d for one situatio n shoul d
end up bein g used in new situation s as well . As you go along , you shoul d need fewe r new facts ,
and fewe r new predicates . Thi s will only happe n if one write s rule s at the mos t general  leve l
at whic h the knowledg e is applicable . In a goodJyjpwledg e base^BearOfVerySmallBmin^ooh)
woul d be replace d by somethin g like the following : "  /  "­­.­ •
1. Poo h is a bear ; bear s are animals ; animal s are physica l things .
Bear(Pooh)
Vfe Bear(b)  =>
Va Animal(a)Animal(b)
=> PhysicalThing(a)
Thes e sentence s help to tie knowledg e abou t Pooh into a broade r context . The y also enabl e
knowledg e to be expresse d at an appropriat e leve l of generality , dependin g on whethe r the
informatio n is applicabl e to bears , animals , or all physica l objects. ^
2. Poo h has a very smal l brain .
RelativeSize(BrainOf(Pooh),  BrainOf(TypicalBear))  = Very(Small)
This provide s a precis e sens e of "very small, " whic h woul d otherwis e be highl y ambiguous .
Is Pooh' s brai n very smal l compare d to a molecul e or a moon ?
3. All animal s (and only animals ) have a brain , whic h is a part of the animal .
Va Animal(a)  <£ > Brain(BrainOf(a))
Va PartOf(BminOf(a),a)
This allow s us to connec t Pooh' s brain to Pooh himself , and introduce s some useful , genera l
vocabulary .
4. If somethin g is part of a physica l thing , then it is also a physica l thing :
Mx,y  PartOf(x,y)  A PhysicalThing(y)  => PhysicalThing(x)
This is a very genera l and importan t fact that is seldo m seen in physic s textbooks !
5. Animal s with brain s that are smal l (or below ) relativ e to the norma l brai n size for their
specie s are silly.2
V a RelativeSize(BrainOf(a),  BrainOf(TypicalMember(SpeciesOf(a))))  < Small
=> Silly(a)
V b Bear(b)  <=/ • SpeciexOf(b)  = Ursidae
TypicalBear  = TypicalMember(  Ursidae)
2 It is importan t to remembe r that the goal for this knowledg e base was to be consisten t and usefu l withi n a worl d of
talkin g stuffe d animals , not to be a mode l of the real world . Althoug h biologist s are in agreemen t that brai n size is not a
good predicto r for silliness , the rule s give n here are the righ t ones for this world .
Sectio n 8.2. Knowledg e Engineerin g 221
6. Ever y physica l thin g has a size. Size s are arrange d on a scale from Tiny to Huge . A relativ e
size is a ratio of two sizes .
MX PhysicalThing(x)  =>• 3s Size(x)  = s
Tiny < Small  < Medium <  Large <  Huge
V a, b RelativeSize(a,  b) =  Size(a)/Size(b)
1. The functio n Very  map s a poin t on a scale to a more extrem e value . Medium  is the neutra l
value for a scale .
Medium  = 1
MX x>  Medium  =>• Very(x)  >x
MX x<  Medium  =? Very(x)  <x
This is mor e work than writin g a singl e rule for BearOfVerySmallBmin,  but it achieve s far more .
It has articulate d som e of the basi c propertie s of physica l thing s and animals , propertie s that will
be used man y times , but need be state d only once . It has begu n to sketc h out a hierarch y of
object s (bears , animals , physica l things) . It has also mad e a representationa l choic e for value s on
scales , whic h will come in hand y later in the chapter .
Every time one write s dow n a sentence , one shoul d ask onesel f the following :
• Wh y is this true ? Coul d I writ e dow n the facts that mak e it true instead ?
• How  generally  is it applicable ? Can I state it for a mor e genera l clas s of objects ?
• Do I need a new predicat e to denot e this class of objects ? How does the class relat e to
other classes ? Is it part of a large r class ? Doe s that class have othe r subclasses ? Wha t are
other propertie s of the object s in this class ?
We canno t provid e a foolproo f recip e for successfu l knowledg e engineering , but we hop e this
exampl e has provide d some pointers .
8.2 KNOWLEDG E ENGINEERIN G
The knowledg e enginee r mus t understan d enoug h abou t the domai n in questio n to represen t
the importan t object s and relationships . He or she mus t also understan d enoug h abou t the
representatio n languag e to correctl y encod e thes e facts . Moreover , the knowledg e enginee r
must also understan d enoug h abou t the implementatio n of the inferenc e procedur e to assur e that
querie s can be answere d in a reasonabl e amoun t of time . To help focu s the developmen t of a
knowledg e base and to integrat e the engineer' s thinkin g at the three levels , the followin g five­ste p
methodolog y can be used :
• Decide  what  to talk about.  Understan d the domai n well enoug h to know whic h object s
and fact s need to be talke d about , and whic h can be ignored . For the early example s in
this chapter , this step is easy . In som e cases , however , it can be the hardes t step . Man y
knowledg e engineerin g project s have faile d becaus e the knowledg e engineer s starte d to
formaliz e the domai n befor e understandin g it. Donal d Michi e (1982 ) give s the exampl e
of a chees e factor y that had a singl e chees e teste r who decide d if the Camember t was
222 Chapte r 8. Buildin g a Knowledg e Base
ONTOLOG Yripe by stickin g his finge r into a sampl e and decidin g if it "felt right. " Whe n the chees e
tester approache d retiremen t age, the factor y investe d muc h time and mone y developin g a
comple x syste m with steel probe s that woul d test for just the righ t surfac e tension , but the
syste m was next to useless . Eventually , it turne d out that feel had nothin g to do with it;
pushin g the finge r in just serve d to brea k the crust and let the arom a out, and that was wha t
the chees e teste r was subconsciousl y relyin g on.
Decide  on a vocabulary  of predicates,  functions,  and  constants.  Tha t is, translat e the
importan t domain­leve l concept s into logic­leve l names . Thi s involve s man y choices ,
some arbitrar y and som e important . Shoul d Size  be a functio n or a predicate ? Woul d
Bigness  be a bette r nam e than Size!  Shoul d Small  be a constan t or a predicate ? Is Small
a measur e of relativ e size or absolut e size? Onc e the choice s have been made , the resul t
is a vocabular y that is know n as the ontolog y of the domain . The word ontolog y mean s a
particula r theor y of the natur e of bein g or existence . Together , this step and the previou s
step are know n as ontologica l engineering . They determin e wha t kind s of thing s exist , but
do not determin e their specifi c propertie s and interrelationships .
Encode  general  knowledge  about the  domain.  Th e ontolog y is an informa l list of the
concept s in a domain . By writin g logica l sentence s or axiom s abou t the term s in the
ontology , we accomplis h two goals : first , we mak e the term s more precis e so that human s
will agre e on their interpretation . Withou t the axioms , we woul d not know , for example ,
whethe r Bear  refer s to real bears , stuffe d bears , or both . Second , we mak e it possibl e to
run inferenc e procedure s to automaticall y deriv e consequence s from the knowledg e base .
Once the axiom s are in place , we can say that a knowledg e base has been produced .
Of course , nobod y expect s a knowledg e base to be correc t and complet e on the first try.
There will be a considerabl e debuggin g process . The main differenc e betwee n debuggin g
a knowledg e base and debuggin g a program  is that it is easie r to look at a singl e logic
sentenc e and tell if it is correct . For example , a typica l erro r in a knowledg e base look s
like this:
MX Animal(x)  => 3b  BrainOf(x)  = b
This says that there is som e object  that is the valu e of the BrainOf  functio n applie d to an
animal . Of course , a functio n has a valu e for any input , althoug h the valu e may be an
undefine d object  for input s that are outsid e the expecte d range . So this sentenc e make s a
vacuou s claim . We can "correct " it by addin g the conjunc t Brain(b).  The n again , if we are
potentiall y dealin g with single­celle d animals , we coul d correc t it again , replacin g Animal
by, say, Vertebrate.
In contrast , a typica l error in a progra m look s like this:
offse t := positio n + 1
It is impossibl e to tell if this statemen t is correc t withou t lookin g at the rest of the progra m
to see if, for example , of f set is used elsewher e in the progra m to refe r to the position ,
or to one beyon d the position ; or whethe r the statemen t was accidentall y include d twic e in
differen t places .
Programmin g languag e statement s therefor e tend to depen d on a lot of context , wherea s
logic sentence s tend to be mor e self­contained . In that respect , a sentenc e in a knowledg e
base is more like an entir e procedur e in a program , not like an individua l statement .
Sectio n 8.3. Th e Electroni c Circuit s Domai n 223
• Encode  a description of  the specific  problem  instance.  If the ontolog y is wel l though t
out, this step will be easy . It will mostl y involv e writin g simpl e atomi c sentence s abou t
instance s of concept s that are alread y part of the ontology .
• Pose  queries  to the  inference  procedure  and get answers.  This is wher e the rewar d is: we
can let the inferenc e procedur e operat e on the axiom s and problem­specifi c facts to deriv e
the facts we are intereste d in knowing .
To understan d this five­ste p proces s better , we turn to som e example s of its use. We first conside r
the domai n of Boolea n electroni c circuits .
8.3 TH E ELECTRONI C CIRCUIT S DOMAI N
Withi n the domai n of discret e digita l electroni c circuits , we woul d like to analyz e the circui t
show n in Figur e 8.1. The circui t purport s to be a one­bi t full adder , wher e the first two input s
are the two bits to be added , and the third inpu t is a carry bit. The first outpu t is the sum , and the
secon d outpu t is a carry bit for the next adder . The goal is to provid e an analysi s that determine s
if the circui t is in fact an adder , and that can answe r question s abou t the valu e of curren t flow at
variou s point s in the circuit.3 We follo w the five­ste p proces s for knowledg e engineering .
3^iixiS 1  1IL~SC1
=£>
I A y —— — I
—— '~^°*)^ '
Figur e 8.1 A  digita l circui t C 1 , with three input s and two outputs , containin g two XOR gates ,
two AND gate s and one OR gate . The input s are bit value s to be added , and the output s are the
sum bit and the carry bit.
Decid e wha t to talk abou t
Digita l circuit s are compose d of wire s and gates . Signal s flow alon g wire s to the inpu t terminal s
of gates , and each gate produce s a signa l on the outpu t termina l that flow s alon g anothe r wire .
3 If you are intimidate d by the electronics , try to get a feel for how the knowledg e base was constructe d withou t worryin g
abou t the details .
224 Chapte r 8. Buildin g a Knowledg e Base
There are four type s of gates : AND , OR, and XOR gate s have exactl y two inpu t terminals , and
NOT gate s have one. All gate s have exactl y one outpu t terminal . Circuits , whic h are compose d
of gates , also have inpu t and outpu t terminals .
Our main purpos e is to analyz e the desig n of circuit s to see if they matc h their specification .
Thus , we need to talk abou t circuits,  their terminals,  and the signals  at the terminals . To determin e
what thes e signal s will be, we need to know abou t individua l gates,  and gate types:  AND , OR,
XOR , and NOT .
Not everythin g that is in the domai n need s to show up in the ontology . We do not need
to talk abou t the wire s themselves , or the path s the wire s take , or the junction s wher e two wire s
come together . All that matter s is the connectivit y of terminals—w e can say that one outpu t
termina l is connecte d to anothe r inpu t termina l withou t havin g to mentio n the wire that actuall y
connect s them . Ther e are man y othe r factor s of the domai n that are irrelevan t to our analysis ,
such as the size, shape , color , or cost of the variou s components .
A special­purpos e ontolog y such as this depend s not only on the domain , but also on the
task to be solved . If our purpos e were somethin g othe r than verifyin g design s at the gate level ,
the ontolog y woul d be different . For example , if we were intereste d in debuggin g fault y circuits ,
then it woul d probabl y be a good idea to includ e the wire s in the ontology , becaus e a fault y wire
can corrup t the signa l flowin g alon g it. For resolvin g timin g faults , we woul d need to includ e
gate delays . If we were intereste d in designin g a produc t that woul d be profitable , then the cost
of the circui t and its spee d relativ e to othe r product s on the marke t woul d be important .
Decid e on a vocabular y
We now know that we wan t to talk abou t circuits , terminals , signals , gates , and gate types . The
next step is to choos e functions , predicates , and constant s to nam e them . We will start from
individua l gate s and mov e up to circuits .
First, we need to be able to distinguis h a gate from othe r gates . This is handle d by namin g
gates with constants : X\,X2,  and so on. Next , we need to know the type of a gate.4 A functio n
is appropriat e for this: Type(X\  )=XOR.  'Thi s introduce s the constan t XOR  for a particula r gate
type; the othe r constant s will be calle d OR,  AND,  and NOT.  The Type  functio n is not the only
way to encod e the ontologica l distinction . We coul d have used a type predicate : Type(X\,  XORJp  i
or severa l predicates , such as XOR(X\).  Eithe r of these choice s woul d work fine, but by choosin g
the functio n Type,  we avoi d the need for an axio m that says that each individua l gate can have
only one type . The semantic s of function s alread y guarantee s this.
Next we conside r terminals . A gate or circui t can have one or mor e inpu t terminal s and
one or more outpu t terminals . We coul d simpl y nam e each one with a constant , just as we name d
gates . Thus , gateX i coul d have terminal s name d X\In\,X\Iri2,  an&X\Out\.  Name s as long and
structure d as these , however , are as bad as BearOfVerySmallBrain.  The y shoul d be replace d with
a notatio n that make s it clea r that X\Outi  is a termina l for gate Xi, and that it is the first outpu t
terminal . A functio n is appropriat e for this; the functio n Out(l,X\)  denote s the first (and only )
outpu t termina l for gate Xi. A simila r functio n In is used for inpu t terminals .
4 Not e that we hav e used name s beginnin g with appropriat e letters— A\, X\, and so on­
easie r to read . The knowledg e base mus t still contai n type informatio n for the gates .­purel y to mak e the exampl e
ISectio n 8.3. Th e Electroni c Circuit s Domai n 225
The connectivit y betwee n gate s can be represente d by the predicat e Connected,  whic h
takes two terminal s as arguments , as in Connected(Out(l,X\),In(l,X 2)).
Finally , we need to know if a signa l is on or off. One possibilit y is to use a unar y predicate ,
On, whic h is true whe n the signa l at a termina l is on. Thi s make s it a little difficult , however , to
pose question s such as "Wha t are all the possibl e value s of the signal s at the followin g terminal s
... ?" We will therefor e introduc e as object s two "signa l values " On or Off, and a functio n Signal
whic h take s a termina l as argumen t and denote s a signa l value .
Encod e genera l rule s
One sign that we have a good ontolog y is that there are very few genera l rule s that need to be
specified . A sign that we have a good vocabular y is that each rule can be state d clearl y and
concisely . Wit h our example , we need only seve n simpl e rules :
1. If two terminal s are connected , then they have the same signal :
Vf|,r 2 Connected(t\,t 2) ==> Signal(t\)=  Signal^
2. The signa l at ever y termina l is eithe r on or off (but not both) :
V t Signal(t)  = On V Signal(t)  = Off
On^Off
3. Connecte d is a commutativ e predicate :
V?i,? 2 Connected(t\,t 2) <£> Connected(t 2,t\)
4. An OR gate' s outpu t is on if and only if any of its input s are on:
Vg Type(g)  = OR  =>
Signal(Out(\,g))=On  <& 3n Signal(In(n,g))=On
5. An AND gate' s outpu t is off if and only if any of its input s are off:
Vg Type(g)=AND  =>
Signal(Out(l,g))=Off  «• 3n  Signal(In(n,g))  = Off
6. An XOR gate' s outpu t is on if and only if its input s are different :
Vg Type(g)=XOR  =>
Signal(Out(\,g))  = On &  Signal(In(l,g)}=£Signal(In(2,g))
1. A NOT gate' s outpu t is differen t from its input :
Vg (Type(g)=NOT)  =>  Signal(Out(l,g))^Signal(In(\,g))
Encod e the specifi c instanc e
The circui t show n in Figur e 8.1 is encode d as circui t C\ with the followin g description . First , we
categoriz e the gates :
Type(Xi ) = XOR  Type(X 2 ) = XOR
Type(A  l) = AND  Type(A 2 ) = AND
Type(O\)  = OR
226 Chapte r 8. Buildin g a Knowledg e Base
Then , the connection s betwee n them :
, Connected(Out(l,Xi),In(l,X2))
' Connected(Out(l,X }),In(2,A 2))
Connected(Out(  1, A2), In( 1, O,))
Connected(Out(l,A,),In(2,­O l))Connected(In(\,  C\
Connected(In(l,C\),In(l,Ai))
Connected(In(2,C\),In(2,X })) '
Connected(In(2,  CO, In(2,  A,')) /
Connected(Out(l,X 2), Out(\,  CO) Connected(In(3,  C\\In(2,X 2))
Connected(Out(\,  OO, Owr(2 , CO) Connected(In(3,  C\
CIRCUI T
VERIFICATIO NPose querie s to the inferenc e procedur e
What combination s of input s woul d caus e the first outpu t of Ci (the sum bit) to be off and the
secon d outpu t of C\ (the carry bit) to be on?
3 i], ('2, (3 Signal(In(\,  C\)) = i\ A Signal(In(2,C\))  = i2 A signal(In(3,  C0) = *3
A Signal(Out(\,  CO) = Of A Signal(Out(2,  CO) = On
The answe r is
(zi = On A /2 = On A ;3 = O/f) V
(/i = On A ;2 = P/f A z3 = On) V
(z,=Oj( f A/2 = OnA/ 3 = On)
Wha t are the possibl e sets of value s of all the terminal s for the adde r circuit ?
3 i i, /2, '3, o\,t>2  Signal(In(\,C]))  = i\ A Signal(In(2,  CO) = z'2
A Signal(In(3,\ )) = z3 A Signal(Out(\,  CO) =01 A Signal(Out(2,  CO) = 02
This fina l quer y will retur n a complet e input/outpu t table for the device , whic h can be used to
check that it does in fact add its input s correctly . This is a simpl e exampl e of circui t verification .
We can also use the definitio n of the circui t to buil d large r digita l systems , for whic h the same
kind of verificatio n procedur e can be carrie d out (see Exercise s 8.1 and 8.3) . Man y domain s are
amenabl e to the same kind of structure d knowledge­bas e development , in whic h more comple x
concept s are define d on top of simple r concepts .
8.4 GENERA L ONTOLOG Y
This sectio n is abou t a genera l ontolog y that incorporate s decision s abou t how to represen t a
broad selectio n of object s and relations . It is encode d withi n first­orde r logic , but make s man y
ontologica l commitment s that first­orde r logic does not make . A genera l ontolog y is rathe r more
demandin g to construct , but once done has man y advantage s over special­purpos e ontologies .
Conside r agai n the ontolog y for circuit s in the previou s section . It make s a large numbe r of
simplifyin g assumptions . For example , time is omitte d completely . Signal s are fixed , and there is
no propagatio n of signals . The structur e of the circui t remain s constant . Now we could take a step
towar d generalit y by considerin g signal s at particula r times , and includin g the wire length s and
propagatio n delay s in wire s and devices . Thi s woul d allow us to simulat e the timin g propertie s
Sectio n 8.4. Genera l Ontolog y 227
of the circuit , and indee d such simulation s are ofte n carrie d out by circui t designers . We coul d
also introduc e more interestin g classe s of gates , for exampl e by describin g the technolog y (TTL ,
MOS , CMOS , and so on) as well as the input/outpu t specification . If we wante d to discus s
reliabilit y or diagnosis , we woul d includ e the possibilit y that the structur e of the circuit , or the
propertie s of the gates , migh t chang e spontaneously . To accoun t for stray capacitances , we
woul d need to mov e from a purel y topologica l representatio n of connectivit y to a more realisti c
descriptio n of geometri c properties .
If we look at the wumpu s world , simila r consideration s apply . Althoug h we do includ e
time, it has a very simpl e structure . Nothin g happen s excep t whe n the agen t acts, and all change s
can be considere d instantaneous . A more genera l ontology , bette r suite d for the real world , woul d
allow for simultaneou s change s extende d over time . We also used the constan t symbo l Pit to say
that there was a pit in a particula r square , becaus e all pits were identical . We coul d have allowe d
for differen t kind s of pits by havin g severa l individual s belongin g to the class of pits but havin g
differen t properties . Similarly , we migh t wan t to allow for severa l differen t kind s of animals ,
not just wumpuses . It migh t not be possibl e to pin dow n the exac t specie s from the availabl e
percepts , so we woul d need to build up a wumpus­worl d biologica l taxonom y to help the agen t
predic t behavio r from scant y clues .
For any area of a special­purpos e ontology , it is possibl e to make change s like these to mov e
towar d greate r generality . An obviou s questio n then arises : do all thes e ontologie s converg e on
a general­purpos e ontology ? The answe r is, "Possibly. " In this section , we will presen t one
version , representin g a synthesi s of ideas from man y knowledg e representatio n effort s in AI and
philosophy . Ther e are two majo r characteristic s of general­purpos e ontologie s that distinguis h
them from collection s of special­purpos e ontologies :
• A general­purpos e ontolog y shoul d be applicabl e in mor e or less any special­purpos e
domai n (with the additio n of domain­specifi c axioms) . This mean s that as far as possible ,
no representationa l issu e can be finesse d or brushe d unde r the carpet . For example , a
genera l ontolog y canno t use situatio n calculus , whic h finesse s the issue s of duratio n and
simultaneity , becaus e domain s such as circui t timin g analysi s requir e thos e issue s to be
handle d properly .
• In any sufficientl y demandin g domain , differen t areas of knowledg e mus t be unified  becaus e
reasonin g and proble m solvin g may involv e severa l area s simultaneously . A robo t circuit ­
repai r system , for instance , need s to reaso n abou t circuit s in term s of electrica l connectivit y
and physica l layout , and abou t time both for circui t timin g analysi s and estimatin g labo r
costs . The sentence s describin g time therefor e mus t be capabl e of bein g combine d with
those describin g spatia l layout , and mus t work equall y well for nanosecond s and minutes ,
and for angstrom s and meters .
After we presen t the genera l ontology , we will appl y it to writ e sentence s describin g the domai n
of grocer y shopping . A brief reveri e on the subjec t of shoppin g bring s to mind a vast arra y of
topic s in need of representation : locations , movement , physica l objects , shapes , sizes , grasping ,
releasing , colors , categorie s of objects , anchovies , amount s of stuff , nutrition , cooking , nonstic k
fryin g pans , taste , time , money , direc t debi t cards , arithmetic , economics , and so on. The domai n
is mor e than adequat e to exercis e our ontology , and leave s plent y of scop e for the reade r to do
some creativ e knowledg e representatio n of his or her own .
228 Chapte r 8. Buildin g a Knowledg e Base
Our discussio n of the general­purpos e ontolog y is organize d unde r the followin g headings ,
each of whic h is reall y wort h a chapte r by itself :
0 Categories : Rathe r than bein g an entirel y rando m collectio n of objects , the worl d exhibit s
a good deal of regularity . For example , there are man y case s in whic h severa l object s have a
numbe r of propertie s in common . It is usua l to defin e categories5 that includ e as member s
all object s havin g certai n properties . For example , we migh t wish to have categorie s for
tomatoe s or ten­dolla r bills , for peache s or poun d notes , for fruit s or monetar y instruments .
We describ e how categorie s can be object s in their own right , and how they are linke d into
a unifie d taxonomi c hierarchy .
<C> Measures : Man y usefu l propertie s such as mass , age, and price relat e object s to quantitie s
of particula r types , whic h we call measures . We explai n how measure s are represente d in
logic , and how they relat e to unit s of measure .
0 Composit e objects : It is very commo n for object s to belon g to categorie s by virtu e of
their constituen t structure . For example , cars have wheels , an engine , and so on, arrange d
in particula r ways ; typica l basebal l game s have nine inning s in whic h each team alternate s
pitchin g and batting . We show how such structure s can be represented .
0 Time , Space , and Change : In orde r to allow for action s and event s that have differen t
duration s and can occu r simultaneously , we enlarg e our ontolog y of time . The basic pictur e
is of a univers e that is continuou s in both tempora l and spatia l dimensions . Times , places ,
and object s will be parts of this universe .
<) Event s and Processes : Event s such as the purchas e of a tomat o will also becom e individ ­
uals in our ontology . Lik e tomatoes , they are usuall y groupe d into categories . Individua l
event s take plac e at particula r time s and places . Processe s are event s that are continuou s
and homogeneou s in nature , such as rainin g or cookin g tomatoes .
0 Physica l Objects : We are alread y familia r with the representatio n of ordinar y object s such
as AND­gate s and wumpuses . As thing s that are extende d in both time and space , physica l
object s have muc h in commo n with events .
0 Substances : Wherea s object s such as tomatoe s are relativel y easy to pin down , substance s
such as tomat o juice are a littl e slippery . Natura l languag e usag e seem s to provid e con­
flictin g intuitions . Is there an objec t calle d TomatoJuicel  Is it a category , or a real physica l
object ? Wha t is the connectio n betwee n it and the liter of tomat o juice I bough t yesterday ?
Betwee n it and constituen t substance s such as Water!  We will see that these question s can
be resolve d by carefu l use of the ontolog y of space , time , and physica l objects .
0 Menta l Object s and Beliefs : An agen t will ofte n need to reaso n abou t its own beliefs ,
for example , whe n tryin g to decid e why it though t that anchovie s wer e on sale. It will
also need to reaso n abou t the belief s of others , for example , in orde r to decid e who m to
ask abou t the righ t aisle to find the tomatoes . In our ontology , sentence s are explicitl y
represented , and are believe d by agents .
In this section , we will try to cove r the highes t level s of the ontology . The top level s of the
hierarch y of categorie s are show n in Figur e 8.2. Whil e the scop e of the effor t migh t seem
5 Categorie s are also calle d classes , collections , kinds , types , and concept s by othe r authors . The y have little or nothin g
to do with the mathematica l topic of categor y theory .
Sectio n 8.4. Genera l Ontolog y 129
Anythin g
AbstractObject s Event s
Sets Number s RepresentationalObject s Interval s Place s PhysicalObject s Processe s
Categorie s Sentence s Measurement s Moment s Thing s
Time s Weight sStuff
Animal s Agent s Soli d Liqui d Gas
Human s
Figur e 8.2 Th e top­leve l ontolog y of the world , showin g the topic s to be covere d later in the
chapter . Arc s indicat e subse t relations .
dauntin g at first , representin g knowledg e of the commonsens e worl d can be highl y illuminating .
One is constantl y amaze d by how muc h one know s but neve r took the time to thin k about . Wit h a
good ontology , writin g it dow n become s more of a pleasur e than a problem . Connection s betwee n
seemingl y disparat e area s becom e obvious , and one is awe d by the scop e of huma n thought .
The followin g subsection s fill in the detail s of each topic . W e shoul d first state one
importan t caveat . We have chose n to discus s the conten t and organizatio n of knowledg e usin g
first­orde r logic . Certai n aspect s of the real worl d are hard to captur e in this language . The
principa l featur e we mus t omi t is the fact that almos t all generalization s hav e exceptions , or
have the statu s of a defaul t in the absenc e of mor e exac t information , or only hold to a degree .
For example , althoug h "tomatoe s are red" is a usefu l rule , som e tomatoe s are green , yellow , or
orange . Simila r exception s can be foun d to almos t all the genera l statement s in this section .
The abilit y to handl e exception s and uncertai n rule s is extremel y important , but is orthogona l to
the task of understandin g the genera l ontology . For this reason , we will dela y the discussio n of
exception s and default s unti l Chapte r 10, and the mor e genera l topi c of uncertai n informatio n
until Chapte r 14.
Representin g Categorie s
CATEGORIE S Th e organizatio n of object s into categorie s is a vital part of knowledg e representation . Althoug h
\ff:^ interactio n with the worl d take s plac e at the leve l of individua l objects , much of  reasoning
takes  place  at the level  of categories.  For example , a shoppe r migh t have the goal of buyin g a
cantaloupe,  rathe r than a particula r cantaloup e such as Cantaloupe?,­]^  Categorie s also serv e to
We ofte n use subscript s as a reminde r that a constan t refer s to an individua l rathe r than a collection .
230 Chapte r 8. Buildin g a Knowledg e Base
make prediction s abou t object s once they are classified . One infer s the presenc e of certai n object s
from perceptua l input , infer s categor y membershi p from the perceive d propertie s of the objects ,
and then uses categor y informatio n to mak e prediction s abou t the objects . For example , from its
green , mottle d skin , larg e size, and ovoi d shape , one can infe r that an objec t is a watermelon ;
from this, one infer s that it woul d be usefu l for fruit salad .
There are two main choice s for representin g categorie s in first­orde r logic . The first we
have alread y seen : categorie s are represente d by unar y predicates . The predicat e symbo l Tomato,
for example , represent s the unar y relatio n that is true only for object s that are tomatoes , and
Tomato(x)  mean s that x is a tomato .
REFLATIO N Th e secon d choic e is to reify the category . Reificatio n is the proces s of turnin g a predicat e
or functio n into an objec t in the language.7 We will see severa l example s of reificatio n in this
chapter . In this case, we use Tomatoes  as a constan t symbo l referrin g to the objec t that is the set of \
all tomatoes.  We use x G Tomatoes  to say that x is a tomato . Reifie d categorie s allow us to mak e
assertion s abou t the categor y itself , rathe r than abou t member s of the category . For example , we
can say Population(Humans)=5, 000,000,000,  even thoug h there is no individua l huma n with a
populatio n of five billion .
Categorie s perfor m one mor e importan t role : the y serv e to organiz e and simplif y the;
INHERITANC E knowledg e base throug h inheritance . If we say that all instance s of the categor y Food  are edible, :
and if we asser t that Fruit  is a subclas s of Food  and Apples  is a subclas s of Fruit,  then we know '.
that ever y appl e is edible . We say that the individua l apple s inheri t the propert y of edibility , in
this case from their membershi p in the Food  category .
TAXONOM Y Subclas s relation s organiz e categorie s into a taxonom y or taxonomi c hierarchy . Tax­ ,
onomie s have been used explicitl y for centurie s in technica l fields . For example , systematic ;
biolog y aims to provid e a taxonom y of all livin g and extinc t species ; librar y scienc e has de­;
velope d a taxonom y of all field s of knowledge , encode d as the Dewe y Decima l system ; tax, j
authoritie s and othe r governmen t department s have develope d extensiv e taxonomie s of occupa­ 1
tions and commercia l products . Taxonomie s are also an importan t aspec t of genera l commonsens e |
knowledge , as we will see in our investigation s that follow .
First­orde r logic make s it easy to state facts abou t categories , eithe r by relatin g object s toj
categorie s or by quantifyin g over their members :
• An objec t is a membe r of a category . For example :
Tomatou  G Tomatoes
• A categor y is a subclas s of anothe r category . For example :
Tomatoes  C Fruit
• All member s of a categor y have som e properties . For example :
V x x G Tomatoes  =>• Red(x)  A Round(x)
• Member s of a categor y can be recognize d by som e properties . For example :
MX Red(Interior(x))  A Green(Exterior(x))  A x G Melons  =$• x&  Watermelons
• A categor y as a whol e has som e properties . For example :
Tomatoes  G DomesticatedSpecies
7 Th e term "reification " come s from the Lati n wor d res, or thing . Joh n McCarth y propose d the term "thingification ,
but it neve r caugh t on.
Sectio n Genera l Ontolog y 231
PARTITIO NNotic e that becaus e Tomatoes  is a category , and is a membe r of DomesticatedSpecies,  then
DomesticatedSpecies  mus t be a categor y of categories . One can even have categorie s of categorie s
of categories , but they are not muc h use.
Althoug h subclas s and instanc e relation s are the mos t importan t ones for categories , we
also wan t to be able to state relation s betwee n categorie s that are not subclasse s of each other .
For example , if we just say that Males  and Females  are subclasse s of Animals,  then we have not
said that a male canno t be a female . We say that two or more categorie s are disjoin t if they have
no member s in common . And even if we know that male s and female s are disjoint , we will not
know that an anima l that is not a male mus t be a femal e unles s we say that male s and female s
constitut e an exhaustiv e decompositio n of the animals . A disjoin t exhaustiv e decompositio n is
know n as a partition . The followin g example s illustrat e these three concepts :
Disjoint({Animals,  Vegetables})
ExhaustiveDecomposition(  {Americans,  Canadians,  Mexicans},  NorthAmericans)
Pa rtition({  Males,  Females],  An ima Is)
(Note that the ExhaustiveDecomposition  of NorthAmericans  is not a Partition  becaus e som e
peopl e have dual citizenship. ) The definition s of these three predicate s are as follows :
Intersection(c  \ , c2) = EmptySet)V.v Disjoints)  O
(V ci, c2 c\ e s A C2 € s A c\ ^c2
V s, c ExhaustiveDecomposition(s,  c) <=>
(V« i€c O 3c 2 c 2<E.sA/(Ec 2)
V s, c Partitions,  c) O Disjoint(s)  A ExhaustiveDecomposition(s,  c)
Categorie s can also be defined  by providin g necessar y and sufficien t condition s for mem ­
bership . For example , a bachelo r is an unmarried , adul t male :
V.v Bachelor(x)  O  Male(x)  A Adult(x)  A Unmarried(x)
As we discus s in the sideba r on natura l kinds , stric t logica l definition s for categorie s are not
alway s possible , nor alway s necessary .
Measure s
In both scientifi c and commonsens e theorie s of the world , object s have height , mass , cost , and
MEASURE S s o on. The value s that we assig n for these propertie s are calle d measures . Ordinary , quantitativ e
measure s are quit e easy to represent . We imagin e that the univers e include s abstrac t "measur e
objects, " such as the length  that is the lengt h of this line segment : I———————————————I .
We can call this lengt h 1.5 inches , or 3.81 centimeters . Thus , the same lengt h has differen t name s
UNITS FUNCTIO N i n our language . Logically , this can be done by combinin g a units functio n with a number . If LI
is the nam e of the line segment , then we can writ e
Length(Li)  = lnches(  1.5) = Centimeters(3.8 1)
Conversio n betwee n unit s is done with sentence s such as
V/ Centimeters(2.54  x l)=Inches(l)
Vr Centigrade(t)=Fahrenheit(32+  1.8 x t)
232 Chapte r 8. Buildin g a Knowledg e Base
NATURA L KIND S
Some categorie s have stric t definitions : an object  is a triangl e if and only if it is a
polygo n with three sides . On the othe r hand , mos t categorie s in the shoppin g worl d
and in the real worl d are natura l kind categorie s with no clear­cu t definition . We
know , for example , that tomatoe s tend to be a dull scarlet , roughl y spherical , with an
indentatio n at top wher e the stem was , abou t three to four inche s in diameter , with a
thin but toug h skin and with flesh , seeds , and juice inside . We also know that there is
variation : unrip e tomatoe s are green , som e are smalle r or large r than average , cherr y
tomatoe s are uniforml y small . Rathe r than havin g a complet e definitio n of tomatoes ,
we have a set of feature s that serve s to identif y object s that are clearl y typica l tomatoes ,
but may not be able to decid e for othe r objects . (Coul d there be a squar e tomato ? a
yello w one? a tomat o three feet across? )
This pose s a proble m for a logica l agent . The agen t canno t be sure that an objec t
it has perceive d is a tomato , and even if it was sure, it coul d not be certai n whic h of the
propertie s of typica l tomatoe s this one has. This proble m is an inevitabl e consequenc e
of operatin g in inaccessibl e environments . The followin g mechanis m provide s a way
to deal with natura l kind s withi n a logica l system .
The key idea is to separat e wha t is true of all instances  of a categor y from what is
true only of typica l instance s of a category . So in additio n to the categor y Tomatoes,
we will also have the categor y Typical(Tomatoes).  Her e Typical  is a functio n that map s
a categor y to the subclas s of that categor y that contain s only the typica l instances :
V c Typical(c)  C c
Most of the knowledg e abou t natura l kind s will actuall y be abou t the typica l instances :
Vjc x 6 Typical(Tomatoes)  => Red(x)  A Spherical(x)
In this way , we can still writ e dow n usefu l fact s abou t categorie s withou t providin g
exact definitions .
The difficult y of providin g exac t definition s for mos t natura l categorie s was
explaine d in dept h by Wittgenstei n (1953) , in his book Philosophical  Investigations.
He used the exampl e of games  to show that member s of a categor y share d "famil y
resemblances " rathe r than necessar y and sufficien t characteristics . The Investigations
also revolutionize d our understandin g of language , as we discus s furthe r in Chapte r 22.
The utilit y of the notio n of stric t definitio n was also challenge d by Quin e (1953) .
He pointe d out that even  the definitio n of "bachelor " give n befor e is suspect ; one might ,
for example , questio n a statemen t such as "the Pop e is a bachelor. " The categor y
"bachelor " still play s a usefu l role in natura l languag e and in forma l knowledg e
representation , becaus e it simplifie s man y sentence s and inferences .
Sectio n 8.4. Genera l Ontolog y 233
Simila r axiom s can be writte n for pound s and kilograms ; second s and days ; dollar s and cents .
(Exercis e 8.9 asks you to represen t exchang e rate s betwee n currencies , wher e thos e exchang e
rates can vary over time. )
Measure s can be used to describ e object s as follows :
Mass(Tomato\2)  = Kilograms(QA6}
Price(Tomato\2)  = $(0.32 )
Vd d^Days  => ­ Duration(d)=Hours(24)
It is very importan t to be able to distinguis h betwee n monetar y amount s and monetar y instruments :
Vfo b^DollarBills  => CashValue(b)  = $(l.OO)
This will be usefu l whe n it come s to payin g for thing s later in the chapter .
Simple , quantitativ e measure s are easy to represent . Ther e are othe r measure s that presen t
more of a problem , becaus e they hav e no agree d scal e of values . Exercise s have difficulty ,
dessert s have deliciousness , and poem s have beauty , yet number s canno t be assigne d to thes e
qualities . One might , in a momen t of pure accountancy , dismis s such propertie s as useles s for the
purpos e of logica l reasoning ; or, still worse , attemp t to impos e a numerica l scale on beauty . This
woul d be a grav e mistake , becaus e it is unnecessary . The mos t importan t aspec t of measure s is
not the particula r numerica l values , but the fact that measure s can be ordered.
Althoug h measure s are not numbers , we can still compar e them usin g an orderin g symbo l
such as >. For example , we migh t well believ e that Norvig' s exercise s are toughe r than Russell's ,
and that one score s less on toughe r exercises :
V e\, 62 e\ £ Exercises  A e^ £ Exercises  A Wrote(Norvig,  e\) A Wrote(Russell,  e^) =>
Difficulty(e }) > Difficulty^)
Ve\, €2 ei £ Exercises  A e2 £ Exercises  A Difficulty(e\)  > Difficulty(e 2) => •
ExpectedScore(e\ ) < ExpectedScorefa)
This is enoug h to allow one to decid e whic h exercise s to do, even thoug h no numerica l value s for
difficult y were ever used . (On e does , however , have to determin e who wrot e whic h exercises. )
These sort s of monotoni c relationship s amon g measure s form the basi s for the field of quali ­
tativ e physics , a subfiel d of AI that investigate s how to reaso n abou t physica l system s withou t
plungin g into detaile d equation s and numerica l simulations . Qualitativ e physic s is discusse d in
the historica l note s section .
Composit e object s
The idea that one objec t can be part of anothe r is a familia r one. One' s nose is part of one's head ;
Romani a is part of Europe ; this chapte r is part of this book . We use the genera l PartOf  relatio n
to say that one thin g is part of another . PartOf  is transitiv e and reflexive . Object s can therefor e
be groupe d into PartOf  hierarchies , reminiscen t of the Subset  hierarchy :
PartOf  (Bucharest,  Romania)
PartOf  (Romania,  EasternEurope)
PartOf  (EasternEurope,  Europe)
From these , give n the transitivit y of PartOf,  we can infe r that PartOf  (Bucharest,  Europe).
234 Chapte r 8. Buildin g a Knowledg e Base
COMPOSIT E OBJEC T An y objec t that has parts is calle d a composit e object . Categorie s of composit e object s
STRUCTUR E ar e often characterize d by the structur e of thos e objects , that is, the parts and how the part s are
related . For example , a bipe d has exactl y two legs that are attache d to its body :
Va Biped(a)  =>
3lt,l2,b Leg(l\  ) A ^Leg(h)  A Body(b)  A
PartOf(li,a)  A PartOf(l 2, a) A PartOf(b,  a) A
Attached(l\,b)  A Attached^,  b} A
/i^/ 2AV/ 3 Leg(k)f\PartOf(h,a)  => (/.­, = /, V /3 =/ 2)
This genera l form of sentenc e can be used to defin e the structur e of any composit e object ,
includin g events : for example , for all basebal l games , ther e exis t nine inning s such that each
is a part of the game , and so on. A generi c even t descriptio n of this kind is ofte n calle d a
SCHEM A schem a or script , particularl y in the area of natura l languag e understanding . Som e approache s
SCRIP T t o text understandin g rely mainl y on the abilit y to recogniz e instance s of schemati c event s from
description s of their parts , so that the text can be organize d into coheren t events , and question s can
be answere d abou t parts not explicitl y mentioned . We discus s these issue s furthe r in Chapte r 22.
We can defin e a PartPartition  relatio n analogou s to the Partition  relatio n for categorie s
(see Exercis e 8.4). An objec t is compose d of the parts in its PartPartition,  and can be viewe d as
derivin g som e propertie s from thos e parts . For example , the mas s of a composit e objec t is the
sum of the masse s of the parts . Notic e that this is not the case with categories : categorie s have
no mass , even thoug h their element s might .
It is also usefu l to defin e composit e object s with definit e parts but no particula r structure .
For example , we migh t wan t to say, "The apple s in this bag weig h three pounds. " Rathe r than
commi t the erro r of assignin g the weigh t to the category  of apples­in­the­bag , we can mak e a
BUNC H bunc h out of the apples . For example , if the apple s are Apple\  , Apple 2, and Apple^,  then
BunchOf(  {Apple  \ , Apple 2 ,
denote s the composit e objec t with the three apple s as parts . We can then use the bunc h as a normal ,
albeit unstructured , object.  Notic e that BunchOf  (Apples)  is the composit e objec t consistin g of
all apple s — not to be confuse d with Apples,  the category .
Representin g chang e with event s
Sectio n 7.6 showe d how situatio n calculu s coul d be used to represen t change . Situatio n calculu s
is perfec t for the vacuu m world , the wumpu s world , or any worl d in whic h a singl e agen t takes
discret e actions . Unfortunately , situatio n calculu s has two problem s that limi t its applicability .
First, situation s are instantaneou s point s in time , whic h are not very usefu l for describin g the
gradua l growt h of a kitte n into a cat, the flow of electron s alon g a wire , or any othe r proces s
wher e chang e occur s continuousl y over time . Second , situatio n calculu s work s best whe n only
one actio n happen s at a time . Whe n ther e are multipl e agent s in the world , or whe n the worl d
can chang e spontaneously , situatio n calculu s begin s to brea k down . It is possibl e to prop it back
up for a whil e by definin g composit e actions , as in Exercis e 7.12 . If there are action s that have
differen t durations , or whos e effect s depen d on duration , then situatio n calculu s in its intende d
form canno t be used at all.
ISectio n 8.4. Genera l Ontolog y 235
Becaus e of thes e limitations , we now turn to a differen t approac h towar d representin g
EVENT CALCULU S change , whic h we call the even t calculus , althoug h the nam e is not standard . Even t calculu s
is rathe r like a continuou s versio n of the situation­calculu s "movie " show n in Figur e 7.3. We
think of a particula r univers e as havin g both a "spatial " and a tempora l dimension . The "spatial "
dimensio n range s over all of the object s in an instantaneou s "snapshot " or "cross­section " of the
EVEN T universe.8 The tempora l dimensio n range s over time . An even t is, informally , just a "chunk " of
this univers e with both tempora l and spatia l extent . Figur e 8.3 give s the genera l idea .
SUBEVENT S
INTERVA L"space"
WorldWarl l
Figur e 8.3 Th e majo r entitie s in even t calculus . Intervals  suc h as the TwentiethCentury
contai n as subevent s all of the event s occurrin g withi n a give n time period . Ordinar y event s such
as WorldWarll  have tempora l and "spatial " extent .
Let us look at an example : Worl d War II, referre d to by the symbo l WorldWarll.  Worl d
War II has parts that we refer to as subevents:9
SubEvent(BattleOfBritain,  WorldWarll)
Similarly , Worl d War II is a subeven t of the twentiet h century :
SubEvent(  WorldWarll,  TwentiethCentury)
The twentiet h centur y is a specia l kind of even t calle d an interval . An interva l is an even t
that include s as subevent s all event s occurrin g in a give n time period . Interval s are therefor e
entire tempora l section s of the universe , as the figur e illustrates . In situatio n calculus , a give n
fact is true in a particula r situation . In even t calculus , a give n even t occur s durin g a particula r
interval . The previou s SubEvent  sentence s are example s of this kind of statement .
8 We put "spatial " in quote s becaus e it is possibl e that the set of object s bein g considere d does not includ e place s at all;
nonetheless , it is a helpfu l metaphor . From now on, we will leav e off the quotes .
9 Not e that SubEvent  is a specia l case of the PartOf  relation , and is also transitiv e and reflexive .
236 Chapte r 8. Buildin g a Knowledg e Base
Like any othe r sort of object , event s can be groupe d into categories . Fo r example ,
WorldWarll  belong s to the categor y Wars.  To say that a war occurre d in the Middl e East in
1967, we woul d say
3 w w G Wars  A SubEvent(w,  AD 1967 ) A PartOf(Location(w),  MiddleEast)
To say that Shanka r travelle d from New York to New Delh i yesterday , we migh t use the categor y
Journeys,  as follows :
3y j E Journeys  A Origin(NewYork,j)  A Destination(NewDelhi,j)
A Traveller(Shankar,j)  A SubEvent(j,  Yesterday)
This notatio n can get a littl e tedious , particularl y becaus e we are ofte n intereste d mor e in the
event' s propertie s than in the even t itself . We can simplif y the description s by usin g comple x term s
to nam e even t categories . For example , Go(Shankar,  NewYork,  NewDelhi)  name s the categor y of
event s in whic h Shanka r travel s from New York to New Delhi . The functio n symbo l Go can be
define d by the followin g sentence :
V e, x, o,d e£  Go(x,  o, d) o­
e e Journeys  A Traveller(x,  e) A Origin(o,  e) A Destination(d,  e)
Finally , we use the notatio n E(c, i) to say that an even t of category  c is a subeven t of the even t (or
interval)  /:
V c, i E(c,  i) <=> 3 e e e c A SubEvent(e,  i)
Thus , we have
E(Go(Shankar,  NewYork,  NewDelhi),  Yesterday)
whic h mean s "ther e was an even t that was a goin g by Shanka r from New York to New Delh i that
took plac e sometim e yesterday. "
PLACE S
MINIMIZATIO NPlace s
Places , like intervals , are specia l kind s of space­tim e chunks . A plac e can be though t of as a
constan t piec e of space , extende d throug h time.10 New Yor k and the Middl e East are places , at
least as far as recen t histor y is concerned . We use the predicat e In to denot e the specia l kind of i
subeven t relatio n that hold s betwee n places ; for example :
ln(NewYork,  USA)
Place s com e in differen t varieties ; for example , NewYork  is an Area,  wherea s the SolarSystem  is ;
a Volume.  The Location  function , whic h we used earlier , map s an objec t to the smalles t plac e
that contain s it:
VJT, / Location(x)  = I <=>
At(x,l)f\Ml 2 At(x,l 2) => In(l,l 2)
This last sentenc e is an exampl e of a standar d logica l constructio n calle d minimization .
10 We will not worr y abou t loca l inertia l coordinat e frame s and othe r thing s that physicist s need to pin dow n exactly ;
what this means . Place s that change  over time are deal t with in a late r section .
Sectio n 8.4. Genera l Ontolog y 237
DIgcRET E EVENT S
PROCES S
LIQUI D EVEN T
STATE SProcesse s
The event s we have seen so far have been wha t we call discret e events —the y have a definit e
structure . Shankar' s trip has a beginning , middle , and end . If interrupte d halfway , the even t
woul d be different—i t woul d not be a trip from New York to New Delhi , but instea d a trip from
New York to somewher e in the Easter n Mediterranean . On the othe r hand , the categor y of event s
denote d by Flying(Shankar)  has a differen t quality . If we take a smal l interva l of Shankar' s flight ,
say, the third twenty­minut e segmen t (whil e he wait s anxiousl y for a secon d bag of honey­roaste d
peanuts) , that even t is still a membe r of Flying(Shankar).  In fact , this is true for any subinterval .
Categorie s of event s wit h this propert y are calle d proces s categorie s or liqui d even t
categories . Any subinterva l of a proces s is also a membe r of the same proces s category . We can
use the sam e notatio n used for discret e event s to say that , for example , Shanka r was flyin g at
some time yesterday :
E(Flying(Shankar),  Yesterday)
We often wan t to say that som e proces s was goin g on throughout  some interval , rathe r than just
in som e subinterva l of it. To do this, we use the predicat e T:
T( Working(Stuart),  TodayLunchH  our)
T(c. i) mean s that som e even t of type c occurre d over exactl y the interva l i—that is, the even t
begin s and ends at the same time as the interval . Exercis e 8.6 asks you to defin e T formally .
As well as describin g processe s of continuou s change , liqui d event s can describ e processe s
of continuou s non­change . Thes e are ofte n calle d states . For example , "Mar y bein g in the loca l
supermarket " is a categor y of state s that we migh t denot e by In(Mary,  Supermarket] ). To say she
was in the supermarke t all this afternoon , we woul d writ e
T(In(Mary,  Supermarket]),  ThisAfternoon)
An interva l can also be a discontinuou s sequenc e of times ; we can represen t the fact that the
supermarke t is close d ever y Sunda y with
T(Closed(Superinarket\ ), BunchOf  (Sundays))
Specia l notatio n for combinin g proposition s
It is temptin g to writ e somethin g like
T((At(Agent,Loc\)  A At(Tomato\,Loc\))Jj,)
but technicall y this is nonsense , becaus e a sentenc e appear s as the first argumen t of the predicat e T,
and all argument s to predicate s mus t be terms , not sentences . Thi s is easil y fixe d by introducin g
a functio n calle d And  that take s two even t categorie s as argument s and return s a categor y of
composit e event s of the appropriat e kind :
T(And(At(Agent,  Loc\ ), At(Tomato  \, Loc\ )), E)
We can defin e the functio n And with the axio m
Vp,q,e  T(And(p,q),e)  O  T(p,  e) A T(q,  e)
238 Chapte r 8. Buildin g a Knowledg e Base
Thus , And(p,  q) is the categor y of composit e "p­^­events, " wher e ap­g­even t is an even t in whic h
both a p and a q occur . If you thin k of a /7­even t as a piec e of "videotape " of a p happening , and
a c/­even t as a "videotape " of a q happening , then the p­q­evenl  is like havin g the two piece s of j
tape splice d togethe r in paralle l (see Figur e 8.4(a)) .
(a) (b) (c)
Figur e 8.4 A  depictio n of comple x events , (a) T(p A q, e) (b) T(p\/_q,  e) (c) Tip V q, e)
Once a metho d for conjoinin g even t categorie s is defined , it is convenien t to exten d the
synta x to allow regula r infix connectiv e symbol s to be used in plac e of the functio n name :
T(p/\q,e)  & T(And(p,q),e)  & T(p,  e) A T(q,  e)
This is fine as long as you remembe r that in T(p A q, s), the expressio n p A q is a term denotin g a
categor y of events , not a sentence .
One migh t thin k that we can just go ahea d and defin e simila r function s for disjunctiv e
and negate d events . In fact , becaus e the T predicat e is essentiall y a conjunction  (ove r all the
subinterval s of the interva l in question) , it can interac t in two differen t way s with disjunctio n and
negation . For example , conside r the Englis h sentenc e "On e of the two shop s was open all day on
Sunday. " This coul d mean , "Eithe r the first shop was open all day on Sunday , or the secon d shop
was open all day on Sunday " (Figur e 8.4(b)) , or it coul d mean , "At any give n time on Sunday , at
least one of the two shop s was open " (Figur e 8.4(c)) . Bot h of these meaning s are usefu l sorts of;
thing s to say, so we will need a concis e representatio n for each . Ther e are no accepte d notations ;
in this case , so we will mak e som e up. Let V be used for the first kind of disjunctiv e event , and
V be used for the second . For the first , we have the followin g definition :
T(pV.q,e)  &  T(p,e)VT(q,e)
We leav e the secon d as an exercise , alon g with the definition s for negate d events .
Times , intervals , and action s
In this section , we flesh out the vocabular y of time intervals . Becaus e it is a limite d domain , we
can be more complet e in decidin g on a vocabular y and encodin g genera l rules . Tim e interval s
are partitione d into moment s and extende d intervals . The distinctio n is that only moment s have
zero duration :
Partition({Moments,  Extendedfntervals},  Intervals)
V (' ;'e Intervals  => (/' G Moments  ­t> Duration(i)  =G)
Now we inven t a time scale and associat e point s on that scal e with moments , givin g us absolut e
times . The time scal e is arbitrary ; we will measur e it in second s and say that the momen t at
Sectio n 8.4. Genera l Ontolog y 239
midnigh t (GMT ) on Januar y 1 , 1900 , has time 0. The function s Start  and End pick out the earlies t
and lates t moment s in an interval , and the functio n Time  deliver s the poin t on the time scale for
a moment . The functio n Duration  give s the differenc e betwee n the end time and the start time .
V/ Interval(i)  => Duration(i)=(Time(End(i)~)­Time(Start(i)))
Time(Start(ADl900))  =Seeonds(0)
Time(Start(AD  1 99 1 )) =Seconds(281  1 694800 )
Time(End(AD  1 99 1 )) = Secon<fr(2903230800 )
Duration(AD  1 99 1 ) =Seconds(3  1 536000 )
To mak e these number s easie r to read , we also introduc e a functio n Date,  whic h take s six
argument s (hours , minutes , seconds , month , day, and year ) and return s a poin t on the time scale :
Time(Start(ADl99l))=SecondsDate(00,00,00,Jan,  1, 1991 )
Date(l2,34,56,Feb,  14, 1993 ) =293868209 6
The simples t relatio n betwee n interval s is Meet.  Two interval s Meet  if the end time of the
first equal s the start time of the second . It is possibl e to defin e predicate s such as Before, After,
During,  and Overlap  solel y in term s of Meet,  but it is mor e intuitiv e to defin e them in term s of
point s on the time scale . (See Figur e 8.5 for a graphica l representation. )
Vz'j Meet(iJ)  <=>
V i,j Before(i,j)  <
Vzj After(j,i)  •£>
V i,j During(iJ)  <
V i,j Overlap(iJ)Time(End(i))  = Time(Start(f))
> Time(End(i))  < Time(Start(j))
Before(iJ)
> Time(Start(j))  < Time(Start(i))  A Time(End(i))  < Time(End(j)}
<^> 3 k During(k,  i) A During(kJ)
Meet(iJ)
Before(ij)  \­
After(j,i)
During(ij)  I——— j
Overlap(iJ)  \­
Overlap(j,i)H i
I———
Figur e 8.5 Predicate s on time intervals .
240 Chapte r 8. Buildin g a Knowledg e Base
For example , to say that the reig n of Elizabet h II followe d that of Georg e VI, and the reig n of
Elvis overlappe d with the 1950s , we can writ e the following :
After(ReignOf(ElizabethU),ReignOf(GeorgeVl))
Overlap(Fifties,ReignOf  (Elvis))
Start(Fifties)  =Start(ADl950)
En d( Fifties)  = End(AD  1959 )
Tempora l relation s amon g interval s are used principall y in describin g actions . This is done
in muc h the same way in even t calculu s as it is in situatio n calculus . The differenc e is that instea d
of definin g a resultin g situatio n and describin g it, one define s a resultin g interval , in whic h a
certai n state occurs . The followin g example s illustrat e the genera l idea:
1. If two peopl e are engaged , then in some futur e interval , they will eithe r marr y or brea k the
engagement .
\/x,y, /o T(Engaged(x,y),io)  =>
3 /, (Meet(i 0, /,) V After(i\,  /<,)) A
T(Marry(x,y)  V BreakEngagement(x,y),  i\)
2. Whe n two peopl e marry , they are spouse s for som e interva l startin g at the end of the
marryin g event .
V.x,>',/ o T(Marry(x,y),io)  =>• 3/i T(Spouse(x,  y), i\) A Meet(io,  i\)
3. The resul t of goin g from one place to anothe r is to be at that othe r place .
Vx,a,b,i 0 ,3/ i T(Go(x,a,b),i 0) => T(ln(x,b),i\)  /\Meet(i 0,i\)
We shall have more to say on the subjec t of action s and interval s in Part IV, whic h cover s plannin g
with thes e sorts of actio n descriptions .
Object s revisite d
One purpos e of situatio n calculu s was to allo w object s to have differen t propertie s at differen t
times . Even t calculu s achieve s the sam e goal . For example , we can say that Poland' s area in
1426 was 233,00 0 squar e miles , wherea s in 1950 it was 117,00 0 squar e miles :
T(Area(Poland,SqMiles(233(m)),ADl426)
T(Area(Poland,  SqMiles(\  17000)) , AD 1950 )
In fact, as well as growin g and shrinking , Polan d has move d abou t somewha t on the map . We^
could plot its land area over time , as show n in Figur e 8.6. We see that Polan d has a tempora l
as wel l as spatia l extent . It turns  out to be perfectly  consistent  to view  Poland  as an event.
We can then use tempora l subevent s such as 19thCenturyPoland,  and spatia l subevent s such as
CentralPoland.
The USA has also change d variou s aspect s over time . One aspec t that change s ever y four
or eigh t years , barrin g mishaps , is its president . In even t calculus , President(USA)  denote s an
objec t that consist s of differen t peopl e at differen t times . President(USA)  is the objec t that is
Georg e Washingto n from 1789 to 1796 , John Adam s from 1796 to 1800 , and so on (Figur e 8.7).
To say that the presiden t of the USA in 1994 is a Democrat , we woul d use
T(Democrat(President(  USA)),  AD 1994 )
Sectio n 8.4. Genera l Ontolog y 241
"SPEce"
ll n
ll n
n l l
n l l
n l ln __ _ n
1 1 1  1
;W:fi5§¥ f S » 1  1
............ . ...:' • Tlv'­ i = :;­­;V : I I
':•: ,.: . ••• : . .. . tl . , ••;;',".  ...•.•; : ,1 ,  —————  , I I
^^ft^^:.,::4^:..;;:^:::^:;^;.:^;,. f^^:^.:.~:Ui l
m^&^^&^^m^^^^^^m^i^^^^m^^,J­y:­: :51.VV:=! l :iV,.' : :V­!:iflr' :"V»» :=,rw'.rt:c: . vl^:/^^^^^^ ^iSftB^i&X ^
•:.::".­•" . Vr­ . ~~ V ." " ": .". ­"­F*?:V:K . .:~y:i.p's.isW­"v " :'••• V~"! "'..."...•::." : .:': .•"o"~l~~:­!*.~ .:>:."..­~J' :i":»r:. ­ ':..*...:­ • .." ".."' ™ il**­­»"o"v~3.­" i
II h.­:V v I I
II —— — I I
II I I
II I I
II I I
II I I
II I I
II I I
II I I
11 n tim pII I I Ilm e
II I I142 6 195 0
Figur e 8.6 Viewin g Polan d as an objec t of varyin g size and locatio n over time .
FLUENT S Object s such as Poland  and President(USA)  are calle d fluents . The dictionar y says that a fluen t
is somethin g that is capabl e of flowing , like a liquid . For our purposes , a fluen t is somethin g that
flow s or change s acros s situations .
It may seem odd to reify object s that are as transien t as President(USA),  yet fluent s allow us
to say som e thing s that woul d otherwis e be cumbersom e to express . For example , "The presiden t
of the USA was male throughou t the 19th century " can be expresse d by
T(Male(President(USA)),  19thCentury )
even thoug h ther e were 24 differen t president s of the USA in the nineteent h century .
There are som e thing s that can be easily  expresse d with fluent s but not with situatio n
calculus . For example , Location(x)  denote s the plac e in whic h x is located , even if that plac e
varie s over time . We can then use sentence s abou t the locatio n of an objec t to expres s the fact
that, for example , the locatio n of the Empir e Stat e Buildin g is fixed :
Fixed(Location(EmpireStateBuilding))
Withou t fluents , we coul d find a way to talk abou t objects  bein g widesprea d or immobile , but we
could not talk directl y abou t the object' s locatio n over time . We leav e it as an exercis e to defin e
the Fixed  predicat e (Exercis e 8.7) .
Substance s and object s
The real worl d perhap s can be seen as consistin g of primitiv e object s (particles ) and composit e
object s buil t from them . By reasonin g at the leve l of larg e objects  such as apple s and cars ,
we can overcom e the complexit y involve d in dealin g wit h vast number s of primitiv e object s
242 Chapte r 8. Buildin g a Knowledg e Base
time
Figur e 8.7
existence .A schemati c view of the objec t President(USA)  for the firs t fiftee n year s of its
INDIVIDUATIO N
STUF F
TEMPORA L
SUBSTANCE S
SPATIA L
SUBSTANCE S
COUN T NOUN S
MASS NOUN Sindividually . Ther e is, however , a significan t portio n of realit y that seem s to defy any obviou s
individuation —divisio n into distinc t objects . We give this portio n the generi c nam e stuff . For
example , suppos e I have som e butte r and an aardvar k in fron t of me. I can say ther e is one
aardvark , but there is no obviou s numbe r of "butter­objects, " becaus e any part of a butter­objec t
is also a butter­object , at least  unti l we get to very smal l parts indeed . Thi s is the majo r distinctio n
betwee n stuff and things . If we cut an aardvar k in half , we do not get two aardvarks , unfortunately .
The distinctio n is exactl y analogou s to the differenc e betwee n liqui d and nonliqui d events . In
fact, som e have calle d liqui d even t type s tempora l substances , wherea s thing s like butte r are
spatia l substance s (Lena t and Guha , 1990) .
Notic e that Englis h enforce s the distinctio n betwee n stuff and things . We say "an aardvark, "
but, excep t in pretentiou s Californi a restaurants , one canno t say "a butter. " Linguist s distinguis h
betwee n coun t nouns , such as aardvarks , holes , and theorems , and mas s nouns , such as butter ,
water , and energy.  Shoul d we also enforc e this distinctio n in our representatio n by treatin g butte r
and aardvark s differently , or can we treat them usin g a unifor m mechanism ?
To represen t stuf f properly , we begi n with the obvious . We will need to have as object s
in our ontolog y at leas t the gros s "lumps " of stuf f that we interac t with . For example , we migh t
recogniz e the butte r as the sam e butte r that was left on the table the nigh t before ; we migh t pick
it up, weig h it, sell it, or whatever . In thes e senses , it is an objec t just like the aardvark . Let us
call it Butter^.  We will also defin e the categor y Butter.  Informally , its element s will be all thos e
thing s of whic h one migh t say "It's butter, " includin g Butter?,.
I
Sectio n 8.4. Genera l Ontolog y 243
INTRINSI C
EXTRINSI CThe next thin g to state was mentione d earlier : wit h som e caveat s abou t very smal l parts
that we will omit for now , any part of a butter­objec t is also a butter­object :
V x, y x G Butter  A PartOf(y,  x) => y £ Butter
Individua l aardvark s deriv e propertie s such as approximat e shape , size , weight , and diet
from membershi p in the categor y of aardvarks . Wha t sorts of propertie s does an objec t deriv e
from bein g a membe r of the Butter  category ? Butte r melt s at aroun d 30 degree s centigrade :
Vx Butter(x)  => MeltingPoint(x,Centigrade(30))
Butte r is yellow , less dens e than water , soft at room temperature , has a high fat content , and
so on. On the othe r hand , butte r has no particula r size, shape , or weight . We can defin e more
specialize d categorie s of butte r such as UnsaltedButter,  whic h is also a kind of stuff becaus e any
part of an unsalted­butter­objec t is also an unsalted­butter­object . On the other hand , if we defin e
a categor y PoundOfButter,  whic h include s as member s all butter­object s weighin g one pound ,
we no longe r have a substance ! If we cut a poun d of butte r in half , we do not get two pound s of
butter—anothe r of those annoyin g thing s abou t the worl d we live in.
What is actuall y goin g on is this: ther e are some propertie s that are intrinsic : they belon g to
the very substanc e of the object , rathe r than to the objec t as a whole . Whe n you cut somethin g in
half, the two piece s retai n the same set of intrinsi c properties—thing s like density , boilin g point ,
flavor , color , ownership , and so on. On the othe r hand , extrinsi c propertie s are the opposite :
propertie s such as weight , length , shape , function , and so on are not retaine d unde r subdivision .
A class of object s that include s in its definitio n only intrinsic  propertie s is then a substance ,
or mas s noun ; a class that include s any extrinsi c propertie s in its definitio n is a coun t noun . The
categor y Stuff  is the mos t genera l substanc e category , specifyin g no intrinsi c properties . The
categor y Thing  is the mos t genera l discret e objec t category , specifyin g no extrinsi c properties .
It follow s that an objec t belong s to both mass and coun t classes . For example , LakeMichigan
is an elemen t of both Water  and Lakes.  Lake  specifie s such extrinsi c propertie s as (approximate )
size, topologica l shape , and the fact that it is surrounde d by land . Not e that we can also handl e
the fact that the wate r in Lake Michiga n change s over time , simpl y by viewin g the lake as an
even t whos e constituen t object s chang e over time , in muc h the same way as President(USA).
This approac h to the representatio n of stuff is not the only possibility . The majo r competin g
approac h consider s Butter  to be wha t we woul d call BunchOf  (Butter),  namely , the objec t com ­
posed of all butte r in the world . All individua l butter­object s are thus PartOf  butter , rathe r than
instance s of butter . Lik e any consisten t knowledg e representatio n scheme , it canno t be proved
incorrect,  but it does seem to be awkwar d for representin g specialize d kind s of substance s such
as UnsaltedButter  and its relatio n to Butter.
Menta l event s and menta l object s
The agent s we have constructe d so far have belief s and can deduc e new beliefs . Yet none of them
has any knowledg e about  belief s or deduction . For single­agen t domains , knowledg e abou t one' s
own knowledg e and reasonin g processe s is usefu l for controllin g inference . For example , if one
know s that one does not know anythin g abou t Romania n geography , then one need not expen d
enormou s computationa l effor t tryin g to calculat e the shortes t path from Arad to Bucharest . In
244 Chapte r 8. Buildin g a Knowledg e Base
PROPOSITIONA L
ATTITUDE S
REFERENTIA L
TRANSPARENC Y
OPAQU E'multiagen t domains , it become s importan t for an agen t to reaso n abou t the menta l processe s of
the othe r agents . Suppos e a shoppe r in a supermarke t has the goal of buyin g som e anchovies .
The agen t deduce s that a good plan is to go wher e the anchovie s are, pick som e up, and brin g
them to the checkou t stand . A key step is for the shoppe r to realiz e that it canno t execut e this plan
until it know s wher e the anchovie s are, and that it can com e to kno w wher e they are by askin g
someone . The shoppe r shoul d also deduc e that it is bette r to ask a store employe e than anothe r
customer , becaus e the employe e is more likel y to know the answer . To do this kind of deduction ,
an agen t need s to have a mode l of wha t othe r agent s know , as well as som e knowledg e of its own
knowledge , lack of knowledge , and inferenc e procedures .
In effect , we wan t to have a mode l of the menta l object s that are in someone' s head (or
something' s knowledg e base ) and of the menta l processe s that manipulat e those menta l objects .
The mode l shoul d be faithful , but it does not have to be detailed . We do not have to be able to
predic t how man y millisecond s it will take for a particula r agen t to mak e a deduction , nor do we
have to predic t wha t neuron s will fire whe n an anima l is face d with a particula r visua l stimulus .
But we do wan t an abstrac t mode l that says that if a logica l agen t believe s P V Q and it learn s
­i/3, then it shoul d com e to believ e Q.
The first step is to ask how menta l object s are represented . Tha t is, if we have a relatio n
Believes(Agent,x),  wha t kind of thin g is xl Firs t of all, its clear that x canno t be a logica l sentence .
If FUes(Superman)  is a logica l sentence , we can' t say Believes(Agent,  Flies(Superman))  becaus e
only term s (not sentences ) can be argument s of relations . But if Flies(Superman)  is reifie d as a
fluent , then it is a candidat e for bein g a menta l object , and Believes  can be a relatio n that takes an
agen t and a prepositiona l fluen t that the agen t believe s in. We coul d defin e othe r relation s such
as Knows  and Wants  to expres s othe r relationship s betwee n agent s and propositions . Relation s
of this kind are calle d propositiona l attitudes .
This appear s to give us wha t we want : the abilit y for an agen t to reaso n abou t the belief s
of agents . Unfortunately , ther e is a proble m with this approach . If Clar k and Superma n are one
and the same  (i.e. , Clark  = Superman)  then Clar k flyin g and Superma n flyin g are one and the
same event . Thus , if the objec t of propositiona l attitude s are reifie d events , we mus t conclud e
that if Lois believe s that Superma n can fly, she also believe s that Clar k can fly, even if she doesn' t
believ e that Clar k is Superman . Tha t is,
(Superman  = Clark)  |=
(Believes(Lc)is,  Flies(Superman))  <= > Believes(Lois,  Flies(Clark)))
There is a sens e in whic h this is right : Lois does believ e of a certai n person , who happen s to be
called Clar k sometimes , that that perso n can fly. But there is anothe r sense in whic h this is wrong :
if you aske d Lois "Can Clar k fly? " she woul d certainl y say no. Reifie d object s and event s wor k
fine for the first sens e of Believes,  but for the secon d sens e we need to reify descriptions  of those
object s and events , so that Clar k and Superma n can be differen t description s (eve n thoug h they
refer to the sam e object) .
Technically , the propert y of bein g able to freel y substitut e a term for an equa l term is calle d
referentia l transparency . In first­orde r logic , ever y relatio n is referentiall y transparent . We
woul d like to defin e Believes  (and the othe r propositiona l attitudes ) as relation s whos e secon d
argumen t is referentiall y opaque —tha t is, one canno t substitut e an equa l term for the secon d
argumen t withou t changin g the meaning .
Sectio n 8.4. Genera l Ontolog y 245
SYNTACTI C THEOR Y W e will concentrat e on wha t is calle d a syntacti c theor y of menta l objects. " In this
STRiNGS approach , we represen t menta l object s with string s writte n in a representatio n language . (W e
will use first­orde r logic itsel f as the representatio n language , but we are not require d to.) A strin g
is just a list of symbols , so the even t Flies(Clark)  can be represente d by the strin g of character s
[F,l,i,e,s,(,C,l,a,r,k,)\,  whic h we wil l abbreviat e as "Flies(Clark)".  In this formulation ,
"Clark"  ^"Superman"  becaus e they are two differen t string s consistin g of differen t symbols . The
idea is that a knowledge­base d agen t has a knowledg e base consistin g of string s that were adde d
eithe r via TLL L or throug h inference . The syntacti c theor y model s the knowledg e base and the
string s that are in it.
Now all we hav e to do is provid e a syntax , semantics , and proo f theor y for the strin g
representatio n language , just as we did in Chapte r 6. The differenc e is that we hav e to defin e
them all in first­orde r logic . We start by definin g Den  as the functio n that map s a strin g to the
objec t that it denotes , and Name  as a functio n that map s an objec t to a strin g that is the nam e of
a constan t that denote s the object . For example , the denotatio n of both "Clark"  and "Superman"
is the objec t referre d to by the constan t symbo l ManOfSteel,  and the nam e of that objec t coul d
be eithe r "Superman",  "Clark",  or or som e othe r constant , such as "K\ \".
Den("Clark")  ­ ManOfSteel  A Den("Supennan")  = ManOfSteel
Name(ManOfSteel)  ­"K\ ,"
The next step is to defin e inferenc e rule s for logica l agents . For example , we migh t wan t to say
that a logica l agen t can do Modu s Ponens : if it believe s /; and believe s p => q then it will also
believ e q. The first attemp t at writin g this axio m is
V a,p,  q LogicalAgent(a)  A Believes(a,p)  A Believesia,  "p => q") => Believes(a,  q)
But this is not righ t becaus e the strin g "p=>c/ " contain s the letter s 'p' and 'q' but has nothin g
to do with the string s that are the value s of the variable s p and q. In fact , "p => q" is not even
a syntacticall y correc t sentence , becaus e only variable s can be lower­cas e letters . The correc t
formulatio n is:
V a,p,  q LogicalAgent(a)  A Believes(a,p)  A Believes(a,  Concat(p,  =>, q)
=> Believesia,  q)
wher e Concat  is a functio n on string s that concatenate s their element s together . We will abbreviat e
Concat(p,  ["=>"J , q) as "/? =>• q". Tha t is, an occurrenc e of x_ withi n a strin g mean s to substitut e in
the valu e of the variabl e x. Lisp programmer s will recogniz e this as the backquot e operator .
Once we add in the othe r inferenc e rule s beside s Modu s Ponens , we wil l be able to
answe r question s of the form "give n that a logica l agen t know s thes e premises , can it draw that
conclusion? " Beside s the norma l inferenc e rules , we need som e rule s that are specifi c to belief .
For example , the followin g rule says that if a logica l agen t believe s something , then it believe s
that it believe s it.
Va,/ 7 LogicalAgent(a)/\Believes(a,p)  => Believes(a,"Believes(Name(a),p)")
Note that it woul d not do to have just a as part of the string , becaus e a is an agent , not a descriptio n
of an agent . We use Name(a)  to get a strin g that name s the agent .
An alternativ e base d on moda l logic is covere d in the historica l note s section .
246 Chapte r 8. Buildin g a Knowledg e Base
LOGICA L
OMNISCIENC EThere are at leas t thre e direction s we coul d go from here . One is to recogniz e that it is
unrealisti c to expec t that ther e will be any real logica l agents . Suc h an agen t can, accordin g to
our axioms , deduc e any vali d conclusio n instantaneously . This is calle d logica l omniscience . It
woul d be mor e realisti c to defin e limite d rationa l agents , whic h can mak e a limite d numbe r of
deduction s in a limite d time . But it is very hard to axiomatiz e such an agent . Pretendin g that all
agent s are logicall y omniscien t is like pretendin g that all problem s with polynomia l time bound s
are tractable—i t is clearl y false , but if we are careful , it does not get us into too muc h trouble .
A secon d directio n is to defin e axiom s for othe r propositiona l attitudes . Th e relatio n
betwee n believin g and knowin g has been studie d for centurie s by philosopher s of the mind . It is
commonl y said that knowledg e is justifie d true belief . Tha t is, if you believ e something , and if it
is actuall y true , and if you have a proo f that it is true, then you know it. The proo f is necessar y
to preven t you from sayin g "I know this coin flip will com e up heads " and then takin g credi t for
being righ t whe n the coin does end up heads , whe n actuall y you just mad e a luck y guess . If you
accep t this definitio n of knowledge , then it can be define d in term s of belie f and truth :
Va,p  Knows(a,p)  O Believes(a,p)  A T(Den(p))  A T(Den(KB(a))  =>• Den(p))
This versio n of Knows  can be read as "know s that. " It is also possibl e to defin e othe r kind s of
knowing . For example , here is a definitio n of "knowin g whether" :
M a.p KnowsWhether(a,p)  O  Knowfi(a,p)  V Knows(a,"~^p^)
Continuin g our example , Lois know s whethe r Clar k can fly if she eithe r know s that Clar k can fly
or know s that he cannot .
The concep t of "knowin g what " is mor e complicated . On e is tempte d to say that an
agent know s wha t Bob' s phon e numbe r is if ther e is som e x for whic h the agen t know s
x­PhoneNumber(Bob).  But that is not right , becaus e the agen t migh t know that Alic e and Bob
have the sam e number , but not know wha t it is (i.e., PhoneNumber(Alice)­PhoneNumber(Bob)),
or the agen t migh t know that there is som e Skole m constan t that is Bob' s numbe r withou t knowin g
anythin g at all abou t it (i.e. , K23 = PhoneNumber(Bob)).  A bette r definitio n of "knowin g what "
says that the agen t has to know of some x that is a strin g of digit s and that is Bob' s number :
Vfl,f c KnowsWhat(a,"PhoneNumber(bJ")  o­
3 x Knows(a,  "x = PhoneNumber(b)")  A DigitString(x)
Of course , for othe r question s we have differen t criteri a for wha t is an acceptabl e answer . For
the questio n "wha t is the capita l of New York, " an acceptabl e answe r is a prope r name , "Albany, "
not somethin g like "the city wher e the state hous e is." To handl e this, we will mak e KnowsWhat
a three plac e relation : it take s an agent , a term , and a predicat e that mus t be true of the answer .
For example :
KnowsWhat(Agent,  Capital(NewYork),  ProperName)
KnowsWhat(Agent,  PhoneNumber(Bob),  DigitString)
A third directio n is to recogniz e that propositiona l attitude s chang e over time . Whe n
we recognize d that processe s occu r over a limite d interva l of time , we introduce d the relatio n
T(pmcess,  interval).  Similarly , we can use Believe(agent,  string,  interval)  to mea n that an agen t
believe s in a propositio n over a give n interval . For example , to say that Lois believe d yesterda y
that Superma n can fly, we writ e
Believes(Lois,  Flies(Superman),  Yesterday)
Sectio n 8.5. Th e Grocer y Shoppin g Worl d 247
Actually , it woul d be mor e consisten t to have Believes  be an even t fluen t just as Flies  is. The n
we coul d say that it will be true tomorro w that Lois knew that Superma n coul d fly yesterday :
T(Believes(Lois,  Flies(Superman),  Yesterday), Tomorrow)
We can even say that it is true now that Jimm y know s toda y that Lois believe s that Superma n
could fly yesterday :
T (Know  s( Jimmy,  Believes(Lois,  Flies(Superman),  Yesterday), Today),  Now)
KNOWLEDG E
PRECONDITION S
KNOWLEDG E
EFFECT SKnowledg e and actio n
We have been so busy tryin g to represen t knowledg e that there is a dange r of losin g track of what
knowledg e is/or . Recal l that we are intereste d in buildin g agent s that perfor m well . Tha t mean s
that the only way knowledg e can help is if it allow s the agen t to do som e actio n it coul d not
have done before , or if it allow s the agen t to choos e a bette r actio n than it woul d otherwis e have
chosen . For example , if the agen t has the goal of speakin g to Bob , then knowin g Bob' s phon e
numbe r can be a grea t help . It enable s the agen t to perfor m a dialin g actio n and have a muc h
bette r chanc e of reachin g Bob than if the agen t did not know the numbe r and diale d randomly .
One way of lookin g at this is to say that action s have knowledg e precondition s and
knowledg e effects . For example , the actio n of dialin g a person' s numbe r has the preconditio n of
knowin g the number , and the actio n of callin g director y assistanc e sometime s has the effec t of
knowin g the number .
Note that each actio n has its own requirement s on the form of the knowledge , just as each
questio n to KnowsWhat  had its own requirements . Suppos e I am in China , and the telephon e
there has Chines e numeral s on the buttons.12 The n knowin g wha t Bob' s numbe r is as a digi t
string is not enough— 1 need to kno w it as a strin g of Chines e digits . Similarly , the questio n
of whethe r I kno w wher e Bob lives has a differen t answe r dependin g on how I wan t to use the
information . If I'm plannin g to go thereb y taxi, all I need is an address ; if I'm drivin g myself , I
need directions ; if I'm parachutin g in, I need exac t longitud e and latitude .
THE GROCER Y SHOPPIN G WORL D
In this section , all our hard wor k in definin g a genera l ontolog y pays off: we will be able to
defin e the knowledg e that an agen t need s to shop for a mea l in a market . To demonstrat e that
the knowledg e is sufficient , we will run a knowledge­base d agen t in our environmen t simulator .
That mean s providin g a simulate d shoppin g world , whic h will by necessit y be simple r than the
real world . But muc h of the knowledg e show n here is the same for simulate d or real worlds . The
big difference s are in the complexit y of vision , motion , and tactil e manipulation . (Thes e topic s
will be covere d in Chapter s 24 and 25.)
12 Actually , Chines e phone s have Arabi c numerals , but bear with the example .
248 Chapte r 8. Buildin g a Knowledg e Base
Complet e descriptio n of the shoppin g simulatio n
We start by givin g a PAG E (percepts , actions , goals , and environment ) descriptio n of the shoppin g
simulation . Firs t the percepts :
1. The agen t receive s thre e percept s at each time step: feel , sound , and vision .
2. The feel percep t is just a bum p or no bump , as in the vacuu m world . The agen t perceive s
a bum p only whe n on the previou s time step it execute d a Forward  actio n and there is not
enoug h room in the locatio n it tried to mov e to.
3. The soun d percep t is a list of spoke n words . The agen t perceive s word s spoke n by agent s
withi n two square s of it.
4. If the agent' s camer a is zoome d in, it perceive s detaile d visua l image s of each objec t in the
squar e it is zoome d at.
5. If the agent' s camer a is not zoome d in, it perceive s coars e visua l image s of each objec t in
the three square s directl y and diagonall y ahead .
6. A visua l percep t consist s of a relativ e location , approximat e size, color , shape , and possibl y
some othe r features . It will be explaine d in detai l later .
Now for the actions :
1. An agen t can spea k a strin g of words .
2. An agen t can go one squar e forward .
3. An agen t can turn 90° to the righ t or left.
4. An agen t can zoom its camer a in at its curren t square , or at any of the three square s directl y
or diagonall y ahead .
5. An agen t can also zoom its camer a out.
6. An agen t can grab an objec t that is withi n one squar e of it. To do so, it need s to specif y
the relativ e coordinate s of the object , and it need s to be empty­handed .
7. An agen t can releas e an objec t that it has grabbed . To do so, it need s to specif y the relativ e
coordinate s of the poin t wher e it want s to releas e the object .
The agent' s goal initiall y will be to buy all the item s on a shoppin g list. This goal can be modifie d
if som e item s are unavailabl e or too expensive . The agen t shoul d also try to do the shoppin g
quickly , and avoi d bumpin g into things . A mor e ambitiou s proble m is to give the agen t the goal
of makin g dinner , and let it compos e the shoppin g list.
The environmen t is the interio r of a store , alon g with all the object s and peopl e in it. As
in the vacuu m and wumpu s worlds , the store is represente d by a grid of squares , with aisle s
separatin g row s of displa y cases . At one end of the stor e are the checkou t stand s and thei r
attendan t clerks . Othe r customer s and store employee s may be anywher e in the store . The agen t
begin s at the entrance , and mus t leave the store from the same square . Ther e is an EXI T sign there
in case the agen t forgets . Ther e are also sign s markin g the aisles , and smalle r sign s (readabl e
only whe n the camer a zoom s in) markin g som e (but not necessaril y all) of the item s for sale.
A real agen t woul d have to deciphe r the vide o signal s from the camer a (or som e digitizatio n
of them) . We assum e that this wor k has alread y been done . Still , the visio n componen t of a
percep t is a comple x list of descriptions . The first componen t of each descriptio n is its relativ e
Sectio n 8.5. Th e Grocer y Shoppin g Worl d 249
positio n with respec t to the agent' s positio n and orientation . For example , the relativ e positio n
[­2,1] is the squar e two square s to the agent' s left and one squar e ahead . The secon d componen t
is the size of the object , give n as the averag e diamete r of the objec t in meters . Nex t is the colo r
of the object , give n as a symbo l (red, green , yellow , orange , ...), followe d by the object' s shap e
(flat, round , square , ...). Finally , we assum e that an optica l characte r recognitio n routin e has
run over the vide o image ; if there are any letter s in the visua l field , they are give n as a list of
words . Figur e 8.8 show s an overvie w of a supermarket , with the agen t at [4,5] still dresse d for the
wumpu s world . The agen t is facin g left. Figur e 8.9(a ) show s wha t the agen t perceive s with the
camer a zoome d out, and Figur e 8.9(b ) show s the agent' s visua l percept  with the camer a zoome d
in at the square  [3,6] .
Organizin g knowledg e
The grocer y shoppin g domai n is too big to handl e all at once . Instead , we will brea k it dow n
into smalle r cluster s of knowledge , wor k on each cluste r separately , and then see how they fit
together . One good way of decomposin g the domai n into cluster s is to conside r the task s facin g
the agent . This is calle d a functiona l decomposition . We divid e the domai n into five clusters :
<> Men u Planning : The agen t will need to know how to modif y the shoppin g list whe n the
store is out of stock of an item .
0 Navigating : As in the wumpu s world , the agen t will need to understan d the effec t of
movemen t action s and creat e an interna l map of the world .
0 Gathering : The agen t mus t be able to find and gathe r the item s it wants . Par t of this
involve s inducin g object s from percepts : the agen t will need recognitio n rules to infe r that
a red roughl y spherica l objec t abou t three inche s in diamete r coul d be a tomato .
0 Communicating : The agen t shoul d be able to ask question s whe n there is somethin g it
canno t find out on its own .
0 Paying : Eve n a shy agen t that prefer s not to ask question s will need enoug h interagen t
skills to be able to pay the checkou t clerk . The agen t will need to know that $5.0 0 is too
much for a singl e tomato , and that if the total price is $17.35 , then it shoul d receiv e $2.6 5
in chang e from a $20 bill.
An advantag e of functiona l decompositio n is that we can pose a proble m completel y withi n a
cluste r and see if the knowledg e can solv e it. Othe r kind s of decompositio n often requir e the
whol e knowledg e base to be fleshe d out befor e the first questio n can be posed .
Menu­Plannin g
A good cook can walk into a market , pick out the best bargai n from the variou s fish , fowl , or
other main cours e ingredient s that look fresh that day, selec t the perfec t accompanyin g dishes ,
and simultaneousl y figur e out how to mak e tomorrow' s mea l from whateve r will be left over . A
competen t erran d runne r can take a shoppin g list and find all the item s on it. Our agen t will be
close r to the secon d of these,  but we will give it som e abilit y to mak e intelligen t choices .
Suppos e the store is out of tomatoe s one day. An agen t with the shoppin g list "tomatoes ,
lettuce , cucumber , oliv e oil, vinegar " shoul d recogniz e that the ingredient s form a salad , and that
250 Chapte r 8. Buildin g a Knowledg e Base
'%
••«*
00 m
B«>*•
Vege ­
tablesFruit Soup
Sauce sMeat
JQ.
8
Figur e 8.8 A n overvie w of a supermarket . Not e the agen t at [4,5] , the othe r shopper s at [2,2 ]
and [6,2] , the checkou t clerk s at [4,2 ] and [8,2] , the sign s in the fourt h row, and the grocerie s
sprea d throughou t the world .
a red peppe r woul d be a good substitute , as it woul d add colo r and flavo r to the salad . An agen t
with the list "tomatoes , yello w onions , celery , a carrot , groun d beef , milk , whit e wine , tagliatelle "
shoul d infe r a Bolognes e sauc e (Hazan , 1973) , and therefor e that it is appropriat e to substitut e
canne d tomatoes .
To mak e thes e inferences , an agen t need s to understan d that the item s on a shoppin g list fit
togethe r to form one or mor e composit e object s know n as dishes , that the dishe s go togethe r to
Sectio n 8.5. Th e Grocer y Shoppin g Worl d 251
m •*•*[1,1], 0.87, [Orange,With(White,Red)] , Blob
[0,1],0.92 , [Red,With(White)] , Blob[1.9,1.9] , .07, [Orange] , Roun d
[1.8,1.3] , .09,[White,With(Black)] , Rectangle ,
Words([0ranges,$,.75,/,lb] )
[1.9,1.1] , .08, [Orange] , Roun d
[1.7,1.7] , .07, [Orange] , Roun d
[1.3,1.2] , .07, [Red] , Roun d
[1.1,1.3] , .06, [Red] , Roun d
(a) Zoome d Out (b) Zoome d In at [3,6]
Figur e 8.9 Th e percept s for the shoppin g agen t at [4,5] . (a) Camer a zoome d out. (b) Camer a
zoome d in at [3,6] . Eac h percep t is a serie s of objec t description s (one per line) . Eac h descriptio n
lists a relativ e position , a size, a colo r summary , a shape , and a characte r strin g (if any) .
form composit e object s know n as meals , and that an objec t can be recognize d by its components .
Our agen t will be calle d upon to mak e two classe s of inference . First , from a list of parts it shoul d
induc e the composit e objec t that these part s mak e up. Thi s is mad e difficul t becaus e the parts
(the item s on the shoppin g list) may mak e up severa l composit e objects , and becaus e not all the
parts will be liste d (som e are alread y at hom e in the cupboard) . Second , the agen t shoul d be able
to decid e how to replac e an unavailabl e part to complet e the intende d composit e object . This can
be don e at two levels : replacin g one ingredien t with anothe r to complet e a dish , and if that is
not possible , replacin g the whol e dish with anothe r to complet e the meal . Som e of the necessar y
knowledg e will involv e individua l dishes , and som e of it will be at a genera l leve l that will also
be usefu l for, say, replacin g a fault y muffle r in a car.
The first step is to conver t a shoppin g list— a list of words—int o a parts list—a  list of
categories . A dictionar y is used to associat e word s with their referents :
Referent("tomatoes",Tomatoes)
Referent("  onions",  Onions)
The next step is to describ e object s in term s of their require d and optiona l parts . If we
wante d to actuall y prepar e a dish , we woul d have to know more abou t the relation s betwee n the
parts . Bu t to do shopping , all we need is a list of the parts . We defin e RequiredParts  so that
RequiredParts({Lettuce,  Dressing},  GreenSalads)  mean s that ever y objec t that is an elemen t of
GreenSalads  has one RequiredPart  that is an elemen t of Lettuce,  and anothe r that is an elemen t
of Dressing.  For lettuc e to be a require d part of gree n salad s mean s that ever y elemen t of gree n
salad s has an elemen t of lettuc e as one of its parts . Simila r reasonin g hold s for OptionalParts,
excep t that only som e element s of a categor y have to manifes t the optiona l parts .
Vr,w RequiredParts(r,w)  => Mp p£r  =>• RequiredPart(p,w)
Vo, w OptionalParts(o,w)  =>• V/? p £ o  =>• OptionalPart(p,w)
V r, w RequiredPart(r,  w) O V c c e w =$• 3i i 6 r A PartOf(i,  c)
Vo, w OptionalPart(o,w)  O 3c c&w  => 3; /' € o A PartOf(o,  c)
Chapte r 8. Buildin g a Knowledg e Base
The next step is to describ e meal s and dishe s in term s of thei r parts :
RequiredParts({MainCourses\,  Meals)
Optional  Parts({FirstCourses,  SideDishes,  Salads,Desserts,...},Meals)
RequiredParts({Lettuce,  Dressing},  GreenSalads)
Optional  Part s({Tomatoes,  Cucumbers,  Peppers,  Carrots,...},  GreenSalads)
RequiredParts({Pasta,  BologneseSauce},  PastaBolognese,)
Optional  Parts(  { GratedCheese],  PastaBolognese)
RequiredParts(  {Onions,  OliveOil,  Butter,  Celery, Carrots,  GroundBeef,  Salt,
WhiteWines,Milk,  TomatoStuff},  BologneseSauce)
Then we need taxonomi c informatio n for dishe s and foods :
GreenSalads  C Salads
Salads  C Dishes
PastaBolognese  C FirstCourses
FirstCourses  C Dishes
Tomatoes  C TomatoStuff
CannedTomatoes  C TomatoStuff
Tagliatelle  C Pasta
Now we wan t to be able to determin e wha t dishe s can be mad e from the shoppin g list "tomatoes ,
yello w onions , celery , a carrot , groun d beef , milk , whit e wine , tagliatelle. " As mentione d earlier ,
this is complicate d by the fact that the salt, butter , and olive oil that are require d for the Bolognes e
dish are not on the shoppin g list. We defin e the predicat e CanMake  to hold betwee n a shoppin g
list and a dish if the categorie s on the list, whe n combine d with typica l staples , cove r all the
require d parts of the dish . >A '
V /, d CanMake(l,  d) O d e Dishes  A Required  Part  s(p, d)  A p C Union(l,  Staples)
{Salt,  Butter,  OliveOil}  C Staples
With wha t we have so far, this woul d allo w us to infe r that Past a Bolognes e is the only dish
that can be mad e from the shoppin g list. The nex t questio n is wha t to do if fresh tomatoe s are
not available . It turn s out that all we have to do is replac e the shoppin g list (or the part of the
shoppin g list that make s up this dish ) with the list of part categorie s for this dish . In this case , that
mean s that Tomatoes  woul d be replace d by TomatoStuff,  whic h coul d be satisfie d by gatherin g
an instanc e of CannedTomatoes.
Navigatin g
An agen t that want s to find a book in a librar y coul d travers e the entir e library , lookin g at each
book unti l the desire d one is found . But it woul d be mor e efficien t to find the call numbe r for the
book , find a map describin g wher e that numbe r can be found , and go directl y there . It is the same
way with a supermarket , althoug h the catalo g syste m is not as good . A shoppin g agen t shoul d
know that supermarket s are arrange d into aisles , that aisle s have sign s describin g thei r content s
(in roug h terms) , and that object s that are near each othe r in the taxonomi c hierarch y are likel y to
be near each othe r in physica l space . For example , Britis h immigrant s to the Unite d State s learn
that to find a packag e of tea, they shoul d look for the aisle marke d "Coffee. "
Sectio n 8.5. Th e Grocer y Shoppin g Worl d 253
Most of the navigatio n proble m is the sam e as in the vacuu m or wumpu s world . The agen t
needs to remembe r wher e it started , and can comput e its curren t locatio n from the movement s it
has mad e wher e it is now . The supermarke t is not as hostil e as the wumpu s worl d so it is safe r
to explore , but an agen t can find bette r route s if it know s that supermarket s are generall y laid
out in aisles . Supermarket s also provid e aisle numbers , unlik e the wumpu s world , so that the
agen t does not need to rely on dead reckoning . On the secon d trip to a store , it can save a lot of
wanderin g by rememberin g wher e thing s are. It is not helpful , however , to remembe r the exac t
locatio n of each individua l item , becaus e the tomat o that is at locatio n [x,y]  toda y will probabl y
be gone tomorrow . Becaus e muc h of the logi c is the sam e as for the wumpu s world , it will not
be repeate d here .
A typica l navigatio n proble m is to locat e the tomatoes . The followin g strateg y is usuall y
believe d to work :
1. If the agen t know s the locatio n of the tomatoe s from a previou s visit , calculat e a path to
that spot from the curren t location .
2. Otherwise , if the locatio n of the vegetabl e aisle is known , plan a path there .
3. Otherwise , mov e alon g the fron t of the store unti l a sign for the vegetabl e aisle is spotted .
4. If none of these work , wande r abou t and find someon e to ask wher e the tomatoe s are. (Thi s
is covere d in the "Communicating " section. )
5. Onc e the vegetabl e aisle is found , mov e dow n the aisl e with the camer a zoome d out,
lookin g for somethin g red. Whe n spotted , zoom in to see if they are in fact tomatoes . (Thi s
is covere d in the "Gathering " section. )
Gatherin g
Once in the righ t aisle , the agen t still need s to find the item s on its list. This is done by matchin g
the visua l percept s agains t the expecte d percept s from each categor y of objects . In the wumpu s
world , this kind of perceptio n is trivial— a breez e signal s a pit and a stenc h signal s a wumpus .
But in the grocer y shoppin g world , there are thousand s of differen t kind s of object s and man y of
them (suc h as tomatoe s and apples ) presen t simila r percepts . The agen t neve r can be sure that it
has classifie d an objec t correctl y base d on its percepts , but it can know whe n it has mad e a good
guess . The shoppin g agen t can use the followin g classificatio n rules :
1. If only one know n categor y matche s a percept , assum e the objec t is a membe r of that
category . (Thi s may lead to an erro r whe n an unknow n objec t is sighted. )
2. If a percep t matche s severa l categories , but ther e is a sign nearb y that identifie s one of
them , assum e the objec t is a membe r of that category .
3. If ther e is an aisle sign that identifie s one categor y (or one supercategory) , assum e the
objec t is of that category . For example , we coul d categoriz e a round , red percep t as a
tomat o rathe r than an appl e if we were in an aisle marke d "Vegetables " and not "Fruit. " At
a cricke t match , it woul d be somethin g else altogether .
254 Chapte r 8. Buildin g a Knowledg e Base
To implemen t this, we start with a set of causal  rules for percepts :
V x x G Tomatoes
\fx x£  Oranges  •
V;c x£ Apples  =>
\l x xE Tomatoes
V x x G Oranges  •=> SurfaceColor(x,  Red)
=>• SurfaceColor(x,  Orange)
SurfaceColor(x,  Red)  V SurfaceColor(x,  Green)
=> Shape(x,  Round)
3­ Shape(x,  Round)
DOMAI N CLOSUR EV* SurfaceColor(x,  c) A Visible(x)  => CausesColorPercept(x,  c)
MX Shape(x,  s) A Visible(x)  =>• CausesShapePercept(x,  s)
These rules , and man y more like them , give a flavo r of a causa l theor y of how percept s are forme d
by object s in the world . Notic e how simplisti c it is. For example , it does not mentio n lightin g at
all. (Fortunately , the light s are alway s on in our supermarket. ) From these rules , the agen t will
be able to deduc e a set of possibl e object s that migh t explai n its percepts . Knowledg e abou t what
sorts of object s appea r wher e will usuall y eliminat e all but one category . Of course , the fact that
the agen t only knows  about  one sort of objec t that migh t produc e a give n percept  does not mean
that the percep t mus t be produce d by that sort of object . Logically , there migh t be othe r sorts of
object s (e.g. , plasti c tomatoes ) that produc e the same percep t as the know n category . Thi s can
be handle d eithe r by a domai n closur e axiom , statin g that the know n categorie s are all the ones
there are, or by a defaul t assumption , as describe d in Chapte r 15.
The othe r part of the gatherin g proble m is manipulation : bein g able to pick up object s and
carry them . In our simulation , we just assum e a primitiv e actio n to gras p an object , and that the
agent can carry all the item s it will need . In the real world , the action s require d to pick up a
bunc h of banana s withou t bruisin g them and a gallo n jug of milk withou t droppin g it pose seriou s
problems . Chapte r 25 consider s them in more detail .
Communicatin g
The successfu l shoppe r know s whe n to ask question s (Wher e are the anchovies ? Are these ;
tomatillas?) . Unfortunately , bein g able to carr y on a conversatio n is a difficul t task , so we
will dela y coverin g it unti l Chapte r 22. Instead , we will cove r a simple r form of one­wa y
communication : readin g signs . If a wor d appear s on an aisle' s sign , then member s of the>
categor y that the word refer s to will be locate d in that aisle .
Va (a e Aisles  f\3s,w  SignOf(s,  a) A w G Words(s))  =>
3 x, c Referent(w,  c) A A: G c A At(x, a)
If a word appear s on a smal l sign , then item s of that categor y will be locate d nearby .
V s, w, I (s G Signs  A Size(s)  < Meters(3)  A w  G Words(s)  A At(s,  I)) =>
3 x, c Referent(w,  c) A x G c A At(x,  AreaAround(l))
Sectio n 8.5. Th e Grocer y Shoppin g Worl d 255
Payin g
The shoppin g agen t also has to know enoug h so that it will not overpa y for an item . First , it need s
to know typica l fair price s for items , for example :
Vg gETypical(GmundBeef)f\  Weight(g)  = Pounds(l)  => $(1 ) < FairPrice(g)  < $(2 )
The agen t shoul d know that total price is roughl y proportiona l to quantity , but that often discount s
are give n for buyin g large r sizes . The followin g rule says that this discoun t can be up to 50% :
V q, c, w, p q G c A Weight(q)  — w A Price(q)  = p =>
V m, q2 m > 1 A qi £ c A Weight(qi)  = m x w =>
(1 + ^p1) x p < FairPrice(q 2) < m x p
Most importantly , the agen t shoul d know that it is a bad deal to pay more than the fair price for
an item , and that buyin g anythin g that is a bad deal is a bad action :
V / Priced)  > FairPrice(i)  => BadDeal(i)
W BadDeal(i)  => Ma  Bad(Buy(aJ))
Buyin g event s belon g to the categor y Buy(b,x,s,p)— buyer  b buyin g objec t x from selle r s for
price p. The complet e descriptio n of buyin g is quit e complex , but follow s the genera l patter n
laid dow n earlie r in the chapte r for marriage . The precondition s includ e the fact that p is the
price of the objec t x', that b has at leas t that muc h mone y in the form of one or mor e monetar y
instruments ; and that s own s x. The even t include s a monetar y exchang e that result s in a net gain
of p for s, and finall y b own s x. Exercis e 8.10 asks you to complet e this description .
One fina l thin g an agen t need s to know abou t shopping : it is bad form to exit a shop whil e
carryin g somethin g that the shop owns .
V a, x,.?,  / .v £ Shops  A T(Carrying(a,  x) A At(x,  s) A Owns(s,  x)) =>
T(Bad(Exit(a)),  i)
An agen t with the goal of exitin g will use this goal to set up the subgoa l of ownin g all the object s
it is carrying . So all we need now is a descriptio n of the parts of a buyin g event , so that the agen t
can execut e the buying . In a supermarket , a buyin g even t consist s of goin g to a checkou t stand ,
placin g all the item s on the stand , waitin g for the cashie r to ring them up, placin g a sum of mone y
equal to the total price on the stand , and pickin g up the item s again . Not e that if the total is $4, it
will not do to plac e the sam e dolla r bill onto the checkou t stand four times .
Vb,m,s,p, e eg SupermarketBuy(b,m,s,p)  =>
3 e\, 62, €3,64,  e=, e\ = Go(b,  c) A CheckoutStand(c)  A
€2 = Put(b,  m, c) A e?, = TotalUpPrice(s,  m) A
€4 = Put(b,  p, c) A 65 = Grab(b,  m) A
Before(e\,  e­i) A Before(e2,e^)  A Before(e^,e4)  A Before(ea,,  £5) A
PartOf(e\,  e) A PartOf(e 2, e) A PartOf(e 3, e) A PartOf(e 4, e) A PartOf(e 5, e)
Now we have touche d on all the majo r area s of knowledg e necessar y for an agen t to cope
with the grocer y shoppin g world . A complet e specificatio n woul d mak e this chapte r too long ,
but we have outline d the approac h one woul d take to complet e this specification . Althoug h it is
hard work , buildin g actua l knowledg e base s of this kind is an invaluabl e experience .
256 Chapte r 8. Buildin g a Knowledg e Base
8.6 SUMMAR Y
This has been the most detaile d chapte r of the book so far. As we said earlie r in the chapter , one
cannot  understan d knowledg e representatio n withou t doin g it, or at least seein g it. The followin g
are som e of the majo r point s of the chapter :
• The proces s of representin g knowledg e of a domai n goes throug h severa l stages . The first ,
informa l stage involve s decidin g wha t kind s of object s and relation s need to be represente d
(the ontology) . The n a vocabular y is selected , and used to encod e genera l knowledg e of
the domain . Afte r encodin g specifi c proble m instances , automate d inferenc e procedure s
can be used to solv e them .
• Goo d representation s eliminat e irrelevan t detail , captur e relevan t distinctions , and expres s
knowledg e at the mos t genera l leve l possible .
• Constructin g knowledge­base d system s has advantage s over programming : the knowledg e
enginee r has to concentrat e only on what' s true abou t the domain , rathe r than on solvin g
the problem s and encodin g the solutio n process ; the same knowledg e can often be used in
severa l ways ; debuggin g knowledg e is often simple r than debuggin g programs .
• Special­purpos e ontologies , such as the one constructe d for the circuit s domain , can be
effectiv e withi n the domai n but often need to be generalize d to broade n their coverage .
• A general­purpos e ontolog y need s to cove r a wide variet y of knowledge , and shoul d be
capabl e in principl e of handlin g any domain .
• We presente d a genera l ontolog y base d aroun d categorie s and the even t calculus . We
covere d structure d objects , time and space , change , processes , substances , and beliefs .
• We presente d a detaile d analysi s of the shoppin g domain , exercisin g the genera l ontolog y
and showin g how the domai n knowledg e can be used by a shoppin g agent .
Finally , it is wort h recallin g that the natur e of an appropriat e representatio n depend s on the worl d
being represente d and the intende d rang e of uses of the representation . The representatio n choice s
in this chapte r are specifi c to the worl d of huma n experience , but this is unavoidable .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
There are plausibl e claim s (Briggs , 1985 ) that forma l knowledg e representatio n researc h bega n
with classica l India n theorizin g abou t the gramma r of Shastri c Sanskrit , whic h date s back to the
first millenniu m B.C. Shastri c Sanskri t grammatica l theor y propose d not only a forma l synta x and
vocabular y for a general­purpos e language , but also provide d an analysi s of its semantics . Shastn c
Sanskri t grammatica l theor y therefor e can be regarde d as the earlies t instanc e of systemati c
representatio n of knowledg e in a specifi c area in orde r to facilitat e inference . In the West , the
use of definition s of term s in ancien t Gree k mathematic s can be regarde d as the earlies t instance .
Indeed , the developmen t of technica l terminolog y or artificia l language s in any field can be
regarde d as a form of knowledg e representatio n research . The connectio n betwee n knowledg e
Sectio n 8.6. Summar y 257
representatio n in this sense , and knowledg e representatio n in AI, is close r than it may seem ;
twentiet h centur y AI researc h draw s widel y upon the formalism s of othe r fields , especiall y logi c
and philosophy . Aristotl e (384­32 2 B.C. ) develope d a comprehensiv e syste m of what we woul d
now call ontolog y and knowledg e representatio n in connectio n with his wor k in logic , natura l
science , and philosophica l metaphysics .
Beside s the logicis t traditio n starte d by McCarthy , whic h we discusse d in Chapte r 6, there
have been man y othe r thread s in the histor y of representatio n in AI. Earl y discussion s in the
field tende d to focu s on "problem  representation " rathe r than "knowledge  representation. " The
emphasi s was on formulatin g the proble m to be solved , rathe r than formulatin g the resource s
availabl e to the program . A consciou s focu s on knowledg e representatio n had to awai t the dis­
cover y that high performanc e in AI proble m solvin g require d the accumulatio n and use of large
amount s of problem­specifi c knowledge . The realizatio n that AI system s neede d such knowl ­
edge was largel y drive n by two type s of research . The first was the attemp t to matc h huma n
performanc e in the everyda y world , particularl y in understandin g natura l huma n language s and
in rapid , content­base d retrieva l from a general­purpos e memory . The secon d was the desig n of
"exper t systems"—also , significantly , calle d "knowledge­base d systems"—tha t coul d matc h (or,
in som e cases , exceed ) the performanc e of huma n expert s on narrowl y define d tasks . The need
for problem­specifi c knowledg e was stresse d forcefull y by the designer s of DENDRAL , the first
exper t system , whic h interprete d the outpu t of a mass spectrometer , a type of instrumen t used for
analysi s of the structur e of organi c chemica l compounds . An earl y statemen t of the DENDRA L
philosophica l perspectiv e can be foun d in Feigenbaum , Buchanan , and Lederberg ( 1971) ; Lindsa y
et al. (1980 ) provid e a book­lengt h descriptio n of the DENDRA L project , alon g with a complet e
bibliograph y from 1964 to 1979 . Althoug h the succes s of DENDRA L was instrumenta l in bringin g
the AI researc h communit y as a whol e to realiz e the importanc e of knowledg e representation ,
the representationa l formalism s used in DENDRA L are highl y specifi c to the domai n of chemistry .
As exper t system s continue d to succee d and proliferate , exper t syste m researcher s becam e inter ­
ested in standardize d knowledg e representatio n formalism s and ontologie s that coul d reduc e the
difficult y of creatin g a new exper t syste m in yet anothe r previousl y unexplore d field . In so doing ,
they venture d into territor y previousl y explore d by philosopher s of scienc e and of language . The
disciplin e impose d in AI by the need for one' s theorie s to "work " has led to mor e rapi d and
deepe r progres s than was the case whe n these problem s were the exclusiv e domai n of philosoph y
(althoug h it has at time s also led to the repeate d reinventio n of the wheel) .
Researc h in memor y and natura l languag e processin g (NLP ) also had to deal with the
need for general­purpos e knowledg e representatio n language s from the very start . Indeed , Ros s
Quillian' s (1961 ) wor k on "semanti c networks " predate s DENDRAL . Becaus e of the need to
get heterogeneou s bodie s of knowledg e to interac t fruitfully , memor y and NLP researc h was
the origina l spar k for semanti c networks , frames , and othe r very genera l formalisms . Suc h
"knowledg e representatio n languages " are covere d in greate r detai l in Chapte r 10. The presen t
chapte r focuse s instea d on the conten t of the knowledg e itsel f and on the representationa l concept s
that are commo n to a numbe r of distinc t formalisms .
The creatio n of comprehensiv e taxonomie s or classification s dates back to ancien t times .
Aristotl e strongl y emphasize d classificatio n and categorizatio n schemes . His Organon,  a col­
lectio n of work s on logi c assemble d by his student s afte r his death , include d a treatis e calle d
Categories  in whic h he attempte d a comprehensiv e high­leve l classificatio n and also introduce d
258 Chapte r 8. Buildin g a Knowledg e Base
TEMPORA L LOGI Cthe use of genu s and specie s for lower­leve l classification , althoug h these term s did not have the
precis e and specificall y biologica l sens e whic h is now attache d to them . Our presen t syste m of
biologica l classification , includin g the use of "binomia l nomenclature " (classificatio n via genu s
and specie s in the technica l sense) , was invente d by the Swedis h biologis t Carolu s Linnaeus ,
or Carl von Linn e (1707­1778). , Lakof f (1987 ) present s a mode l of classificatio n base d on
prototype s rathe r than stric t categorica l boundaries .
Withi n moder n AI specifically , comprehensiv e taxonomie s have usuall y been develope d
as part of large project s that also include d researc h in othe r area s of knowledg e representation .
Thes e includ e the "commonsens e summer " projec t led by Jerry Hobb s (1985 ) and the knowledg e
representatio n portio n of the ensuin g TACITU S natura l languag e interpretatio n projec t (Hobbs ,
1986 ; Hobb s et al., 1990) , as well as the massiv e CYC projec t (Lena t and Guha , 1990) . The
taxonom y used in this chapte r was develope d by the authors , base d in part on thei r experienc e
in the CYC projec t and in part on work by Hwan g and Schuber t (1993 ) and Davi s (1990) . An .
inspirationa l discussio n of the genera l projec t of commonsens e knowledg e representatio n appear s
in Hayes' s (1978 ; 1985b ) "The Naiv e Physic s Manifesto. "
The philosophica l stud y of the part­whol e relatio n was initiate d by the Polis h logicia n i
Lesniewsk i (1916) , who was a hardcor e "nominalist " or skepti c abou t abstrac t entitie s such as i
sets and numbers . He intende d his "mereology " (the nam e is derive d from the Gree k word for \
"part" ) as a substitut e for mathematica l set theory . Althoug h mereolog y showe d promis e as a ^
way of analyzin g the distinctio n betwee n mass noun s and coun t nouns , Lesniewski' s publication s I
are extremel y difficul t to follow , becaus e they are writte n in a very idiosyncrati c forma l notatio n :
with (in som e cases ) almos t no natural­languag e commentary . A more readabl e expositio n and i
axiomatizatio n of mereolog y was provide d in 1940 by the philosopher s Nelso n Goodma n (another '
hardcor e nominalist ) and Henr y Leonar d underth e nam e of "the calculu s of individuals " (Leonard ;
and Goodman , 1940) . Goodman' s The Structure of  Appearance  (1977 ) applie s the calculu s of |
individual s to wha t in AI woul d be calle d knowledg e representation . Quin e (1960 ) also support s ]
the nominalis t view of substances . Harr y Bun t (1985 ) has provide d an extensiv e analysi s of its j
use in knowledg e representation .
Few if any AI researcher s have any problem s with abstrac t entities ; the pragmati c "Onto ­ i
logica l Promiscuity " endorse d by Hobb s (1985 ) in the articl e of that title is mor e typical . The ]
positio n adopte d in this chapter , in whic h substance s are categorie s of objects , was champione d 1
by Richar d Montagu e (1973) . It has also been adopte d in the CYC project . Copelan d (1993) j
mount s a seriou s but not invincibl e attack .
Severa l differen t approache s have been take n in the stud y of time and events . The oldes t :|
approac h is tempora l logic , whic h is a form of moda l logi c in whic h moda l operator s are used \
specificall y to refe r to the time s at whic h facts are true . Typically , in tempora l logic , "Dp" |
mean s "p will be true at all time s in the future, " and "Op " mean s "p will be true at some time in i
the future. " The stud y of tempora l logic was initiate d by Aristotl e and the Megaria n and Stoic j
school s in ancien t Greece .
In moder n times , Findla y (1941 ) was the first to conceiv e the idea of a "forma l calculus " for 1
reasonin g abou t time ; Findla y also sketche d a few propose d laws of tempora l logic . The furthe r j
developmen t of moder n tempora l logi c was carrie d out by a numbe r of researchers , including !
Arthu r Prio r (1967) . The moder n developmen t was actuall y strongl y influence d by historica l i
studie s of Megaria n and Stoi c tempora l logic . Burstal l (1974 ) introduce d the idea of using ]
Sectio n 8.6. Summar y 259
moda l logi c to reaso n abou t compute r programs . Soo n thereafter , Vaugha n Prat t (1976 ) designe d
dynami c logic , in whic h moda l operator s indicat e the effect s of program s or othe r actions . For
instance , in dynami c logic , if a is the nam e of a program , then "[a]/? " migh t mea n "p woul d
be true in all worl d state s resultin g from executin g progra m a in the curren t worl d state" , and
"(a )p" migh t mea n "p woul d be true in at least one worl d state resultin g from executin g progra m
Q in the curren t worl d state. " Dynami c logic was applie d to the actua l analysi s of program s by
Fische r and Ladne r (1977) . Pnuel i (1977 ) introduce d the idea of usin g classica l tempora l logi c
to reaso n abou t programs . Shoha m (1988 ) discusse s the use of tempora l logic in AI.
Despit e the long histor y of tempora l logic , the considerabl e mathematica l theor y buil t up
aroun d it, and its extensiv e use in othe r branche s of compute r science , AI researc h on tempora l
reasonin g has more often take n a differen t approach . A tempora l logic is usuall y conceptualize d
aroun d an underlyin g mode l involvin g events , worl d states , or tempora l intervals . The tendenc y
in AI has been to refe r to thes e events , states , and interval s directly , usin g term s that denot e
them , rathe r than indirectl y throug h the interpretatio n of the sentenc e operator s of tempora l
logic . Th e languag e used is typicall y eithe r first­orde r logi c or, in som e cases , a restricte d
algebrai c formalis m geare d towar d efficien t computatio n (but still capabl e of bein g embedde d
withi n first­orde r logic) . This approac h may allow for greate r clarit y and flexibilit y in some cases .
Also, tempora l knowledg e expresse d in first­orde r logic can be more easil y integrate d with othe r
knowledg e that has been accumulate d in that notation .
One of the earlies t formalism s of this kind was McCarthy' s situatio n calculus , mentione d
in Chapte r 7. McCarth y (1963 ) introduce d situationa l fluent s and mad e extensiv e use of them
in later papers . Recen t wor k by Raymon d Reite r (1991 ) and other s in the "cognitiv e robotics "
projec t at the Universit y of Toront o (Scher l and Levesque , 1993 ) has re­emphasize d the use
of situatio n calculu s for knowledg e representation . Th e relationshi p betwee n tempora l logi c
and situatio n calculu s was analyze d by McCarth y and Haye s (1969) . The y also brough t to
the attentio n of AI researcher s the wor k of philosopher s such as Donal d Davidso n on events .
Davidson' s research , collecte d in (Davidson , 1980) , had a heav y emphasi s on the analysi s
of natura l language , particularl y of the adverb . It has strongl y influence d late r AI research ,
particularl y in natura l languag e understanding . Othe r philosophica l and linguisti c approache s to
event s that are of significanc e for AI researc h are thos e of Zeno Vendle r (1967 ; 1968) , Alexande r
Mourelato s (1978) , and Emmo n Bach (1986) .
Jame s Alien' s introductio n of time intervals , and a small , fixe d set of relationship s betwee n
them , as the primitive s for reasonin g abou t time (Alien , 1983 ; Alien , 1984 ) marke d a majo r
advanc e over situatio n calculu s and othe r system s base d on time point s or instantaneou s events .
Preliminar y version s of Alien' s work were availabl e as technica l report s as early as 1981 . Pete r
Ladki n (1986a ; 1986b ) introduce d "concave " time interval s (interval s with gaps ; essentially ,
union s of ordinar y "convex " time intervals ) and applie d the technique s of mathematica l abstrac t
algebr a to time representation . Alie n (1991 ) systematicall y investigate s the wid e variet y of
technique s currentl y availabl e for time representation . Shoha m (1987 ) describe s the reificatio n
of event s and sets fort h a nove l schem e of his own for the purpose . The term "even t calculus "
is also used by Kowalsk i and Sergo t (1986) , who show how to reaso n abou t event s in a logi c
programmin g system .
The syntacti c theor y of menta l object s was firs t studie d in dept h by Kapla n and Mon ­
tague (1960) , who showe d that it led to paradoxe s if not handle d carefully . Becaus e it has a
260 Chapte r 8. Buildin g a Knowledg e Base
natura l mode l in term s of belief s as physica l configuration s of a compute r or a brain , it has been
popula r in AI in recen t years . Konolig e (1982 ) and Haa s (1986 ) used it to describ e inferenc e
engine s of limite d power , and Morgenster n (1987 ) showe d how it coul d be used to describ e knowl ­
edge precondition s in planning . The method s for plannin g observatio n action s in Chapte r 13 are
based on the syntacti c theory .
MODA L LOGI C Moda l logi c is the classica l metho d for reasonin g abou t knowledg e in philosophy . Moda l
logic augment s first­orde r logi c wit h moda l operators , suc h as B (believes ) and K (knows) ,
that take sentences  as argument s rathe r than terms . The proo f theor y for moda l logi c restrict s
substitutio n withi n moda l contexts , thereb y achievin g referentia l opacity . The moda l logi c of '
knowledg e was invente d by Jaakk o Hintikk a (1962) . Sau l Kripk e (1963 ) define d the semantic s
POSSIBL E WORLD S o f the moda l logi c of knowledg e in term s of possibl e worlds . Roughl y speaking , a worl d is
possibl e for an agen t if it is consisten t with everythin g the agen t knows . Fro m this , one can
deriv e rule s of inferenc e involvin g the K operator . Rober t C. Moor e relate s the moda l logi c of i
knowledg e to a styl e of reasonin g abou t knowledg e whic h refer s directl y to possibl e world s in
first­orde r logic (Moore , 1980 ; Moore , 1985a) . Moda l logic can bean intimidatingl y arcan e field ,
but has also foun d significan t application s in reasonin g abou t informatio n in distribute d compute r ;
system s (Halpern , 1987) . For an excellen t compariso n of the syntacti c and moda l theorie s of ;
knowledge , see (Davis , 1990) .
For obviou s reasons , this chapte r does not cove r every  area of knowledg e representatio n in
depth . The thre e principa l topic s omitte d are the following :
<C> Qualitativ e physics : A subfiel d of knowledg e representatio n concerne d specificall y with
constructin g a logical , nonnumeri c theor y of physica l object s and processes . The term
was coine d by Joha n de Klee r (1975) , althoug h the enterpris e coul d be said to have starte d
in Fahlman' s (1974 ) BUILD . BUIL D was a sophisticate d planne r for constructin g comple x
tower s of blocks . Fahlma n discovere d in the proces s of designin g it that mos t of the effort ;
(80% , by his estimate ) wen t into modellin g the physic s of the block s worl d to determin e •
the stabilit y of variou s subassemblie s of blocks , rathe r than into plannin g per se. He \
sketche s a hypothetica l naive­physic s like proces s to explai n why youn g childre n can solve
BuiLD­lik e problem s withou t acces s to the high­spee d floating­poin t arithmeti c used in:
BUILD' S physica l modelling . Haye s (1985a ) uses "histories, " four­dimensiona l slice s of j
space­tim e simila r to Davidson' s events , to construc t a fairl y comple x naiv e physic s of ;j
liquids . Haye s was the first to prov e that a bath with the plug in will eventuall y overflo w :
if the tap keep s running ; and that a perso n who fall s into a lake will get wet all over .
De Klee r and Brow n (1985 ) and Ken Forbu s (1985 ) attempte d to construc t something ;
like a general­purpos e theor y of the physica l world , base d on qualitativ e abstraction s of I
physica l equations . In recen t years , qualitativ e physic s has develope d to the poin t wher e :
it is possibl e to analyz e an impressiv e variet y of comple x physica l system s (Sack s and
Joskowicz , 1993 ; Yip , 1991) . Qualitativ e technique s have been also used to construct ;
nove l design s for clocks , windscree n wipers , and six­legge d walker s (Subramanian , 1993 ; \
Subramania n and Wang , 1994) . The collectio n Readings in  Qualitative  Reasoning  about j
Physical  Systems  (Wel d and de Kleer , 1990 ) provide s a good introductio n to the field .
SPATIA L REASONIN G < > Spatia l reasoning : The reasonin g necessar y to navigat e in the wumpu s worl d and super ­
marke t worl d is trivia l in compariso n to the rich spatia l structur e of the real world . The ;QUALITATIV E
PHYSIC S
Sectio n 8.6. Summar y 261
PSYCHOLOGICA L
PEASONIN Gmost complet e attemp t to captur e commonsens e reasonin g abou t spac e appear s in the work
of Ernes t Davi s (1986 ; 1990) . As with qualitativ e physics , it appear s that an agen t can go
a long way , so to speak , withou t resortin g to a full metri c representation . Whe n such a
representatio n is necessary , technique s develope d in robotic s (Chapte r 25) can be used .
0 Psychologica l reasoning : ,The developmen t of a workin g psychology  for artificia l agents
to use in reasonin g abou t themselve s and othe r agents . Thi s is ofte n base d on so­calle d
"folk psychology, " the theor y that human s in genera l are believe d to use in reasonin g
abou t themselve s and othe r humans . Whe n AI researcher s provid e their artificia l agent s
with psychologica l theorie s for reasonin g abou t othe r agents , the theorie s are frequentl y
based on the researchers ' descriptio n of the logica l agents ' own design . Also , this type
of psychologica l theorizin g frequentl y take s plac e withi n the contex t of natura l languag e
understanding , wher e divinin g the speaker' s intention s is of paramoun t importance . For
this reason , the historica l and bibliographica l backgroun d for this topic has been relegate d
to othe r chapters , especiall y Chapte r 22.
The proceeding s of the internationa l conference s on Principles of  Knowledge  Represen­
tation  and Reasoning  provid e the mos t up­to­dat e source s for work in this area . Readings  in
Knowledge  Representation  (Brachma n and Levesque , 1985 ) and Formal Theories  of the Common­
sense  World  (Hobb s and Moore , 1985 ) are excellen t anthologie s on knowledg e representation ;
the forme r focuse s mor e on historicall y importan t paper s in representatio n language s and for­
malisms , the latte r on the accumulatio n of the knowledg e itself . Representations  of Commonsense
Knowledge  (Davis , 1990 ) is a good recen t textboo k devote d specificall y to knowledg e represen ­
tation rathe r than AI in general . Hughe s and Cresswel l (1968 ; 1984 ) and Chella s (1980 ) are
introductor y texts on moda l logic ; A Manual  of Intensional  Logic  (van Benthem , 1985 ) provide s
a usefu l surve y of the field . The proceeding s of the annua l conferenc e Theoretical  Aspects  of
Reasoning  About  Knowledge  (TARK ) contai n man y interestin g paper s from AI, distribute d sys­
tems, and gam e theory . Resche r and Urquhar t (1971 ) and van Benthe m (1983 ) cove r tempora l
logic specifically . Davi d Hare l (1984 ) provide s an introductio n to dynami c logic .
EXERCISE S
8.1 Exten d the vocabular y from Sectio n 8.3 to defin e additio n and an adde r circuit .
8.2 Represen t the followin g six sentence s usin g the representation s develope d in the chapter .
a. Wate r is a liqui d betwee n 0 and 100 degrees .
b. Wate r boils at 100 degrees .
c. The wate r in John' s wate r bottl e is frozen .
d. Perrie r is a kind of water .
e. John has Perrie r in his wate r bottle .
f. All liquid s have a freezin g point . (Don' t use HasFreezingPoint! )
262 Chapte r 8. Buildin g a Knowledg e Base
^'"('\^  ^ isi°^ '••.' 'i'.^ * ~Now repea t the exercis e usin g a representatio n base d on the mereologica l approach , in which ,
for example , Water  is an objec t containin g as parts all the wate r in the world .
8.3 Encod e the descriptio n of the 4­bit adde r in Figur e 8.10 and pose querie s to verif y that it is
in fact correct .
xo
YO
X1
Y1
X2
Y2
X3
Y3ZO
Z1
Z2
Z3
­ Z4
Figur e 8.10 A  4­bit adder .X3 X2 X1 XO
+ Y3 Y2 Y1 YO
Z4 Z3 Z2 Z1 ZO
8.4 Writ e definition s for the following :
a. ExhaustivePartDecomposition
b. PartPartition
c. PartwiseDisjoint
These shoul d be analogou s to those for ExhaustiveDecomposition,  Partition,  and Disjoint.
8.5 Writ e a set of sentence s that allow s one to calculat e the price of an individua l tomat o (or
other object) , give n the price per pound . Exten d the theor y to allow the price of a bag of tomatoe s
to be calculated .
8.6 Thi s exercis e concern s the relationship s betwee n even t categorie s and the time interval s in
whic h they occur .
a. Defin e the predicat e T(c, i) in term s of SubEvent  and G .
b. Explai n precisel y why we do not need two differen t notation s (A and A) to describ e
conjunctiv e even t categories .
c. Giv e a forma l definitio n for T(p V q, i).
d. Giv e forma l definition s for T(^p,  i) and T(^p,  i), analogou s to thos e usin g V and V.
8.7 Defin e the predicat e Fixed,  wher e Fixed(Location(x))  mean s that the locatio n of objec t x is ij
fixed over time .
8.8 Defin e the predicate s Before,  After,  During,  and Overlap  usin g the predicat e Meet  and the j
function s Start  and End,  but not the functio n Time  or the predicat e <.
Sectio n 8.6. Summar y 263
8.9 Construc t a representatio n for exchang e rates betwee n currencie s that allow s fluctuation s
on a daily basis .
8.10 In this chapter , we sketche d som e of the propertie s of buyin g events . Provid e a forma l
logica l descriptio n of buyin g usin g even t calculus .
8.11 Describ e the even t of tradin g somethin g for somethin g else. Describ e buyin g as a kind of
tradin g wher e one of the object s is a sum of money .
8.12 Th e exercise s on buyin g and tradin g used a fairl y primitiv e notio n of ownership . For
example , the buye r start s by owning  the dolla r bills . This pictur e begin s to brea k dow n when , for
example , one' s mone y is in the bank , becaus e there is no longe r any specifi c collectio n of dolla r
bills that one owns . The pictur e is complicate d still furthe r by borrowing , leasing , renting , and
bailment . Investigat e the variou s commonsens e and legal concept s of ownership , and propos e a
schem e by whic h they can be represente d formally .
8.13 Yo u are to creat e a syste m for advisin g compute r scienc e undergraduate s on wha t course s
to take over an extende d perio d in orde r to satisf y the progra m requirements . (Us e whateve r
requirement s are appropriat e for your institution. ) First , decid e on a vocabular y for representin g
all the information , then represen t it; then use an appropriat e quer y to the system , that will retur n
a legal progra m of stud y as a solution . You shoul d allow for some tailorin g to individua l students ,
in that your syste m shoul d ask the studen t wha t course s or equivalent s he has alread y taken , and
not generat e program s that repea t those courses .
Sugges t way s in whic h you r syste m coul d be improved , for exampl e to take into accoun t
knowledg e abou t studen t preferences , workload , good and bad instructors , and so on. For each
kind of knowledge , explai n how it coul d be expresse d logically . Coul d you r syste m easil y
incorporat e this informatio n to find the best  progra m of stud y for a student ?
8.14 Figur e 8.2 show s the top level s of a hierarch y for everything . Exten d it to includ e as man y
real categorie s as possible . A good way to do this is to cove r all the thing s in your everyda y life.
This include s object s and events . Star t with wakin g up, procee d in an orderl y fashio n notin g
everythin g that you see, touch , do, and thin k about . For example , a rando m samplin g produce s
music , news , milk , walking , driving , gas, Soda Hall , carpet , talking , Professo r Fateman , chicke n
curry , tongue , $4, sun, the daily newspaper , and so on.
You shoul d produc e both a singl e hierarch y char t (larg e shee t of paper ) and a listin g of
object s and categorie s with one or more relation s satisfie d by member s of each category . Ever y
objec t shoul d be in a category , and ever y categor y shoul d be incorporate d in the hierarchy .
8.15 (Adapte d from an exampl e by Dou g Lenat. ) You r missio n is to capture , in logica l form ,
enoug h knowledg e to answe r a serie s of question s abou t the followin g simpl e sentence :
Yesterda y John wen t to the Nort h Berkele y Safewa y and bough t two pound s of
tomatoe s and a poun d of groun d beef .
Start by tryin g to represen t its conten t as a serie s of assertions . You shoul d write sentence s that
have straightforwar d logica l structur e (e.g. , statement s that object s have certai n properties ; that
object s are relate d in certai n ways ; that all object s satisfyin g one propert y satisf y another) . The
followin g may help you get started :
264 Chapte r 8. Buildin g a Knowledg e Base
• Whic h classes , individuals , relations , and so on, woul d you need ? Wha t are their parents ,
sibling s and so on? (You will need event s and tempora l ordering , amon g othe r things. )
• Wher e woul d they fit in a more genera l hierarchy ?
• Wha t are the constraint s and interrelationship s amon g them ?
• How detaile d mus t you be abou t each of the variou s concepts ?
The knowledg e base you construc t mus t be capabl e of answerin g a list of question s that we will
give shortly . Som e of the question s deal with the materia l state d explicitl y in the story , but mos t
of them requir e one to know othe r backgroun d knowledge—t o read betwee n the lines . You'l l
have to deal with wha t kind of thing s are at a supermarket , wha t is involve d with purchasin g the
thing s one selects , wha t will purchase s be used for, and so on. Try to mak e your representatio n
as genera l as possible . To give a trivia l example : don' t say "Peopl e buy food from Safeway, "
becaus e that won' t help you with thos e who shop at anothe r supermarket . Don' t say "Joe mad e
spaghett i with the tomatoe s and groun d beef, " becaus e that won' t help you with anythin g else at
all. Also , don' t turn the question s into answers ; for example , questio n (c) asks "Did John buy
any meat?"—no t "Did John buy a poun d of groun d beef? "
Sketc h the chain s of reasonin g that woul d answe r the questions . In the proces s of doin g so,
you will no doub t need to creat e additiona l concepts , mak e additiona l assertions , and so on. If
possible , use a logica l reasonin g syste m to demonstrat e the sufficienc y of your knowledg e base .
Many of the thing s you writ e may only be approximatel y correc t in reality , but don' t worr y too
much ; the idea is to extrac t the commo n sens e that lets you answe r thes e question s at all.
a. Is John a child or an adult ? [Adult ]
b. Doe s John now have at least 2 tomatoes ? [Yes ]
c. Did John buy any meat ? [Yes ]
d. If Mary was buyin g tomatoe s at the same time as John , did he see her? [Yes ]
e. Are the tomatoe s mad e in the supermarket ? [No ]
f. Wha t is John goin g to do with the tomatoes ? [Eat them ]
g. Doe s Safewa y sell deodorant ? [Yes ]
h. Did John bring any mone y to the supermarket ? [Yes ]
i. Doe s John have less mone y after goin g to the supermarket ? [Yes ]
8.16 Mak e the necessar y additions/change s to your knowledg e base from the previou s exercis e
so that the followin g question s can be answered . Sho w that they can indee d be answere d by
the KB, and includ e in your repor t a discussio n of the fixes , explainin g why they were needed ,
whethe r they were mino r or major , and so on.
a. Are ther e othe r peopl e in Safewa y whil e John is there ? [Yes—staff! ]
b. Did Mar y see John ? [Yes ]
c. Is John a vegetarian ? [No ]
d. Wh o own s the deodoran t in Safeway ? [Safewa y Corporation ]
e. Did John have an ounc e of groun d beef ? [Yes ]
f. Doe s the Shel l statio n next door have any gas? [Yes ]
g. Do the tomatoe s fit in John' s car trunk ? [Yes ]
9INFERENC E IN
FIRST­ORDE R LOGI C
In which  we define  inference mechanisms  that  can efficiently  answer questions  posed
in first­order  logic.
Chapte r 6 define d the notio n of inference , and showe d how soun d and complet e inferenc e can
be achieve d for prepositiona l logic . In this chapter , we exten d thes e result s to first­orde r logic .
In Sectio n 9.1 we provid e som e additiona l basi c inferenc e rule s to deal with quantifiers . In
Sectio n 9.2, we show how thes e inferenc e rules , alon g with thos e for prepositiona l logic , can
be chaine d togethe r to mak e proofs . By examinin g thes e proofs , we can com e up with mor e
powerfu l inferenc e rule s that mak e proof s muc h shorte r and mor e intuitive . Thi s make s it
possibl e to desig n inferenc e procedure s that are sufficientl y powerfu l to be of use to a knowledge ­
based agent.  Thes e rule s and procedure s are discusse d in Section s 9.3 and 9.4. Sectio n 9.5
describe s the proble m of completenes s for first­orde r inference , and discusse s the remarkabl e
result , obtaine d by Kur t Godel , that if we exten d first­orde r logi c with additiona l construct s
to handl e mathematica l induction , then there is no complet e inferenc e procedure ; even thoug h
the needl e is in the metaphorica l haystack , no procedur e can guarante e to find it. Sectio n 9.6
describe s an inferenc e procedur e calle d resolutio n that is complet e for any set of sentence s in
first­orde r logic , and Sectio n 9.7 prove s that it is complete .
INFERENC E RULE S INVOLVIN G QUANTIFIER S
In Sectio n 6.4, we saw the inferenc e rule s for propositiona l logic : Modu s Ponens , And ­
Elimination , And­Introduction , Or­Introduction , and Resolution . Thes e rules hold for first­orde r
logic as well . But we will need additiona l inferenc e rule s to handl e first­orde r logic sentence s
with quantifiers . The thre e additiona l rule s we introduc e here are mor e comple x than previou s
ones, becaus e we have to talk abou t substitutin g particula r individual s for the variables . We will
use the notatio n SUBST(# , a) to denot e the resul t of applyin g the substitutio n (or bindin g list) 6
to the sentenc e a. For example :
SVEST({x/Sam,  ylPam],  Likes(x,y))  = Likes(Sam,Parri)
265
266 Chapte r 9. Inferenc e in First­Orde r Logi c
The three new inferenc e rules are as follows :
UNIVERSA L
ELIMINATIO N
EXISTENTIA L
ELIMINATIO N
EXISTENTIA L
INTRODUCTIO N0Universa l Elimination : For any sentenc e a, variabl e v, and groun d term1 g:
V v a
0SUBST({v/g},a ) ­
For example , from V* Likes(x,IceCream),  we can use the substitutio n {x/Ben}  and infe r
Likes(Ben,  IceCream).
Existentia l Elimination : For any sentenc e a, variabl e v, and constan t symbo l k that does
not appea r elsewher e in the knowledg e base :
3 v a
SUBST({v/fc},a )
For example , from 3x Kill(x,  Victim),  we can infe r Kill(Murderer,  Victim),  as long as
Murderer  does not appea r elsewher e in the knowledg e base .
<C> Existentia l Introduction : For any sentenc e o, variabl e v that does not occu r in a, and
groun d term g that does occu r in a:
3v SUBST({g/v},a )
For example , from Likes(Jerry,  IceCream)  we can infe r 3 x Likes(x,  IceCream).
You can chec k these rules usin g the definitio n of a universa l sentenc e as the conjunctio n of all its
possibl e instantiation s (so Universa l Eliminatio n is like And­Elimination ) and the definitio n of
an existentia l sentenc e as the disjunctio n of all its possibl e instantiations .
It is very importan t that the constan t used to replac e the variable  in the Existentia l Elimi ­
natio n rule is new . If we disregar d this requirement , it is easy to produc e consequence s that do
not follo w logically . For example , suppos e we have the sentenc e 3 x Father(x,  John)  ("Joh n has
a father") . If we replac e the variabl e x by the constan t John,  we get Father(John,  John),  whic h is
certainl y not a logica l consequenc e of the origina l sentence . Basically , the existentia l sentenc e
says there is some object  satisfyin g a condition , and the eliminatio n proces s is just givin g a nam e
to that object . Naturally , that nam e mus t not alread y belon g to anothe r object . Mathematic s •
provide s a nice example : suppos e we discove r that there is a numbe r that is a little bigge r than
2.7182 8 and that satisfie s the equatio n d(xy)/dy=xy for x. We can give this numbe r a name , such i
as e, but it woul d be a mistak e to give it the nam e of an existin g object , like TT.
9.2 AN EXAMPL E PROO F
Havin g define d some inferenc e rules , we now illustrat e how to use them to do a proof . From here,
it is only a shor t step to an actua l proo f procedure , becaus e the applicatio n of inferenc e rules is
simpl y a questio n of matchin g their premis e pattern s to the sentence s in the KB and then addin g
1 Recal l from Chapte r 7 that a groun d term is a term that contain s no variables—tha t is, eithe r a constan t symbo l or a
functio n symbo l applie d to som e groun d terms .
Sectio n 9.2. A n Exampl e Proo f 267
their (suitabl y instantiated ) conclusio n patterns . We will begi n with the situatio n as it migh t be
describe d in English :
The law says that it is a crim e for an America n to sell weapon s to hostil e nations .
The countr y Nono , an enem y of America , has som e missiles , and all of its missile s
were sold to it by Colone l West , who is American .
Wha t we wish to prov e is that Wes t is a criminal . We first represen t thes e fact s in first­orde r
logic , and then show the proo f as a sequenc e of application s of the inferenc e rules.2
"... it is a crim e for an America n to sell weapon s to hostil e nations" :
V x, y, z American(x)  A Weapon(y)  A Nation(z)  A Hostile(z)
A Sells(x,  z, y) => Criminal(x)
"Non o ... has some missiles" :
3;c Owns(Nono,x)f\Missile(x)  (9.2 )
"All of its missile s were sold to it by Colone l West" :
V* Owns(Nono,x)  A Missile(x)  => Sells(West,Nono,x)  (9.3 )
We will also need to know that missile s are weapons :
MX Missile(x)  => Weapon(x)  (9.4 )
and that an enem y of Americ a count s as "hostile" :
MX Enemy(x,America)  ­=> Hostile(x)  (9.5 )
"West , who is America n ...":
American(West)  (9.6 )
"The countr y Non o ...":
Nation(Nono)  (9.7 )
"Nono , an enem y of Americ a ...":
Enemy  (Nono,  America)  (9.8 )
Nation(America)  (9.9 )
The proo f consist s of a serie s of application s of the inferenc e rules :
From (9.2) and Existentia l Elimination :
Owns(Nono,  M1) A Missile(M  1) (9.10 )
From (9.10 ) and And­Elimination :
Owns(Nono,M\)  (9.11 )
Missile(M\)  (9.12 )
From (9.4 ) and Universa l Elimination :
Missile(M  1) => Weapon(M  1) (9.13 )
2 Ou r representatio n of the fact s will not be idea l accordin g to the standard s of Chapte r 8, becaus e this is mainl y an
exercis e in proo f rathe r than knowledg e representation .
268 Chapte r 9. Inferenc e in First­Orde r Logi c
Sells(West,Nono,M\)(9.14 )
(9.15 )
(9­16 )
(9.17 )
(9.18 )
(9­19 )
(9.20 )
(9.21 )From (9.12) , (9.13) , and Modu s Ponens :
Weapon(Ml)
From (9.3) and Universa l Elimination :
Owns(Nono,M\)  A Missile(Ml)  =;
From (9.15) , (9.10) , and Modu s Ponens :
Sells(West,Nono,M\)
From (9.1) and Universa l Eliminatio n (thre e times) :
American(West)  A Weapon(M\)  A Nation(Nond)  A Hostile(Nono}
A Sells(West,Nono,MY)  => Criminal(Wesi)
From (9.5) and Universa l Elimination :
Enemy(Nono,  America)  => Hostile(Nono)
From (9.8) , (9.18) , and Modu s Ponens :
Hostile(Nono)
From (9.6) , (9.7) , (9.14) , (9.16) , (9.19) , and And­Introduction :
American(West)  A Weapon(M\)  A Nation(Nono)
A Hostile(Nono)  A Sells(West,Nono,Ml)
From (9.17) , (9.20) , and Modu s Ponens :
Criminal(West)
If we formulat e the proces s of findin g a proo f as a searc h process , then obviousl y this proo f is the
solutio n to the searc h problem , and equall y obviousl y it woul d have to be a prett y smar t progra m
to find the proo f withou t followin g any wron g paths . As a searc h problem , we woul d have
Initia l state = KB (sentence s 9.1­9.9 )
Operator s = applicabl e inferenc e rule s
Goal test = KB containin g Criminal(West)
This exampl e illustrate s some importan t characteristics :
• The proo f is 14 step s long .
• The branchin g facto r increase s as the knowledg e base grows ; this is becaus e som e of the
inferenc e rule s combin e existin g facts .
• Universa l Eliminatio n can have an enormou s branchin g facto r on its own , becaus e we can
replac e the variable  by any groun d term .
• We spen t a lot of time combinin g atomi c sentence s into conjunctions , instantiatin g universa l •.
rules to match , and then applyin g Modu s Ponens .
Thus , we have a seriou s difficulty , in the form of a collectio n of operator s that give long proof s ;
and a large branchin g factor , and henc e a potentiall y explosiv e searc h problem . We also have
an opportunity , in the form of an identifiabl e patter n of applicatio n of the operator s (combinin g j
atomi c sentences , instantiatin g universa l rules , and then applyin g Modu s Ponens) . If we can, ­
defin e a bette r searc h spac e in whic h a singl e operato r take s thes e three steps , then we can find <
proof s more efficiently .
Sectio n 9.3. Generalize d Modu s Ponen s 269
GENERALIZE D MODU S PONEN S
GENERALIZE D
MODU S PONEN SIn this section , we introduc e a generalizatio n of the Modu s Ponen s inferenc e rule that does in a
singl e blow wha t require d an And ;Introduction , Universa l Elimination , and Modu s Ponen s in the
earlie r proof . The idea is to be able to take a knowledg e base containing , for example :
Missile(Ml)
Owns(Nono,Ml)
V ' x Missile(x)  A Owns(Nono,x)  =>• Sells(West,  Nono,x)
and infe r in one step the new sentenc e
Sells(West,Nono,M])
Intuitively , this inferenc e seem s quit e obvious . The key is to find som e x in the knowledg e base
such that x is a missil e and Non o own s x, and then infe r that Wes t sells this missil e to Nono .
More generally , if there is some substitutio n involving s that make s the premis e of the implicatio n
identica l to sentence s alread y in the knowledg e base , then we can asser t the conclusio n of the
implication , afte r applyin g the substitution . In the precedin g case , the substitutio n {x/M\}
achieve s this.
We can actuall y mak e Modu s Ponen s do even more work . Suppos e that instea d of knowin g
Owns(Nono,M\),  we knew that everyon e own s Ml (a communa l missile , as it were) :
Vy Owns(y,M\)
Then we woul d still like to be able to conclud e that Sells(West,  Nono,M\}.  This inferenc e coul d
be carrie d out if we first applie d Universa l Eliminatio n with the substitutio n {ylNono}  to get
Owns(Nono,Ml).  The generalize d versio n of Modu s Ponen s can do it in one step by findin g a
substitutio n for both the variable s in the implicatio n sentenc e and the variable s in the sentence s to
be matched . In this case , applyin g the substitutio n {xlM\  , y/Nono}  to the premis e Owns(Nono,x)
and the sentenc e Owns(y,M\)  will mak e them identical . If this can be done for all the premise s
of the implication , then we can infe r the conclusio n of the implication . The rule is as follows :
0 Generalize d Modu s Ponens : For atomi c sentence s p/, p/, and q, wher e ther e is a substi ­
tution 0 such that SuBST(6>,p/ ) = SUBST(0,p/) , for all i:
pi', p 2', ...,  p,,',  (pi Ap 2 A.. . Ap n =><? )
There are n + 1 premise s to this rule: the n atomi c sentence s p' and the one implication . Ther e
is one conclusion : the resul t of applyin g the substitutio n to the consequen t q. For the exampl e
with Wes t and the missile :
p\ is Missile(Ml)  p i is Missile(x)
P2 is Owns(y,M\)  p 2 is Owns(Nono,x)
0 is {x/Ml,y/Nono}  q  is Sells(West,Nono,x)
SUBST(6» , q) is Sells(West,  Nono,  Ml)
Generalized  Modus  Ponens  is an efficient  inference  rule  for three  reasons:
1 . It take s bigge r steps , combinin g severa l smal l inference s into one.
270 Chapte r 9. Inferenc e in First­Orde r Logi c
2. It takes sensibl e steps—i t uses substitution s that are guarantee d to help rathe r than randoml y
UNIFICATIO N tryin g Universa l Eliminations . The unificatio n algorith m take s two sentence s and return s
a substitutio n that make s them look the sam e if such a substitutio n exists .
3. It make s use of a precompilatio n step that convert s all the sentence s in the knowledg e base
CANONICA L FORM int o a canonica l form . Doin g this once and for all at the start mean s we need not wast e
time tryin g conversion s durin g the cours e of the proof .
We will deal with canonica l form and unificatio n in turn .
Canonica l Form
We are attemptin g to build an inferencin g mechanis m with one inferenc e rule—th e generalize d
versio n of Modu s Ponens . Tha t mean s that all sentence s in the knowledg e base shoul d be in a
form that matche s one of the premise s of the Modu s Ponen s rule—otherwise , they coul d neve r
be used . In othe r words , the canonica l form for Modu s Ponen s mandate s that each sentenc e in
the knowledg e base be eithe r an atomi c sentenc e or an implicatio n with a conjunctio n of atomi c
sentence s on the left hand side and a singl e atom on the right . As we saw on page 174, sentence s
HORN SENTENCE S o f this form are calle d Hor n sentences , and a knowledg e base consistin g of only Hor n sentence s
is said to be in Horn Norma l Form .
We conver t sentence s into Horn sentence s whe n they are first entere d into the knowledg e
base, usin g Existentia l Eliminatio n and And­Elimination.3 For example , 3 x Owns(Nono,x)  A
Missile(x)  is converte d into the two atomi c Hor n sentence s Owns(Nono,M\)  and Missle(Ml).
Once the existentia l quantifier s are all eliminated , it is traditiona l to drop the universa l quantifiers ,
so that Vy Owns(y,Ml)  woul d be writte n as Owns(y,Ml).  Thi s is just an abbreviation—th e
meanin g of y is still a universall y quantifie d variable , but it is simple r to write and read sentence s
withou t the quantifiers . We retur n to the issue of canonica l form on page 278.
UNIFIE RUnificatio n
The job of the unificatio n routine , UNIFY , is to take two atomi c sentence s p and q and retur n a
substitutio n that woul d mak e p and q look the same . (If there is no such substitution , then UNIF Y
shoul d retur n fail.)  Formally ,
UNIF Y (p, q) = 6 wher e SuBST(0,p ) = SuBST(0,# )
0 is calle d the unifie r of the two sentences . We will illustrat e unificatio n in the contex t of an
example , delayin g discussio n of detaile d algorithm s unti l Chapte r 10. Suppos e we have a rule
Knows(John,x)  =>• Hates(John,x)
("Joh n hate s everyon e he knows" ) and we wan t to use this with the Modu s Ponen s inferenc e rule
to find out who m he hates . In othe r words , we need to find thos e sentence s in the knowledg e base
3 We will see in Sectio n 9.5 that not all sentence s can be converte d into Hor n form . Fortunately , the sentence s in our
exampl e (and in man y othe r problems ) can be.
Sectio n 9.3. Generalize d Modu s Ponen s 271
STANDARDIZ E APAR Tthat unif y with Knows(John,  x), and then appl y the unifie r to Hates(John,  x). Let our knowledg e
base contai n the followin g sentences :
Knows(John,  Jane)
Knows(y,  Leonid)
Knows(y,  Mother(y))
Knows(x,  Elizabeth)
(Remembe r that x and y are implicitl y universall y quantified. ) Unifyin g the anteceden t of the
rule agains t each of the sentence s in the knowledg e base in turn give s us:
\jNlFY(Knows(John,x),  Knows(  John,  Jane))  ­ {x/Jane}
UNlFY(Knows(John,  x), Knows(y,  Leonid))  = {x/Leonid,  ylJohn}
UNiFY(Knows(John,x),  Know  s(y,Mother  (y))) = {ylJohn,xlMother(John)}
UNlF\(Knows(John,x),  Know  s(x, Elizabeth))  =fail
The last unificatio n fails becaus e x canno t take on the valu e John  and the valu e Elizabeth  at the
same time . But intuitively , from the facts that John hate s everyon e he know s and that everyon e
know s Elizabeth , we shoul d be able to infer that John hate s Elizabeth . It shoul d not matte r if the
sentenc e in the knowledg e base is Knows(x,  Elizabeth)  or Knows(y,  Elizabeth).
One way to handl e this proble m is to standardiz e apar t the two sentence s bein g unified ,
whic h mean s renamin g the variable s of one (or both ) to avoi d nam e clashes . Afte r standardizin g
apart , we woul d have
\jNlFY(Knows(John,x\),  Know  s(x 2, Elizabeth))  = {x\ I Elizabeth,  x2l John}
The renamin g is vali d becaus e Vx Knows(x,  Elizabeth)  and Vx2 Knows(x 2, Elizabeth)  have the
same meaning . (See also Exercis e 9.2.)
There is one more complication : we said that UNIF Y shoul d retur n a substitutio n that make s
the two argument s look the same . But if there is one such substitution , then there are an infinit e
number :
UNlFY(Knows(John,x),  Knows(y,  z)) = {y/John,x/z}
or {ylJohn,xlz,wlFreda}
or {y/John,xlJohn,  zlJohn]
or • • •
Thus , we insis t that UNIF Y return s the most genera l unifie r (or MGU) , whic h is the substitutio n
that make s the least commitmen t abou t the binding s of the variables . In this case it is {ylJohn,  x/z}.
Sampl e proo f revisite d
Let us solv e our crim e proble m usin g Generalize d Modu s Ponens . To do this, we first need to
put the origina l knowledg e base into Horn form . Sentence s (9.1) throug h (9.9) becom e
American(x)  A Weapon(y)  A Nation(z)  A Hostile(z)
A Sells(x,  z, y) => CriminaKx)
Owns(Nono,M\)(9.22 )
(9.23 )
272 Chapte r 9. Inferenc e in First­Orde r Logi c
Missile(Ml)  (9.24 )
Owns(Nono,  x) A Missile(x)  => Sells(West,  Nono,  x) (9.25 )
Missile(x)  => Weapon(x)  (9.26 )
Enemy(x,  America)  => Hostile(x)  (9.27 )
American(West)  (9.28 )
Nation(Nono)  (9.29 )
Enemy(Nono,  America)  (9.30 )
Nation(America)  (9.31 )
The proo f involve s just four steps . From (9.24 ) and (9.26 ) usin g Modu s Ponens :
Weapon(M\)  (9.32 )
From (9.30 ) and (9.27 ) usin g Modu s Ponens :
Hosnle(Nono)  (9.33 )
From (9.23) , (9.24) , and (9.25 ) usin g Modu s Ponens :
Sells(West,Nono,Ml)  (9.34 )
From (9.28) , (9.32) , (9.29) , (9.33) , (9.34 ) and (9.22) , usin g Modu s Ponens :
Criminal(West)  (9.35 )
This proo f show s how natura l reasonin g with Generalize d Modu s Ponen s can be. (In fact, one
migh t ventur e to say that it is not unlik e the way in whic h a huma n migh t reaso n abou t the
problem. ) In the next section , we describ e systemati c reasonin g algorithm s usin g Modu s Ponens .
These algorithm s form the basis for man y large­scal e application s of AI, whic h we describ e in
Chapte r 10.
9.4 FORWAR D AND BACKWAR D CHAININ G
FORWAR D CHAININ G
BACKWAR D
CHAININ GNow that we have a reasonabl e languag e for representin g knowledge , and a reasonabl e inferenc e
rule (Generalize d Modu s Ponens ) for usin g that knowledge , we will stud y how a reasonin g
progra m is constructed .
The Generalize d Modu s Ponen s rule can be used in two ways . We can start with the
sentence s in the knowledg e base and generat e new conclusion s that in turn can allow mor e
inference s to be made . This is calle d forwar d chaining . Forwar d chainin g is usuall y used whe n
a new fact is adde d to the databas e and we wan t to generat e its consequences . Alternatively ,
we can start with somethin g we wan t to prove , find implicatio n sentence s that woul d allow us
to conclud e it, and then attemp t to establis h their premise s in turn . Thi s is calle d backwar d
chaining , becaus e it uses Modu s Ponen s backwards . Backwar d chainin g is normall y used whe n
there is a goal to be proved .
Sectio n 9.4. Forwar d and Backwar d Chainin g 273
Forward­chainin g algorith m
Forwar d chainin g is normall y triggere d by the additio n of a new fact p to the knowledg e base . It
can be incorporate d as part of the TEL L process , for example . The idea is to find all implication s
that have p as a premise ; then if the othe r premise s are alread y know n to hold , we can add the
consequen t of the implicatio n to the knowledg e base , triggerin g furthe r inferenc e (see Figur e 9.1).
RENAMIN G Th e FORWARD­CHAI N procedur e make s use of the idea of a renaming . One sentenc e is
a renamin g of anothe r if they are identica l excep t for the name s of the variables . For example ,
Likes(x,  IceCream)  and Likes(y,  IceCream)  are renaming s of each othe r becaus e they only diffe r
in the choic e of x or y, but Likes(x,  x) and Likes(x,  y) are not renaming s of each other .
COMPOSITIO N W e also need the idea of a compositio n of substitutions . COMPOSE ^ , 02) is the substitutio n
whos e effec t is identica l to the effec t of applyin g each substitutio n in turn . Tha t is,
SUBST(COMPOSE(0|,0 2),p) = SUBST(0 2,SUBST(#i,p) )
We will use our crim e proble m agai n to illustrat e how FORWARD­CHAI N works . We will
begin with the knowledg e base containin g only the implication s in Horn form :
American(x)  A Weaponry)  A Nation(z)  A Hostile(z)
A Sells(x,  z, y) => Criminal(x)  (9.36 )
Owns(Nono,  x) A Missile(x)  => Sells(West,  Nono,  x) (9.37 )
Missile(x)  => Weapon(x)  (9.38 )
Enemy(x,  America)  => Hostile(x)  (9.39 )
procedur e FORWARD­CHAIN'S , p)
if there is a sentenc e in KB that is a renamin g of p then retur n
Add p to KB
for each (p\ A ... A p,, => q) in KB such that for som e i, UNIFY(P,,/? ) = 0 succeed s do
FlND­AND­lNFER(A"B , [p\, ... ,p­,­\,p M,. .. ,p,,],q,9)
end
procedur e FIND­AND­lNFER'fi , premises,  conclusion,  0)
if premises  = [ \ then
FORWARD­CHAIN'S , SuBST(0 , conclusion))
else for each/; ' in KB such that UNlFY(p' , SUBST(0 , FlRST(  premises)))  = #2 do
FlND­AND­lNFER(/fS , REST(premises),  conclusion,  COMPOSE^ , #2))
end
Figur e 9.1 Th e forward­chainin g inferenc e algorithm . It adds to KB all the sentence s that can
be inferre d from the sentence/; . If/j is alread y in KB, it does nothing . If/; is new , conside r each
implicatio n that has a premis e that matche s p. For each such implication , if all the remainin g
premise s are in KB, then infer the conclusion . If the premise s can be matche d severa l ways , then
infer each correspondin g conclusion . The substitutio n 6> keep s track of the way thing s match .
274 Chapte r 9. Inferenc e in First­Orde r Logi c
DATA­DRIVE NNow we add the atomi c sentence s to the knowledg e base one by one, forwar d chainin g each time
and showin g any additiona l fact s that are added :
FORWAKD­CHAJN(KB,American(West))
Add to the KB. It unifie s with a premis e of (9.36) , but the othe r premise s of (9.36 ) are not known ,
so FORWARD­CHAI N return s withou t makin g any new inferences .
FOKWARD­CUMN(KB,Nation(Nono))
Add to the KB. It unifie s with a premis e of (9.36) , but ther e are still missin g premises , so
FORWARD­CHAI N returns .
FORWARD­CHAI N (KB, Enemy(Nono,  America))
Add to the KB. It unifie s with the premis e of (9.39) , with unifie r {x/Nono}.  Call
FORWARD­CHAI N (KB, Hostile(Nono))
Add to the KB. It unifie s with a premis e of (9.36) . Onl y two othe r premise s are known , so
processin g terminates .
FORWARD­CHAI N (KB, Owns(Nono,Ml))
Add to the KB. It unifie s with a premis e of (9.37) , with unifie r {x/Ml}.  The othe r premise , now
Missile(Ml),  is not known , so processin g terminates .
FORWARD­CHAIN(A'B , Missile(M\))
Add to the KB. It unifie s with a premis e of (9.37 ) and (9.38) . We will handl e them in that order .
• Missile(Ml)  unifie s with a premis e of (9.37 ) with unifie r {xlM\}.  The othe r premise , now
Owns(Nono,M\),  is known , so call
FORWARD­CHAIN(/fB , Sells(West,  Nono,M\))
Add to the KB. It unifie s with a premis e of (9.36) , with unifie r {x/West,y/Ml,z/Nono}.
The premis e Weapon(Ml)  is unknown , so processin g terminates .
• Missile(Ml)  unifie s with a premis e of (9.38 ) with unifie r {x/Ml}.  Cal l
FORWARD­CHAIN(£B , Weapon(Ml))
Add to the KB. It unifie s with a premis e of (9.36) , with unifie r {y/Ml}.  The othe r premise s
are all known , with accumulate d unifie r {xlWest,y/M\,zlNono}.  Call
FORWARD­CHAlN(A:fi , Criminal(  West))
Add to the KB. Processin g terminates .
As can be seen from this example , forwar d chainin g build s up a pictur e of the situatio n graduall y
as new data come s in. Its inferenc e processe s are not directe d towar d solvin g any particula r
problem ; for this reaso n it is calle d a data­drive n or data­directe d procedure . In this example ,
there were no rules capabl e of drawin g irrelevan t conclusions , so the lack of directednes s was not
a problem . In othe r case s (for example , if we have severa l rule s describin g the eatin g habit s of
Americans  and the price of missiles) , FORWARD­CHAI N will generat e many irrelevan t conclusions .
In such cases , it is ofte n bette r to use backwar d chaining , whic h direct s all its effor t towar d the
questio n at hand .
Sectio n 9.4. Forwar d and Backwar d Chainin g 275
Backward­chainin g algorith m
Backwar d chainin g is designe d to find all answer s to a questio n pose d to the knowledg e base .
Backwar d chainin g therefor e exhibit s the functionalit y require d for the ASK procedure . The
backward­chainin g algorith m BACK­CHAI N work s by first checkin g to see if answer s can be
provide d directl y from sentences'i n the knowledg e base . It then find s all implication s whos e
conclusio n unifie s with the query , and tries to establis h the premise s of those implications , also by
backwar d chaining . If the premis e is a conjunction , then BACK­CHAI N processe s the conjunctio n
conjunc t by conjunct , buildin g up the unifie r for the whol e premis e as it goes . The algorith m is
show n in Figur e 9.2.
Figur e 9.3 is the proo f tree for derivin g Criminal(West)  from sentence s (9.22 ) throug h
(9.30) . As a diagra m of the backwar d chainin g algorithm , the tree shoul d be read depth­first ,
left to right . To prov e Criminal(x)  we have to prov e the five conjunct s belo w it. Som e of these
are in the knowledg e base , and other s requir e furthe r backwar d chaining . Eac h leaf node has
the substitutio n used to obtai n it writte n below . Not e that once one branc h of a conjunctio n
succeeds , its substitutio n is applie d to subsequen t branches . Thus , by the time BACK­CHAI N gets
to Sells(x,  z, v), all the variable s are instantiated . Figur e 9.3 can also be seen as a diagra m of
forwar d chaining . In this interpretation , the premise s are adde d at the bottom , and conclusion s
are adde d once all their premise s are in the KB. Figur e 9.4 show s wha t can happe n if an incorrec t
choic e is mad e in the search—i n this case , choosin g Americ a as the natio n in question . Ther e is
no way to prov e that Americ a is a hostil e nation , so the proo f fails to go through , and we have to
back up and conside r anothe r branc h in the searc h space .
functio n BACK­CHAIN(/CB , q) return s a set of substitution s
BACK­CHAIN­LlST(/f5 , [<?],{} )
functio n BACK­CHAlN­LiST(Ar5, qlist,  9) return s a set of substitution s
inputs : KB,  a knowledg e base
qlist,  a list of conjunct s formin g a quer y (0 alread y applied )
6, the curren t substitutio n
static : answers,  a set of substitutions , initiall y empt y
if qlist  is empt y then retur n {6}
q <— FlRST(<?/«? )
for each q\ in KB such that #, <— UNiFY(g , <?,') succeed s do
Add COMPOSE^ , #,) to answers
end
for each sentenc e (pi A ... A pn => q' t) in KB such that 8t <— UNiFY(g , <?,') succeed s do
answers^­  BACK­CHAiN­LlST(£B,SUBST(0j , [p\ .. ./?„]),COMPOSE(0,0;) ) U answers
end
retur n the unio n of BACK­CHAIN­LlST(/fB , REST(^Wif) . #) f°r each 6 € answers
Figur e 9.2 Th e backward­chainin g algorithm .
276 Chapte r 9. Inferenc e in First­Orde r Logi c
American(x)  \HighTech(y)  | | Weapon(Ml)  | | Nation(z)  \ \Hostile(Nono)  \ \ SellsfWest,Nono,Ml)
Yes, {x/Westf  Yes,  {y/Mlf Yes, Iz/Nono}
\ Missile(Ml)  \
Yes, {/Enemy(  Nona,America)
Yes, I}
Owns(Nono,Ml)  \Missile(Ml)
Yes, {I Yes, {}
Figur e 9.3 Proo f tree to infe r that Wes t is a criminal .
fx/West/Atnerican(x)  Weaponry)  Nation(z)  Hostile(America)  Sells(West,America,Ml)
/z/America/ Fail
Missile(y)
Figur e 9.4 A  faile d proo f tree: nothin g can be inferred .
9.5 COMPLETENES S
Suppos e we have the followin g knowledg e base :
V* P(x)  => Q(x)
V* ­./>(* ) => R(x)
Vx Q(x)  => S(x)
MX R(x)  => S(x)
Then we certainl y wan t to be able to conclud e S(A);  S(A)  is true if Q(A)  or R(A)  is true, and one
of thos e mus t be true becaus e eithe r P(A ) is true or ­*P(A)  is true .(9­40) 1
ction 9.6. Sectio n Resolution : A Complet e Inferenc e Procedur e 277
COMPLETENES S
THEORE MUnfortunately , chainin g with Modu s Ponen s canno t deriv e S(A) for us. The proble m is that
MX ­>P(x)  =>• R(x)  canno t be converte d to Horn form , and thus canno t be used by Modu s Ponens .
INCOMPLET E Tha t mean s that a proo f procedur e usin g Modu s Ponen s is incomplete : ther e are sentence s
entaile d by the knowledg e base that the procedur e canno t infer .
The questio n of the existenc e of complet e proo f procedure s is of direc t concer n to mathe ­
maticians . If a complet e proo f procedur e can be foun d for mathematica l statements , two thing s
follow : first , all conjecture s can be establishe d mechanically ; second , all of mathematic s can
be establishe d as the logica l consequenc e of a set of fundamenta l axioms . A complet e proo f
procedur e for first­orde r logi c woul d also be of grea t valu e in AI: barrin g practica l issue s of
computationa l complexity , it woul d enabl e a machin e to solve any proble m that can be state d in
the language .
The questio n of completenes s has therefor e generate d som e of the mos t importan t mathe ­
matica l work of the twentiet h century . This work culminate d in the result s prove d by the Germa n
mathematicia n Kurt Godel  in 1930 and 1931 . Gode l has som e good new s for us; his complete ­
ness theore m showe d that , for first­orde r logic , any sentenc e that is entaile d by anothe r set of
sentence s can be prove d from that set. Tha t is, we can find inferenc e rules that allow a complet e
proof procedur e R:
if KB |= Q then KB \­R a
The completenes s theore m is like sayin g that a procedur e for findin g a needl e in a haystac k does
exist . Thi s is not a trivia l claim , becaus e universall y quantifie d sentence s and arbitraril y neste d
functio n symbol s add up to haystack s of infinit e size. Gode l showe d that a proo f procedur e exists ,
but he did not demonstrat e one; it was not unti l 1965 that Robinso n publishe d his resolutio n
algorithm , whic h we discus s in the next section .
There is one proble m with the completenes s theore m that is a real nuisance . Not e that
we said that if a sentenc e follows , then it can be proved . Normally , we do not know unti l the
proof is don e that the sentenc e does  follow ; wha t happen s whe n the sentenc e doesn' t follow ?
Can we tell? Well , for first­orde r logic , it turn s out that we cannot ; our proo f procedur e can go
on and on, but we will not know if it is stuc k in a hopeles s loop or if the proo f is just abou t to
pop out. (Thi s is like the haltin g proble m for Turin g machines. ) Entailmen t in first­orde r logic
SEMIDECIDABL E i s thus  semidecidablc , that is, we can show that sentence s follo w from premises , if they do, but
we canno t alway s show it if they do not. As a corollary , consistenc y of sets of sentence s (the
questio n of whethe r there is a way to mak e all the sentence s true ) is also semidecidable .RESOLUTIO N
ALGORITH M
RESOLUTION : A COMPLET E INFERENC E PROCEDUR E
Recal l from Chapte r 6 that the simpl e versio n of the resolutio n inferenc e rule for propositiona l
logic has the followin g form :
aV/3,  ­i/g V 7
a V 7or equivalentl y­ia => /?, /3 => 7
278 Chapte r 9. Inferenc e in First­Orde r Logi c
The rule can be understoo d in two ways . First , we can see it as reasonin g by cases . If 3 is false ,
then from the first disjunction , a mus t be true ; but if l3 is true , then from the secon d disjunctio n
~ mus t be true . Hence , eithe r a or ­ mus t be true . Th e secon d way to understan d it is as
transitivit y of implication : from two implications , we deriv e a third that link s the premis e of the
first to the conclusio n of the second . Notic e that Modu s Ponen s does not allow us to deriv e new
implications ; it only derive s atomi c conclusions . Thus , the resolutio n rule is more powerfu l than
Modu s Ponens . In this section , we will see that a generalizatio n of the simpl e resolutio n rule can
serve as the sole inferenc e rule in a complet e inferenc e procedur e for first­orde r logic .
GENERALIZE D
RESOLUTIO N
(DISJUNCTIONS )
GENERALIZE D
RESOLUTIO N
(IMPLICATIONS )The resolutio n inferenc e rule
In the simpl e form of the resolutio n rule, the premise s have exactl y two disjuncts . We can exten d
that to get a mor e genera l rule that says that for two disjunction s of any length , if one of the
disjunct s in one claus e (/?,) unifie s with the negatio n of a disjunc t in the othe r (qk),  then infe r the
disjunctio n of all the disjunct s excep t for those two:
0 Generalize d Resolutio n (disjunctions) : For literals/? , and q,,
wher e UNlFY(/7 ;, ^q k) = 0:
p\ V.. . pj ...Vp m,
q\ V • • • q
SUBST(# , (/?, V . . . pj. , V Pj+i . . . pn, V <?, . . . qk­ 1 V qM ... V q,,))
Equivalently , we can rewrit e this in term s of implications :
<C> Generalize d Resolutio n (implications) : For atom s /?,•,<?,• , r,,s/
wher e UNIF Y (ph qk) = 0:
q\ V . . . qk . . . V q,,4
CONJUNCTIV E
NORMA L FOR M
IMPLICATIV E
NORMA L FOR MCanonica l form s for resolutio n
In the firs t versio n of the resolutio n rule , ever y sentenc e is a disjunctio n of literals . Al l the
disjunction s in the KB are assume d to be joine d in one big, implici t conjunctio n (as in a norma l
KB), so this form is calle d conjunctiv e norma l form (or CNF) , even thoug h each individua l
sentenc e is a disjunctio n (confusing , isn't it?).
In the secon d versio n of the resolutio n rule, each sentenc e is an implicatio n with a conjunc ­
tion of atom s on the left and a disjunctio n of atom s on the right . We call this implicativenorma l
form (or INF) , althoug h the nam e is not standard . We can transfor m the sentence s in (9.40 ) into
eithe r of the two forms , as we now show . (Notic e that we have standardize d apar t the variabl e
name s in these sentences. )
Sectio n 9.6. Resolution : A Complet e Inferenc e Procedur e 279
Conjunctiv e Norma l Form
­.P(w ) V Q(w)
P(x) V R(x)
­GO­ ) V S(y)
­>R(z)  V S(z)Implicativ e Norma l Form
P(w) =1
True  => P(x)V
Q(y) => 500(9.41 )
The two form s are notationa l variant s of each other , and as we will see on page 281 , any set
of sentence s can be translate d to eithe r form . Historically , conjunctiv e norma l form is mor e
common , but we will use implicativ e norma l form , becaus e we find it more natural .
It is importan t to recogniz e that resolution is  a generalization  of modus  ponens.  Clearly ,
the implicativ e norma l form is mor e genera l than Horn form , becaus e the right­han d side can be a
disjunction , not just a singl e atom . But at first glanc e it seem s that Modu s Ponen s has the abilit y
to combin e atom s with an implicatio n to infe r a conclusio n in a way that resolutio n canno t do.
This is just an illusion—onc e we realiz e that an atomi c sentenc e o in implicativ e norma l form is
writte n as True  => a, we can see that modu s ponen s is just a specia l case of resolution :
Q­, a =>• i3 True  =>• a, a => J————— — i s equivalen t to ————————­— —ft True  => 3
Even thoug h True  => a is the "correct " way to writ e an atomi c sentenc e in implicativ e norma l
form , we will sometime s writ e a as an abbreviation .
Resolutio n proof s
One coul d use resolutio n in a forward ­ or backward­chainin g algorithm , just as Modu s Ponen s is
used. Figur e 9.5 show s a three­ste p resolutio n proo f of S(A)  from the KB in (9.41) .
P(w)  => S(w) True  => P(x)  vR(x)
True ­> S(x) vR(x)
True => S(A)
Figur e 9.5 A  proo f that 5(A) follow s from the KB in (9.41) , usin g resolution . Eac h "vee " in
the proo f tree represent s a resolutio n step : the two sentence s at the top are the premises , and the
one at the botto m is the conclusio n or resolvent . The substitutio n is show n for each resolution .
280 Chapte r 9. Inferenc e in First­Orde r Logi c
Technically , the fina l resolven t shoul d be True  => S(A)  V S(A) , but we have take n the
libert y of removin g the redundan t disjunct . In som e systems , ther e is a separat e inferenc e rule
FACTORIN G calle d factorin g to do this, but it is simple r to just mak e it be part of the resolutio n rule.
Chainin g with resolutio n is more powerfu l than chainin g with Modu s Ponens , but it is still
not complete . To see that , conside r tryin g to prov e P V ~^P from the empt y KB. The sentenc e is
valid , but with nothin g in the KB, ther e is nothin g for resolutio n to appl y to, and we are unabl e
to prov e anything .
REFUTATIO N On e complet e inferenc e procedur e usin g resolutio n is refutation , also know n as proo f by
contradictio n and reducti o ad absurdum . The idea is that to prov e P, we assum e P is false
(i.e., add ­>P to the knowledg e base ) and prov e a contradiction . If we can do this, then it mus t be
that the knowledg e base implie s P. In othe r words :
(KB A ­./> => False)  O (KB  => P)
Proof by contradictio n is a powerfu l tool throughou t mathematics , and resolutio n give s us a
simple , sound , complet e way to appl y it. Figur e 9.6 give s an exampl e of the method . We start
with the knowledg e base of (9.41 ) and are attemptin g to prov e S(A).  We negat e this to get
­<S(A),  whic h in implicativ e norma l form is S(A ) => False,  and add it to the knowledg e base .
Then we appl y resolutio n until we arriv e at a contradiction , whic h in implicativ e norma l form is
True  =>• False.  It take s one mor e step than in Figur e 9.5, but that is a smal l pric e to pay for the
securit y of a complet e proo f method .
True  ­­> S(x) S(A) =>False
True =>False
Figur e 9.6 A  proo f that S(A)  follow s from the KB in (9.41 ) usin g resolutio n with refutation .
Sectio n 9.6. Resolution : A Complet e Inferenc e Procedur e 281
Conversio n to Norma l Form
So far, we have claime d that resolutio n is complete , but we have not show n it. In this sectio n we
show that any first­orde r logic sentenc e can be put into implicativ e (or conjunctive ) norma l form ,
and in the followin g sectio n we will show that from a set of sentence s in norma l form we can
prove that a give n sentenc e follow s from the set.
We presen t the procedur e for convertin g to norma l form , step by step , showin g that each
step does not chang e the meaning ; it shoul d be fairl y clea r that all possibl e sentence s are deal t
with properly . You shoul d understan d why each of the step s in this procedur e is valid , but few
peopl e actuall y manag e to remembe r them all.
<> Eliminat e implications : Recal l that p => q is the sam e as ~^p V q. So replac e all
implication s by the correspondin g disjunctions .
<) Mov e ­i inwards : Negation s are allowe d only on atom s in conjunctiv e norma l form , and
not at all in implicativ e norma l form . We eliminat e negation s with wide scop e usin g de
Morgan' s laws (see Exercis e 6.2) , the quantifie r equivalence s and doubl e negation :
i(p V q)
i(/> A q)
iV x,p
<3x,pbecome s
become s
become s
become s
become s­>p A ~^
­>p V ­*
3x ­>p
0 Standardiz e variables : For sentence s like (Vx P(x))  V (3* Q(x))  that use the sam e
variabl e nam e twice , chang e the nam e of one of the variables . This avoid s confusio n later
when we drop the quantifiers .
0 Mov e quantifier s left: The sentenc e is now in a form in whic h all the quantifier s can be
move d to the left, in the orde r in whic h they appear , withou t changin g the meanin g of the
sentence . It is tediou s to prov e this properly ; it involve s equivalence s such as
p V V x q become s V x p V q
whic h is true becaus e p here is guarantee d not to contai n an x.
SKOLEMIZATIO N 0  Skolemize : Skolemizatio n is the proces s of removin g existentia l quantifier s by elimina ­
tion. In the simpl e case , it is just like the Existentia l Eliminatio n rule of Sectio n 9.1 —
translat e 3x P(x)  into P(A),  wher e A is a constan t that does not appea r elsewher e in
the KB. But there is the adde d complicatio n that som e of the existentia l quantifiers , even
thoug h move d left, may still be neste d insid e a universa l quantifier . Conside r "Everyon e
has a heart" :
Vx Person(x)  =>• 3y Heart(y)  A Has(x,y)
If we just replace d y with a constant , H, we woul d get
V;c Person(x)  => Heart(H)  A Has(x,H)
whic h says that everyon e has the same hear t H. We need to say that the hear t they have is
not necessaril y shared , that is, it can be foun d by applyin g to each perso n a functio n that
maps from perso n to heart :
V* Person(x)  =? Heart(F(x))  A Has(x,F(x))
282 Chapte r 9. Inferenc e in First­Orde r Logi c
wher e F is a functio n nam e that does not appea r elsewher e in the KB. F is calle d a
SKOLE M FUNCTIO N Skole m function . In general , the existentiall y quantifie d variabl e is replace d by a term
that consist s of a Skole m functio n applie d to all the variable s universall y quantifie d outside
the existentia l quantifie r in question . Skolemizatio n eliminate s all existentiall y quantifie d
variables , so we are now free to drop the universa l quantifiers , becaus e any variabl e mus t
be universall y quantified .
<> Distribut e A over V: (a A b) V c become s (a V c) A (b V c).
<C> Flatte n neste d conjunction s and disjunctions : (a V b} V c become s (a V b V c), and
(a A b) A c become s (a A b A c).
At this point , the sentenc e is in conjunctiv e norma l form (CNF) : it is a conjunctio n wher e
every conjunc t is a disjunctio n of literals . This form is sufficien t for resolution , but it may
be difficul t for us human s to understand .
0 Conver t disjunction s to implications : Optionally , you can take one more step to conver t
to implicativ e norma l form . For each conjunct , gathe r up the negativ e literal s into one list,
the positiv e literal s into another , and buil d an implicatio n from them :
(­>a V ­1/7 V c V d) become s (a A b =>• c V d)
Exampl e proo f
We will now show how to appl y the conversio n procedur e and the resolutio n refutatio n procedur e
on a more complicate d example , whic h is state d in Englis h as:
Jack own s a dog.
Every dog owne r is an anima l lover .
No anima l love r kills an animal .
Eithe r Jack or Curiosit y kille d the cat, who is name d Tuna .
Did Curiosit y kill the cat?
First, we expres s the origina l sentence s (and som e backgroun d knowledge ) in first­orde r logic :
A. 3 x Dog(x)  A Owns(Jack,x)
B. VJK (3 y Dog(y)  A Owns(x,y)~)  => AnimalLover(x)
C.Vx  AnimalLover(x)  => My Animal(y)  =>• ­>Kills(x,y)
D. Kills(Jack,  Tuna)  V Kills(Curiosity,  Tuna)
E. Cat(Tuna)
F. \/x Cat(x)  => Animal(x)
Now we have to appl y the conversio n procedur e to conver t each sentenc e to implicativ e norma l
form . We will use the shortcu t of writin g P instea d of True  => P:
Al. Dog(D)
A2. Owns(Jack,D)
B. Dog(y)  A Owns(x,y)  => AnimalLover(x)
C. AnimalLover(x)  A Animal(y)  A Kills(x,  y) => False
D. Kills(Jack,  Tuna)  V Kills(Curiosity,  Tuna}
E. Cat(Tuna)
F. Cat(x)  => Animal(x)
Sectio n 9.6. Resolution : A Complet e Inferenc e Procedur e 283
The proble m is now to show that Kills(Curiosity,  Tuna)  is true . We do that by assumin g the
negation , Kills(Curiosity,  Tuna)  => False,  and applyin g the resolutio n inferenc e rule seve n
times , as show n in Figur e 9.7. We eventuall y deriv e a contradiction , False,  whic h mean s that the
assumptio n mus t be false , and Kills(Curiosity,  Tuna)  is true after all. In English , the proo f could
be paraphrase d as follows :
Suppos e Curiosit y did not kill Tuna . We kno w that eithe r Jack or Curiosit y did,
thus Jack mus t have . But Jack own s D, and D is a dog, so Jack is an anima l lover .
Furthermore , Tuna is a cat, and cats are animals , so Tuna is an animal . Anima l lover s
don't kill animals , so Jack couldn' t have kille d Tuna . But this is a contradiction ,
becaus e we alread y conclude d that Jack mus t have kille d Tuna . Hence , the origina l
suppositio n (tha t Curiosit y did not kill Tuna ) mus t be wrong , and we have prove d
that Curiosit y did kill Tuna .
The proo f answer s the questio n "Did Curiosit y kill the cat? " but ofte n we wan t to pose mor e
genera l questions , like "Wh o kille d the cat?" Resolutio n can do this, but it take s a little more
work to obtai n the answer . The quer y can be expresse d as 3 w Kills(w,  Tuna).  If you repea t the
proof tree in Figur e 9.7, substitutin g the negatio n of this query , Kills(w,  Tuna)  =>• False  for the
old query , you end up with a simila r proo f tree, but with the substitutio n {w/Curiosity}  in one of
the steps . So findin g an answe r to "Wh o kille d the cat" is just a matte r of lookingi n the proo f tree
to find the bindin g of w. It is straightforwar d to maintai n a compose d unifie r so that a solutio n is
availabl e as soon as a contradictio n is found .
Dog(D)  Dog(y)  A Owns(x.y)  =* Animal  Lover(x)  AnimalLover(x)  A Animal(y)  A Kills(x.y)  => False
AnimalLoveiix)  A Killsix,  Tuna)  =s> False KillsfJack,  Tuna}  v Kilk(Citriosit\,  Tuna)
Figur e 9.7 A  proo f that Curiosit y kille d the cat, usin g resolutio n with refutation .
284 Chapte r 9. Inferenc e in First­Orde r Logi c
Dealin g with equalit y
There is one proble m that we have not deal t with so far, namely , findin g appropriat e inferenc e
rules for sentence s containin g the equalit y symbol . Unificatio n does a good job of matchin g
variable s with othe r terms : P(x)  unifie s with P(A).  But P(A)  and P(B)  fail to unify , even if the
sentenc e A = B is in the knowledg e base. The proble m is that unificatio n only does a syntacti c test
based on the appearanc e of the argumen t terms , not a true semanti c test base d on the object s they
represent . Of course , no semanti c test is availabl e becaus e the inferenc e syste m has no acces s
to the object s themselves , but it shoul d still be able to take advantag e of what knowledg e it has
concernin g the identitie s and difference s amon g objects .
One way to deal with this is to axiomatiz e equality , by writin g dow n its properties . We
need to say that equalit y is reflexive , symmetric , and transitive , and we also have to say that we
can substitut e equal s for equal s in any predicat e or function . So we need three basic axioms , and
then one for each predicat e and function :
MX x=x
Vx,y  x = y => y = x
V 'x,y,z  x=y/\y  = z =?• x = z
\/x,y  x = y => (/>,(* ) & Pi(y))
Vx,y x=y  => (P 2(x) &  P 2(y})
Vw,x,y,z  w=y/\x  = z =>
Vw,x,y,z  w = y/\x=z  =>•
The othe r way to deal with equalit y is with a specia l inferenc e rule . The demodulatio n
rule takes an equalit y statemen t x=y and any sentenc e with a neste d term that unifie s with x and
derive s the same  sentenc e with y substitute d for the neste d term . Mor e formally , we can defin e
the inferenc e rule as follows :
DEMODULATIO N 0  Demodulation : For any term s x,y,  and z, wher e UNIFY(JC,Z ) = 6:
x=y,  (...z... )
(...SUBST(0 , >')... )
If we writ e all our equalitie s so that the simple r term is on the righ t (e.g. , (x + 0) = 0), then
demodulatio n will do simplification , becaus e it alway s replace s an expressio n on the left with
PARAMODULATIO N on e on the right . A more powerfu l rule calle d paramodulatio n deals with the case wher e we do
not know x­y,  but we do know,  say, x = y V P(x).
Resolutio n strategie s
We know that repeate d application s of the resolutio n inferenc e rule will find a proo f if one exists ,
but we have no guarante e of the efficienc y of this process . In this sectio n we look at four of the
strategie s that have been used to guid e the searc h towar d a proof .
Sectio n 9.6. Resolution : A Complet e Inferenc e Procedur e 285
Unit preferenc e
This strateg y prefer s to do resolution s wher e one of the sentence s is a singl e litera l (also know n
UNIT CLAUS E a s a unit clause) . Th e idea behin d the strateg y is that we are tryin g to produc e a very shor t
sentence , True  =>• False,  and therefor e it migh t be a good idea to prefe r inference s that produc e
shorte r sentences . Resolvin g a "uni t sentenc e (suc h as P) with any othe r sentenc e (suc h as
P A Q =>• R) alway s yield s a sentenc e (in this case , Q =>• R) that is shorte r than the othe r
sentence . Whe n the unit preferenc e strateg y was first tried for prepositiona l inferenc e in 1964 ,
it led to a dramati c speedup , makin g it feasibl e to prov e theorem s that coul d not be handle d
withou t the preference . Uni t preferenc e by itsel f does not, however , reduc e the branchin g facto r
in medium­size d problem s enoug h to mak e them solvabl e by resolution . It is, nonetheless , a
usefu l heuristi c that can be combine d with othe r strategies .
Set of suppor t
Preference s that try certai n resolution s first are helpful , but in genera l it is more effectiv e to try to
eliminat e some potentia l resolution s altogether . The set of suppor t strateg y does just that. It start s
SET OF SUPPOR T b y identifyin g a subse t of the sentence s calle d the set of support . Ever y resolutio n combine s
a sentenc e from the set of suppor t with anothe r sentence , and adds the resolven t into the set of
support . If the set of suppor t is smal l relativ e to the whol e knowledg e base , this will cut the
searc h spac e dramatically .
We have to be carefu l with this approach , becaus e a bad choic e for the set of suppor t will
make the algorith m incomplete . However , if we choos e the set of suppor t S so that the remainde r
of the sentence s are jointl y satisfiable , then set­of­suppor t resolutio n will be complete . A commo n
approac h is to use the negate d quer y as the set of support , on the assumptio n that the origina l
knowledg e base is consistent . (Afte r all, if it is not consistent , then the fact that the quer y follow s
from it is vacuous. ) The set­of­suppor t strateg y has the additiona l advantag e of generatin g proo f
trees that are ofte n easy for human s to understand , becaus e they are goal­directed .
Inpu t resolutio n
WPUT RESOLUTIO N I n the inpu t resolutio n strategy , ever y resolutio n combine s one of the inpu t sentence s (fro m the
KB or the query ) with som e othe r sentence . The proof s in Figur e 9.5 and Figur e 9.6 use only
inpu t resolutions ; they have the characteristi c shap e of a diagona l "spine " with singl e sentence s
combinin g onto the spine . Clearly , the spac e of proo f trees of this shap e is smalle r than the
space of all proo f graphs . In Horn knowledg e bases , Modu s Ponen s is a kind of inpu t resolutio n
strategy , becaus e it combine s an implicatio n from the origina l KB with som e othe r sentences .
Thus , it shoul d not be surprisin g that inpu t resolutio n is complet e for knowledg e base s that are
in Horn form , but incomplet e in the genera l case .
L|NEAR RESOLUTIO N Th e linea r resolutio n strateg y is a sligh t generalizatio n that allow s P and Q to be resolve d
togethe r if eithe r P is in the origina l KB or if P is an ancesto r of Q in the proo f tree. Linea r
resolutio n is complete .
286 Chapte r 9. Inferenc e in First­Orde r Logi c
Subsumptio n
SUBSUMPTIO N Th e subsumptio n metho d eliminate s all sentence s that are subsume d by (i.e., more specifi c than )
an existin g sentenc e in the KB. For example , if P(x) is in the KB, then there is no sens e in addin g
P(A),  and even less sens e in addin g P(A)  V Q(B).  Subsumptio n help s keep the KB small , whic h
helps keep the searc h spac e small :
9.7 COMPLETENES S OF RESOLUTIO N
REFUTATION ­
COMPLET E
HERBRAN D
UNIVERS EThis sectio n prove s that resolutio n is complete . It can be safel y skippe d by thos e who are willin g
to take it on faith .
Befor e we show that resolutio n is complete , we need to be more precis e abou t the particula r
flavo r of completenes s that we will establish . Resolutio n is refutation­complete , whic h mean s
that if a set of sentence s is unsatisfiable , then resolutio n will deriv e a contradiction . Resolutio n
canno t be used to generat e all logica l consequence s of a set of sentences , but it can be used to
establis h that a give n sentenc e is entaile d by the set. Hence , it can be used to find all answer s to
a give n question , usin g the negated­goa l metho d describe d before .
We will take it as give n that any sentenc e in first­orde r logi c (withou t equality ) can be
rewritte n in norma l form . Thi s can be prove d by inductio n on the form of the sentence , usin g
atomi c sentence s as the base case (Davi s and Putnam , 1960) . Our goal therefor e is to prov e the
following : ifS is an unsatisfiable  set of sentences  in clausal  form,  then  the application  of a finite
number  of resolution  steps to S will  yield a  contradiction.
Our proo f sketc h follow s the origina l proo f due to Robinson , with som e simplification s
from Geneseret h andNilsso n (1987) . The basi c structur e of the proo f is show n in Figur e 9.8, and
is as follows :
1. We begi n by observin g that if S is unsatisfiable , then there exist s a particula r set of ground
instances  of the sentence s of 5", such that this set is also unsatisfiabl e (Herbrand' s theorem) .
2. We then show that resolutio n is complet e for groun d sentence s (i.e., propositiona l logic) .
This is easy becaus e the set of consequence s of a propositiona l theor y is alway s finite .
3. We then use a liftin g lemm a to show that, for any resolutio n proo f usin g the set of groun d
sentences , ther e is a correspondin g proo f usin g the first­orde r sentence s from whic h the
groun d sentence s were obtained .
To carry out the first step, we will need thre e new concepts :
0 Herbran d universe : If S is a set of clauses , then Hs, the Herbran d univers e of S, is the set
of all groun d term s constructibl e from the following :
a. The functio n symbol s in S, if any.
b. The constan t symbol s in S, if any; if none , then the constan t symbo l A.
For example , if S contain s just the claus e P(x, F(x, A)) A Q(x,  A) => R(x,  B), then Hs is the
followin g infinit e set of groun d sentences :
{A, B, F(A,  A), F(A,  B), F(B,  A), F(B,  B), F(A,  F(A,  A)), F(A,  F(A, B)),...}
Sectio n 9.7. Completenes s of resolutio n 287
Anyset of sentence s S is representabl e in clausa l form
I
Assum e S is unsatisfiable , and in clausa l form
J ———————— —— Herbrand' s theore m
Some set S' of groun d instance s is unsatisfiabl e
There
Figur e 9.81 ———————— —
Resolutio n can find a contradictio n in S'
J ———————— —
is a resolutio n proo f for the contradictio n in S
Structur e of a completenes s proo f for resolution .— Groun d resolutio n
theore m
— Liftin g lemm a
SATURATIO N <0 > Saturation : If S is a set of clauses , and P is a set of groun d terms , then P(S),  the saturatio n
of S with respec t to P, is the set of all groun d clause s obtaine d by applyin g all possibl e
consisten t substitution s for variable s in S with groun d term s in P.
HERBRAN D BASE < ) Herbran d base : The saturatio n of a set of clause s S with respec t to its Herbran d univers e
is calle d the Herbran d base of S, writte n as Hs(S).  For example , if 5 contain s just the claus e
given above , then
HS(S) = {P(A,F(A,A))/\Q(A,A)  => R(A,B),
P(B,F(B,A))/\Q(B,A)  => R(B,B),
P(F(A,A),F(F(A,A),A))f\Q(F(A,A),A)  =>  R(F(A,A),B),
P(F(A,B),F(F(A,B),A))  /\Q(F(A,B),A)  =>  R(F(A,B),B),
Notic e that both the Herbran d univers e and the Herbran d base can be infinit e even if the
origina l set of sentence s S is finite .
Thes e definition s allow us to state a form of Herbrand' s theore m (Herbrand , 1930) :
If a set of clause s S is unsatisfiable , then ther e exist s a finit e subse t of H$(S)  that is also
unsatisfiable .
Let S' be this finit e subset . No w it will be usefu l to introduc e the resolutio n closur e of S',
whic h i s the set of all clause s derivabl e by repeate d applicatio n of the resolutio n inferenc e step to
clause s in S' or their derivatives . (To construc t this closure , we coul d run a breadth­firs t searc h to
completio n usin g the resolutio n inferenc e rule as the successo r generator. ) Let T be the resolutio n
closur e of S', and let Ay = {A],A­2,...,At}be  the set of atomi c sentence s occurrin g in S'. Notic e
that becaus e 5' is finite , A$' mus t also be finite . An d becaus e the clause s in T are constructe d
288 Chapte r 9. Inferenc e in First­Orde r Logi c
GODEL' s INCOMPLETENES S THEORE M
By slightl y extendin g the languag e of first­orde r logic to allow for the mathematica l
inductio n schem a in arithmetic , Godel  was able to show , in his incompletenes s
theorem , that there are true aYithmeti c sentence s that canno t be proved .
The proo f of the incompletenes s theore m is somewha t beyon d the scop e of this
book , occupying , as it does , at leas t 30 pages , but we can give a hint here . We
begin with the logica l theor y of numbers . In this theory , ther e is a singl e constant ,
0, and a singl e function , S (the successo r function) . In the intende d model , 5(0)
denote s 1, S(S(0) ) denote s 2, and so on; the languag e therefor e has name s for all
the natura l numbers . The vocabular y also include s the functio n symbol s +, x, and
Expt (exponentiation) , and the usua l set of logica l connective s and quantifiers . The
first step is to notic e that the set of sentence s that we can writ e in this languag e can
be enumerated . (Imagin e definin g an alphabetica l orde r on the symbol s and then
arrangin g in alphabetica l orde r each of the sets of sentence s of lengt h 1, 2, and so
on.) We can then numbe r each sentenc e a with a uniqu e natura l numbe r #a (the
Gode l number) . This is crucial : numbe r theor y contain s a nam e for each of its own
sentences . Similarly , we can numbe r each possibl e proo f P with a Gode l numbe r
G(P),  becaus e a proo f is simpl y a finit e sequenc e of sentences .
Now suppos e we have a set A of sentence s that are true statement s abou t the
natura l numbers . Recallin g that A can be name d by a give n set of integers , we can
imagin e writin g in our languag e a sentenc e a(j,A)  of the followin g sort:
V i i is not the Gode l numbe r of a proo f of the sentenc e whos e Gode l
numbe r is j, wher e the proo f uses only premise s in A.
Then let <r be the sentenc e a(#a,  A), that is, a sentenc e that state s its own unprovabilit y
from A. (Tha t this sentenc e alway s exist s is true but not entirel y obvious. )
Now we mak e the followin g ingeniou s argument . Suppos e that a is provabl e
from A; then a is false (becaus e a says it canno t be proved) . Bu t then we have a
false sentenc e that is provabl e from A, so A canno t consis t of only true sentences— a
violatio n of our premise . Therefor e a is no? provabl e from A. But this is exactl y wha t
u itsel f claims ; henc e u is a true sentence .
So, we have show n (barrin g 29 and a half pages ) that for any set of true sentence s
of numbe r theory , and in particula r any set of basi c axioms , ther e are othe r true
sentence s that cannot  be prove d from thos e axioms . Thi s establishes , amon g othe r
things , that we can neve r prov e all the theorem s of mathematic s within  any given
system  of axioms.  Clearly , this was an importan t discover y for mathematics . Its
significanc e for AI has been widel y debated , beginnin g with speculation s by Gode l
himself . We take up the debat e in Chapte r 26.
Sectio n 9.7. Completenes s of resolutio n 289
GROUN D
RESOLUTIO N
THEORE M
LIFTIN G LEMM Aentirel y from member s of As> , T mus t be finit e becaus e only a finit e numbe r of distinc t clause s
can be constructe d from a finit e vocabular y of atomi c sentences . To illustrat e thes e definitions ,
we will use a slimmed­dow n example :
S' = { P(A),  P(A)  => Q(A),  Q(A)  => False  }
AS­ = {P(A),  Q(A),  False}
T = {P(A),  P(A)  => Q(A),  Q(A)  => False,  Q(A),  P(A)  => False,  False}
Now we can state a completenes s theore m for resolutio n on groun d clauses . Thi s is calle d the
groun d resolutio n theorem :
If a set of groun d clause s is unsatisfiable , then the resolutio n closur e of thos e clause s
contain s the claus e False.
We prov e this theore m by showin g its contrapositive : if the closur e T does not contai n False,
then S' is satisfiable ; in fact, we can construc t a satisfyin g assignmen t for the atomi c sentence s in
S' ' . The constructio n procedur e is as follows :
Pick an assignmen t (True  or False)  for each atomi c sentenc e in Ay in som e fixe d orde r
­ If there is a claus e in T containin g the litera l ­iA/ , such that all its othe r literal s are
false unde r the assignmen t chose n for AI, . . . ,A,_i , then assig n A, to be False.
­ Otherwise , assig n A, to be True.
It is easy to show that the assignmen t so constructe d will satisf y S' , provide d T is close d unde r
resolutio n and does not contai n the claus e False  (Exercis e 9.10) .
Now we hav e establishe d that ther e is alway s a resolutio n proo f involvin g som e finit e
subse t of the Herbran d base of S. The next step is to show that there is a resolutio n proo f usin g
the clause s of S itself , whic h are not necessaril y groun d clauses . We start by considerin g a singl e
applicatio n of the resolutio n rule . Robinson' s basic lemm a implie s the followin g fact:
Let Cj and €2 be two clause s with no share d variables , and let C\ and C'2 be groun d
instance s of C\ and C2. If C' is a resolven t of C\ and C'2, then there exist s a claus e
C such that (1) C is a resolven t of C\ and €2, and (2) C' is a groun d instanc e of C.
This is calle d a liftin g lemma , becaus e it lifts a proo f step from groun d clause s up to genera l
first­orde r clauses . In orde r to prov e his basic liftin g lemma , Robinso n had to inven t unificatio n
and deriv e all of the propertie s of mos t genera l unifiers . Rathe r than repea t the proo f here , we
simpl y illustrat e the lemma :
Ci = P(x,F(x,A))  t\Q(x,A)  => R(x,B)
C2 = N(G(y),z)  => P(H(y),z)
C\ = P(H(B),F(H(B},A))f\Q(H(B),A)  =>  R(H(B),B}
C'2 = N(G(B),F(H(B),Ay)  =>  P(H(B},F(H(B),A))
C' = N(G(B),F(H(B),A))f\Q(H(B},A)  =>  R(H(B),B)
C = N(G(y),F(H(y),A))/\Q(H(y),A)  => R(H(y),B)
We see that indee d C' is a groun d instanc e of C. In general , for C{ and C'2 to have any resolvents ,
they mus t be constructe d by firs t applyin g to C\ and C2 the mos t genera l unifie r of a pair of
290 Chapte r 9. Inferenc e in First­Orde r Logi c
complementar y literal s in C\ and €2­ Fro m the liftin g lemma , it is easy to deriv e a simila r
statemen t abou t any sequenc e of application s of the resolutio n rule:
For any claus e C' in the resolutio n closur e of S', there is a claus e C in the resolutio n
closur e of S, such that C' is a groun d instanc e of C and the derivatio n of C is the
same lengt h as the derivatio n of C'.
From this fact , it follow s that if the claus e False  appear s in the resolutio n closur e of S', it mus t
also appea r in the resolutio n closur e of S. Thi s is becaus e False  canno t be a groun d instanc e
of any othe r clause . To recap : we hav e show n that if 5 is unsatisfiable , then ther e is a finit e
derivatio n of the claus e False  usin g the resolutio n rule.
The liftin g of theore m provin g from groun d clause s to first­orde r clause s provide d a vast
increas e in power . This derive s from the fact that the first­orde r proo f need instantiat e variable s
only as far as necessar y for the proof , wherea s the ground­claus e method s wer e require d to
examin e a huge numbe r of arbitrar y instantiations .
9.8 SUMMAR Y
We have presente d an analysi s of logica l inferenc e in first­orde r logic , and a numbe r of algorithm s
for doin g it.
• A simpl e extensio n of the prepositiona l inferenc e rule s allow s the constructio n of proof s
for first­orde r logic . Unfortunately , the branchin g facto r for the quantifie r is huge .
• The use of unificatio n to identif y appropriat e substitution s for variable s eliminate s the 5
instantiatio n step in first­orde r proofs , makin g the proces s muc h mor e efficient .
• A generalize d versio n of Modu s Ponen s uses unificatio n to provid e a natura l and powerfu l I
inferenc e rule , whic h can be used in a backward­chainin g or forward­chainin g algorithm .
• The canonica l form for Modu s Ponen s is Hor n form :
p\ A ... A pn => q, wher e pt and q are atoms .
This form canno t represen t all sentences , and Modu s Ponen s is not a complet e proo f system .
• The generalize d resolutio n inferenc e rule provide s a complet e syste m for proo f by refuta­ 1
tion. It require s a norma l form , but any sentenc e can be put into the form .
• Resolutio n can work with eithe r conjunctiv e norma l form —eac h sentenc e is a disjunctio n |
of literals—o r implicativ e norma l form —eac h sentenc e is of the form
p\ A ... A pn =>• q\ V ... V qm, wher e /?/ and q­, are atoms .
• Severa l strategie s exis t for reducin g the searc h spac e of a resolutio n syste m withou t com ­ |
promisin g completeness .
Sectio n 9.8. Summar y 29 1
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
Logica l inferenc e was studie d extensivel y in Gree k mathematics . The type of inferenc e mos t
SYLLOGIS M carefull y studie d by Aristotl e was the syllogism . The syllogis m is divide d into "figures " and
"moods, " dependin g on the orde r of the term s (whic h we woul d call predicates ) in the sentences ,
the degre e of generalit y (whic h we woul d toda y interpre t throug h quantifiers ) applie d to each
term, and whethe r each term is negated . The mos t fundamenta l syllogis m is that of the first moo d
of the first figure :
All S are M.
All M are P.
Therefore , all S are P.
Aristotl e tried to prov e the validit y of othe r syllogism s by "reducing " them to thos e of the first
figure . He was muc h less precis e in describin g wha t this "reduction " shoul d involv e than he was
in characterizin g the syllogisti c figure s and mood s themselves .
Rigorou s and explici t analysi s of inferenc e rule s was one of the stron g point s of Megaria n
and Stoi c propositiona l logic . The Stoic s took five basi c inferenc e rule s as vali d withou t proo f
and then rigorousl y derive d all other s from thes e five . The first and mos t importan t of the five
rules was the one know n toda y as Modu s Ponens . The Stoic s were muc h mor e precis e abou t
what was mean t by derivatio n than Aristotl e had been abou t wha t was mean t by reductio n of one
syllogis m to another . The y also used the "Principl e of Conditionalization, " whic h state s that if
q can be inferre d validl y from p, then the conditiona l "p => q" is logicall y true . Bot h thes e
principle s figur e prominentl y in contemporar y logic .
Inferenc e rules , othe r than logicall y valid schemas , were not a majo r focu s eithe r for Bool e
or for Frege . Boole' s logi c was closel y modele d on the algebr a of numbers . It relie d mainl y
on the equalit y substitutio n inferenc e rule , whic h allow s one to conclud e P(t) give n P(s) and
s = t. Logicall y valid schema s were used to obtai n equation s to whic h equalit y substitutio n coul d
be applied . Wherea s Frege' s logi c was muc h mor e genera l than Boole's , it too relie d on an
abundanc e of logicall y valid schema s plus a singl e inferenc e rule that had premises—i n Frege' s
case, this rule was Modu s Ponens . Freg e took advantag e of the fact that the effec t of an inferenc e
rule of the form "Fro m p infe r q" can be simulate d by applyin g Modu s Ponen s to p alon g with a
logicall y valid schem a p => q. This "axiomatic " style of exposition , usin g Modu s Ponen s plus
a numbe r of logicall y valid schemas , was employe d by a numbe r of logician s after Frege ; mos t
notably , it was used in Principia  Mathematica  (Whitehea d and Russell , 1910) .
One of the earlies t type s of system s (afte r Frege ) to focu s prominentl y on inferenc e rules was
natura l deduction , introduce d by Gerhar d Gentze n (1934 ) and by Stanisla w Jaskowsk i (1934) .
Natura l deductio n is calle d "natural " becaus e it does not requir e sentence s to be subjecte d to
extensiv e preprocessin g befor e it can be applie d to them (as man y othe r proo f procedure s do) and
becaus e its inferenc e rules are though t to be more intuitiv e than , say, the resolutio n rule. Natura l
deductio n make s frequen t use of the Principl e of Conditionalization . The object s manipulate d
by Gentzen' s inferenc e rule s are calle d sequents.  A sequen t can be regarde d eithe r as a logica l
argumen t (a pair of a set of premise s and a set of alternativ e conclusions , intende d to be an instanc e
of som e vali d rule of inference ) or as a sentenc e in implicativ e norma l form . Prawit z (1965 )
292 Chapte r 9. Inferenc e in First­Orde r Logi c
offer s a book­lengt h treatmen t of natura l deduction . Gallie r (1986 ) uses Gentze n sequent s to
expoun d the theoretica l underpinning s of automate d deduction .
Conjunctiv e norma l form and disjunctiv e norma l form for propositiona l formula s were
know n to Schrode r (1877) , althoug h the principle s underlyin g the constructio n of these form s
go back at leas t to Boole . Th e use of clausa l form (conjunctiv e norma l form for first­orde r
logic ) depend s upon certai n technique s for manipulatin g quantifiers , in particula r skolemization .
Whitehea d and Russel l (1910 ) expounde d the so­calle d rules  of passage  (the actua l term is from
Herbran d (1930) ) that are used to mov e quantifier s to the fron t of formulas . (Movin g all the
quantifier s to the fron t of a formul a is calle d prenexing  the formula , and a formul a with all
the quantifier s in fron t is said to be in prene x form. ) Hor n form was introduce d by Alfre d
Horn (1951) . Wha t we have calle d "implicativ e norma l form " was used (wit h a right­to­lef t
implicatio n symbol ) in Rober t Kowalski' s (1979b ) Logic  for Problem  Solving,  and this way of
writin g clause s is sometime s calle d "Kowalsk i form. "
Skole m constant s and Skole m function s were introduced , appropriatel y enough , by Thoral f
Skole m (1920) . The genera l procedur e for skolemizatio n is give n in (Skolem , 1928) , along with
the importan t notio n of the Herbran d universe .
Herbrand' s theorem , name d after the Frenc h logicia n Jacque s Herbran d (1930) , has playe d
a vital role in the developmen t of automate d reasonin g methods , both befor e and after Robinson' s
introductio n of resolution . This is reflecte d in our referenc e to the "Herbran d universe " rathe r than
the "Skole m universe, " even thoug h Skole m reall y invente d this concep t (and indee d Herbran d
does not explicitl y use Skole m function s and constants , but a less elegan t althoug h roughl y
equivalen t device) . Herbran d can also be regarde d as the invento r of unification , becaus e a
varian t of the unificatio n algorith m occur s in (Herbrand , 1930) .
First­orde r logic was show n to have complet e proo f procedure s by Gode l (1930) , usin g
method s simila r to those of Skole m and Herbrand . Alan Turin g (1936 ) and Alonz o Churc h (1936 )
simultaneousl y showed , usin g very differen t proofs , that validit y in first­orde r logi c was not
decidable . The excellen t text by Enderto n (1972 ) explain s all of these result s in a rigorou s yet
moderatel y understandabl e fashion .
The first mechanica l devic e to carr y out logica l inference s was constructe d by the third
Earl of Stanhop e (1753­1816) . The Stanhop e Demonstrato r coul d handl e syllogism s and certai n
inference s in the theor y of probability . The first mechanica l device  to carr y out inference s in
mathematical  logic was Willia m Stanle y Jevons' s "logica l piano, " constructe d in 1869 . Jevon s
was one of the nineteenth­centur y logician s who expande d and improve d Boole' s work ; the
logica l pian o carrie d out reasonin g in Boolea n logic . An entertainin g and instructiv e histor y of
these and othe r early mechanica l device s for reasonin g is give n by Marti n Gardne r (1968) .
The first publishe d result s from researc h on automate d deductio n usin g electroni c com ­
puter s were thos e of Newell , Shaw , and Simo n (1957 ) on the Logi c Theorist . Thi s progra m
was base d on an attemp t to mode l huma n though t processes . Marti n Davi s (1957 ) had actuall y
designe d a progra m that cam e up with a proo f in 1954 , but the Logi c Theorist' s result s were
publishe d slightl y earlier . Bot h Davis's  1954 progra m and the Logi c Theoris t were base d on
somewha t ad hoc method s that did not strongl y influenc e later automate d deduction .
It was Abraha m Robinso n who suggeste d attemptin g to use Herbrand' s Theore m to gener ­
ate proof s mechanically . Gilmor e (1960 ) wrot e a progra m that uses Robinson' s suggestio n in a
way influence d by the "Bet h tableaux " metho d of proo f (Beth , 1955) . Davi s and Putna m (1960 ) |
Sectio n 9.8. Summar y 293
introduce d clausa l form , and produce d a progra m that attempte d to find refutation s by substitutin g
member s of the Herbran d univers e for variable s to produc e groun d clause s and then lookin g for
prepositiona l inconsistencie s amon g the groun d clauses . Prawit z (1960 ) develope d the key idea
of lettin g the ques t for prepositiona l inconsistenc y drive the searc h process , and generatin g term s
from the Herbran d univers e only whe n it was necessar y to do so in orde r to establis h prepositiona l
inconsistency . Afte r furthe r developmen t by othe r researchers , this idea led J. A. Robinso n (no
relation ) to develop  the resolutio n metho d (Robinson , 1965) , whic h used unificatio n in its mod ­
ern form to allo w the demonstratio n of propositiona l inconsistenc y withou t necessaril y makin g
explici t use of term s from the Herbran d universe . The so­calle d "invers e method " develope d at
abou t the same time by the Sovie t researche r S. Maslo v (1964 ; 1967 ) is base d on principle s some ­
what differen t from Robinson' s resolutio n metho d but offer s simila r computationa l advantage s
in avoidin g the unnecessar y generatio n of term s in the Herbran d universe . The relation s betwee n
resolutio n and the invers e metho d are explore d by Maslo v (1971 ) and by Kuehne r (1971) .
The demodulatio n rule describe d in the chapte r was intende d to eliminat e equalit y axiom s
by combinin g equalit y substitutio n with resolution , and was introduce d by Wos (1967) . Ter m
rewritin g system s such as the Knuth­Bendi x algorith m (Knut h and Bendix , 1970 ) are base d on
demodulation , and have foun d wide applicatio n in programmin g languages . The paramodulatio n
inferenc e rule (Wo s and Robinson , 1968 ) is a more genera l versio n of demodulatio n that provide s
a complet e proo f procedur e for first­orde r logic with equality .
In additio n to demodulatio n and paramodulatio n for equalit y reasoning , othe r special ­
purpos e inferenc e rule s hav e been introduce d to aid reasonin g of othe r kinds . Boye r and
Moor e (1979 ) provid e powerfu l method s for the use of mathematica l inductio n in automate d
reasoning , althoug h their logi c unfortunatel y lack s quantifier s and does not quit e have the full
powe r of first­orde r logic . Stickel' s (1985 ) "theor y resolution " and Mann a and Waldinger' s (1986 )
metho d of "specia l relations " provid e genera l way s of incorporatin g special­purpos e inferenc e
rules into a resolution­styl e framework .
A numbe r of contro l strategie s have been propose d for resolution , beginnin g with the unit
preferenc e strateg y (Wo s et al., 1964) . The set of suppor t strateg y was propose d by Wos et
al. (1965),  to provid e a degre e of goal­directednes s in resolution . Linear  resolution  first appeare d
in (Loveland , 1968) . Wolfgan g Bibe l (1981 ) develope d the connectio n metho d whic h allow s
comple x deduction s to be recognize d efficiently . Development s in resolutio n contro l strategies ,
and the accompanyin g growt h in the understandin g of the relationshi p betwee n completenes s and
syntacti c restriction s on clauses , contribute d significantl y to the developmen t of logic program ­
ming (see Chapte r 10). Geneseret h and Nilsso n (1987 , Chapte r 5) provid e a shor t but thoroug h
analysi s of a wide variet y of contro l strategies .
There are a numbe r of good general­purpos e introduction s to automate d deductio n and to
the theor y of proo f and inference ; som e were mentione d in Chapte r 7. Additiona l textbook s on
matter s relate d to completenes s and undecidabilit y includ e Computability  and Logic  (Boolo s and
Jeffrey , 1989) , Metalogic  (Hunter , 1971) , and (for an entertainin g and unconventional , yet highl y
rigorou s approach ) A Course  in Mathematical  Logic  (Manin,  1977) . Man y of the mos t importan t
paper s from the turn­of­the­centur y developmen t of mathematica l logi c are to be foun d in From
Frege  to Godel:  A Source  Book  in Mathematical  Logic  (van Heijenoort , 1967) . Th e journa l
of recor d for the field of pure mathematica l logi c (as oppose d to automate d deduction ) is The
Journal  of Symbolic  Logic.
294 Chapte r 9. Inferenc e in First­Orde r Logi c
Textbook s geare d towar d automate d deductio n includ e (in additio n to thos e mentione d in
Chapte r 7) the classi c Symbolic  Logic and Mechanical  Theorem  Proving  (Chan g and Lee, 1973 )
and, mor e recently , Automated  Reasoning:  Introduction  and Applications  (Wo s et al., 1992) .
The two­volum e antholog y Automation  of Reasoning  (Siekman n and Wrightson , 1983 ) include s
many importan t paper s on automate d deductio n from 1957 to 1970 . The historica l summarie s
prefacin g each volum e provid e a concis e yet thoroug h overvie w of the histor y of the field . Furthe r
importan t historica l informatio n is availabl e from Loveland' s "Automate d Theore m Proving : A
Quarter­Centur y Review " (1984 ) and from the bibliograph y of (Wo s et al., 1992) .
The principa l journa l for the field of theore m provin g is the Journal of Automated Reason­
ing; importan t result s are also frequentl y reporte d in the proceeding s of the annua l Conference s
on Automate d Deductio n (CADE) . Researc h in theore m provin g is also strongl y relate d to the use
of logic in analyzin g program s and programmin g languages , for whic h the principa l conferenc e
is Logi c in Compute r Science .
EXERCISE S
9.1 Fo r each of the followin g pairs of atomi c sentences , give the most genera l unifier , if it exists .
a. P(A,B,B),P(x,y,z).
b. QKy,G(A,B)),Q(G(x,x),y).
c. Older(Father(y),y),  Older(Father(x),John).
d. Knows(Father(y),y),  Knows(x,x).
9.2 On e migh t suppos e that we can avoi d the proble m of variabl e conflic t in unificatio n by
standardizin g apar t all of the sentence s in the knowledg e base once and for all. Sho w that for j
some sentences , this approac h canno t work . (Hint:  Conside r a sentence , one part of whic h unifie s
with another. )
9.3 Sho w that the final state of the knowledg e base after a serie s of calls to FORWARD­CHAI N is i
independen t of the orde r of the calls . Doe s the numbe r of inferenc e steps require d depen d on the)
order in whic h sentence s are added ? Sugges t a usefu l heuristi c for choosin g an order .
9.4 Writ e dow n logica l representation s for the followin g sentences , suitabl e for use with Gen­
eralize d Modu s Ponens :
a. Horses , cows , and pigs are mammals .
b. An offsprin g of a hors e is a horse .
c. Bluebear d is a horse .
d. Bluebear d is Charlie' s parent .
e. Offsprin g and paren t are invers e relations .
f. Ever y mamma l has a parent .
Sectio n 9.8. Summar y 295
9.5 In this questio n we will use the sentence s you wrot e in Exercis e 9.4 to answe r a questio n
using a backward­chainin g algorithm .
a. Dra w the proo f tree generate d by an exhaustiv e backward­chainin g algorith m for the quer y
3h Horse(h).
b. Wha t do you notic e abou t this domain ?
c. How man y solution s for h actuall y follo w from your sentences ?
d. Can you thin k of a way to find all of them ? (Hint:  You migh t wan t to consul t (Smit h et al.,
1986). )
9.6 A popula r children' s riddl e is "Brother s and sister s have I none , but that man' s fathe r is my
father' s son." Use the rules of the famil y domai n (Chapte r 7) to show who that man is. You may
use any of the inferenc e method s describe d in this chapter .
9.7 Ho w can resolutio n be used to show that a sentenc e is
a. Valid ?
b. Unsatisfiable ?
9.8 Fro m "Horse s are animals, " it follow s that "The head of a hors e is the head of an animal. "
Demonstrat e that this inferenc e is valid by carryin g out the followin g steps :
a. Translat e the premis e and the conclusio n into the languag e of first­orde r logic . Use three
predicates : HeadOf(h,x),  Horse(x),  andAnimal(x).
b. Negat e the conclusion , and conver t the premis e and the negate d conclusio n into conjunctiv e
norma l form .
c. Use resolutio n to show that the conclusio n follow s from the premise .
9.9 Her e are two sentence s in the languag e of first­orde r logic :
(A):V * 3y (x>y)
(B):3> ­ MX (x>y)
a. Assum e that the variable s rang e over all the natura l number s 0,1,2,... , oo, and that the
">" predicat e mean s "greate r than or equa l to." Unde r this interpretation , translat e these
sentence s into English .
b. Is (A) true unde r this interpretation ?
c. Is (B) true unde r this interpretation ?
d. Doe s (A) logicall y entai l (B)?
e. Doe s (B) logicall y entai l (A)?
f. Try to prov e that (A) follow s from (B) usin g resolution . Do this even if you think that (B)
does not logicall y entai l (A); continu e unti l the proo f break s dow n and you canno t procee d
(if it does brea k down) . Sho w the unifyin g substitutio n for each resolutio n step . If the
proof fails , explai n exactl y where , how , and why it break s down .
g. Now try to prov e that (B) follow s from (A).
296 Chapte r 9. Inferenc e in First­Orde r Logi c
9.10 In this exercise , you will complet e the proo f of the groun d resolutio n theore m give n in the
chapter . The proo f rests on the claim that if T is the resolutio n closur e of a set of groun d clause s
S', and T does not contai n the claus e False,  then a satisfyin g assignmen t can be constructe d for
S' usin g the constructio n give n in the chapter . Sho w that the assignmen t does indee d satisf y S',
as claimed .
10LOGICA L REASONIN G
SYSTEM S
In which we  show  how  to build  efficient  programs  that reason  with  logic.
10.1 INTRODUCTIO N
THEORE M PROVER S
LUG ic
PROGRAMMIN G
LANGUAGE SWe have explaine d that it is a good idea to buil d agent s as reasonin g systems—system s that
explicitl y represen t and reaso n with knowledge . The main advantag e of such system s is a high
degre e of modularity . The contro l structur e can be isolate d from the knowledge , and each piec e
of knowledg e can be largel y independen t of the others . This make s it easie r to experimen t with
the syste m and modif y it, make s it easie r for the syste m to explai n its working s to anothe r agent ,
and, as we will see in Part VI, make s it easie r for the syste m to learn on its own .
In this chapte r the rubbe r hits the road , so to speak , and we discus s way s in whic h these
advantage s can be realize d in an actual , efficien t system . Automate d reasonin g system s com e in
severa l flavors , each designe d to addres s differen t kind s of problems . We grou p them into four
main categories :
<) Theore m prover s and logic programmin g languages : Theore m prover s use resolutio n
(or som e othe r complet e inferenc e procedure ) to prov e sentence s in full first­orde r logic ,
often for mathematica l and scientifi c reasonin g tasks . The y can also be used to answe r
questions : the proo f of a sentenc e containin g variable s serve s as an answe r to a questio n
becaus e it instantiate s the variables . Logi c programmin g language s typicall y restric t the
logic , disallowin g full treatmen t of negation , disjunction , and/o r equality . The y usuall y use
backwar d chaining , and may includ e som e nonlogica l feature s of programmin g language s
(such as inpu t and output) . Example s of theore m provers : SAM , AURA , OTTER . Example s
of logic programmin g languages : Prolog , MRS , LIFE .
<> Productio n systems : Like logic programmin g languages , thes e use implication s as their
primar y representation . The consequen t of each implicatio n is interprete d as an actio n
recommendation , rathe r than simpl y a logica l conclusion . Action s includ e insertion s and
297
298 Chapte r 10 . Logica l Reasonin g System s
FRAM E SYSTEM S
SEMANTI C
NETWORK S
DESCRIPTIO N LOGI C
SYSTEM S
TERMINOLOGICA L
LOGIC Sdeletion s from the knowledg e base as well as inpu t and output . Productio n system s operat e
with a forward­chainin g contro l structure . Som e have a conflic t resolutio n mechanis m
to decid e whic h actio n to take whe n more than one is recommended . Examples : OPS­5 ,
CLIPS , SOAR .
<} Fram e system s and semanti c networks : Thes e system s use the metapho r that object s are
nodes in a graph , that these node s are organize d in a taxonomi c structure , and that links
betwee n node s represen t binar y relations . In fram e system s the binar y relation s are though t
of as slots in one fram e that are fille d by another , wherea s in semanti c networks , they are
though t of as arrow s betwee n nodes . The choic e betwee n the fram e metapho r and the
semanti c networ k metapho r determine s whethe r you draw the resultin g network s as neste d
boxes or as graphs , but the meanin g and implementatio n of the two type s of system s can
be identical . In this chapte r we will say "semanti c network " to mean "semanti c networ k or
frame system. " Example s of fram e systems : OWL , FRAIL , KODIAK . Example s of semanti c
networks : SNEPS , NETL , Conceptua l Graphs .
0 Descriptio n logic systems : Thes e system s evolve d from semanti c network s due to pressur e
to formaliz e wha t the network s mean whil e retainin g the emphasi s on taxonomi c structur e '•
as an organizin g principle . The idea is to expres s and reaso n with comple x definitions }
of, and relation s among , object s and classes . Descriptio n logic s are sometime s calle d
terminologica l logics , becaus e they concentrat e on definin g terms . Recen t wor k has \
concentrate d on the trade­of f betwee n expressivit y in the languag e and the computationa l |
complexit y of certai n operations . Examples : KL­ONE , CLASSIC , LOOM .
In this chapter , we will see how each of the four type s of system s can be implemented , and how ;
each of the followin g five task s is addressed :
1. Add a new fact to the knowledg e base . Thi s coul d be a percept , or a fact derive d via]
inference . We call this functio n TELL .
2. Give n a knowledg e base and a new fact, deriv e some of the facts implie d by the conjunctio n j
of the knowledg e base and the new fact. In a forward­chainin g system , this is part of TELL . \
3. Decid e if a quer y is entaile d by the knowledg e base . We call this functio n ASK . Differen t 1
version s of ASK do differen t things , rangin g from just confirmin g that the quer y is entailed !
to returnin g a set of all the possibl e substitution s that mak e the quer y true.
4. Decid e if a quer y is explicitl y store d in the knowledg e base— a restricte d versio n of ASK .
5. Remov e a sentenc e from the knowledg e base . It is importan t to distinguis h between !
correctin g a sentenc e that prove s to be false , forgettin g a sentenc e that is no longe r usefu l j
(perhap s becaus e it was only relevan t in the past) , and updatin g the knowledg e base to I
reflec t a chang e in the world , whil e still rememberin g how the worl d was in the past , j
(Som e system s can mak e this distinction ; other s rely on the knowledg e enginee r to keep j
thing s straight. )
All knowledge­base d system s rely on the fundamenta l operatio n of retrievin g sentence s satisfyin g 1
certai n conditions—fo r example , findin g an atomi c sentenc e that unifie s with a query , or findin g j
an implicatio n that has a give n atomi c sentenc e as one of its premises . We will therefor e begi n j
with technique s for maintainin g a knowledg e base in a form that support s efficien t retrieval .
Sectio n 10.2 . Indexing , Retrieval , and Unificatio n 299
INDEXING , RETRIEVAL , AND UNIFICATIO N
The function s TEL L and ASK can in genera l do complicate d reasonin g usin g forwar d and backwar d
chainin g or resolution . In this section , we conside r two simple r function s that implemen t the part
of TEL L and ASK that deal s directl y with the physica l implementatio n of the knowledg e base .
We call these function s STOR E and FETCH . We also describ e the implementatio n of a unificatio n
algorithm , anothe r basic componen t of knowledge­base d systems .
Implementin g sentence s and term s
The first step in buildin g a reasonin g syste m is to defin e the data type s for sentence s and terms .
This involve s definin g both the synta x of sentences—th e forma t for interactin g with the user at
the logi c level—an d the interna l representatio n in whic h the syste m will stor e and manipulat e
sentence s at the implementatio n level . Ther e may be severa l interna l representation s for differen t
aspect s of sentences . For example , there may be one form that allow s the syste m to prin t sentence s
and anothe r to represen t sentence s that have been converte d to clausa l form .
Our basic data type will represen t the applicatio n of an operato r (whic h coul d be a predicate ,
a functio n symbo l or a logica l connective ) to a list of argument s (whic h coul d be term s or
sentences) . We will call this genera l data type a COMPOUND . It has field s for the operato r (OP )
and argument s (ARCS) . For example , let c be the compoun d P(x)  A Q(x);  then OP[C ] = A  and
ARGS[C ] = [/>(*) , Q(x)].
Store and fetch
Now that we have a data type for sentence s and terms , we need to be able to maintai n a set
of sentence s in a knowledg e base , whic h mean s storin g them in such a way that they can be
fetche d efficiently . Typically , FETC H is responsibl e for findin g sentence s in the knowledg e base
that unif y with the query , or at least have the same syntacti c structure . ASK is responsibl e for
the inferenc e strategy , whic h result s in a serie s of calls to FETCH . The  computational  cost  of
inference  is dominated  by two aspects:  the search  strategy  used  by ASK and the data  structures
used to implement  FETCH .
The call STORE(KB,S)  adds each conjunc t of the sentenc e S to the knowledg e base KB. The
simples t approac h is to implemen t the knowledg e base as an array or linke d list of conjuncts . For
example , afte r
TELL(KB,  A A ­.£)
TELL(KB,  ­.C A D)
the KB will contai n a list with the element s
The call FETCH(KB,Q)  mus t then go throug h the element s of the knowledg e base one at a time
until it eithe r find s a conjunc t that matche s Q or reache s the end. Wit h this approac h FETC H take s
300 Chapte r 10 . Logica l Reasonin g System s
O(ri) time on an n­elemen t KB. STOR E take s O( 1) time to add a conjunc t to the KB, but if we wan t
to ensur e that no duplicate s are added , then STOR E is also O(n).  This is impractica l if one want s
to do seriou s inference .
Table­base d indexin g
A bette r approac h is to implemen t the knowledg e base as a hash table.1 If we only had to deal
with groun d litera l sentences2 we coul d implemen t STOR E so that when give n P it store s the valu e
true in the hash table unde r the key P, and whe n give n ­>P, it stores/a/s e unde r the key P. Then
FETC H coul d do a simpl e looku p in the hash table , and both FETC H and STOR E woul d be 0(1) .
There are two problem s with this approach : it does not deal with comple x sentence s
other than negate d sentences , and it does not deal with variable s withi n sentences . So FETC H
woul d not be able to find "an implicatio n with P as consequent, " such as migh t be require d by
a backward­chainin g algorithm ; nor coul d it find Brother(Richard,  John)  whe n give n a quer y
3x Brother(Richard,x).
The solutio n is to mak e STOR E maintai n a more comple x table . We assum e the sentence s
are all converte d to a norma l form . (We use implicativ e norma l form. ) The keys to the table will
be predicat e symbols , and the valu e store d unde r each key will have four components :
• A list of positiv e literal s for that predicat e symbol .
• A list of negativ e literals .
• A list of sentence s in whic h the predicat e is in the conclusion .
• A list of sentence s in whic h the predicat e is in the premise .
So, give n the knowledg e base :
Brother(Richard,  John)
Brother(Ted,Jack)  A Brother(Jack,  Bobbie)
­^Brother(Ann,  Sam)
Brother(x,y)  => Male(x)
Brother(x,  y) A Male(y)  =>• Brother(y,  x)
Male(Jack)  A Male(Ted)  A ... A ­>Male(Ann)  A ...
the table for the knowledg e base woul d be as show n in Figur e 10.1 .
Now suppos e that we ask the quer y
ASK(KB,Brother(Jack,  Ted))
and that ASK uses backwar d chainin g (see Sectio n 9.4). It woul d first call FETC H to find a positiv e
litera l matchin g the query . Becaus e that fails , FETC H is calle d to find an implicatio n with Brother
as the consequent . The quer y matche s the consequent , the anteceden t become s the new goal after
the appropriat e substitutio n is applied , and the procedur e begin s again .
' A hash table is a data structur e for storin g and retrievin g informatio n indexe d by fixe d keys.  For practica l purposes ,
a hash table can be considere d to have constan t storag e and retrieva l times , even whe n the tabl e contain s a vary large
numbe r of items .
2 Remembe r that a groun d litera l contain s no variables . It is eithe r an atomi c sentenc e such as Brother(Richard,  John)
or a negate d atomi c sentenc e such as ^Brother(Ann,  Victoria).
L
Sectio n 10.2 . Indexing , Retrieval , and Unificatio n 301
Key
Brother
MalePositiv e
Brother(Richard,John)
Bwther(Ted,  Jack)
Brother(Jack,  Bobbie)
Male(Jack)
Male(Ted)Negativ e
­iBrother(Ann,  Sam)
­iMale(Ann)Conclusio n
Brother(x,y)  A Male(y)
=f­ Brother(y,x)
Brother(x,y)  => Male(x)Premis e
Brother(x,y)  A Male(y)
=> Brother(y,x)
Brother(x,y)  => Male(x)
Brother(x,y)  A Mak(y)
=> Brother(y,x)
Figur e 10.1 Table­base d indexin g for a collectio n of logica l sentences .
Becaus e in first­orde r logic the predicat e is alway s fixe d in a query , the simpl e devic e of
dividin g up the knowledg e base by predicat e reduce s the cost of fetchin g considerably . But why
stop with just the predicate ?
TREE­BASE D
INDEXIN G
COMBINE D INDEXIN GTree­base d indexin g
Table­base d indexin g is idea l whe n there are man y predicat e symbol s and a few clause s for each
symbol . Bu t in som e applications , ther e are man y clause s for a give n predicat e symbol . For
example , in a knowledg e base for the U.S. Censu s Burea u that uses socia l securit y number s to
represen t people , the quer y Brother(0\ 2­34­5678, x) woul d requir e a searc h throug h million s of
Brother  literals .
To mak e this searc h efficient , we need to inde x on the argument s as well as on the predicat e
symbol s themselves . One way to do this is to chang e the table entr y for Brother  so that each
entry is itsel f a table , indexe d by the first argument , rathe r than just a list. So to answe r the
query Brother(Q  12­34­5678 , x), we first look in the predicat e table unde r Brother,  and then look
in that entr y unde r 012­34­5678 . Bot h table lookup s take a smal l constan t amoun t of time , so
the complet e FETC H is efficient . We can view the proces s of searchin g for a matchin g litera l as
a walk dow n a tree, wher e the branc h at each poin t is dictate d by the symbo l at the appropriat e
point in the quer y sentenc e (Figur e 10.2) .
Tree­base d indexin g is one form of combine d indexing , in that it essentiall y make s a
combine d key out of the sequenc e of predicat e and argumen t symbol s in the query . Unfortunately ,
it provide s little help whe n one of the symbol s in the sequenc e is a variable , becaus e ever y branc h
has to be followe d in that case . Suppos e our censu s knowledg e base has a predicat e Taxpayer
with four arguments : a person , a zip code , a net incom e to the neares t thousand , and the numbe r
of dependents , for example :
roxpa);er(012­34­5678,02138,32000,10 )
Suppos e we were intereste d in findin g all the peopl e in zip code 0213 8 with exactl y 10 dependents :
FETCH(7a*7wyer(/7,02138 , i, 10))
There are tens of thousand s of peopl e with that zip code , and hundred s of thousand s of peopl e in
the countr y with 10 dependents , but there are probabl y only a few peopl e that matc h both criteria .
To find thos e peopl e withou t undu e effort , we need a combine d inde x base d on both the secon d
and fourt h argument . If we are to deal with ever y possibl e set of variable  positions , we will need
302 Chapte r 10 . Logica l Reasonin g System s
Predicate  ?
Figur e 10.2 Tree­base d indexin g organize s a knowledg e base into a neste d serie s of hash
tables . Each node in the tree is a hash table indexe d by the valu e at a particula r sentenc e position .
2" combine d indices , wher e n is the numbe r of symbo l position s in the sentence s bein g stored .
Whe n sentence s includ e comple x terms , n can easil y grow quit e large . At som e point , the extra
storag e neede d for the indice s and the extra work that STOR E mus t do to maintai n them outweig h
the benefits . We can respon d by adoptin g a fixed policy , such as maintainin g indice s only on keys
compose d of predicat e plus each argument ; or by usin g an adaptiv e polic y that create s indice s to
meet the demand s of the kind s of querie s bein g asked .
CROSS­INDEXIN G Th e cross­indexin g strateg y indexe s entrie s in severa l places , and whe n faced with a quer y
choose s the mos t promisin g place for retrieval . Suppos e we have the quer y
FETCH(Taxpayer(p,  02138,20000,3) )
and the fou r availabl e indice s key on Taxpayer  plu s each of the fou r argumen t position s
separately . A  sentenc e matchin g the quer y wil l be indexe d unde r Taxpayer(­,Q2l3&,­,­),
Taxpayer(­,  _, 20000 , _), and Taxpayer^.,  _, _, 3). The best strateg y is usuall y to searc h throug h
whicheve r of these collection s of sentence s is smallest .
The unificatio n algorith m
In Sectio n 9.3 we showe d how two statement s such as
Knows(John,x)  => • Hates(John,x)
Knows(John,  Jane)
can be combine d to infe r Hates(John,Jane).  Th e key to this Modu s Ponen s inferenc e is to
unify Knows(John,x)  and Knows(John,Jane).  This unificatio n yield s as a resul t the substitutio n
{x/Jane},  whic h can then be applie d to Hates(John,  x) to give the solutio n Hates(John,  Jane).
We have seen that by cleve r indexing , we can reduc e the numbe r of calls to the unificatio n
algorithm , but this numbe r still can be quit e large . Thus , the unificatio n algorith m shoul d be
efficient . The algorith m show n in Figur e 10.3 is reasonabl y simple . It recursivel y explore s the
Sectio n 10.2 . Indexing , Retrieval , and Unificatio n 303
two expression s simultaneously , buildin g up a unifie r as it goes alon g but failin g if it ever find s
two correspondin g point s in the structure s that do not match . Unfortunately , it has one expensiv e
step. Th e occur­chec k take s time linea r in the size of the expressio n bein g checked , and is
done for each variabl e found . It make s the time complexit y of the algorith m O(n2) in the size
of the expression s bein g unified . Late r we will see how to mak e this algorith m mor e efficien t
by eliminatin g the need for explici t representation s of substitutions . On page 308, we see how
unificatio n can be extende d to handl e more informatio n beside s equality .
functio n UNIFY(> , y) return s a substitutio n to mak e x and y identical , if possibl e
UNIFY­lNTERNALCr,}' , {})
functio n UNIFY­!NTERNAL(J:,}' , 9) return s a substitutio n to mak e x and y identica l (give n 9)
inputs : x, a variable , constant , list, or compoun d
y, a variable , constant , list, or compoun d
9, the substitutio n buil t up so far
if 9 = failur e then retur n failur e
else if x = y then retur n f)
else if VARIABLE?(X ) then retur n UNIFY­ VAK(x,y , 9)
else if VARIABLE?O> ) then retur n UNIFY­VAR(y,Jc,0 )
else if COMPOUND KX) and COMPOUND?(y ) then
return UNIFY­!NTERNAL(ARGS[^] , ARGSty] , UNIFY­INTERNAL(OP[JC] , Op\y],  9)
else if LIST?(JC ) and LIST?(V ) then
retur n UNIFY­lNTERNAL(RESTW , REST[y] , UNIFY­lNTERNAL(FlRST[;t] , FlRST[y] , 9))
else retur n failur e
functio n UNIFY­VAR(var , x, 9)  return s a substitutio n
inputs : var,  a variabl e
x, any expressio n
9, the substitutio n buil t up so far
if [varlval]  € 9
then retur n UNiFY­lNTERNAL(va/,jf , 6)
else if {x/val}  e 9
then retur n UNIFY­lNTERNAL(var , val, 9)
else if var occur s anywher e in x I * occur­check  * I
then retur n failur e
else retur n add {x/var}  to ff
Figur e 10.3 Th e unificatio n algorithm . The algorith m work s by comparin g the structure s of
the inputs , elemen t by element . The substitutio n 9 that is the argumen t to UNIFY­!NTERNA L is
built up alon g the way , and used to mak e sure that later comparison s are consisten t with binding s
that were establishe d earlier .
304 Chapte r 10 . Logica l Reasonin g System s
10.3 LOGI C PROGRAMMIN G SYSTEM S
PROLO GWe now turn from the detail s of implementin g a knowledg e base to a compariso n of way s in
which a knowledg e base can be constructe d and used . We start with logic programming .
We have seen that the declarativ e approac h has man y advantage s for buildin g intelligen t
systems . Logi c programmin g tries to exten d thes e advantage s to all programmin g tasks . Any
computation  can be viewed  as a process  of making  explicit  the consequences  of choosing  a
particular  program  for a particular machine  and providing  particular  inputs.  Logi c programmin g
view s the progra m and input s as logica l statement s abou t the world , and the proces s of makin g
consequence s explici t as a proces s of inference . The relatio n betwee n logi c and algorithm s is
summe d up in Rober t Kowalski' s equatio n
Algorithm  = Logic  + Control
A logi c programmin g languag e make s it possibl e to writ e algorithm s by augmentin g logica l
sentence s with informatio n to contro l the inferenc e process . Prolo g is by far the mos t widel y
used logi c programmin g language . Its user s numbe r in the hundred s of thousands . It is used
primaril y as a rapid­prototypin g languag e and for symbol­manipulatio n task s such as writin g
compiler s (Van Roy , 1990 ) and parsin g natura l languag e (Pereir a and Warren , 1980) . It has also
been used to develo p exper t syste m application s in legal , medical , financial , and othe r domains .
NEGATIO N AS
FAILUR EThe Prolo g languag e
We have alread y explaine d the notationa l convention s of Prolo g in Chapte r 7. Viewe d as a logica l
knowledg e base, a Prolo g progra m has the followin g characteristics :
• A progra m consist s of a sequenc e of sentences , implicitl y conjoined . All variable s have
implici t universa l quantification , and variable s in differen t sentence s are considere d distinct .
• Onl y Hor n claus e sentence s are acceptable . Thi s mean s that each sentenc e is eithe r an
atomi c sentenc e or an implicatio n with no negate d antecedent s and an atomi c consequent .
• Term s can be constan t symbols , variables , or functiona l terms .
• Querie s can includ e conjunctions , disjunctions , variables , and functiona l terms .
• Instea d of negate d antecedent s in implications , Prolo g uses a negatio n as failur e operator :
a goal not P is considere d prove d if the syste m fails to prov e P.
• All syntacticall y distinc t term s are assume d to refer to distinc t objects . Tha t is, you canno t
asser t A = B or A = F(x),  wher e A is a constant . You can asser t x = B or x ­ F(y),  wher e x is
a variable .
• Ther e is a large set of built­i n predicate s for arithmetic , input/output , and variou s syste m
and knowledg e base functions . Literal s usin g thes e predicate s are "proved " by executin g
code rathe r than doin g furthe r inference . In Prolo g notatio n (wher e capitalize d name s are
variables) , the goal X is 4 + 3 succeed s with X boun d to 7. However , the goal 5 is X+Y
canno t be proved , becaus e the built­i n function s do not do arbitrar y equatio n solving.3
3 Not e that if prope r axiom s are provide d for addition , such goal s can be solve d by inferenc e withi n a Prolo g program .
Sectio n 10.3 . Logi c Programmin g System s 305
HEAD
BODYAs an example , here is a Prolo g progra m for the Member  relation , given both in norma l first­orde r
logic notatio n and in the forma t actuall y used by Prolog :
Vx, I Member(x,  [x\l])
Vx,y,l  Member(x,  I) =>
Member(x,[y\l])member(X,[x|L]) .
member(X,[Y|L] )
member(X,L) .
As we mentione d in Chapte r 7, the Prolo g representatio n has the consequent , or head , on the left­
hand side, and the antecedents , or body , on the right . A Prolo g claus e is often read as "To prov e
(the head) , prov e (the body}. " To preserv e this intuitiv e readin g alon g with our logica l notation ,
we will compromis e and writ e Prolo g clause s usin g a leftwar d implication . For example , the
secon d claus e of the Member  definitio n become s
Member(x,  [y\l])  •$= Member(x,  I)
The definitio n of Member  can be used to answe r severa l kind s of queries . It can be used to
confir m that Member(2,  [1,2,3] ) is true . It can also enumerat e the three value s of x that mak e
Member(x,  [1,2,3] ) true . It can be used to find the valu e of x such that Member(2,[  1,*, 3]) is
true. It even can be used to enumerat e the lists for whic h Member(\,  list) is true.
Implementatio n
The designer s of Prolo g mad e a numbe r of implementatio n decision s designe d to provid e a
simple , fast executio n model :
• All inference s are done by backwar d chaining , with depth­firs t search . Tha t mean s that
wheneve r there is a dead end in the attemp t to prov e a sentence , Prolo g back s up to the
most recen t step that has alternatives .
• The orde r of searc h throug h the conjunct s of an anteceden t is strictl y left to right , and
clause s in the knowledg e base are applie d in first­to­las t order .
• The occur­chec k is omitte d from the unificatio n routine .
The omissio n of the occur­chec k make s Prolo g inferenc e unsound , but actua l error s happe n very
seldo m in practice . The use of depth­firs t searc h make s Prolo g incomplete , becaus e of infinit e
paths create d by circula r sentence s (but see page 311 for anothe r approach) . Programmer s mus t
therefor e keep terminatio n in mind whe n writin g recursiv e sentences . Despit e these caveats , one
good poin t of Prolo g is that the execution  model  is simple enough  that a trained  programmer  can
add control  information  to yield  an efficient  program.
Like our BACK­CHAI N algorith m (page 275) , Prolo g enumerate s all the solution s to a query ,
but it does not gathe r them into a set. Instead , it is up to the user' s progra m to do wha t it will
with each solutio n as it is enumerated . The mos t commo n thing to do is to print the answers . In
fact, Prolog' s top leve l does this automatically . A quer y such as
member(loc(X,X),[loc(l,l),loc(2,l),loc(2,2)]) ?
result s in the user seein g two piece s of output , "X = 1" and "X = 2".
The executio n of a Prolo g progra m can happe n in two modes : interprete d and compiled .
Compilatio n is discusse d in the next subsection . Interpretatio n essentiall y amount s to runnin g
306 Chapte r 10 . Logica l Reasonin g System s
CHOIC E POIN T
TRAI Lthe BACK­CHAI N algorith m from Sectio n 9.4, with the progra m as the knowledg e base . We
say "essentially, " becaus e Prolo g interpreter s contai n a variet y of improvement s designe d to
maximiz e speed . Here we conside r only two.
First, instea d of constructin g the list of all possibl e answer s for each subgoa l befor e
continuin g to the next , Prolo g interpreter s generat e one answe r and a "promise " to generat e the
rest whe n the curren t answe r has been full y explored . Thi s promis e is calle d a choic e point .
When the depth­firs t searc h complete s its exploratio n of the possibl e solution s arisin g from the
curren t answe r and back s up to the choic e point , the choic e poin t is expande d to yield a new
answe r for the subgoa l and a new choic e point . This approac h save s both time and space . It also
provide s a very simpl e interfac e for debuggin g becaus e at all time s there is only a singl e solutio n
path unde r consideration .
Second , our simpl e implementatio n of BACK­CHAI N spend s a good deal of time generatin g
substitution s and applyin g them to quer y lists. Prolo g eliminate s the need for a substitution  data
type by implementin g logic variable s that can remembe r their curren t binding . At any poin t in
time every variabl e in the progra m is eithe r unboun d or is boun d to some value . Together , these
variable s and value s implicitl y defin e a substitution . Of course , there is only one such substitutio n
at a time , but that is all we need . The substitutio n is the righ t one for the curren t path in the
searc h tree. Extendin g the path can only add new variabl e bindings , becaus e an attemp t to add a
differen t bindin g for an already­boun d variabl e result s in a failur e of unification . Whe n a path in
the searc h fails , Prolo g will back up to a previou s choic e point , and then it may have to unbin d
some variables . This is done by keepin g track of all the variable s that have been boun d in a stack
called the trail . As each new variabl e is boun d by UNIFY­VAR , the variabl e is pushe d onto the
trail stack.  Whe n a goal fails and it is time to back up to a previou s choic e point , each of the
variable s is unboun d as it is remove d from the trail .
OPEN­COD ECompilatio n of logic program s
It is possibl e to mak e a reasonabl y efficien t Prolo g interprete r by followin g the guideline s in the
previou s subsection . But interpretin g program s in any language , includin g Prolog , is necessaril y
slowe r than runnin g compile d code . Thi s is becaus e the interprete r alway s behave s as if it has
never seen the progra m before . A Prolo g interprete r mus t do databas e retrieva l to find sentence s
that matc h the goal , and analysi s of sentenc e structur e to decid e wha t subgoal s to generate . All
seriou s heavy­dut y Prolo g programmin g is done with compile d code . The grea t advantag e of
compilatio n is that whe n it is time to execut e the inferenc e process , we can use inferenc e routine s
specificall y designe d for the sentence s in the knowledg e base . Prolo g basicall y generate s a
miniatur e theore m prove r for each differen t predicate , thereb y eliminatin g muc h of the overhea d
of interpretation . It is also possibl e to open­cod e the unificatio n routin e for each differen t call,
thereb y avoidin g explici t analysi s of term structure . (Fo r detail s of open­code d unification , see
Warre n et al. (1977). )
The instructio n sets of today' s computer s give a poor matc h with Prolog' s semantics , so
most Prolo g compiler s compil e into an intermediat e languag e rathe r than directl y into machin e
language . The mos t popula r intermediat e languag e is the Warre n Abstrac t Machine , or WAM ,
name d after Davi d H. D. Warren , one of the implementor s of the first Prolo g compiler . The WAM
Sectio n 10.3 . Logi c Programmin g System s 307
is an abstrac t instructio n set that is suitabl e for Prolo g and can be eithe r interprete d or translate d
into machin e language . Othe r compiler s translat e Prolo g into a high­leve l languag e such as Lisp
or C, and then use that language' s compile r to translat e to machin e language . For example , the
definitio n of the Member  predicat e can be compile d into the code show n in Figur e 10.4 .
procedur e MEMBER(item,  list, continuation)
trail  <— GLOBAL­TRAIL­POINTER( )
if UNTFY([i're m I NEW­VARIABLE()] , list) then CALL(continuation)
RESET­TRAIL(frai7 )
res*­— NEW­VARIABLE( )
if UNiFY(/wf , [NEW­VARIABLE Q I rest])  then MEMBER((tem , rest,  continuation)
Figur e 10.4 Pseudocod e representin g the resul t of compilin g the Member  predicate . Th e
functio n NEW­VARIABL E return s a new variable , distinc t from all othe r variable s so far used . The
procedur e CALL(continuation)  continue s executio n with the specifie d continuation .
There are severa l point s wort h mentioning :
• Rathe r than havin g to searc h the knowledg e base for Member  clauses , the clause s are built
into the procedur e and the inference s are carrie d out simpl y by callin g the procedure .
• As describe d earlier , the curren t variabl e binding s are kept on a trail . The first step of the
procedur e save s the curren t state of the trail , so that it can be restore d by RESET­TRAI L if
the first claus e fails . This will undo any binding s generate d by the first call to UNIFY .
CONTINUATION S •  The trickies t part is the use of continuation s to implemen t choic e points . You can thin k
of a continuatio n as packagin g up a procedur e and a list of argument s that togethe r defin e
what shoul d be done next wheneve r the curren t goal succeeds . It woul d not do j ust to retur n
from a procedur e like MEMBE R whe n the goal succeeds , becaus e it may succee d in severa l
ways , and each of them has to be explored . The continuatio n argumen t solve s this proble m
becaus e it can be calle d each time the goal succeeds . In the MEMBE R code , if item unifie s
with the first elemen t of the list, then the MEMBE R predicat e has succeeded . We then CAL L
the continuation , with the appropriat e binding s on the trail, to do whateve r shoul d be done
next. For example , if the call to MEMBE R were at the top level , the continuatio n woul d
print the binding s of the variables .
Befor e Warren' s work on compilatio n of inferenc e in Prolog , logi c programmin g was too
slow for genera l use. Compiler s by Warre n and other s allowe d Prolo g to achiev e speed s of up
to 50,00 0 LIPS (logica l inference s per second ) on standar d 1990­mode l workstations . Mor e
recently , applicatio n of moder n compile r technology , includin g type inference , open­coding , and
interprocedura l data­flo w analysi s has allowe d Prolo g to reac h speed s of severa l millio n LIPS ,
makin g it competitiv e with C on a variet y of standar d benchmark s (Van Roy , 1990) . Of course ,
the fact that one can writ e a planne r or natura l languag e parse r in a few doze n lines of Prolo g
make s it somewha t mor e desirabl e than C for prototypin g most small­scal e AI researc h projects .
308 Chapte r 10 . Logica l Reasonin g System s
OR­PARALLELIS M
AND­PARALLELIS M
CONSTRAIN T LOGI C
PROGRAMMIN G
TYPE PREDICATE SOthe r logic programmin g language s
Althoug h Prolo g is the only accepte d standar d for logic programming , there are man y othe r usefu l
systems , each extendin g the basi c Prolo g mode l in differen t ways .
The parallelizatio n of Prolo g is an obviou s directio n to explore . If we examin e the work
done by a Prolo g program , ther e are two principa l source s of parallelism . Th e first , calle d
OR­parallelism , come s from the possibilit y of a goal unifyin g with man y differen t literal s and
implication s in the knowledg e base . Each give s rise to an independen t branc h in the searc h spac e
that can lead to a potentia l solution , and all such branche s can be solve d in parallel . The second ,
called AND­parallelism , come s from the possibilit y of solvin g each conjunc t in the body of an
implicatio n in parallel . And­parallelis m is mor e difficul t to achieve , becaus e solution s for the
whol e conjunctio n requir e consisten t binding s for all the variables . Each conjunctiv e branc h must
communicat e with the othe r branche s to ensur e a globa l solution . A numbe r of project s have
been successfu l in achievin g a degre e of paralle l inference , but the mos t advance d is probabl y
the PIM (Paralle l Inferenc e Machine ) project , part of the Fifth Generatio n Computin g System s
projec t in Japan . PIM has achieve d speed s of 64 millio n LIPS .
Prolo g can be enriched , rathe r than just accelerated , by generalizin g the notio n of the
bindin g of a variable . Prolog' s logic variable s are very usefu l becaus e they allow a programme r
to generat e a partia l solutio n to a problem , leavin g som e of the variable s unbound , and then
later fill in the value s for thos e variables . Unfortunately , ther e is no way in Prolo g to specif y
constraints  on values : for example , to say that X < 3, and then later in the computatio n determin e
the exac t valu e of X. The constrain t logic programmin g (CLP ) formalis m extend s the notion s
of variable s and unificatio n to allow such constraints . Conside r this definitio n of a triangl e based
on the length s of the three sides :
Triangle(x,  y, z) (y > 0) A (z > 0) A (x + y > z) A (y + z > x) A (x + z > y)
In eithe r Prolo g or CLP , this definitio n can be used to confir m Triangle(3,  4, 5). But only in CLP
woul d the quer y Triangle(x,  4, 5) give the bindin g specifie d by the constrain t {x > 1 A x < 9}; in
standar d Prolog , this quer y woul d fail.
As well as usin g arithmetica l constraint s on variables , it is possibl e to use logica l constraints .
For example , we can insis t that a particula r variabl e refer to a Person.  In standar d Prolog , this
can only be done by insertin g the conjunc t Person(p)  into the body of a clause . Then , whe n the
claus e is used , the syste m will attemp t to solv e the remainde r of the claus e with p boun d to each
differen t perso n in the knowledg e base . In language s such as Logi n and Life , literal s containin g
type predicate s such as Person  are implemente d as constraints . Inferenc e is delaye d unti l the
constraint s need to be resolved . Thus , Person(p)  just mean s that the variabl e p is constraine d
to be a person ; it does not generat e alternativ e binding s for p. The use of type s can simplif y
programs , and the use of constraint s can spee d up their execution .
Advance d contro l facilitie s
Goin g back to our censu s knowledg e base , conside r the quer y "Wha t is the incom e of the spous e
of the president? " This migh t be state d in Prolo g as
Income(s,  i) A Married(s,p)  A Occupation(p,  President)
ISectio n 10.3 . Logi c Programmin g System s 309
METAREASONIN G
CHRONOLOGICA L
BACKTRACKIN G
BACKJUMPIN G
DEPENDENCY ­
DIRECTE D
BACKTRACKIN GThis quer y will be expensiv e to compute , becaus e it mus t enumerat e all person/incom e pairs , then
fetch the spous e for each perso n (failin g on thos e that are not married , and loopin g on anyon e who
is marrie d multipl e times) , and finall y chec k for the one perso n whos e occupatio n is president .
Conjunctiv e querie s like this often can be answere d mor e efficientl y if we first spen d some time
to reorde r the conjunct s to reduc e the expecte d amoun t of computation . For this query , a bette r
orderin g is
Occupation(p,  President)  A Married(s,  p) A Income(s,  i)
This yield s the same answer , but with no backtrackin g at all, assumin g that the Occupation  and
Married  predicate s are indexe d by their secon d arguments .
This reorderin g proces s is an exampl e of metareasoning , or reasonin g abou t reasoning . As
with constrain t satisfactio n searc h (Sectio n 3.7), the heuristi c we are usin g for conjunc t orderin g
is to put the mos t constrainin g conjunct s first . In this case it is clear that only one p satisfie s
Occupation(p,  President),  but it is not alway s easy to predic t in advanc e how man y solution s
there will be to a predicate . Eve n if it were , it woul d not be a good idea to try all n\ permutation s
of an n­plac e conjunctio n for large n. Language s such as MRS (Geneseret h and Smith , 1981 ;
Russell , 1985 ) allow the programme r to write metarule s to decid e whic h conjunct s are tried first.
For example , the user coul d writ e a rule sayin g that the goal with the fewes t variable s shoul d be
tried first .
Some system s chang e the way backtrackin g is don e rathe r than attemptin g to reorde r
conjuncts . Conside r the proble m of findin g all peopl e x who com e from the same town as the
president . One inefficien t orderin g of this quer y is:
Resident(p,  town)  A Resident(x,  town)  A Occupation(p,  President)
Prolo g woul d try to solv e this by enumeratin g all resident s p of any town , then enumeratin g all
resident s x of that town , and then checkin g if pis the president . Whe n the Occupation(p,  President)
goal fails , Prolo g backtrack s to the most recent  choic e point , whic h is the Resident(x,  town)  goal .
This is calle d chronologica l backtracking ; althoug h simple , it is sometime s inefficient . Clearly ,
generatin g a new x canno t possibl y help p becom e president !
The techniqu e of backjumpin g avoid s such pointles s repetition . In this particula r problem ,
a backjumpin g searc h woul d backtrac k two steps to Resident(p,  town)  and generat e a new bindin g
for p. Discoverin g wher e to backjum p to at compilatio n time is easy for a compile r that keep s
globa l dataflo w information . Sometimes , in additio n to backjumpin g to a reasonabl e spot , the
syste m will cach e the combinatio n of variable s that lead to the dead end, so that they will
not be repeate d agai n in anothe r branc h of the search . Thi s is calle d dependency­directe d
backtracking . In practice , the overhea d of storin g all the dead ends is usuall y too great—a s with
heuristi c search , memor y is ofte n a stronge r constrain t than time . In practice , ther e are man y
more backjumpin g system s than full dependency­directe d backtrackin g systems .
The fina l kin d of metareasonin g is the mos t complicated : bein g able to remembe r a
previousl y compute d inferenc e rathe r than havin g to deriv e it all over again . Thi s is importan t
becaus e mos t logica l reasonin g system s are give n a serie s of relate d queries . For example , a
logic­base d agen t repeatedl y ASK S its knowledg e base the questio n "wha t shoul d I do now? "
Answerin g this questio n will involv e subgoal s that are simila r or identica l to ones answere d the
previou s time around . The agen t coul d just store ever y conclusio n that it is able to prove , but
this woul d soon exhaus t memory . Ther e mus t be som e guidanc e to decid e whic h conclusion s are
310 Chapte r 10 . Logica l Reasonin g System s
worth storin g and whic h shoul d be ignored , eithe r becaus e they are easy to recomput e or becaus e
they are unlikel y to be aske d again . Chapte r 21 discusse s thes e issue s in the genera l contex t of
an agen t tryin g to take advantag e of its previou s reasonin g experiences .
10.4 THEORE M PROVERS________________________ _
Theore m prover s (also know n as automate d reasoners ) diffe r from logic programmin g language s
in two ways . First , mos t logi c programmin g language s only handl e Hor n clauses , wherea s
theore m prover s accep t full first­orde r logic . Second , Prolo g program s intertwin e logi c and
control . The programmer' s choic e in writin g A <= B A C instea d of A •£= CA B affect s the
executio n of the program . In theore m provers , the user can writ e eithe r of these , or anothe r form
such as ­*B •<= C A ­>A, and the result s will be exactl y the same . Theore m prover s still need contro l
informatio n to operat e efficiently , but this informatio n is kept distinc t from the knowledg e base ,
rathe r than bein g part of the knowledg e representatio n itself . Mos t of the researc h in theore m
prover s is in findin g contro l strategie s that are generall y useful . In Sectio n 9.6 we covere d three
generi c strategies : unit preference , linea r inpu t resolution , and set of support .
Desig n of a theore m prove r
In this section , we describ e the theore m prove r OTTE R (Organize d Technique s for Theorem ­
provin g and Effectiv e Research ) (McCune , 1992) , with particula r attentio n to its contro l strategy .
In preparin g a proble m for OTTER , the user mus t divid e the knowledg e into four parts :
• A set of clause s know n as the set of suppor t (or sos),  whic h define s the importan t facts
abou t the problem . Ever y resolutio n step resolve s a membe r of the set of suppor t agains t
anothe r axiom , so the searc h is focuse d on the set of support .
• A set of usabl e axiom s that are outsid e the set of support . Thes e provid e backgroun d
knowledg e abou t the proble m area . The boundar y betwee n wha t is part of the proble m
(and thus in sos) and what  is backgroun d (and thus in the usabl e axioms ) is up to the user' s
judgment .
• A set of equation s know n as rewrite s or demodulators . Althoug h demodulator s are
equations , they are alway s applie d in the left to righ t direction . Thus , they defin e a
canonica l form into whic h all term s will be simplified . For example , the demodulato r
x + 0 = x say s that ever y term of the form x + 0 shoul d be replace d by the term x.
• A set of parameter s and clause s that define s the contro l strategy . In particular , the user
specifie s a heuristi c functio n to contro l the searc h and a filterin g functio n that eliminate s
some subgoal s as uninteresting .
OTTE R work s by continuall y resolvin g an elemen t of the set of suppor t agains t one of the usabl e
axioms . Unlik e Prolog , it uses a form of best­firs t search . Its heuristi c functio n measure s the
"weight " of each clause , wher e lighte r clause s are preferred . The exac t choic e of heuristi c is up
to the user, but generally , the weigh t of a claus e shoul d be correlate d with its size and/o r difficulty .
Sectio n 10.4 . Theore m Prover s 31:
Unit clause s are usuall y treate d as very light , so that the searc h can be seen as a generalizatio n
of the unit preferenc e strategy . At each step , OTTE R move s the "lightest " claus e in the set of
suppor t to the usabl e list, and adds to the usabl e list som e immediat e consequence s of resolvin g
the lightes t claus e with element s of the usabl e list. OTTE R halts whe n it has foun d a refutatio n or
when ther e are no mor e clause s in the set of support . The algorith m is show n in mor e detai l in
Figur e 10.5 .
procedur e OTTER(,«O.S , usable)
inputs : sos,  a set of support—clause s definin g the proble m (a globa l variable )
usable,  backgroun d knowledg e potentiall y relevan t to the proble m
repea t
claus e <— the lightes t membe r of sos
move clause  from sos to usable
PROCESS(lNFER(cfai«e , usable),  sos)
until sos ­ [ ] or a refutatio n has been foun d
functio n lNFER(c/<mve , usable)  return s clause s
resolv e clause  with each membe r of usable
retur n the resultin g clause s after applyin g FILTE R
procedur e PROCESS(clauses,  sos)
for each clause  in clauses  do
clause  r­ SlMPLIFY(c/<2M5<? )
merg e identica l literal s
discar d claus e if it is a tautolog y
sos'—  [clause  I sos]
if clause  has no literal s then a refutatio n has been foun d
if clause  has one litera l then look for unit refutatio n
end
Figur e 10.5 Sketc h of the OTTE R theore m prover . Heuristi c contro l is applie d in the selectio n
of the "lightest " clause , and in the FILTE R functio n that eliminate s uninterestin g clause s from
consideration .
Extendin g Prolo g
An alternativ e way to buil d a theore m prove r is to start with a Prolo g compile r and exten d it to
get a soun d and complet e reasone r for full first­orde r logic . Thi s was the approac h take n in the
Prolo g Technolog y Theore m Prover , or PTT P (Stickel , 1988) . PTT P include s five significan t
change s to Prolo g to restor e completenes s and expressiveness :
• The occur s chec k is put back into the unificatio n routin e to mak e it sound .
312 Chapte r 10 . Logica l Reasonin g System s
LOCKIN G• The depth­firs t searc h is replace d by an iterativ e deepenin g search . This make s the searc h
strateg y complet e and takes only a constan t facto r more time .
• Negate d literal s (suc h as ~^P(x))  are allowed . In the implementation , there are two separat e
routines , one tryin g to prov e P and one tryin g to prov e ­iP.
• A claus e with n atom s is store d as n differen t rules . For example , A ­4= B A C woul d also be
stored as ­<B <= C A ­>A and as ­>C <= B A ­>A. This technique , know n as locking , mean s
that the curren t goal need only be unifie d with the head of each claus e but still allow s for
prope r handlin g of negation .
• Inferenc e is mad e complet e (even for non­Hor n clauses ) by the additio n of the linea r inpu t
resolutio n rule: If the curren t goal unifie s with the negatio n of one of the goal s on the
stack , then the goal can be considere d solved . This is a way of reasonin g by contradiction .
Suppos e we are tryin g to prov e P and that the curren t goal is ­>P. Thi s is equivalen t to
sayin g that ­>P => P, whic h entail s P.
Despit e these changes , PTT P retain s the feature s that make Prolo g fast. Unification s are still done
by modifyin g variable s directly , with unbindin g done by unwindin g the trail durin g backtracking .
The searc h strateg y is still base d on inpu t resolution , meanin g that every resolutio n is agains t one
of the clause s give n in the origina l statemen t of the proble m (rathe r than a derive d clause) . This
make s it feasibl e to compil e all the clause s in the origina l statemen t of the problem .
The main drawbac k of PTTP is that the user has to relinquis h all contro l over the searc h
for solutions . Eac h inferenc e rule is used by the syste m both in its origina l form and in the
contrapositiv e form . This can lead to unintuitiv e searches . For example , suppos e we had the rule
<f(x, y) =f(a,  b)) ^.( X = d)/\(y  = b)
As a Prolo g rule, this is a reasonabl e way to prov e that two / term s are equal . But PTT P woul d
also generat e the contrapositive :
(xfr)  <= (f(x,y)#(a,b))  A (y = b)
It seem s that this is a wastefu l way to prov e that any two term s x and a are different .
PROOF­CHECKE R
SOCRATI C
REASONE RTheore m prover s as assistant s
So far, we have though t of a reasonin g syste m as an independen t agen t that has to mak e decision s
and act on its own . Anothe r use of theore m prover s is as an assistant , providin g advic e to, say, a
mathematician . In this mod e the mathematicia n acts as a supervisor , mappin g out the strateg y for
determinin g wha t to do next and askin g the theore m prove r to fill in the details . Thi s alleviate s
the proble m of semi­decidabilit y to som e extent , becaus e the superviso r can cance l a quer y and
try anothe r approac h if the quer y is takin g too muc h time . A theore m prove r can also act as a
proof­checker , wher e the proo f is give n by a huma n as a serie s of fairl y large steps ; the individua l
inference s require d to show that each step is soun d are fille d in by the system .
A Socrati c reasone r is a theore m prove r whos e ASK functio n is incomplete , but whic h can
alway s arriv e at a solutio n if aske d the righ t serie s of questions . Thus , Socrati c reasoner s mak e
good assistants , provide d ther e is a superviso r to mak e the righ t serie s of calls to ASK . ONTI C
(McAllester , 1989 ) is an exampl e of a Socrati c reasonin g syste m for mathematics .
314 Chapte r 10 . Logica l Reasonin g System s
of an agent—o n each cycle , we add the percept s to the knowledg e base and run the forwar d
chainer , whic h choose s an actio n to perfor m accordin g to a set of condition­actio n rules .
Theoretically , we coul d implemen t a productio n syste m with a theore m prover , usin g
resolutio n to do forwar d chainin g over a full first­orde r knowledg e base . A mor e restricte d
language , on the othe r hand , can provid e greate r efficienc y becaus e the branchin g facto r is
reduced . The typica l productio n syste m has these features :
• The syste m maintain s a knowledg e base calle d the workin g memory . This contain s a set
of positiv e literal s with no variables .
• The syste m also maintain s a separat e rule memory . This contain s a set of inferenc e rules ,
each of the form p\ A pi • • • =>• act\  A act^ • • •, wher e the /?, are literals , and the act, are
action s to take whe n the pi are all satisfied . Allowabl e action s are addin g and deletin g
element s from the workin g memory , and possibl y othe r action s (suc h as printin g a value) .
• In each cycle , the syste m compute s the subse t of rules whos e left­han d side is satisfie d by
the curren t content s of the workin g memory . This is calle d the matc h phase .
• The syste m then decide s whic h of the rules shoul d be executed . This is calle d the conflic t
resolutio n phase .
• The fina l step in each cycl e is to execut e the action(s)  in the chose n rule(s) . This is calle d
the act phase .
RETEMatc h phas e
Unificatio n addresse s the proble m of matchin g a pair of literals , wher e eithe r litera l can contai n
variables . We can use unificatio n in a straightforwar d way to implemen t a forward­chainin g
productio n system , but this is very inefficient . If there are w element s in workin g memor y and
r rules each with n element s in the left­han d side, and solvin g a proble m require s c cycles , then
the naiv e matc h algorith m mus t perfor m wrnc  unifications . A simpl e exper t syste m migh t have
w= 100, r = 200, n = 5, c = 1000 , so this is a hundre d millio n unifications . The rete algorithm4
used in the OPS­ 5 productio n syste m was the first to seriousl y addres s this problem . The rete
algorith m is best explaine d by example . Suppos e we have the followin g rule memory :
A(x) A B(x)  A C(y) =>• add D(x)
A(x) A BOO A D(x)  => add E(x)
A(x) A B(x)  A E(x)  => delet e A(x)
and the followin g workin g memory :
{A(1),A(2),B(2),B(3),B(4),C(5) >
The rete algorith m first compile s the rule memor y into the networ k show n in Figur e 10.6 . In
this diagram , the circula r node s represen t fetche s (not unifications ) to workin g memory . Unde r
node A, the workin g memor y element s A(l) and A(2) are fetche d and stored . The squar e node s
indicat e unifications . Of the six possibl e A x B combination s at the A = B node , only A(2) and
B(2) satisf y the unification . Finally , rectangula r boxe s indicat e actions . Wit h the initia l workin g
4 Ret e is Latin for net. It rhyme s with treaty .
Sectio n 10.5 . Forward­Chainin g Productio n System s 315
( n j —— — ^ AD —.» .
X\^_^ x/c A j "^ /^ •• AB "1 ° ) *
A(1),A(2 ) B(2),B(3),B(4 ) A(2) \ M& )
B<2,\_ ^
XL) ———addE
addD
delet e AD(2)
Figur e 10.6 A  rete network . Circle s represen t predicat e tests . A squar e containing , for
example , A=B  represent s a constrain t that the solution s to the A and B tests mus t be equal .
Rectangle s are actions .
memor y the "add  D" rule is the only one that fires , resultin g in the additio n of the sentenc e D(2)
to workin g memory .
One obviou s advantag e of the rete networ k is that it eliminate s duplicatio n betwee n rules .
All three of the rule s start with a conjunctio n of A and B, and the networ k allow s that part to be
shared . The secon d advantag e of rete network s is in eliminatin g duplicatio n over time . Mos t
productio n system s mak e only a few change s to the knowledg e base on each cycle . This mean s
that most of the tests at cycle t+\ will give the same resul t as at cycle t. The rete networ k modifie s
itself after each additio n or deletion , but if there are few changes , then the cost of each updat e
will be smal l relativ e to the whol e job of maintainin g the indexin g information . The networ k
thus represent s the save d intermediat e state in the proces s of testin g for satisfactio n of a set of
conjuncts . In this case , addin g D(2)  will resul t in the activatio n of the "add E" rule , but will not
have any effec t on the rest of the network . Addin g or deletin g an A, however , will have a bigge r
effec t that need s to be propagate d throug h muc h of the network .
Conflic t resolutio n phas e
Some productio n system s execut e the action s of all rule s that pass the matc h phase . Othe r
productio n system s treat these rule s only as suggestions , and use the conflic t resolutio n phas e to
decid e whic h of the suggestion s to accept . Thi s phas e can be though t of as the contro l strategy .
Some of the strategie s that have been used are as follows :
• No duplication.  Do not execut e the sam e rule on the same argument s twice .
• Recency.  Prefe r rule s that refe r to recentl y create d workin g memor y elements .
316 Chapte r 10 . Logica l Reasonin g System s
• Specificity.  Prefe r rules that are mor e specific.5 For example , the secon d of these two rules
woul d be preferred :
Mammal(x)  => add Legs(x,  4)
Mammal(x)  A Human(x)  =>• add Legs(x,  2)
• Operation  priority.  Prefer  action s with highe r priority , as specifie d by som e ranking . For
example , the secon d of the followin g two rules shoul d probabl y have a highe r priority :
ControlPanel(p)  A Dusty(p)  => Action(Dust(p))
ControlPanel(p)  A MeltdownLightOn(p)  => Action(Evacuate)
Practica l uses of productio n system s
Forward­chainin g productio n system s forme d the foundatio n of muc h early work in AI. In par­
ticular , the XCO N syste m (originall y calle d Rl (McDermott , 1982) ) was built usin g a productio n
syste m (rule­based ) architecture . XCO N contain s severa l thousan d rules for designin g configura ­
tions of compute r component s for customer s of the Digita l Equipmen t Corporation . It was one of
the first clear commercia l successe s in the emergin g field of exper t systems . Man y othe r simila r
system s have been buil t usin g the sam e underlyin g technology , whic h has been implemente d
in the general­purpos e languag e Ops­5 . A good deal of work has gone into designin g match ­
ing algorithm s for productio n syste m languages , as we have seen ; implementation s on paralle l
hardwar e have also been attempte d (Achary a et al., 1992) .
ARCHITECTURE S Productio n system s are also popula r in cognitiv e architectures —tha t is, model s of huma n
reasoning—suc h as ACT (Anderson , 1983 ) and SOA R (Lair d et al., 1987) . In such systems , the
"workin g memory " of the syste m model s huma n short­ter m memory , and the production s are
part of long­ter m memory . Bot h ACT and SOA R have sophisticate d mechanism s for conflic t
resolution , and for savin g the result s of expensiv e reasonin g in the form of new productions .
These can be used to avoi d reasonin g in futur e situation s (see also Sectio n 21.2) .
10.6 FRAM E SYSTEM S AND SEMANTI C NETWORK S
In 1896 , seve n year s after Pean o develope d what is now the standar d notatio n for first­orde r logic ,
GRAPHSTIAL Charle s Peirc e propose d a graphica l notatio n calle d existentia l graph s that he calle d "the logic
of the future. " Thu s bega n a long­runnin g debat e betwee n advocate s of "logic " and advocate s
of "semanti c networks. " Wha t is unfortunat e abou t this debat e is that it obscure d the underlyin g
unity of the field . It is now accepte d that ever y semanti c networ k or fram e syste m coul d just
as well have been define d as sentence s in a logic , and mos t accep t that it coul d be first­orde r
logic.6 (We will show how to execut e this translatio n in detail. ) The importan t thin g with any
representatio n languag e is to understan d the semantics , and the proo f theory ; the detail s of the
synta x are less important . Whether  the language  uses  strings or  nodes  and links,  and  whether it
5 Fo r more on the use of specificit y to implemen t defaul t reasoning , see Chapte r 14.
6 Ther e are a few problem s havin g to do with handlin g exceptions , but they too can be handle d with a little care .Ill
Sectio n 10.6 . Frame  System s and Semanti c Network s 317
is called  a semantic  network  or a logic,  has no effect  on its meaning  or on its implementation.
Havin g said this, we shoul d also say that the forma t of a languag e can have a significant
effec t on its clarit y for a huma n reader . Som e thing s are easie r to understan d in a graphica l
notation ; som e are bette r show n as string s of characters . Fortunately , there is no need to choos e
one or the other ; the skille d AI practitione r can translat e back and fort h betwee n notations ,
choosin g the one that is best for the task at hand , but drawin g intuition s from othe r notations .
Some systems , such as the CYC syste m mentione d in Chapte r 8, provid e both kind s of interfaces .
Beside s the appea l of prett y node­and­lin k diagrams , semanti c network s have been success ­
ful for the same reaso n that Prolo g is more successfu l than full first­orde r logic theore m provers :
becaus e mos t semanti c networ k formalism s have a very simpl e executio n model . Programmer s
can build a large networ k and still have a good idea abou t wha t querie s will be efficient , becaus e
(a) it is easy to visualiz e the steps that the inferenc e procedur e will go through , and (b) the quer y
languag e is so simpl e that difficul t querie s canno t be posed . Thi s may be the reaso n why man y
of the pioneerin g researcher s in commonsens e ontolog y felt more comfortabl e developin g their
theorie s with the semanti c networ k approach .
Synta x and semantic s of semanti c network s
Semanti c network s concentrat e on categorie s of object s and the relation s betwee n them . It is
very natura l to draw the link
Cats Subset Mammals
to say that cats are mammals . Of course , it is just as easy to writ e the logica l sentenc e Cats  C
Mammals,  but whe n semanti c network s were first used in AI (aroun d 1961) , this was not widel y
appreciated ; peopl e though t that in orde r to use logic they woul d have to writ e
\/x Cat(x)  => Mammal(x)
whic h seeme d a lot more intimidating . It was also felt that V* did not allow exceptions , but that
Subset  was somehow more forgiving.7
We now recogniz e that semantic s is mor e importan t than notation . Figur e 10.7 give s an
exampl e of a typica l frame­base d network , and a translatio n of the networ k into first­orde r logic .
This networ k can be used to answe r the quer y "How man y legs does Opu s have? " by followin g
the chai n of Member and Suhset link s from Opu s to Penguin s to Birds , and seein g that bird s have
two legs . This is an exampl e of inheritance , as describe d in Sectio n 8.4. Tha t is clea r enough ,
but wha t happens , when , say, ther e are two differen t chain s to two differen t number s of legs?
Ironically , semanti c network s sometime s lack a clear semantics . Ofte n the user is left to induc e
the semantic s of the languag e from the behavio r of the progra m that implement s it. Consequently ,
users ofte n thin k of semanti c network s at the implementatio n leve l rathe r than the logica l leve l
or the knowledg e level .
7 In man y systems , the nam e IsA was give n to both subse t and set­membershi p links , in correspondenc e with Englis h
usage : "a cat is a mammal " and "Fif i is a cat." Thi s can lead directl y to inconsistencies , as pointe d out by Dre w
McDermot t (1976 ) in his articl e "Artificia l Intelligenc e Meet s Natura l Stupidity. " Som e system s also faile d to distinguis h
betwee n propertie s of member s of a categor y and propertie s of the categor y as a whole .
318 Chapte r 10 . Logica l Reasonin g System s
.Q
E
0)L
Pat
Name : Pa t
(a) A frame­base d knowledg e baseRel(Alive,Animals,T )
Rel(Flies,Animals,F )
Birds C Animal s
Mammal s C Animal s
Rel(Flies , Birds,! )
Rel(Legs,Birds,2 )
Rel(Legs,Mammals,4 )
Penguin s C Bird s
Cats C Mammal s
Bats C Mammal s
Rel(Flies,Penguins,F )
Rel(Legs,Bats,2 )
Rel(Flies,Bats,T )
Opus G Penguin s
Bill SCat s
Pat 6 Bats
Name(Opus,"Opus" )
Name(Bill,"Bill" )
Friend(Opus,Bill )
Friend(Bill,Opus )
Name(Pat,"Pat" )
(b) Translatio n into first­orde r logic
Figur e 10.7 A  frame­base d networ k and a translatio n of the networ k into first­orde r logic .
Boxe d relatio n name s in the networ k correspon d to relation s holdin g for all member s of the set
of objects .
The semantic s of a simpl e semanti c networ k languag e can be state d by providin g first­orde r
logica l equivalent s for assertion s in the networ k language . We first defin e a versio n in whic h
exception s are not allowed . In additio n to Suhset and Member  links , we find that ther e is a need for
at least three othe r kind s of links : one that says a relatio n R hold s betwee n two objects , A and 5;
one that says R hold s betwee n ever y elemen t of the clas s A and the objec t B; and one that says
that R hold s betwee n ever y elemen t of A and som e elemen t of B. The five standar d link type s
are summarize d in Figur e 10.8.8 Notic e that a theore m prove r or logic programmin g languag e
could take the logica l translation s of the link s and do inheritanc e by ordinar y logica l inference . A
semanti c networ k syste m uses uses special­purpos e algorithm s for followin g links , and therefor e
can be faste r than genera l logica l inference .
8 Becaus e assertion s of the form A L^J B are so common , we use the abbreviatio n Rel(R.  A. B) as syntacti c suga r in the
logica l translatio n (Figur e 10.7) .
Sectio n 10.6 . Fram e System s and Semanti c Network s 319
Link Typ e
^ Subset  fi
A Member  D
A_£_ B
A^B
AMBSemantic s
ACB
Aefi
R(A,B)
\/x x£A  => R(x,B)
MX 3y  x<EA  => y£Bf\R(x,y)Exampl e
Cats C Mammals
Bill G Cats
Bill Ase, 12
n . /  1 Lees  \ /­*Birds  ' — M 2
n • 7  1 Parent  \\ j­»­ »/nras ' ,  ' Birds
Figur e 10.8 Lin k type s in semanti c networks , and their meanings .
Inheritanc e with exception s
As we saw in Chapte r 8, natura l kind s are full of exceptions . The diagra m in Figur e 10.7 says that
mammal s have 4 legs, but it also says that bats, whic h are mammals , have 2 legs. Accordin g to
the straightforwar d logica l semantics , this is a contradiction . To fix the problem , we will chang e
the semanti c translatio n of a boxed­/ ? link from A to B to mean that every membe r of A mus t have
an R relatio n to B unless there is  some  intervening  A' for which  Rel(R,  A',B').  The n Figur e 10.7
will unambiguousl y mean that bats have 2 legs , not 4. Notic e that Rel(R,A,B)  no longe r mean s
DEFAUL T VALU E tha t ever y A is relate d by R to B; instea d it mean s that B is a defaul t valu e for the R relatio n for
member s of A, but the defaul t can be overridde n by othe r information .
It may be intuitiv e to think of inheritanc e with exception s by followin g links in a diagram ,
but it is also possible—an d instructive—t o defin e the semantic s in first­orde r logic . The first step
in the logica l translatio n is to reify relations : a relatio n R become s an object , not a predicate . Tha t
mean s that Rel(R,A,B)  is just an ordinar y atomi c sentence , not an abbreviatio n for a comple x
sentence . It also mean s that we can no longe r write R(x, B), becaus e R is an object , not a predicate .
We will use Val(R,x,B)  to mea n that the equivalen t of an R(x,B)  relatio n is explicitl y asserte d
in the semanti c network , and Holds(R,x,B)  to mean  that R(x,B)  can be inferred . We then can
defin e Holds  by sayin g that a relatio n R hold s betwee n x and b if eithe r there is an explici t Val
predicatio n or there is a Rel on some paren t class p of whic h x is an element , and there is no Rel
on any intervenin g class i. (A class i is intervenin g if x is an elemen t of i and i is a subse t off. )
In othe r words :
Vr,x,b  Holds(r,x,b)  O
Val(r,x,  b} V (3p x £p A Rel(r,p,  b) A ­iInterveningRel(x,p,  r))
Vx,p,r  InterveningRel(x,p,r)  ­o­
3 i Intervening(x,  i,p) A 3 b' Rel(r,  i, b')
Va,i,p  Intervening(x,i,p)  o­ (x£i)/\(iCp)
Note that the C symbo l mean s prope r subse t (e.g. , / C p mean s that ; is a subse t of p and is
not equa l to p). The next step is to recogniz e that it is importan t not only to kno w wha t Rel
and Val relation s hold , but also wha t ones do not hold . Suppos e we are tryin g to find the n
that satisfie s Holds(Legs,  Opus,  n). We kno w Rel(Legs,  Birds,  2) and we kno w Opu s is a bird ,
but the definitio n of Holds  does not allo w us to infe r anythin g unles s we can prov e ther e is no
Rel(Legs,  i, b) for i = Penguins  or any othe r intervenin g category . If the knowledg e base only
320 Chapte r 10 . Logica l Reasonin g System s \
contain s positiv e Rel atom s (i.e. , Rel(Legs,Birds,2)  A Rel(Flies,Birds,T)),  then we are stuck .
Therefore , the translatio n of a semanti c networ k like Figur e 10.7 shoul d includ e sentence s that •
say that the Rel and Val relation s that are show n are the only ones that are true:
\/r,a,b  Rel(r,a,b)  & [r,a,b]  e {[Alive,  Animal,T],[Flies,  Animal,  F], •••}
\/r,a,b  Val(r,a,b)  O [r,a,b]  £ {[Friend,  Opus,Bill],  [Friend,Bill,  Opus],. ..}
MULTIPL E
INHERITANC EMultipl e inheritanc e
Some semanti c networ k system s allow multipl e inheritance —tha t is, an objec t can belon g to
more than one categor y and can therefor e inheri t propertie s alon g severa l differen t paths . In
some cases , this will work fine . For example , som e peopl e migh t belon g to both the categorie s
Billionaire  and PoloPlayer,  in whic h case we can infer that they are rich and can ride a horse .
It is possible , however , for two inheritanc e path s to produc e conflictin g answers . An
exampl e of this difficult y is show n in Figur e 10.9 . Opu s is a penguin , and therefor e speak s only
in squawks . Opu s is a cartoo n character , and therefor e speak s English.9 In the simpl e logica l
translatio n give n earlier , we woul d be able to infe r both conclusions , which , with appropriat e
backgroun d knowledge , woul d lead to a contradiction . Withou t additiona l informatio n indicatin g
some preferenc e for one path , there is no way to resolv e the conflict .
Speec h1 Vocalizatio n ICartoo n
Characte rPengui n| Vocalizatio n 1
Squawk s
Opus
Figur e 10.9 A n exampl e of conflictin g inference s from multipl e inheritanc e paths .
Inheritanc e and chang e
A knowledg e base is not of muc h use to an agen t unles s it can be expanded . In system s base d
on first­orde r logic , we use TEL L to add a new sentenc e to the knowledg e base , and we enjo y the
9 Th e classica l exampl e of multipl e inheritanc e conflic t is calle d the "Nixo n diamond."  It arise s from the observatio n
that Richar d Nixo n was both a Quake r (and henc e a pacifist ) and a Republica n (and henc e not a pacifist) . Becaus e of
its potentia l for controversy , we will avoi d this particula r example . The othe r canonica l exampl e involve s a bird calle d
Tweety , abou t who m the less said the better .
Sectio n 10.6 . Fram e System s and Semanti c Network s 321
propert y of monotonicity : iff follow s from KB, then it still follow s whe n KB is augmente d by
TELL(KB,S).  In othe r words ,
if KB h P then (KB  A S) h P
NONMONOTONI C Inheritanc e with exception s is nonmonotonic : fro m the semanti c networ k in Figur e 10.7 it
follow s that Bill has 4 legs, but if we were to add the new statemen t Rel(Legs,  Cats,  3), then it no
longe r follow s that Bill has 4 legs. Ther e are two way s to deal with this.
First, we coul d switc h from first­orde r logic to a nonmonotoni c logic that explicitl y deal s
with defaul t values . Nonmonotoni c logic s allow you to say that a propositio n P shoul d be treate d
as true unti l additiona l evidenc e allow s you to prov e that P is false . Ther e has been quit e a lot
of interestin g theoretica l wor k in this area , but so far its impac t on practica l system s has been
smalle r than othe r approaches , so we will not addres s it in this chapter .
Second , we coul d treat the additio n of the new statemen t as a RETRAC T followe d by a TELL .
Given the way we have define d Rel, this make s perfec t sense . We do not make statement s of the
form TELL(£B , Rel(R,A,B)).  Instead , we mak e one big equivalenc e statemen t of the form
TELL(KB,  Mr,a,b  Rel(r,a,b)  <£>... )
wher e the ... indicat e all the possibl e Rel's.  So to add Rel(Legs,  Cats,  3), we woul d have to
remov e the old equivalenc e statemen t and replac e it by a new one. Onc e we have altere d the
knowledg e base by removin g a sentenc e from it (and not just addin g a new one) we shoul d not
be surprise d at the nonmonotonicity . Sectio n 10.8 discusse s implementation s of RETRACT .
Implementatio n of semanti c network s
Once we have decide d on a meanin g for our networks , we can start to implemen t the network . Of
course , we coul d choos e to implemen t the networ k with a theore m prove r or logic programmin g
language , and in som e case s this woul d be the best choice . Bu t for network s with simpl e
semantics , a more straightforwar d implementatio n is possible . A node in a networ k is represente d
by a data structur e with field s for the basi c taxonomi c connections : whic h categorie s it is a
membe r of; wha t element s it has; wha t immediat e subset s and supersets . It also has field s
for othe r relation s in whic h it participates . The RELS­I N and RELS­Ou T field s handl e ordinar y
(unboxed ) links , and the ALL­RELS­l N and ALL­RELS­OU T field s handl e boxe d links . Here is the
data type definitio n for nodes :
datatyp e SEM­NET­NOD E
components : NAME , MEMBERSHIPS , ELEMENTS , SUPERS , SUBS ,
RELS­IN , RELS­OUT , ALL­RELS­IN , ALL­RELS­OU T
Each of the four REL­field s is organize d as a table indexe d by the relation . We use the functio n
LooKVP(key,  table)  to find the valu e associate d with a key in a table . So, if we have the two
links Opus F^ Bill  and Opus Frie"f Steve,  then LoOKUP(Friend,RELS­Ow(Opus))  give s us the
set {Bill,Steve}.
322 Chapte r 10 . Logica l Reasonin g System s
The code in Figur e 10.1 0 implement s everythin g you need in orde r to ASK the networ k
whethe r subset , membership , or othe r relation s hold betwee n two objects . Each of the function s
simpl y follow s the appropriat e links unti l it find s wha t it is lookin g for, or runs out of links . The
code does not handl e double­boxe d links , nor does it handl e exceptions . Also , the code that
TELL S the networ k abou t new relation s is not shown , becaus e it is straightforward .
The code can be extende d with othe r function s to answe r othe r questions . One proble m
with this approac h is that it is easy to becom e carrie d away with the data structure s and forge t their
underlyin g semantics . For example , we coul d easil y defin e a NUMBER­OF­SU B KIND S functio n that
return s the lengt h of the list in the SUB S slot. For Figur e 10.7 , NUMBER­OF­SUBKlNDS(Am'ma/ )
= 2. This may well be the answe r the user wanted , but its logica l statu s is dubious . Firs t of all, it
functio n MEMRERl(e/ement,  category)  return s True  or False
for each c in MEMBERSHiPS[e/<?menf ] do
if SUBSET?(c , category)  then retur n True
retur n False
functio n SUBSET?(SW£> , super)  return s True  or False
if sub = super  then retur n True
for each c in SUPER S [sub]  do
if SUBSET?(c , super)  then retur n True
retur n False
functio n RELATED­Jol(source , relation,  destination)  return s True  or False
if relation  appear s in RELS­OUT(.vowrce ) then
retur n MEMftER([relation,  destination],RELS­O\jT(node))
else for each c in MEMBERSHiPS^osirce ) do
if AEE­RELATED­To?(c , relation,  destination)  then retur n True
end
retur n False
functio n AEE­RELATED­TO?(.vowrce , relation,  destination)  return s True  or False
if relation  appear s in ALE­RELS­OUT(.yowrce ) then
retur n MEMBER([relation,  destination],  ALL­RELS­OUT(node) )
else for each c in SupERS(category ) do
if ALL­RELATED­TO?(c , relation,  destination)  then retur n True
end
retur n False
Figur e 10.1 0 Basi c routine s for inheritanc e and relatio n testin g in a simpl e exception­fre e
semanti c network . Not e that the functio n MEMBER ? is define d here to operat e on semanti c
networ k nodes , whil e the functio n MEMBE R is a utilit y that operate s on sets.
Sectio n 10.7 . Descriptio n Logic s 323
is likel y that there are specie s of animal s that are not represente d in the knowledg e base . Second ,
it may be that som e node s denot e the same object . Perhap s Dog  and Chien  are two node s with
an equalit y link betwee n them . Do thes e coun t as one or two? Finally , is Dog­With­Black­Ears
a kind of animal ? Ho w abou t Dog­On­My­Block­Last­Thursdayl  It is easy to answe r thes e
question s base d on wha t is store d in the knowledg e base , but it is bette r to have a clear semantic s
so that the question s can be answere d abou t the world , rathe r than abou t the curren t state of the
interna l representation .
PARTITIONE D
SEMANTI C
NETWORK S
PROCEDURA L
ATTACHMEN TExpressivenes s of semanti c network s
The network s we have discusse d so far are extremel y limite d in their expressiveness . For example ,
it is not possibl e to represen t negatio n (Opu s does not ride a bicycle) , disjunctio n (Opu s appear s
in eithe r the Times  or the Dispatch),  or quantificatio n (all of Opus ' friend s are cartoo n characters) .
These construct s are essentia l in man y domains .
Some semanti c network s exten d the notatio n to allow all of first­orde r logic . Peirce' s orig­
inal existentia l graphs , partitione d semanti c network s (Hendrix , 1975) , and SNEP S (Shapiro ,
1979) all take this approach . A more commo n approac h retain s the limitation s on expressivenes s
and uses procedura l attachmen t to fill in the gaps . Procedura l attachmen t is a techniqu e wher e
a functio n writte n in a programmin g languag e can be store d as the valu e of som e relation , and
used to answe r ASK calls abou t that relatio n (and sometime s TEL L calls as well) .
Wha t do semanti c network s provid e in retur n for givin g up expressiveness ? We have
alread y seen two advantages : the y are able to captur e inheritanc e informatio n in a modula r
way, and their simplicit y make s them easy to understand . Efficienc y is ofte n claime d as a third
advantage : becaus e inferenc e is done by followin g links , rathe r than retrievin g sentence s from a
knowledg e base and performin g unifications , it can operat e with only a few machin e cycle s per
inferenc e step . But if we look at the kind s of computation s done by compile d Prolo g programs ,
we see ther e is not muc h difference . A compile d Prolo g progra m for a set of subse t and set­
membershi p sentences , combine d with genera l propertie s of categories , does almos t the sam e
computation s as a semanti c network .
1QJ DESCRIPTIO N LOGIC S
|2>JjJ« " Th e synta x of first­orde r logic is designe d to mak e it easy to say thing s abou t objects . Description
'"^' logics  are designed  to focus  on categories and  their definitions.  The y provid e a reasonabl y
sophisticate d facilit y for definin g categorie s in term s of existin g relations , with muc h greate r
expressivenes s than typica l semanti c networ k languages . Th e principa l inferenc e task s are
SUBSUMPTIO N subsumption —checkin g if one categor y is a subse t of anothe r base d on thei r definitions—an d
CLASSIFICATIO N classification —checkin g if an objec t belong s to a category . In som e descriptio n logics , object s
are also viewe d as categorie s define d by the object' s descriptio n and (presumably ) containin g
only one member . Thi s way of lookin g at representatio n is a significan t departur e from the
object­centere d view that is mos t compatibl e with first­orde r logica l syntax .
324 Chapte r 10 . Logica l Reasonin g System s I
The CLASSI C languag e (Borgid a et al., 1989 ) is a typica l descriptio n logic . The synta x
of CLASSI C description s is show n in Figur e 10.II.10 For example , to say that bachelor s are
unmarried , adul t male s we woul d write
Bachelor  = And(Untnarried,  Adult,  Male)
The equivalen t in first­orde r logic woul d be
\/x Bachelor(x)  O  Unmarried(x)  A Adult(x)  A Male(x)
Notic e that the descriptio n logi c effectivel y allow s direc t logica l operation s on predicates , rathe r
than havin g to first creat e sentence s to be joine d by connectives . Any descriptio n in CLASSI C can
be writte n in first­orde r logic , but som e description s are mor e straightforwar d in CLASSIC . For 1
example , to describ e the set of men with at least  three sons who are all unemploye d and marrie d
to doctors , and at mos t two daughter s who are all professor s in physic s or chemistr y departments ,
we woul d use
And(Man,AtLeast(3,  Son),AtMost(2,  Daughter),
All(Son,And(Unemployed,  Married,  All(Spouse,  Doctor))),
All(Daughter,And(Professor,  Fills(Department,  Physics,  Chemistry))))
We leav e it as an exercis e to translat e this into first­orde r logic .
Concept  — Thing  | ConceptName
And(Concept,...)
AlKRoleName,  Concept)
AtLeast(lnteger,  RoleName)
AtMost(lnteger,  RoleName)
Fills (Role Name,  IndividualName,...)
SameAs(Path,  Path)
| OneOf  (IndividualName,...)
Path  ­>• [RoleName,...]
Figur e 10.1 1 Th e synta x of description s in a subse t of the CLASSI C language .
Perhap s the mos t importan t aspec t of descriptio n logic s is the emphasi s on tractabilit y of ;
inference . A proble m instanc e is solve d by describin g it and askin g if it is subsume d by one of
severa l possibl e solutio n categories . In standar d first­orde r logic systems , predictin g the solutio n
time is often impossible . It is often left to the user to enginee r the representatio n to detou r aroun d
sets of sentence s that seem to be causin g the syste m to take severa l week s to solv e a problem .
The thrus t in descriptio n logics , on the othe r hand , is to ensur e that subsumption­testin g can be
solve d in time polynomia l in the size of the proble m description . The CLASSI C languag e satisfie s
this condition , and is currentl y the mos t comprehensiv e languag e to do so.
10 Notic e that the languag e does not allow one to simpl y state that one concept , or category , is a subse t of another . This
is a deliberat e policy : subsumptio n betwee n categorie s mus t be derivabl e from som e aspect s of the description s of the
categories . If not, then somethin g is missin g from the descriptions .
Sectio n 10.8 . Managin g Retractions , Assumptions , and Explanation s 325
This sound s wonderfu l in principle , unti l one realize s that it can only have one of two conse ­
quences : hard problem s eithe r canno t be state d at all, or requir e exponentiall y large descriptions !
However , the tractabilit y result s do shed light on wha t sorts of construct s caus e problems , and
thus help the user to understan d how differen t representation s behave . For example , descriptio n
logic s usuall y lack negation  and disjunction.  Thes e both forc e first­orde r logica l system s to
essentiall y go throug h an exponentia l case analysi s in orde r to ensur e completeness . For the
same reason , they are exclude d from Prolog . CLASSI C only allow s a limite d form of disjunctio n
in the Fills  and OneOf  constructs , whic h allow disjunctio n over explicitl y enumerate d individual s
but not over descriptions . Wit h disjunctiv e descriptions , neste d definition s can lead easil y to an
exponentia l numbe r of alternativ e route s by whic h one categor y can subsum e another .
Practica l uses of descriptio n logic s
Becaus e they combin e clea r semantic s and simpl e logica l operations , descriptio n logic s have
becom e popula r with both the theoretica l and practica l AI communities . Application s have
include d financia l managemen t (May s et al, 1987) , databas e interface s (Bec k et al, 1989) , and
softwar e informatio n system s (Devanb u etal.,  1991) . Becaus e of the gradua l extensio n of the class
of tractabl e languages , and a bette r understandin g of what kind s of construct s caus e intractability ,
the efficienc y of descriptio n logic system s has improve d by severa l order s of magnitud e over the
last decade .
10.8 MANAGIN G RETRACTIONS , ASSUMPTIONS , AND EXPLANATION S
We have said a great deal abou t TELL and ASK , but so far very little abou t RETRACT . Mos t logica l
reasonin g systems , regardles s of their implementation , have to deal with RETRACT . As we have
seen, there are three reason s for retractin g a sentence . It may be that a fact is no longe r important ,
and we wan t to forge t abou t it to free up spac e for othe r purposes . It may be that the syste m
is trackin g the curren t state of the worl d (withou t worryin g abou t past situations ) and that the
world changes . Or it may be that the syste m assume d (or determined ) that a fact was true, but
now want s to assum e (or come s to determine ) that it is actuall y false . In any case , we wan t to be
able to retrac t a sentenc e from the knowledg e base withou t introducin g any inconsistencies , and
we woul d like the interactio n with the knowledg e base as a whol e (the cycle of TELL , ASK and
RETRAC T requests ) to be efficient .
It take s a little experienc e to appreciat e the problem . First , it is importan t to understan d
the distinctio n betwee n RETRACJ(KB,  P) and TELL(A^5 , ­i/>) . Assumin g that the knowledg e base
alread y contain s P, addin g ­i/> with TEL L will allo w us to conclud e both P and ­>P,  wherea s
removin g P with RETRAC T will allow us to conclud e neithe r P nor ­>P. Second , if the syste m
does any forwar d chaining , then RETRAC T has som e extra wor k to do. Suppos e the knowledg e
base was told P and P => Q, and used that to infe r Q and add it to the knowledg e base . The n
RETRACT(/3? , P) mus t remov e both P and Q to keep the knowledg e base consistent . However , if
there is som e othe r independen t reaso n for believin g Q (perhap s both R and R =>• Q have been
326 Chapte r 10 . Logica l Reasonin g System s
TRUT H
MAINTENANC E
TRUT H
MAINTENANC E
SYSTE M
EXPLANATION S
ASSUMPTION S
JTMSasserted) , then Q does not have to be remove d afte r all. The proces s of keepin g trac k of whic h
additiona l proposition s need to be retracte d whe n we retrac t P is calle d truth maintenance .
The simples t approac h to truth maintenanc e is to use chronologica l backtrackin g (see page
309). In this approach , we keep track of the orde r in whic h sentence s are adde d to the knowledg e
base by numberin g them from PI to P,,. Whe n the call RETRACT(P, ) is made , the syste m revert s
to the state just befor e P, was added . If desired , the sentence s P,+\ throug h Pn can then be adde d
again . This is simple , and it guarantee s that the knowledg e base will be consistent , but it mean s
that retractio n is O(ri),  wher e n is the size of the knowledg e base . We woul d prefe r a more
efficien t approac h that does not requir e us to duplicat e all the work for P/+1 to Pn.
A trut h maintenanc e syste m or TM S is a progra m that keep s trac k of dependencie s
betwee n sentence s so that retractio n (and som e othe r operations ) will be more efficient . A TMS
actuall y perform s four importan t jobs . First , a TMS enable s dependency­directe d backtracking ,
to avoi d the inefficienc y of chronologica l backtracking .
A secon d and equall y importan t job is to provid e explanation s of propositions . A proo f
is one kind of explanation—i f we ask, "Explai n why you believ e P is true? " then a proo f of P
is a good explanation . If a proo f is not possible , then a good explanatio n is one that involve s
assumptions . For example , if we ask, "Explai n why the car won' t start, " there may not be enoug h
evidenc e to prov e anything , but a good explanatio n is, "If we assum e that there is gas in the car
and that it is reachin g the cylinders , then the observe d absenc e of activit y prove s that the electrica l
syste m mus t be at fault. " Technically , an explanatio n £ of a sentenc e P is define d as a set of
sentence s such that E entail s P. The sentence s in E mus t eithe r be know n to be true (i.e., they
are in the knowledg e base) , or they mus t be know n to be assumption s that the problem­solve r
has made . To avoi d havin g the whol e knowledg e base as an explanation , we will insis t that E is
minimal , that is, that there is no prope r subse t of E that is also an explanation .
The abilit y to deal with assumption s and explanation s is critica l for the third job of a TMS :
doing defaul t reasoning . In a taxonomi c syste m that allow s exceptions , statin g that Opu s is a
pengui n does not sanctio n an irrefutabl e inferenc e that Opu s has two legs , becaus e additiona l
informatio n abou t Opu s migh t overrid e the derive d belief . A TMS can delive r the explanatio n
that Opus , bein g a penguin , has two legs provided  he is not an abnormal  penguin.  Here , the
lack of abnormalit y is mad e into an explici t assumption . Finally , TMS s help in dealin g with
inconsistencies . If addin g P to the knowledg e base result s in a logica l contradiction , a TMS can
help pinpoin t an explanatio n of wha t the contradictio n is.
There are severa l type s of TMSs . The simples t is the justification­base d truth maintenanc e
syste m or JTMS . In a JTMS , each sentenc e in the knowledg e base is annotate d with a justificatio n
that identifie s the sentence s from whic h it was inferred , if any. For example , if Q is inferre d by
Modu s Ponen s from P, then the set of sentence s {P, P => Q} coul d serve as a justificatio n of
the sentenc e Q. Som e sentence s will have mor e than one justification . Justification s are used
to do selectiv e retractions . If after addin g P\ throug h Pn we get a call to RETRACT(P,) , then the
JTMS will remov e from the knowledg e base exactl y thos e sentence s for whic h Pt is a require d
part of ever y justification . So, if a sentenc e Q had {P/, P, => • Q] as its only justification , it
woul d be removed ; if it had the additiona l justificatio n {P f, P,­V  R => Q},  then it woul d still be
removed ; but if it also had the justification!/? , P, V/? => Q}, then it woul d be spared .
In mos t JTM S implementations , it is assume d that sentence s that are considere d once will
probabl y be considere d again , so rathe r than removin g a sentenc e from the knowledg e base whe n
Sectio n 10.9 . Summar y 327
ATMSit loses all justification , we merel y mark the sentenc e as bein g out of the knowledg e base . If a
subsequen t assertio n restore s one of the justifications , then we mark the sentenc e as bein g back
in. In this way , the JTM S retain s all of the inferenc e chain s that it uses , and need not rederiv e
sentence s whe n a justificatio n become s valid again .
To solv e the car diagnosi s proble m with a JTMS , we woul d first assum e (that is, assert )
that there is gas in the car and that it is reachin g the cylinders . Thes e sentence s woul d be labelle d
as in. Give n the right backgroun d knowledge , the sentenc e representin g the fact that the car will
not start woul d also becom e labelle d in. We coul d then ask the JTM S for an explanation . On the
other hand , if it turne d out that the assumption s were not sufficien t (i.e., they did not lead to "car
won' t start" bein g in), then we woul d retrac t the origina l assumption s and make some new ones .
We still have a searc h problem—th e TMS does only part of the job.
The JTM S was the first type of TMS , but the most popula r type is the ATM S or assumption ­
based truth maintenanc e system . The differenc e is that a JTM S represent s one consisten t state
of the worl d at a time . The maintenanc e of justification s allow s you to quickl y mov e from one
state to anothe r by makin g a few retraction s and assertions , but at any time only one state is
represented . An ATM S represent s all the state s that have ever been considere d at the same time .
Wherea s a JTM S simpl y label s each sentenc e as bein g in or out, an ATM S keep s track , for
each sentence , of whic h assumption s woul d caus e the sentenc e to be true. In othe r words , each
sentenc e has a labe l that consist s of a set of assumptio n sets. The sentenc e hold s just in thos e
cases wher e all the assumption s in one of the assumptio n sets hold .
To solv e problem s with an ATMS , we can mak e assumption s (suc h as P, or "gas in car")
in any orde r we like. Instea d of retractin g assumption s whe n one line of reasonin g fails , we just
asser t all the assumption s we are intereste d in, even if they contradic t each other . We then can
check a particula r sentenc e to determin e the condition s unde r whic h it holds . For example , the
label on the sentenc e Q woul d be {{P;} , {R}},  meanin g that Q is true unde r the assumptio n that
PI is true or unde r the assumptio n that R is true. A sentenc e that has the empt y set as one of its
assumptio n sets is necessaril y true—i t is true with no assumption s at all. On the othe r hand , a
sentenc e with no assumptio n sets is just false .
The algorithm s used to implemen t truth maintenanc e system s are a little complicated , and
we do not cove r them here . The computationa l complexit y of the truth maintenanc e proble m is
at least as grea t as that of propositiona l inference—tha t is, NP­hard . Therefore , you shoul d not
expec t truth maintenanc e to be a panace a (excep t for triviall y smal l problems) . But whe n used
carefull y (for example , with an informe d choic e abou t wha t is an assumptio n and wha t is a fact
that can not be retracted) , a TMS can be an importan t part of a logica l system .
11L9SUMMAR Y
This chapte r has provide d a connectio n betwee n the conceptua l foundation s of knowledg e rep­
resentatio n and reasoning , explaine d in Chapter s 6 throug h 9, and the practica l worl d of actua l
reasonin g systems . We emphasiz e that real understandin g of these system s can only be obtaine d
by tryin g them out.
328 Chapte r 10 . Logica l Reasonin g System s
We have describe d implementatio n technique s and characteristic s of four majo r classe s of
logica l reasonin g systems :
• Logi c programmin g system s and theore m provers .
• Productio n systems .
• Semanti c networks .
• Descriptio n logics .
We have seen that there is a trade­of f betwee n the expressivenes s of the syste m and its efficiency .
Compilatio n can provid e significan t improvement s in efficienc y by takin g advantag e of the fact
that the set of sentence s is fixed in advance . Usabilit y is enhance d by providin g a clear semantic s
for the representatio n language , and by simplifyin g the executio n mode l so that the user has a
good idea of the computation s require d for inference .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
Work on indexin g and retrieva l in knowledg e base s appear s in the literature s of both AI and
databases . Th e two majo r text s on AI programmin g (Charnia k et ai, 1987 ; Norvig , 1992 )
discus s the topic in depth . The text by Forbu s and de Klee r (1993 ) also cover s muc h of this
ground . The standar d referenc e on managemen t of database s and knowledg e base s is (Ullman ,
1989) . Jack Minke r was a majo r pionee r in the developmen t of the theor y of deductiv e database s
(GallaireandMinker , 1978 ; Minker , 1988) . Colomb ( 1991 ) present s some interestin g ideas abou t
using hardwar e to aid indexin g of Prolo g programs .
As mentione d in Chapte r 9, unificatio n was foreshadowe d by Herbran d (1930) , and formall y
introduce d by Robinso n (1965 ) in the sam e articl e that unveile d the resolutio n inferenc e rule.
Extendin g work by Boye r and Moor e (1972) , Martell i and Montanar i (1976 ) and Paterso n and
Wegma n (1978 ) develope d unificatio n algorithm s that run in linea r time and spac e via sharin g
of structur e amon g representation s of terms . Unificatio n is surveye d by Knigh t (1989 ) and by
Lasse z et al. (1988) . Shiebe r (1986 ) cover s the use of unificatio n in natura l languag e processing .
Prolo g was developed , and the first interprete r written , by the Frenc h researche r Alai n
Colmeraue r in 1972 (Roussel , 1975 ; Colmeraue r et al., 1973) ; Colmeraue r (1985 ) also give s
an English­languag e surve y of Prolog . Muc h of the theoretica l backgroun d was develope d by
Rober t Kowalsk i (1974 ; 1979b ; 1979a ) in collaboratio n with Colmerauer . Kowalsk i (1988 ) and
Cohe n (1988 ) provid e good historica l overview s of the origin s of Prolog . Foundations  of Logic
Programming  (Lloyd , 1987 ) is a theoretica l analysi s of the underpinning s of Prolo g and othe r
logic programmin g languages . Ait­Kac i (1991 ) give s a clear expositio n of the Warre n Abstrac t
Machin e (WAM ) mode l of computatio n (Warren , 1983) .
Recently , muc h of the effor t in logi c programmin g has been aime d towar d increasin g
efficienc y by buildin g informatio n abou t specifi c domain s or specifi c inferenc e pattern s into
the logic programmin g language . The languag e LOGI N (Ait­Kac i and Nasr , 1986 ) incorporate s
efficien t handlin g of inheritanc e reasoning . Constrain t logi c programmin g (CLP ) is base d on
the use of constrain t satisfaction , togethe r with a backgroun d theory , to solv e constraint s on
Sectio n 10.9 . Summar y 329
FUNCTIONA L
PROGRAMMIN Gvariable s (Roac h et al., 1990) , rathe r than the simpl e equalit y propagatio n used in norma l
unification . (Herbrand' s origina l formulatio n had also used constrainin g equation s rathe r than
syntacti c matching. ) CL P is analyze d theoreticall y in (Jaffa r and Lassez , 1987) . Jaffa r et
al. (1992a ) wor k specificall y in the domai n of the real numbers , usin g a logi c programmin g
languag e calle d CLP(R) . Concurren t CLP is addresse d by Saraswa t (1993) . Jaffa r et al. (1992b )
presen t the Constrain t Logi c Abstrac t Machin e (CLAM) , a WAM­lik e abstractio n designe d to
aid in the analysi s of CLP(R) . Ait­Kac i and Podelsk i (1993 ) describ e a sophisticate d constrain t
logic programmin g languag e calle d LIFE , whic h combine s constrain t logic programmin g with
functiona l programmin g and with inheritanc e reasonin g (as in LOGIN) . Prolo g III (Colmerauer ,
1990) build s in severa l assorte d type s of reasonin g into a Prolog­lik e language . Volum e 58 (1992 )
of the journa l Artificial Intelligence  is devote d primaril y to constraint­base d systems . Kohn(1991 )
describe s an ambitiou s projec t to use constrain t logic programmin g as the foundatio n for a real­
time contro l architecture , with application s to fully automati c pilots .
Aside from the developmen t of constrain t logi c and othe r advance d logi c programmin g
languages , there has been considerabl e effor t to spee d up the executio n of Prolo g by highl y opti­
mize d compilatio n and the use of paralle l hardware , especiall y in the Japanes e Fifth Generatio n
computin g project . Van Roy (1990 ) examine s som e of the issue s involve d in fast executio n on
serial hardware . Feigenbau m and Shrob e (1993 ) provid e a genera l accoun t and evaluatio n of
the Fifth Generatio n project . The Fifth Generation' s paralle l hardwar e prototyp e was the PIM ,
or Paralle l Inferenc e Machin e (Taki , 1992) . Logi c programmin g of the PIM was base d on the
formalis m of guarde d Horn clause s (Ueda , 1985 ) and the GHC and KL1 language s that grew out
of it (Furukawa , 1992) . A numbe r of application s of paralle l logic programmin g are covere d by
Nitta et al.  (1992) . Othe r language s for paralle l logi c programmin g includ e Concurren t Prolo g
(Shapiro , 1983 ) and PARLO G (Clar k and Gregory , 1986) .
Logic programmin g is not the only paradig m of programmin g that has been prominen t
in Al. Functiona l programmin g model s program s not as collection s of logica l clause s but
as description s of mathematica l functions . Functiona l programmin g is base d on the lambd a
calculu s (Church , 1941 ) and combinator y logic (Schonfinkel , 1924 ; Curr y and Feys , 1958) , two
sophisticate d mathematica l notation s for describin g and reasonin g abou t functions . The earlies t
functiona l programmin g language , datin g from 1958 , was Lisp , whic h is due to John McCarthy .
Its histor y and prehistor y is describe d in detai l in (McCarthy , 1978) . Incidentally , McCarth y
denie s (p. 190) that Lisp was intende d as an actua l implementatio n of the lambda­calculu s (as has
often been asserted) , althoug h it does borro w certai n features . Lisp stand s for LISt Processing ,
the use of linke d lists whos e element s are connecte d by pointer s (rathe r than by proximit y in
the machine' s addres s space , as array s are) to creat e data structure s of grea t flexibility . The
list processin g techniqu e predate d Lisp and functiona l programmin g (Newel l and Shaw , 1957 ;
Gelernte r et al, 1960) . Afte r its invention , Lisp proliferate d into a wide variet y of dialects , partl y
becaus e the languag e had been designe d to be easy to modif y and extend . In the past two decades ,
there has been an effor t to reunif y the languag e as Commo n Lisp , describe d in grea t detai l by
Steel e (1990) . Bot h of the two majo r Al programmin g text s mentione d abov e assum e the use
of Lisp . A numbe r of othe r functiona l programmin g language s have been develope d aroun d a
small , clean core of definitions . Thes e includ e SCHEME , DYLAN , and ML.
The so­calle d problem­solvin g language s were precursor s of logi c programmin g in that
they attempte d to incorporat e inference­lik e mechanisms , althoug h they were not logi c program ­
330 Chapte r 10 . Logica l Reasonin g System s
ming language s as such and had contro l structure s othe r than backtracking . PLANNE R (Hewitt ,
1969) , althoug h neve r actuall y implemented , was a very comple x languag e that used auto ­
matic backtrackin g mechanism s analogou s to the Prolo g contro l structure . A subse t know n as
MICRO­PLANNE R (Sussma n and Winograd , 1970 ) was implemente d and used in the SHRDL U nat­
ural languag e understandin g syste m (Winograd , 1972) . The CONNIVE R languag e (Sussma n and
McDermott , 1972 ) allowe d finer programme r contro l over backtrackin g than MICRO­PLANNER .
CONNIVE R was used in the HACKE R (Sussman , 1975 ) and BUIL D (Fahlman , 1974 ) plannin g sys­
tems. QLiS P (Sacerdot i et al, 1976 ) used patter n matchin g to initiat e functio n calls , as Prolo g
does; it was used in the NOA H plannin g sy stem (Sacerdoti , 1975 ; Sacerdoti , 1977) . Mor e recently ,
POPLO G (Sloman , 1985 ) has attempte d to incorporat e severa l programmin g languages , includin g
Lisp, Prolog , and POP­1 1 (Barret t et al,  1985) , into an integrate d system .
Reasonin g system s with metaleve l capabilitie s were first propose d by Haye s (1973) , but
his GOLU X syste m was neve r built (a fate that also befel l Doyle' s (1980 ) ambitiou s SEA N system) .
AMORD(deKleere?a/. , 1977 ) put some of these ideas into practice , as did TEIRESIA S (Davis , 1980 )
in the field of rule­base d exper t systems . In the area of logic programmin g systems , MRS (Gene ­
sereth and Smith , 1981 ; Russell , 1985 ) provide d extensiv e metaleve l facilities . Dincba s and Le
Pape (1984 ) describ e a simila r syste m calle d METALOG . The work of Davi d E. Smit h (1989 ) on
controllin g logica l inferenc e build s on MRS . Alan Bund y 's (1983 ) PRES S syste m used logica l rea­
sonin g at the metaleve l to guid e the use of equalit y reasonin g in solvin g algebr a and trigonometr y
problems . It was able to attai n humanlik e performanc e on the Britis h A­leve l exam s for advance d
precolleg e students , althoug h equalit y reasonin g had previousl y been though t to be a very difficul t
proble m for automate d reasonin g systems . Guar d et al.  (1969 ) describ e the early SAM theore m
prover , whic h helpe d to solve an open proble m in lattic e theory . Wos and Winke r (1983 ) give an
overvie w of the contribution s of AUR A theore m prove r towar d solvin g open problem s in variou s
areas of mathematic s and logic . McCun e (1992 ) follow s up on this , recountin g the accomplish ­
ment s of AURA' S successo r OTTE R in solvin g open problems . McAlleste r (1989 ) describe s the
ONTI C exper t assistan t syste m for mathematic s research .
A Computational  Logic  (Boye r and Moore , 1979 ) is the basic referenc e on the Boyer­Moor e
theore m prover . Sticke l (1988 ) cover s the Prolo g Technolog y Theore m Prove r (PTTP) , whic h
incorporate s the techniqu e of lockin g introduce d by Boye r (1971) .
Early work in automate d progra m synthesi s was done by Simo n (1963) , Gree n (1969a) , and
Mann a and Waldinge r (1971) . The transformationa l syste m of Burstal l and Darlingto n (1977 )
used equationa l reasonin g with recursio n equation s for progra m synthesis . Barsto w (1979 )
provide s an early book­lengt h treatment . RAPT S (Paig e and Henglein , 1987 ) take s an approac h
that view s automate d synthesi s as an extensio n of the proces s of compilation . KID S (Smith ,
1990 ) is one of the stronges t moder n systems ; it operate s as an exper t assistant . Mann a and
Waldinge r (1992 ) give a tutoria l introductio n to the curren t state of the art, with emphasi s on
their own deductiv e approach . Automating  Software  Design  (Lowr y and McCartney , 1991 ) is an
anthology ; the article s describ e a numbe r of curren t approaches .
There are a numbe r of textbook s on logi c programmin g and Prolog . Logic  for Problem
Solving  (Kowalski , 1979b ) is an early text on logi c programmin g in general , with a numbe r of
exercises . Severa l textbook s on Prolo g are availabl e (Clocksi n and Mellish , 1987 ; Sterlin g and
Shapiro , 1986 ; O'Keefe , 1990;Bratko , 1990) . Despit e focusin g on Commo n Lisp , Norvig ( 1992 )
gives a good deal of basi c informatio n abou t Prolog , as wel l as suggestion s for implementin g
Sectio n 10.9 . Summar y 331
Prolo g interpreter s and compiler s in Commo n Lisp . Severa l textbook s on automate d reasonin g
were mentione d in Chapte r 9. Asid e from these , a uniqu e text by Bund y (1983 ) provide s
reasonabl y broa d coverag e of the basic s whil e also providin g treatment s of more advance d topic s
such as meta­leve l inferenc e (usin g PRES S as one case study ) and the use of higher­orde r logic .
The Journal  of Logic  Programming  and the Journal  of Automated  Reasoning  are the principa l
journal s for logic programmin g and theore m provin g respectively . The majo r conference s in these
fields are the annua l Internationa l Conferenc e on Automate d Deductio n (CADE ) and Internationa l
Conferenc e on Logi c Programming .
Aside from classica l example s like the semanti c network s used in Shastri c Sanskri t gramma r
describe d in Chapte r 8, or Peirce' s existentia l graph s as describe d by Robert s (1973) , moder n
work on semanti c network s in AI bega n in the 1960 s with the wor k of Quillia n (1961 ; 1968) .
Charniak' s (1972 ) thesi s serve d to underscor e the full exten t to whic h heterogeneou s knowledg e
on a wide variet y of topic s is essentia l for the interpretatio n of natura l languag e discourse .
Minsky' s (1975 ) so­calle d "frame s paper " serve d to place knowledg e representatio n on the
map as a centra l proble m for AI. The specifi c formalis m suggeste d by Minsky , however , that
of so­calle d "frames, " was widel y criticize d as, at best , a trivia l extensio n of the technique s of
object­oriente d programming , such as inheritanc e and the use of defaul t value s (Dah l etal,  1970 ;
Birtwistl e et al, 1973) , whic h predate d Minsky' s frame s paper . It is not clea r to wha t exten t
the latte r paper s on object­oriente d programmin g were influence d in turn by early AI wor k on
semanti c networks .
The questio n of semantic s arose quit e acutel y with respec t to Quillian' s semanti c network s
(and thos e of other s who followe d his approach) , with their ubiquitou s and very vagu e "ISA links, "
as well as othe r early knowledg e representatio n formalism s such as that of MERLI N (Moor e and
Newell , 1973 ) with its mysteriou s "flat " and "cover " operations . Woods ' (1975 ) famou s articl e
"What' s In a Link? " drew the attentio n of AI researcher s to the need for precis e semantic s in
knowledg e representatio n formalisms . Brachma n (1979 ) elaborate d on this poin t and propose d
solutions . Patric k Hayes' s (1979 ) "The Logi c of Frames " cut even  deepe r by claimin g that mos t
of whateve r conten t such knowledg e representation s did have was merel y sugar­coate d logic :
"Mos t of 'frames ' is just a new synta x for parts of first­orde r logic. " Drew McDermott' s (1978b )
"Tarskia n Semantics , or, No Notatio n Withou t Denotation! " argue d that the kind of semantica l
analysi s used in the forma l stud y of first­orde r logic , base d on Tarski' s definitio n of truth , shoul d
be the standar d for all knowledg e representatio n formalisms . Measurin g all formalism s by
the "logi c standard " has man y advocate s but remain s a controversia l idea ; notably , McDermot t
himsel f has reverse d his positio n in "A Critiqu e of Pure Reason " (McDermott , 1987) . NET L
(Fahlman , 1979 ) was a sophisticate d semanti c networ k syste m whos e ISA link s (calle d "virtua l
copy" or VC links ) wer e base d mor e on the notio n of "inheritance " characteristi c of fram e
system s or of object­oriente d programmin g language s than on the subse t relation , and wer e
much more precisel y define d than Quillian' s links from the pre­Wood s era. NET L is particularl y
intriguin g becaus e it was intende d to be implemente d in paralle l hardwar e to overcom e the
difficult y of retrievin g informatio n from larg e semanti c networks . Davi d Touretzk y (1986 )
subject s inheritanc e to rigorou s mathematica l analysis . Selma n and Levesqu e (1993 ) discus s the
complexit y of inheritanc e with exceptions , showin g that in mos t formulation s it is NP­complete .
The developmen t of descriptio n logic s is merel y the mos t recen t stag e in a long line of
researc h aime d at findin g usefu l subset s of first­orde r logic for whic h inferenc e is computationall y
332 Chapte r 10 . Logica l Reasonin g System s
tractable . Hecto r Levesqu e and Ron Brachma n (1987 ) showe d that certai n logica l constructs ,
notabl y certai n uses of disjunctio n and negation , were primaril y responsibl e for the intractabilit y
of logica l inference . Buildin g on the KL­ON E syste m (Schmolz e and Lipkis , 1983) , a numbe r
of system s have been develope d whos e design s incorporat e the result s of theoretica l complexit y
analysis , mos t notabl y KRYPTO N (Brachma n etai,  1983 ) and Classi c (Borgid a etal,  1989) . The
resul t has been a marke d increas e in the spee d of inference , and a muc h bette r understandin g
of the interactio n betwee n complexit y and expressivenes s in reasonin g systems . On the othe r
hand , as Doyl e and Patil (1991 ) argue , restrictin g the expressivenes s of a languag e eithe r make s
it impossibl e to solv e certai n problems , or encourage s the user to circumven t the languag e
restriction s usin g nonlogica l means .
The stud y of truth maintenanc e system s bega n with the TMS (Doyle , 1979 ) and RUP
(McAllester , 1980 ) systems , both of whic h were essentiall y JTMSs . The ATM S approac h was
describe d in a serie s of paper s by Joha n de Klee r (1986c ; 1986a ; 1986b) . Building  Problem
Solvers  (Forbu s and de Kleer , 1993 ) explain s in dept h how TMS s can be used in AI applications .
EXERCISE S
10.1 Recal l that inheritanc e informatio n in semanti c network s can be capture d logicall y by
suitabl e implicatio n sentences . In this exercise , we will conside r the efficienc y of usin g such
sentence s for inheritance .
a. Conside r the informatio n conten t in a used­ca r catalogu e such as Kelly' s "Blu e Book" :
that, for example , 197 3 Dodg e Van s are wort h $575 . Suppos e all this informatio n (for
11,00 0 models ) is encode d as logica l rules , as suggeste d in the chapter . Writ e dow n three
such rules , includin g that for 1973 Dodg e Vans . How woul d you use the rule s to find the
value of & particular  car (e.g. , JB, whic h is a 1973 Dodg e Van ) give n a backward­chainin g
theore m prove r such as Prolog ?
b. Compar e the time efficienc y of the backward­chainin g metho d for solvin g this proble m
with the inheritanc e metho d used in semanti c nets .
c. Explai n how forwar d chainin g allow s a logic­base d syste m to solv e the sam e proble m
efficiently , assumin g that the KB contain s only the 11,00 0 rules abou t price .
d. Describ e a situatio n in whic h neithe r forwar d nor backwar d chainin g on the rules will allow
the price quer y for an individua l car to be handle d efficiently .
e. Can you sugges t a solutio n enablin g this type of quer y to be solve d efficientl y in all cases in
logic systems ? (Hint:  Remembe r that two cars of the sam e categor y have the same price. )
10.2 Th e followin g Prolo g code define s a relatio n R.
R([],X,X) .
R([A|X],Y,[A|Z] ) :­ R(X,Y,Z )
a. Sho w the proo f tree and solutio n obtaine d for the querie s
R( [1,2 ] ,L, [1,2,3,4 ] ) an d R(L,M , [1,2,3,4 ] )
Sectio n10.9. Summar y 333
b. Wha t standar d list operatio n does R represent ?
c. Defin e the Prolo g predicat e last (L, X) (X is the last elemen t of list L) usin g R and no
other predicates .
10.3 In this exercise , we will look at sortin g in Prolog .
a. Writ e Prolo g clause s that defin e the predicat e sorte d (L) , whic h is true if and only if list
L is sorte d in ascendin g order .
b. Writ e a Prolo g definitio n for the predicat e perm (L, M), whic h is true if and only if L is a
permutatio n of M.
c. Defin e sort (L,M ) (M is a sorte d versio n of L) usin g perm and sorted .
d. Run sort on longe r and longe r lists unti l you lose patience . Wha t is the time complexit y
of your program ?
e. Writ e a faste r sortin g algorithm , such as insertio n sort or quicksort , in Prolog .
10.4 In this exercise , we wil l look at the recursiv e applicatio n of rewrit e rule s usin g logi c
programming . A rewrit e rule (or demodulato r in OTTE R terminology ) is an equatio n with a
specifie d direction . For example , the rewrit e rule x + 0 —> x suggest s replacin g any expressio n
that matche s x + 0 with the expressio n x. Th e applicatio n of rewrit e rule s is a centra l part
of mathematica l reasonin g systems , for example , in expressio n simplificatio n and symboli c
differentiation . We will use the predicat e Rewrite(x,y)  to represen t rewrit e rules . For example ,
the earlie r rewrit e rule is writte n as Rewrite(x  + 0,x).  We will also need som e way to defin e
primitiv e term s that canno t be furthe r simplified . For example , we can use Primitive(O)  to say
that 0 is a primitiv e term .
a. Writ e a definitio n of a predicat e Simplify(x,  y), that is true whe n y is a simplifie d versio n of
jc; that is, no furthe r rewrit e rules are applicabl e to any subexpressio n of y.
b. Writ e a collectio n of rules for simplificatio n of expression s involvin g arithmeti c operators ,
and appl y your simplificatio n algorith m to som e sampl e expressions .
c. Writ e a collectio n of rewrit e rule s for symboli c differentiation , and use them alon g with
your simplificatio n rule s to differentiat e and simplif y expression s involvin g arithmeti c
expressions , includin g exponentiation .
10.5 In this exercise , we will conside r the implementatio n of searc h algorithm s in Prolog .
Suppos e that successo r (X, Y) is true when state Y is a successo r of state X; and that goal (X)
is true whe n X is a goal state . Writ e a definitio n for solv e (X, P), whic h mean s that P is a path
(list of states ) beginnin g with X, endin g in a goal state , and consistin g of a sequenc e of legal steps
as define d by successor . You will find that depth­firs t searc h is the easies t way to do this.
How easy woul d it be to add heuristi c searc h control ?
10.6 Wh y do you thin k that Prolo g include s no heuristic s for guidin g the searc h for a solutio n
to the query ?
10.7 Assum e we put into a logica l databas e a segmen t of the U.S. censu s data listin g the age, city
of residence , date of birth , and mothe r of ever y person , and wher e the constan t symbo l for each
perso n is just their socia l securit y number . Thus , Ron' s age is give n by Age(443­65­1282,76) .
334 Chapte r 10 . Logica l Reasonin g System s
Whic h of the indexin g scheme s S1­S 5 followin g enabl e an efficien t solutio n for whic h of the
querie s Q1­Q 4 (assumin g norma l backwar d chaining) .
<} SI: an inde x for each atom in each position .
0 S2: an inde x for each first argument .
<> S3: an inde x for each predicat e atom .
<> S4: an inde x for each combination  of predicat e and first argument .
<) S5: an inde x for each combination  of predicat e and secon d argument , and an inde x for
each first argumen t (nonstandard) .
0 Ql: Age(443­44­432\,x)
0 Q2 : Residesln(x,  Houston)
0 Q3 : Mother(x,y)
0 Q4 : Age(x,  34) A Residesln(x,  TinyTownUSA)
10.8 W e wouldn' t wan t a semanti c networ k to contai n both Age(Bill,  12) and Age(Bill,  10), but
its fine if it contain s both Friend(Bill,  Opus)  and Friend(Bill,  Steve).  Modif y the function s in I
Figur e 10.1 0 so that they mak e the distinctio n betwee n logica l function s and logica l relations , |
and treat each properly .
10.9 Th e code repositor y contain s a logica l reasonin g syste m whos e component s can be replace d
by othe r versions . Re­implemen t som e or all of the followin g components , and mak e sure that
the resultin g syste m work s usin g the circui t exampl e from Chapte r 8.
a. Basi c data type s and acces s function s for sentence s and their components .
b. STOR E and FETC H for atomi c sentence s (disregardin g efficiency) .
c. Efficien t indexin g mechanism s for STOR E and FETCH .
d. A unificatio n algorithm .
e. A forward­chainin g algorithm .
f. A backward­chainin g algorith m usin g iterativ e deepening .
L
Part IV
ACTIN G LOGICALL Y
In Part II, we saw that an agen t canno t alway s selec t action s base d solel y on
the percept s that are availabl e at the moment , or even the interna l mode l of the
curren t state . We saw that problem­solvin g agent s are able to plan ahead—t o
conside r the consequence s of sequences  of actions—befor e acting . In Part III, we
saw that a knowledge­base d agen t can selec t action s base d on explicit , logica l
representation s of the curren t state and the effect s of actions . Thi s allow s the
agent to succee d in complex , inaccessibl e environment s that are too difficul t for
a problem­solvin g agent .
In Part IV, we put these two ideas togethe r to build plannin g agents . At the
most abstrac t level , the task of plannin g is the same as proble m solving . Plannin g
can be viewe d as a type of proble m solvin g in whic h the agen t uses belief s abou t
action s and their consequence s to searc h for a solutio n over the mor e abstrac t
space of plans , rathe r than over the spac e of situations . Plannin g algorithm s can
also be viewe d as special­purpos e theore m prover s that reaso n efficientl y with
axiom s describin g actions .
Chapte r 11 introduce s the basi c idea s of planning , includin g the need to
divid e comple x problem s into subgoal s whos e solution s can be combine d to
provid e a solutio n for the complet e problem . Chapte r 12 extend s these idea s to
more expressiv e representation s of state s and actions , and discusse s real­worl d
plannin g systems . Chapte r 13 consider s the executio n of plans , particularl y for
cases in whic h unknow n contingencie s mus t be handled .
11PLANNIN G
In which  we see how an agent can take advantage of problem structure to construct
complex  plans  of action.
In this chapter , we introduc e the basic ideas involve d in plannin g systems . We begin by specifyin g
PLANNIN G AGEN T a  simpl e plannin g agen t that is very simila r to a problem­solvin g agen t (Chapte r 3) in that it
construct s plan s that achiev e its goals , and then execute s them . Sectio n 11.2 explain s the
limitation s of the problem­solvin g approach , and motivate s the desig n of plannin g systems . The
plannin g agen t differ s from a problem­solvin g agen t in its representation s of goals , states , and
actions , as describe d in Sectio n 11.4 . The use of explicit , logica l representation s enable s the
planne r to direc t its deliberation s muc h mor e sensibly . The plannin g agen t also differ s in the
way it represent s and searche s for solutions . The remainde r of the chapte r describe s in detai l the
basic partial­orde r plannin g algorithm , whic h searche s throug h the spac e of plan s to find one
that is guarantee d to succeed . The additiona l flexibilit y gaine d from the partiall y ordere d plan
representatio n allow s a plannin g agen t to handl e quit e complicate d domains .
ILlASiMPL E PLANNIN G AGEN T
When the worl d state is accessible , an agen t can use the percept s provide d by the environmen t
to buil d a complet e and correc t mode l of the curren t worl d state . Then , give n a goal , it can call
a suitabl e plannin g algorith m (whic h we will call IDEAL­PLANNER ) to generat e a plan of action .
The agen t can then execut e the steps of the plan , one actio n at a time .
The algorith m for the simpl e plannin g agen t is show n in Figur e 11.1 . Thi s shoul d
be compared  with the problem­solvin g agen t show n in Figur e 3.1. Th e plannin g algorith m
IDEAL­PLANNE R can be any of the planner s describe d in this chapte r or Chapte r 12. We assum e
the existenc e of a functio n STATE­DESCRIPTION , whic h take s a percep t as inpu t and return s an
initia l state descriptio n in the forma t require d by the planner , and a functio n MAKE­GOAL­QUERY ,
whic h is used to ask the knowledg e base wha t the next goal shoul d be. Not e that the agen t mus t
deal with the case wher e the goal is infeasibl e (it just ignore s it and tries another) , and the case
337
338 Chapte r 11 . Planning !
functio n SIMPLE­P L ANNiNG­AGENT(/?ercep O return s an action
static : KB,  a knowledg e base (inplud.e s actio n descriptions )
p, a plan , initiall y NoPlftn
t, a counter , initiall y 0, indicatin g time
local variables : G, a goal
current,  a curren t state descriptio n
TELL(KB,  MAKE­PERCEPT­SENTENCE(percep« , ?))
current^­  STATE­DESCRIPT1ON(£B , t)
if p = NoPlan  then
G <— ASK(£B , MAKE­GOAL­QUERY(O )
p <— lDEAL­PLANNER(cwrrenf , G, KB)
ifp = NoPlan  or p is empt y then action  ^ NoOp
else
action  <— FlRST(p )
, MAKE­ACTION­SENTENCE(acf/0« , f
retur n action
Figur e 11.1 A  simpl e plannin g agent . The agen t first generate s a goal to achieve , and then
construct s a plan to achiev e it from the curren t state . Onc e it has a plan , it keep s executin g it until
the plan is finished , then begin s agai n with a new goal .
wher e the complet e plan is in fact empty , becaus e the goal is alread y true in the initia l state . The
agent interact s with the environmen t in a minima l way—i t uses its percept s to defin e the initia l
state and thus the initia l goal , but thereafte r it simpl y follow s the step s in the plan it has con­
structed . In Chapte r 13, we discus s more sophisticate d agen t designs  that allow more interactio n
betwee n the worl d and the planne r durin g plan execution .
11.2 FRO M PROBLE M SOLVIN G TO PLANNIN G
Plannin g and proble m solvin g are considere d differen t subject s becaus e of the difference s in
the representation s of goals , states , and actions , and the difference s in the representatio n and
constructio n of actio n sequences . In this section , we firs t describ e som e of the difficultie s
encountere d by a search­base d problem­solvin g approach , and then introduc e the method s used
by plannin g system s to overcom e these difficulties .
Recal l the basic element s of a search­base d problem­solver :
• Representatio n of actions . Action s are describe d by program s that generat e successo r
state descriptions .
• Representatio n of states . In proble m solving , a complet e descriptio n of the initia l state is
Sectio n 11.2 . Fro m Proble m Solvin g to Plannin g 339
given , and action s are represente d by a progra m that generate s complet e state descriptions ,
Therefore,/al l state representation s are complete . In most problems , a state is a simpl e data
structure:/ a permutatio n of the piece s in the eigh t puzzle , the positio n of the agen t in a
route­findin g problem , or the positio n of the six peopl e and the boat in the missionarie s and
cannibal s problem . Stat e representation s are used only for successo r generation , heuristi c
functio n evaluation , and goal testing .
• Representatio n of goals . The only informatio n that a problem­solvin g agen t has abou t
its goal is in the form of the goal test and the heuristi c function . Bot h of thes e can be
applie d to state s to decid e on their desirability , but they are used as "blac k boxes. " Tha t
is, the problem­solvin g agen t canno t "loo k inside " to selec t action s that migh t be usefu l in
achievin g the goal .
• Representatio n of plans . In proble m solving , a solutio n is a sequenc e of actions , such as
"Go from Arad to Sibi u to Fagara s to Bucharest. " Durin g the constructio n of solutions ,
searc h algorithm s conside r only unbroke n sequence s of action s beginnin g from the initia l
state (or, in the case of bidirectiona l search , endin g at a goal state) .
Let us see how thes e desig n decision s affec t an agent' s abilit y to solv e the followin g simpl e
problem : "Ge t a quar t of milk and a bunc h of banana s and a variable­spee d cordles s drill. "
Treatin g this as a problem­solvin g exercise , we need to specif y the initia l state : the agen t is at
home but withou t any of the desire d objects , and the operato r set: all the thing s that the agen t
can do. We can optionall y suppl y a heuristi c function : perhap s the numbe r of thing s that have
not yet been acquired .
Figur e 11.2 show s a very smal l part of the first two level s of the searc h spac e for this
problem , and an indicatio n of the path towar d the goal . The actua l branchin g facto r woul d be in
the thousand s or millions , dependin g on how action s are specified , and the lengt h of the solutio n
could be dozen s of steps . Obviously , there are too man y action s and too man y state s to consider .
The real difficult y is that the heuristi c evaluatio n functio n can only choos e amon g state s to decid e
whic h is close r to the goal ; it canno t eliminat e action s from consideration . Eve n if the evaluatio n
functio n coul d get the agen t into the supermarket , the agen t woul d then resor t to a guessin g game .
The agen t make s guesse s by considerin g actions—buyin g an orange , buyin g tuna fish , buyin g
corn flakes , buyin g milk—an d the evaluatio n functio n rank s thes e guesses—bad , bad, bad, good .
The agen t then know s that buyin g milk is a good thing , but has no idea wha t to try next and mus t
start the guessin g proces s all over again .
The fact that the problem­solvin g agen t consider s sequence s of action s startin g from the
initia l state also contribute s to its difficulties . It force s the agen t to decid e first wha t to do in the
initia l state , wher e the relevan t choice s are essentiall y to go to any of a numbe r of othe r places .
Until the agen t has figure d out how  to obtai n the variou s items—b y buying , borrowing , leasing ,
growing , manufacturing , stealing—i t canno t reall y decid e wher e to go. The agen t therefor e need s
a mor e flexibl e way of structurin g its deliberations , so that it can wor k on whicheve r part of the
proble m is mos t likel y to be solvabl e give n the curren t information .
The first key idea behin d plannin g is to "open  up " the representation  of states,  goals, and
actions.  Plannin g algorithm s use description s in som e forma l language , usuall y first­orde r logic
or a subse t thereof . State s and goal s and goal s are represente d by sets of sentences , and action s
are represente d by logica l description s of precondition s and effects . Thi s enable s the planne r to
340 Chapte r 11 . Plannin g
Go To Pet Store
/ ­  "// Go To Schoo l
r­rStart G o To SuPermarke t
\\ Go To Slee p
\\ \ Rea d A Book
\\ Sit in Chai r
\ ——————\ Etc. Etc. ...Talk to Parro t
/ Buy a Dog
Go To Clas s
Buy Tuna Fish
\\ Buy Arugul a
\ Buy Milk
Sit Som e More
\ Rea d A Book—— »­ ••• —— •­ Finis h
Figur e 11.2 Solvin g a shoppin g proble m with forwar d searc h throug h the spac e of situation s
in the world .
make direc t connection s betwee n state s and actions . For example , if the agen t know s that the
goal is a conjunctio n that include s the conjunc t Have(Milk),  and that Buy(x)  achieve s Have(x),
then the agen t know s that it is worthwhil e to conside r a plan that include s Buy(Milk).  It need not
conside r irrelevan t action s such as Buy(WhippingCream)  or GoToSleep.
The secon d key idea behin d plannin g is that the planner  is free to add actions  to the plan
wherever  they are  needed,  rather  than  in an incremental  sequence  starting  at the initial  state.  For
example , the agen t may decid e that it is goin g to have to Buy(Milk),  even befor e it has decide d
wher e to buy it, how to get there , or wha t to do afterwards . Ther e is no necessar y connectio n
betwee n the orde r of plannin g and the orde r of execution . By makin g "obvious " or "important "
decision s first , the planne r can reduc e the branchin g facto r for futur e choice s and reduc e the need
to backtrac k over arbitrar y decisions . Notic e that the representatio n of state s as sets of logica l
sentence s play s a crucia l role in makin g this freedo m possible . For example , whe n addin g the
actio n Buy(Milk)  to the plan , the agen t can represen t the state in whic h the actio n is execute d as,
say, At(Supermarket).  This actuall y represent s an entir e class of states—state s with and withou t
bananas , wit h and withou t a drill , and so on. Searc h algorithm s that requir e complet e state
description s do not have this option .
The third and fina l key idea behin d plannin g is that most  parts  of the world  are independent
of most  other  parts.  This make s it feasibl e to take a conjunctiv e goal like "get a quar t of milk and
a bunc h of banana s and a variable­spee d cordles s drill " and solv e it with a divide­and­conque r
strategy . A subpla n involvin g goin g to the supermarke t can be used to achiev e the first two
Sectio n 11.3 . Plannin g in Situatio n Calculu s 34 1
conjuncts , and anothe r subpla n (e.g. , eithe r goin g to the­hardwar e store or borrowin g from a
neighbor ) can be used to achiev e the third . The supermarke t subpla n can be furthe r divide d into
a milk subpla n and a banana s subplan . We can then put all the subplan s togethe r to solv e the
whol e problem . This work s becaus e there is little interactio n betwee n the two subplans : goin g
to the supermarke t does not interfer e with borrowin g from a neighbor , and buyin g milk does not
interfer e with buyin g banana s (unles s the agen t runs out of some resource , like time or money) .
Divide­and­conque r algorithm s are efficien t becaus e it is almos t alway s easie r to solv e
severa l smal l sub­problem s rathe r than one big problem . However , divide­and­conque r fails in
cases wher e the cost of combinin g the solution s to the sub­problem s is too high . Man y puzzle s
have this property . For example , the goal state in the eigh t puzzl e is a conjunctiv e goal : to get
tile 1 in positio n A and tile 2 in positio n B and ... up to tile 8. We coul d treat this as a plannin g
proble m and plan for each subgoa l independently , but the reaso n that puzzle s are "tricky " is that
it is difficul t to put the subplan s together . It is easy to get tile 1 in positio n A, but gettin g tile 2
in positio n B is likel y to mov e tile 1 out of position . For trick y puzzles , the plannin g technique s
in this chapte r will not do any bette r than problem­solvin g technique s of Chapte r 4. Fortunately ,
the real worl d is a largel y benig n place wher e subgoal s tend to be nearl y independent . If this
were not the case , then the shee r size of the real worl d woul d mak e successfu l proble m solvin g
impossible .
L. 3 PLANNIN G IN SITUATIO N CALCULU S
Befor e gettin g into plannin g technique s in detail , we presen t a formulatio n of plannin g as a logica l
inferenc e problem , usin g situatio n calculu s (see Chapte r 7). A plannin g proble m is represente d
in situatio n calculu s by logica l sentence s that describ e the three main parts of a problem :
• Initia l state : An arbitrar y logica l sentenc e abou t a situatio n 50. For the shoppin g problem ,
this migh t be1
At(Home,  50) A ~^Have(Milk,  50) A ­^Have(Bananas,  S0) A ­iHave(Drill,  S0)
• Goa l state : A logica l quer y askin g for suitabl e situations . For the shoppin g problem , the
query woul d be
3 s At(Home,  s) A Have(Milk,  s) A Have(Bananas,  s) A Have(Drill,  s)
• Operators : A set of description s of actions , usin g the actio n representatio n describe d in
Chapte r 7. For example , here is a successor­stat e axio m involvin g the Buy(Milk)  action :
V a, s Have(Milk,  Result(a, .?)) O  [(a  = Buy(Milk)  A At(Supermarket,  s)
V (Have(Milk,  s)Aa  ^Dmp(Milk))]
Recal l that situatio n calculu s is base d on the idea that action s transfor m states : Result(a,  s) name s
the situatio n resultin g from executin g actio n a in situatio n s. For the purpose s of planning , it
1 A bette r representatio n migh t be alon g the line s of ­i3 m Milk(m)  A Have(m,  So), but we have chose n the simple r
notatio n to facilitat e the explanatio n of plannin g methods . Notic e that partia l state informatio n is handle d automaticall y
by a logica l representation , wherea s problem­solvin g algorithm s require d a specia l multiple­stat e representation .
342 Chapte r 11 . Plannin g
PLANNE Rwill be usefu l to handl e actio n sequence s as well as singl e actions . We will use Result'(I,  s) to
mean the situatio n resultin g from executin g the sequenc e of action s / startin g in s. Result1 is
define d by sayin g that an empt y sequenc e of action s has no effec t on a situation , and the resul t
of a nonempt y sequenc e of action s is the same as applyin g the first action , and then applyin g the
rest of the action s from the resultin g situation :
VA­ Result'([},s)  = s
Va,p, s Result'([a\p],s)  = Result'(p,Result(a,s))
A solutio n to the shoppin g proble m is a plan p that whe n applie d to the start state SQ yield s a
situatio n satisfyin g the goal query . In othe r words , ap such that
At(Home,  Result'(p,  So)) A Have(Milk,  Result'(p,S 0)) A Have(Bananas,  Result'(p,S 0))
A Have(Drill,  Result'(p,  S0))
If we hand this quer y to ASK , we end up with a solutio n such as
p = [Go(SuperMarket),  Buy(Milk),  Buy(Banana),
Go(HardwareStore),  Buy(Drill),  Go(Home)]
From the theoretica l poin t of view , ther e is little more to say. We have a formalis m for expressin g
goals and plans , and we can use the well­define d inferenc e procedur e of first­orde r logic to find
plans . It is true that ther e are som e limitation s in the expressivenes s of situatio n calculus , as
discusse d in Sectio n 8.4, but situatio n calculu s is sufficien t for most plannin g domains .
Unfortunately , a good theoretica l solutio n does not guarante e a good practica l solution .
We saw in Chapte r 3 that proble m solvin g take s time that is exponentia l in the lengt h of the
solutio n in the wors t case, and in Chapte r 9, we saw that logica l inferenc e is only semidecidable .
If you suspec t that plannin g by unguide d logica l inferenc e woul d be inefficient , you'r e right .
Furthermore , the inferenc e procedur e give s us no guarantee s abou t the resultin g plan/ ? othe r than
that it achieve s the goal . In particular , note that ifp achieve s the goal , then so do [Nothing\p]  and
[A, A~' \p], wher e Nothing  is an actio n that make s no change s (or at least no relevan t changes ) to
the situation , and A~' is the invers e of A (in the sens e that s = Result(A~t,Result(A,s))).  So we
may end up with a plan that contain s irrelevan t step s if we use unguide d logica l inference .
To mak e plannin g practica l we need to do two things : (1) Restric t the languag e with whic h
we defin e problems . Wit h a restrictiv e language , ther e are fewe r possibl e solution s to searc h
through . (2) Use a special­purpos e algorith m calle d a planne r rathe r than a general­purpos e
theore m prove r to searc h for a solution . The two go hand in hand : ever y time we defin e a new
problem­descriptio n language , we need a new plannin g algorith m to proces s the language . The
remainde r of this chapte r and Chapte r 12 describ e a serie s of plannin g language s of increasin g
complexity , alon g with plannin g algorithm s for thes e languages . Althoug h we emphasiz e the
algorithms , it is importan t to remembe r that we are  always  dealing with  a logic:  a formal
language  with  a well­defined  syntax,  semantics, and  proof theory.  The proo f theor y says wha t
can be inferre d abou t the result s of actio n sequences , and therefor e wha t the lega l plan s are. The
algorith m enable s us to find thos e plans . The idea is that the algorith m can be designe d to proces s
the restricte d languag e mor e efficientl y than a resolutio n theore m prover .
Sectio n 11.4 . Basi c Representation s for Plannin g 34 3
jJL4 BASI C REPRESENTATION S FOR PLANNING_____________ _
The "classical " approac h that most planner s use toda y describe s state s and operator s in a restricte d
languag e know n as the STRIP S language,2 or in extension s thereof . The STRIP S languag e lend s
itself to efficien t plannin g algorithms , whil e retainin g muc h of the expressivenes s of situatio n
calculu s representations .
Representation s for state s and goal s
In the STRIP S language , state s are represente d by conjunction s of function­fre e groun d literals ,
that is, predicate s applie d to constan t symbols , possibl y negated . For example , the initia l state
for the milk­and­banana s proble m migh t be describe d as
At(Home)  A ­^Have(Milk)  A ^Have(Bananas)  A ^Have(Drill)  A • • •
As we mentione d earlier , a state descriptio n does not need to be complete . An incomplet e state
description , such as migh t be obtaine d by an agen t in an inaccessibl e environment , correspond s
to a set of possibl e complet e state s for whic h the agen t woul d like to obtai n a successfu l plan .
Many plannin g system s instea d adop t the convention—analogou s to the "negatio n as failure "
conventio n used in logi c programming—tha t if the state descriptio n does not mentio n a give n
positiv e litera l then the litera l can be assume d to be false .
Goal s are also describe d by conjunction s of literals . For example , the shoppin g goal migh t
be represente d as
At(Home)  A Have(Milk)  A Have(Bananas}  A Have(Drill)
Goals can also contai n variables . For example , the goal of bein g at a store that sells milk woul d
be represente d as
At(x)f\Sells(x,Milk)
As with goal s give n to theore m provers , the variable s are assume d to be existentiall y quantified .
However , one mus t distinguis h clearl y betwee n a goal give n to a planne r and a quer y give n to a
theore m prover . The forme r asks for a sequenc e of action s that makes  the goal true  if executed,
and the latte r asks whethe r the quer y sentenc e is true  give n the trut h of the sentence s in the
knowledg e base .
Althoug h representation s of initia l state s and goal s are used as input s to plannin g systems ,
it is quit e commo n for the plannin g proces s itsel f to maintai n only implici t representation s of
states . Becaus e mos t action s chang e only a smal l part of the state representation , it is mor e
efficien t to keep track of the changes . We will see how this is done shortly .
2 Name d after a.pioneeringplannin g progra m know n as the STanfor d Researc h Institut e Proble m Solver . Ther e are two
unfortunat e thing s abou t the nam e STRIPS . First , the organizatio n no longe r uses the nam e "Stanford " and is now know n
as SRI International . Second , the progra m is wha t we now call a planner , not a proble m solver , but whe n it was develope d
in 1970 , the distinctio n had not been articulated . Althoug h the STRIP S planne r has long since been superseded , the STRIP S
languag e for describin g action s has been invaluable , and man y "STRIPS­like " variant s have been developed .
344 Chapte r 11 . Plannin g
ACTIO N
DESCRIPTIO N
PRECONDITIO N
EFFEC T
OPERATO R SCHEM A
APPLICABL ERepresentation s for action s
Our STRIP S operator s consis t of three components :
• The actio n descriptio n is wha t an agen t actuall y return s to the environmen t in orde r to do
something . Withi n the plarine r it serve s only as a nam e for a possibl e action .
• The preconditio n is a conjunctio n of atom s (positiv e literals ) that says wha t mus t be true
befor e the operato r can be applied .
• The effec t of an operato r is a conjunctio n of literal s (positiv e or negative ) that describe s
how the situatio n change s whe n the operato r is applied.3
Here is an exampl e of the synta x we will use for formin g a STRIP S operato r for goin g from one
place to another :
Op(A.cnON:Go(there),PKECOND:At(here)  A Path(here,there),
EFFECT.At(there)  A ­*At(here))
(We will also use a graphica l notatio n to describ e operators , as show n in Figur e 11.3. ) Notic e
that there are no explici t situatio n variables . Everythin g in the preconditio n implicitl y refer s to
the situatio n immediatel y befor e the action , and everythin g in the effec t implicitl y refer s to the
situatio n that is the resul t of the action .
At(here),  Path(here,  there)
Go(there )
At(there),  ­iAt(here)
Figur e 11.3 Diagrammati c notatio n for the operato r Go(there).  Th e precondition s appea r
above the action , and the effect s below .
An operato r with variable s is know n as an operato r schema , becaus e it does not correspon d
to a singl e executabl e actio n but rathe r to a famil y of actions , one for each differen t instantiatio n of
the variables . Usually , only fully instantiate d operator s can be executed ; our plannin g algorithm s
will ensur e that each variabl e has a valu e by the time the planne r is done . As with state
descriptions , the languag e of precondition s and effect s is quit e restricted . The preconditio n mus t
be a conjunctio n of positiv e literals , and the effec t mus t be a conjunctio n of positiv e and/o r
negativ e literals . All variable s are assume d universall y quantified , and there can be no additiona l
quantifiers . In Chapte r 12, we will relax these restrictions .
We say that an operato r o is applicabl e in a state s if there is som e way to instantiat e the
variable s in o so that ever y one of the precondition s of o is true in s, that is, if Precond(o)  C s.
In the resultin g state , all the positiv e literal s in Effect(o)  hold , as do all the literal s that held in s,
The origina l versio n of STRIP S divide d the effect s into an add list and a delet e list.
Sectio n 11.4 . Basi c Representation s for Plannin g 345
excep t for thos e that are negativ e literal s in Effect(o).  For example , if the initia l situatio n include s
the literal s
At(Home),  Path(Home,  Supermarket),...
then the actio n Go(Supemarket)  is applicable , and the resultin g situatio n contain s the literal s
^At(Home),  At(Supermarket),  Path(Home,  Supermarket),...
SITUATIO N SPAC E
PROGRESSIO N
REGRESSIO N
PARTIA L PLAN
SNEMEN T
OPERATOR SSituatio n Spac e and Plan Spac e
In Figur e 11.2 , we showe d a searc h spac e of situations  in the worl d (in this case the shoppin g
world) . A path throug h this spac e from the initia l state to the goal state constitute s a plan for
the shoppin g problem . If we wanted , we coul d take a proble m describe d in the STRIP S languag e
and solv e it by startin g at the initia l state and applyin g operator s one at a time unti l we reache d
a state that include s all the literal s in the goal . We coul d use any of the searc h method s of Part
II. An algorith m that did this woul d clearl y be considere d a proble m solver , but we coul d also
conside r it a planner . We woul d call it a situatio n spac e planne r becaus e it searche s throug h
the spac e of possibl e situations , and a progressio n planne r becaus e it searche s forwar d from the
initia l situatio n to the goal situation . The main proble m with this approac h is the high branchin g
facto r and thus the huge size of the searc h space .
One way to try to cut the branchin g facto r is to searc h backwards , from the goal state to the
initia l state ; such a searc h is calle d regressio n planning . Thi s approac h is possible  becaus e the
operator s contai n enoug h informatio n to regres s from a partia l descriptio n of a resul t state to a
partia l descriptio n of the state befor e an operato r is applied . We canno t get complet e description s
of state s this way, but we don' t need to. The approac h is desirable  becaus e in typica l problem s the
goal state has only a few conjuncts , each of whic h has only a few appropriat e operators , wherea s
the initia l state usuall y has man y applicabl e operators . (An operato r is appropriat e to a goal if the
goal is an effec t of the operator. ) Unfortunately , searchin g backward s is complicate d somewha t
by the fact that we often have to achiev e a conjunctio n of goals , not just one. The origina l STRIP S
algorith m was a situation­spac e regressio n planne r that was incomplet e (it coul d not alway s find
a plan whe n one existed ) becaus e it had an inadequat e way of handlin g the complicatio n of
conjunctiv e goals . Fixin g this incompletenes s make s the planne r very inefficient .
In summary , the node s in the searc h tree of a situation­spac e planne r correspon d to sit­
uations , and the path throug h the searc h tree is the plan that will be ultimatel y returne d by
the planner . Eac h branc h poin t adds anothe r step to eithe r the beginnin g (regression ) or end
(progression ) of the plan .
An alternativ e is to searc h throug h the spac e of plans  rathe r than the spac e of situations.
That is, we start with a simple , incomplet e plan , whic h we call a partia l plan . The n we conside r
ways of expandin g the partia l plan until we com e up with a complet e plan that solve s the problem .
The operator s in this searc h are operator s on plans : addin g a step , imposin g an orderin g that puts
one step befor e another , instantiatin g a previousl y unboun d variable , and so on. The solutio n is
the final plan , and the path take n to reach it is irrelevant .
Operation s on plan s com e in two categories . Refinemen t operator s take a partia l plan
and add constraint s to it. On e way of lookin g at a partia l plan is as a representatio n for a set
346 Chapte r 11 . Plannin g
MODIFICATIO N
OPERATO Rof complete , full y constraine d plans . Refinemen t operator s eliminat e som e plan s from this set,
but they neve r add new plan s to it. Anythin g that is not a refinemen t operato r is a modificatio n
operator . Som e planner s wor k by constructin g potentiall y incorrec t plans , and then "debugging "
them usin g modificatio n operators . In this chapter , we use only refinemen t operators .
LEAS T COMMITMEN T
PARTIA L ORDE R
TOTA L ORDE R
LINEARIZATIO N
FULL Y INSTANTIATE D
PLAN SRepresentation s for plan s
If we are goin g to searc h throug h a spac e of plans , we need to be able to represen t them . We
can settl e on a good representatio n for plan s by considerin g partia l plan s for a simpl e problem :
puttin g on a pair of shoes . The goal is the conjunctio n of RightShoeOn  A LeftShoeOn,  the initia l
state has no literal s at all, and the four operator s are
Op(A iCTlON:RightShoe,  PRECOND'.RightSockOn,  EFFECT : RightShoeOn)
Op(ACTlON:RightSock,EFFECT.RightSockOn)
Op(AcnON:LeftShoe,  PRECONV.LeftSocWn,  EFFECT.  LeftShoeOn)
Op(AcnON:LeftSock,  EFFECT : LeftSockOn)
A partia l plan for this proble m consist s of the two steps RightShoe  and LeftShoe.  But whic h step
shoul d com e first ? Man y planner s use the principl e of least commitment , whic h says that one
shoul d only mak e choice s abou t thing s that you currentl y care about , leavin g the othe r choice s
to be worke d out later . Thi s is a good idea for program s that search , becaus e if you mak e a
choic e abou t somethin g you don' t care abou t now, you are likel y to mak e the wron g choic e and
have to backtrac k later . A leas t commitmen t planne r coul d leav e the orderin g of the two steps
unspecified . Whe n a third step, RightSock,  is adde d to the plan , we want to mak e sure that puttin g
on the righ t sock come s befor e puttin g on the righ t shoe , but we do not care wher e they com e
with respec t to the left shoe . A planne r that can represen t plan s in whic h som e step s are ordere d
(befor e or after ) with respec t to each othe r and othe r steps are unordere d is calle d a partia l orde r
planner . The alternativ e is a total orde r planner , in whic h plan s consis t of a simpl e list of steps .
A totall y ordere d plan that is derive d from a plan P by addin g orderin g constraint s is calle d a
linearizatio n of P.
The socks­and­shoe s exampl e does not show it, but planner s also have to commi t to binding s
for variable s in operators . For example , suppos e one of your goal s is Have(Milk),  and you have
the actio n Buy(item,  store).  A sensibl e commitmen t is to choos e this actio n with the variabl e item
boun d to Milk.  However , ther e is no good reaso n to pick a bindin g for store,  so the principl e of
least commitmen t says to leav e it unboun d and mak e the choic e later . Perhap s anothe r goal will
be to buy an item that is only availabl e in one specialt y store . If that store also carrie s milk , then
we can bind the variabl e store  to the specialt y store at that time . By delayin g the commitmen t to
a particula r store , we allow the planne r to mak e a good choic e later . This strateg y can also help
prun e out bad plans . Suppos e that for som e reaso n the branc h of the searc h spac e that include s
the partiall y instantiate d actio n Buy(Milk,  store)  lead s to a failur e for som e reaso n unrelate d to
the choic e of store (perhap s the agen t has no money) . If we had committe d to a particula r store ,
then the searc h algorith m woul d force us to backtrac k and conside r anothe r store . But if we have
not committed , then ther e is no choic e to backtrac k over and we can discar d this whol e branc h
of the searc h tree withou t havin g to enumerat e any of the stores . Plan s in whic h ever y variabl e is
boun d to a constan t are calle d fully instantiate d plans .
Sectio n 11.4 . Basi c Representation s for Plannin g 347
PLAN
CAUSA L LINK SIn this chapter , we will use a representatio n for plan s that allow s for deferre d commitment s
abou t orderin g and variabl e binding . A plan is formall y define d as a data structur e consistin g of
the followin g four components :
• A set of plan steps . Each step is one of the operator s for the problem .
• A set of step orderin g constraints . Each orderin g constrain t is of the form 51, ­< Sj, whic h is
read as "S, befor e Sf and mean s that step S/ mus t occu r sometim e befor e step Sj (but not
necessaril y immediatel y before).4
• A set of variabl e bindin g constraints . Each variabl e constrain t is of the form v = x, wher e v
is a variabl e in some step, and x is eithe r a constan t or anothe r variable .
• A set of causa l links.5 A causa l link is writte n as S/ _1+ S/ and read as "S, achieve s c for
Sj" Causa l link s serv e to recor d the purpose(s ) of steps in the plan : here a purpos e of S,­ is
to achiev e the preconditio n c of Sj.
The initia l plan , befor e any refinement s have take n place , simpl y describe s the unsolve d problem .
It consist s of two steps , calle d Start  and Finish,  with the orderin g constrain t Start  X Finish.  Both
Start and Finish  have null action s associate d with them , so whe n it is time to execut e the plan ,
they are ignored . The Start  step has no preconditions , and its effec t is to add all the proposition s
that are true in the initia l state . The Finish  step has the goal state as its precondition , and no
effects . By definin g a proble m this way, our planner s can start with the initia l plan and manipulat e
it unti l they com e up with a plan that is a solution . The shoes­and­sock s proble m is define d by
the four operator s give n earlie r and an initia l plan that we writ e as follows :
/>/an(STEPS: { Si: 0/7(AcTiON:Starf) ,
S2: Op(ACTlON:Finish,
PRECONV.RightShoeOn  A LeftShoeOn)},
ORDERINGS : {Si ­< S2],
BINDINGS : {},
LINKS : {})
As with individua l operators , we will use a graphica l notatio n to describ e plan s (Figur e 11.4(a)) .
The initia l plan for the shoes­and­sock s proble m is show n in Figur e 11.4(b) . Late r in the chapte r
we will see how this notatio n is extende d to deal with more comple x plans .
Figur e 11.5 show s a partial­orde r plan that is a solutio n to the shoes­and­sock s problem ,
and six linearization s of the plan . This exampl e show s that the partial­orde r plan representatio n
is powerfu l becaus e it allow s a planne r to ignor e orderin g choice s that have no effec t on the
correctnes s of the plan . As the numbe r of steps grows , the numbe r of possibl e orderin g choice s
grow s exponentially . For example , if we adde d a hat and a coat to the problem , whic h interac t
neithe r with each othe r nor with the shoe s and socks , then ther e woul d still be one partia l
plan that represent s all the solutions , but ther e woul d be 180 linearization s of that partia l plan .
(Exercis e 11.1 asks you to deriv e this number) .
4 We use the notatio n A ­< B ­< C to mea n (A X B) A (B ­< C).
5 Som e author s call causa l link s protectio n intervals .
348 Chapte r 11 . Plannin g
LeftShoeOn,  ,, RightShoeOn
(a) (b)
Figur e 11.4 (a ) Problem s are define d by partia l plan s containin g only Start  and Finish  steps .
The initia l state is entere d as the effect s of the Start  step, and the goal state is the preconditio n of
the Finish  step. Orderin g constraint s are show n as arrow s betwee n boxes , (b) The initia l plan for
the shoes­and­sock s problem .
LParti
/
Left
Sockal Orde r Plan :
Start
/ \\
Right
Sock
1 1
eftSockOn  RightSockOn
Left
Shoe
\
LefiShk ;
oeOn,  R
FinishRight
Shoe
/Total Orde r Plans :
Start
{
Right
Sock
{
Left
Sock|
Right
Shoe
I
Left
ShoeStart
*
Right
Sock
1
Left
Sock
{
Left
Shoe
{
Right
ShoeStart
I
Left
Sock
'
Right
Sock
I
Right
Shoe
'
Left
ShoeStart
{
Left
Sock
i
Right
Sock
{
Left
Shoe
'
Right
Shoe
ghtShoeOn  1  1  1  1
Finish Finish Finish FinishStart
|
Right
Sock
'
Right
Shoe
1
Left
Sock
1
Left
Shoe
'
FinishStart
'
Left
Sock
i
Left
Shoe
{
Right
Sock
'
Right
Shoe
{
Finish
Figur e 11.5 A  partial­orde r plan for puttin g on shoe s and sock s (includin g precondition s on
steps) , and the six possibl e linearization s of the plan.
Sectio n 11.5 . A  Partial­Orde r Plannin g Exampl e 349
Solution s
A solutio n is a plan that an agen t can execute , and that guarantee s achievemen t of the goal . If
we wante d to mak e it reall y easy to chec k that a plan is a solution , we coul d insis t that only fully
instantiated , totall y ordere d plan s can be solutions . But this is unsatisfactor y for thre e reasons .
First, for problem s like the one in Figur e 11.5 , it is mor e natura l for the planne r to retur n a
partial­orde r plan than to arbitraril y choos e one of the man y linearization s of it. Second , som e
agent s are capabl e of performin g action s in parallel , so it make s sens e to allo w solution s with
paralle l actions . Lastly , whe n creatin g plan s that may late r be combine d with othe r plan s to
solve large r problems , it pays to retai n the flexibilit y afforde d by the partia l orderin g of actions .
SOLUTIO N Therefore , we allow partiall y ordere d plan s as solution s usin g a simpl e definition : a solutio n is
a complete , consisten t plan . We need to defin e these terms .
COMPLET E PLAN A  complet e plan is one in whic h ever y preconditio n of ever y step is achieve d by som e
ACHIEVE D othe r step . A step achieve s a conditio n if the conditio n is one of the effect s of the step , and
if no othe r step can possibl y cance l out the condition . Mor e formally , a step 5, achieve s a
preconditio n c of the step Sj if (1) S/ ­< Sj and c G EFFECTS(S/) ; and (2) ther e is no step S* such
that (­ic) G EFFECTS(St) , wher e 5, ­X Sfc X Sj in som e linearizatio n of the plan .
CONSISTEN T PLAN A  consisten t plan is one in whic h ther e are no contradiction s in the orderin g or bindin g
constraints . A contradictio n occur s whe n both 5,­ ­< Sj and 5} x 5,­ hold or both v = A and v = B
hold (for two differen t constant s A and B). Bot h ­< and = are transitive , so, for example , a plan
with Si ­< 82, £2 X £3, and £3 ­< Si is inconsistent .
The partia l plan in Figur e 11.5 is a solutio n becaus e all the precondition s are achieved .
From the precedin g definitions , it is easy to see that any linearizatio n of a solutio n is also a
solution . Henc e the agen t can execut e the steps in any orde r consisten t with the constraints , and
still be assure d of achievin g the goal .
1L5 A PARTIAL­ORDE R PLANNIN G EXAMPL E
In this section , we sketc h the outlin e of a partial­orde r regressio n planne r that searche s throug h
plan space . The planne r start s with an initia l plan representin g the start and finis h steps , and
on each iteratio n adds one mor e step . If this lead s to an inconsisten t plan , it backtrack s and
tries anothe r branc h of the searc h space . To keep  the search  focused,  the planner  only  considers
adding  steps that  serve  to achieve  a precondition  that has not yet been  achieved.  The causa l link s
are used to keep track of this.
We illustrat e the planne r by returnin g to the proble m of gettin g some milk , a banana , and a
drill, and bringin g them back home . We will mak e som e simplifyin g assumptions . First , the Go
actio n can be used to trave l betwee n any two locations . Second , the descriptio n of the Buy actio n
ignore s the questio n of mone y (see Exercis e 11.2) . The initia l state is define d by the followin g
operator , wher e HWS  mean s hardwar e store and SM mean s supermarket :
Op(AenON:Start,EFFECT.At(Home)  A Sells(HWS,  Drill)
A Sells(SM,  Milk),  Sells(SM,  Banana))
350 Chapte r 11 . Plannin g
The goal stat e is define d by a Finish  step describin g the object s to be acquire d and the fina l
destinatio n to be reached :
Op(ACT[ON:Finish,
PRECOND:Have(Drill)  A Have(Milk)  A Have(Banana)  A At(Home))
The action s themselve s are define d as follows :
Op(AcrnON:Go(there),PRECOND:At(here),
EFFECT.At(there)  A ­^At(here))
Op(ACTlON:Buy(x),PRECOND:At(store)  A Sells(store,x),
EFFECT : Have(x))
Figur e 11.6 show s a diagra m of the initia l plan for this problem . We will develo p a solutio n to
the proble m step by step, showin g at each poin t a figur e illustratin g the partia l plan at that poin t
in the development . As we go along , we will note som e of the propertie s we requir e for the
plannin g algorithm . Afte r we finis h the example , we will presen t the algorith m in detail .
At(Home)  Sells(SM,Banana) Sells(SM,  Milk)  Sells(HWS,  Drill)
Have  Drill)  Have  Milk) Have(Banana)  At(Home)
Figur e 11.6 Th e initia l plan for the shoppin g problem .
The first thin g to notic e abou t Figur e 11.6 is that there are man y possibl e way s in whic h
the initia l plan can be elaborated . Som e choice s will work , and som e will not. As we work out
the solutio n to the problem , we will show som e correc t choice s and som e incorrec t choices . For
simplicity , we will start with som e correc t choices . In Figur e 11.7 (top) , we have selecte d three
Buy action s to achiev e three of the precondition s of the Finish  action . In each case there is only
one possibl e choic e becaus e the operato r librar y offer s no othe r way to achiev e these conditions .
The bold arrow s in the figur e are causa l links . For example , the leftmos t causa l link in the
figur e mean s that the step Buy(Drill)  was adde d in orde r to achiev e the Finish  step' s Have(DriU)
precondition . The planne r will mak e sure that this conditio n is maintaine d by protectin g it: if a
step migh t delet e the Have(DriU)  condition , then it will not be inserte d betwee n the Buy(Drill)
step and the Finish  step. Ligh t arrow s in the figur e show orderin g constraints . By definition , all
action s are constraine d to com e after the Start  action . Also , all cause s are constraine d to com e
befor e their effects , so you can thin k of each bold arrow as havin g a ligh t arrow underneat h it.
The secon d stage in Figur e 11.7 show s the situatio n after the planne r has chose n to achiev e
the Sells  precondition s by linkin g them to the initia l state . Again , the planne r has no choic e here
becaus e ther e is no othe r operato r that achieve s Sells.
Sectio n 11.5 . A  Partial­Orde r Plannin g Exampl e 351
At(s),  Sellsfs,  Drill) At(s),  Sells(s,Milk) At(s),  Sells(s,Bananas)
Buy(Bananas )
Have(Drill),  Have/Milk),  Have(Bananas),  At(Home)
At(HWS),  Sells(HWS,Drill)  At(SM),  Sells(SM,Milk)  At(SM),  Sells(SM,Bananas)
Buy(Bananas )
Have(Drill),  Have(Milk),  Have(Bananas),  At(Home)
Figur e 11.7 Top : A partia l plan that achieve s three of the four precondition s of Finish.  The
heav y arrow s show causa l links . Bottom : Refinin g the partia l plan by addin g causa l link s to
achiev e the Sells  precondition s of the Buy steps .
Althoug h it may not seem like we have done much yet, this is actuall y quite an improvemen t
over wha t we coul d have done with the problem­solvin g approach . First , out of all the thing s that
one can buy, and all the place s that one can go, we were able to choos e just the right  Buy action s
and just the righ t places,  withou t havin g to wast e time considerin g the others . Then , once we
have chose n the actions , we need not decid e how to orde r them ; a partial­orde r planne r can mak e
that decisio n later .
In Figur e 11.8 , we exten d the plan by choosin g two Go action s to get us to the hardwar e
store and supermarket , thus achievin g the At precondition s of the Buy actions .
So far, everythin g has been easy . A planne r coul d get this far withou t havin g to do any
search . Now it gets harder . The two Go action s have unachieve d precondition s that interac t with
each other , becaus e the agen t canno t be At two place s at the sam e time . Eac h Go actio n has a
preconditio n At(x),  wher e x is the locatio n that the agen t was at befor e the Go action . Suppos e
352 Chapte r 11 . Plannin g
At(HWS),  Sells(HWS,Drill)  At(SM),  Sells(SM,Milk)  At(SM),  Se/fefSM, Bananas )
Have(Drill),  Have(Milk),  Have(Bananas),  At(Home)
Figur e 11.8 A  partia l plan that achieve s A? precondition s of the three Buy actions .
the planne r tries to achiev e the precondition s of Go(HWS)  and Go(SM)  by linkin g them to the :
At(Home)  conditio n in the initia l state . This result s in the plan show n in Figur e 11.9 .
Unfortunately , this will lead to a problem . The step Go(HWS)  adds the conditio n At(HWS),
but it also delete s the conditio n At(Home).  So if the agen t goes to the hardwar e store , it can no
longe r go from hom e to the supermarket . (Tha t is, unles s it introduce s anothe r step to go back
home from the hardwar e store—bu t the causa l link mean s that the start step, not some othe r step,
At(HWS),  Sells(HWS,Drill)  At(SM),  Sells(SM,Milk)  At(SM),  Sells(SM,Bananas)
Have/Drill),  Have(Milk),  Have(Bananas),  At(Home)
Figur e 11.9 A  flawe d plan that gets the agen t to the hardwar e store and the supermarket .
Sectio n 11.5 . A  Partial­Orde r Plannin g Exampl e 353
achieve s the At(Home)  precondition. ) On the othe r hand , if the agen t goes to the supermarke t
first, then it canno t go from hom e to the hardwar e store .
At this point , we have reache d a dead end in the searc h for a solution , and mus t back up
and try anothe r choice . The interestin g part is seein g how a planner could  notice  that this partial
I r plan  is a dead end  without wasting  a lot of time  on it. The key is that the the causa l link s in
PROTECTE D LINK S a  partia l plan are protecte d links . A causa l link is protecte d by ensurin g that threats —tha t is,
THREATS step s that migh t delet e (or clobber ) the protecte d condition—ar e ordere d to com e befor e or after
the protecte d link . Figur e 11.10(a ) show s a threat : The causa l link S\ c 82 is threatene d by
the new step S3 becaus e one effec t of 53 is to delet e c. The way to resolv e the threa t is to add
orderin g constraint s to mak e sure that S3 does not interven e betwee n Si and S2. If S3 is place d
DEMOTIO N befor e Si this is calle d demotio n (see Figur e 11.10(b)) , and if it is place d afte r S2, it is calle d
PROMOTIO N promotio n (see Figur e 11.10(c)) .
/\
s, \
T s
<C ~~IC
­4­ /X/
(a)/ S 3
S ­1 C
81: Tc
S2
(b)81
c
S2
\.
^
(c)S3
­ic
Figur e 11.10 Protectin g causa l links . In (a), the step Si threaten s a conditio n c that is establishe d
by Si and protecte d by the causa l link from Si to 52. In (b), Sy has been demote d to com e befor e
Si , and in (c) it has been promote d to come  after 82 .
In Figur e 11.9 , there is no way to resolv e the threa t that each Go step pose s to the other .
Whicheve r Go step come s first will delet e the At (Home)  conditio n on the othe r step. Wheneve r
the planne r is unabl e to resolv e a threa t by promotio n or demotion , it give s up on the partia l plan
and back s up to a try a differen t choic e at som e earlie r poin t in the plannin g process .
Suppos e the next choic e is to try a differen t way to achiev e the At(x)  preconditio n of the
Go(SM)  step, this time by addin g a causa l link from Go(HWS)  to Go(SM).  In othe r words , the
plan is to go from hom e to the hardwar e stor e and then to the supermarket . Thi s introduce s
anothe r threat . Unles s the plan is furthe r refined , it will allow the agen t to go from the hardwar e
store to the supermarke t withou t first buyin g the drill (whic h was why it wen t to the hardwar e
store in the first place) . Howeve r muc h this migh t resembl e huma n behavior , we woul d prefe r our
plannin g agen t to avoi d such forgetfulness . Technically , the Go(SM)  step threaten s the At(HWS)
preconditio n of the  Buy  (Drill)  step, whic h is protecte d by a causa l link . The threa t is resolve d by
constrainin g Go(SM)  to com e afte r Buy(Drill).  Figur e 11.1 1 show s this .
354 Chapte r 11 . Plannin g
Go(HWS )s
At(HW>""^_ Star t
/­^
v X  S;;, se//s(Hws,Dri//j / x»fs/w) , se/fefs/w./w//* ;
Buy(Drill )
\
Have(Drill)  , HBuy(Milk )
y ­
ave(Milk)  , Have(8ana
Finis h/\ffSM; , Se/fefS/ WGo(SM )
Bananas)
Buy(Bananas )
^—^___^^^  ~^~—^.
iasj , At(Home)  "*r­^M(SM)
Go(Home )
$x
Figur e 11.11 Causa l link protectio n in the shoppin g plan . The Go(HWS) A'^^ Buy(Drill)
causa l link is protecte d orderin g the Go(SM)  step afte r Buy(Drill),  and the Go(SM) A^­S
Buy(MilklBananas)  link is protecte d by orderin g Go(Home)  afte r Buy(Milk)  and Buy(Bananas).
Only the At(Home)  preconditio n of the Finish  step remain s unachieved . Addin g a
Go(Home)  step achieve s it, but introduce s an At(x)  preconditio n that need s to be achieved.6
Again , the protectio n of causa l link s will help the planne r decid e how to do this:
• If it tries to achiev e At(x)  by linkin g to At(Home)  in the initia l state , ther e will be no way
to resolv e the threat s cause d by go(HWS)  and Go(SM).
• If it tries to link At(x)  to the Go(HWS)  step , ther e will be no way to resolv e the threa t pose d
by the Go(SM)  step , whic h is alread y constraine d to com e after Go(HWS).
• A link from Go(SM)  to At(x)  mean s that x is boun d to SM,  so that now the Go(Home)
step delete s the At(SM)  condition . Thi s result s in threat s to the At(SM)  precondition s of
Buy(Milk)  and Buy(Bananas),  but thes e can be resolve d by orderin g Go(Home)  to com e
after thes e step s (Figur e 11.11) .
Figur e 11.1 2 show s the complet e solutio n plan , with the step s redraw n to reflec t the orderin g
constraint s on them . The resul t is an almos t totall y ordere d plan ; the only ambiguit y is that
Buy(Milk)  and Buy(Bananas)  can com e in eithe r order .
Let us take stock of wha t our partial­orde r planne r has accomplished . It can take a proble m
that woul d requir e man y thousand s of searc h state s for a problem­solvin g approach , and solve it
with only a few searc h states . Moreover , the least  commitmen t natur e of the planne r mean s it
only need s to searc h at all in place s wher e subplan s interac t with each other . Finally , the causa l
links allo w the planne r to recogniz e whe n to abando n a doome d plan withou t wastin g a lot of
time expandin g irrelevan t parts of the plan .
6 Notic e that the Go(Home)  step also has the effec t ­iAt(x),  meanin g that the step will delet e an At conditio n for some
locatio n yet to be decided . This is a possibl e threa t to protecte d condition s in the plan such as At(SM ), but we will not
worr y abou t it for now . Possibl e threat s are deal t with in Sectio n 11.7 .
Sectio n 11.6 . A  Partial­Orde r Plannin g Algorith m 355
Have/Milk)  At/Home)  Have(Banj  Have/Drill)At(SM)  Sel/sfSM.Milk)
Figur e 11.12 A  solutio n to the shoppin g problem .
11.6 A  PARTIAL­ORDE R PLANNIN G ALGORITH M
In this section , we develo p a mor e forma l algorith m for the planne r sketche d in the previou s
section . W e call the algorith m POP , for Partial­Orde r Planner . Th e algorith m appear s in
Figur e 11.13 . (Notic e that POP is writte n as a nondeterministic  algorithm , usin g choos e and fail
rathe r than explici t loops . Nondeterministi c algorithm s are explaine d in Appendi x B.)
POP start s with a minima l partia l plan , and on each step extend s the plan by achievin g a
preconditio n c of a step Sneed­ It does this by choosin g som e operator—eithe r from the existin g
steps of the plan or from the pool of operators—tha t achieve s the precondition . It record s the
causa l link for the newl y achieve d precondition , and then resolve s any threat s to causa l links .
The new step may threate n an existin g causa l link or an existin g step may threate n the new causa l
link. If at any poin t the algorith m fails to find a relevan t operato r or resolv e a threat , it backtrack s
to a previou s choic e point . An importan t subtlet y is that the selectio n of a step and preconditio n
in SELECT­SUBGOA L is not a candidat e for backtracking . The reaso n is that ever y preconditio n
needs to be considere d eventually , and the handlin g of precondition s is commutative : handlin g
c\ and then 02 lead s to exactl y the sam e set of possibl e plan s as handlin g c­i and then c\. So we
356 Chapte r 11 . Plannin g !
functio n POP(initial,  goal,  operators)  returns  plan
plan  <— MAKE­MlNIMAL­PLAN(/«i'fw/ , goal)
loop do
if SoLUTiON?(p/aw ) then retur n plan
Sneed,  C — SELECT­SUBGOAL(/7/an )
CHOOSE­OPERATOR(/?/an , operators,  Sneed, c)
RESOLVE­THREATS(p/an )
end
functio n SELECT­SuBGOAL(/7/arc ) return s Sneed, c
pick a plan step Sneed from STEPS(plan)
with a preconditio n c that has not been achieve d
retur n Sneed,  c
procedur e CHOOSE­OPERATOR(/?/an , operators,  S,, eed, c)
choos e a step Sadd from operators  or STEPS(p/a« ) that has c as an effec t
if there is no such step then fail
add the causa l link Sadd —^ S,, eed to LtNKS(p/an )
add the orderin g constrain t Sadd  ~< £„«.</t o ORDERiNGS(p/an )
if Sadd is a newl y adde d step from operators  then
add Sadd to STEPS(plan)
add Start  ­< Sadd X Finish  to ORDERINGS ( p/an )
procedur e RESOLVE­THREATS(p/an )
. S, in LlNKS(p/an)d o for each StnKat that threaten s a link S,
choos e eithe r
Promotion:  Add 51,/,,™, ­< S; to ORDERlNGS(/?/an )
Demotion:  Add S, ^ 5,;, rear to ORDERINGS(p/o« )
if not CoNSiSTENT(pfa« ) then fail
end
Figur e 11.1 3 Th e partial­orde r plannin g algorithm , POP .
can just pick a preconditio n and mov e ahea d withou t worryin g abou t backtracking . The pick we
make affect s only the speed , and not the possibility , of findin g a solution .
Notic e that POP is a regressio n planner , becaus e it start s with goal s that need to be
achieve d and work s backward s to find operator s that will achiev e them . Onc e it has achieve d all
the precondition s of all the steps , it is done ; it has a solution . POP is sound  and complete.  Ever y
plan it return s is in fact a solution , and if there is a solution , then it will be foun d (assumin g
a breadth­firs t or iterativ e deepenin g searc h strategy) . At this point , we sugges t that the reade r
retur n to the exampl e of the previou s section , and trace throug h the operatio n of POP in detail .
Sectio n 11.7 . Plannin g with Partiall y Instantiate d Operator s 357
11/7 PLANNIN G WITH PARTIALL Y INSTANTIATE D OPERATOR S
The versio n of POP in Figur e 11.1 3 outline s the algorithm , but leave s som e detail s unspecified .
In particular , it does not deal with variabl e bindin g constraints . For the mos t part, all this entail s
is bein g diligen t abou t keepin g track of bindin g lists and unifyin g the righ t expression s at the
right time . The implementatio n technique s of Chapte r 10 are applicabl e here .
There is one substantiv e decisio n to make : in RESOLVE­THREATS , shoul d an operato r that
has the effect , say, ­*At(x)  be considere d a threa t to the conditio n At(Home)l  Currentl y we can
POSSIBL E THREA T distinguis h betwee n threat s and non­threats , but this is a possibl e threat . Ther e are thre e main
approache s to dealin g with possibl e threats :
• Resolv e now with an equalit y constraint : Modif y RESOLVE­THREAT S so that it resolve s
all possibl e threat s as soon as they are recognized . For example , whe n the planne r choose s
the operato r that has the effec t ­<At(x),  it woul d add a bindin g such as x = HWS  to mak e
sure it does not threate n At(Home).
• Resolv e now with an inequalit y constraint : Exten d the languag e of variabl e bindin g
constraint s to allo w the constrain t x^Home.  Thi s has the advantag e of bein g a lowe r
commitment—i t does not requir e an arbitrar y choic e for the valu e of x—but it is a little
more complicate d to implement , becaus e the unificatio n routine s we have used so far all
deal with equalities , not inequalities .
• Resolv e later : The third possibilit y is to ignor e possibl e threats , and only deal with them
when they becom e necessary  threats . Tha t is, RESOLVE­THREAT S woul d not conside r
­)At(x),  to be a threa t toAt(Home).  But if the constrain t x = Home  were ever adde d to the
plan, then the threa t woul d be resolve d (by promotio n or demotion) . Thi s approac h has
the advantag e of bein g low commitment , but has the disadvantag e of makin g it harde r to
decid e if a plan is a solution .
Figur e 11.1 4 show s an implementatio n of the change s to CHOOSE­OPERATOR , alon g with the
change s to RESOLVE­THREAT S that are necessar y for the third approach . It is certainl y possibl e
(and advisable ) to do some bookkeepin g so that RESOLVE­THREAT S will not need to go throug h
a tripl y neste d loop on each call.
When partiall y instantiate d operator s appea r in plans , the criterio n for solution s need s to
be refine d somewhat . In our earlie r definitio n (pag e 349) , we were concerne d mainl y with the
questio n of partia l ordering ; a solutio n was define d as a partia l plan such that all linearization s
are guarantee d to achiev e the goal . Wit h partiall y instantiate d operators , we also need to ensur e
that all instantiation s will achiev e the goal . We therefor e exten d the definitio n of achievemen t
for a step in a plan as follows :
A step Si achieve s a preconditio n c of the step S/ if (1) 5,­ ^ S/ and 5,­ has an effec t
that necessaril y unifie s with c; and (2) ther e is no step 5* such that S/ ­< Sk ­< Sj in
some linearization  of the plan , and S* has an effec t that possibl y unifie s with ­ic.
The POP algorith m can be seen as constructin g a proo f that the each preconditio n of the goal step
is achieved . CHOOSE­OPERATO R come s up with the 5,­ that achieve s (1), and RESOLVE­THREAT S
make s sure that (2) is satisfie d by promotin g or demotin g possibl e threats . The trick y part is that
358 Chapte r 11 . Plannin g
procedur e CHOOSE­OPERATOR(p/cm , operators,  S,,m/, c)
choos e a step Salia from operators  or STEPS(ptaw ) that has cacu as an effec t
such that M = UNIFY(C , cacU, BiNDiNGS(pfan) )
if there is no such step
then fail
add u to BlNDINGS(pfan )
add Sack! C : S,,eed  tO LINKS ( £>/«« )
add Sadj  ­< Snttd  to ORDERINGS(ptaw )
if Sadd is a newl y adde d step from operators  then
add Sadd  to STEPS(pta« )
add Start ­< S0(w ­< Finish  to ORDERiNGS(/)tei )
procedur e RESOLVE­THREATS(p/an )
for each S, _^ 5,­ in LiNKS(ptan ) do
for each S,i, reat in STEPS(pton ) do
for each c' in EFFECT(S,/, m,,) do
if SUBST(BlNDINGS(p/fl«),<: ) = SUBST(BlNDINGS(p/an),­ic')the n
choos e eithe r
Promotion:  Add S,i, rea, ­< 5, to ORDERINGS(p/on )
Demotion:  Add S/ ­< S,/, ra« to ORDERINGS(p/fl« )
if not CONSISTENT(/7/fltt )
then fail
end
end
end
Figur e 11.1 4 Suppor t for partiall y instantiate d operator s in POP .
if we adop t the "resolve­later " approach , then there will be possibl e threat s that are not resolve d j
away . We therefor e need som e way of checkin g that these threat s are all gone befor e we return !
the plan . It turn s out that if the initia l state contain s no variable s and if every operato r mention s |
all its variable s in its precondition , then any complet e plan generate d by POP is guarantee d to be :
fully instantiated . Otherwis e we will need to chang e the functio n SOLUTION ? to chec k that there |
are no uninstantiate d variable s and choos e binding s for them if there are. If this is done , then |
POP is guarantee d to be a soun d planne r in all cases .
It is harde r to see that POP is complete—tha t is, find s a solutio n wheneve r one exists—bu t |
again it come s dow n to understandin g how the algorith m mirror s the definitio n of achievement .
The algorith m generate s ever y possibl e plan that satisfie s part (1), and then filter s out those plans |
that do not satisf y part (2) or that are inconsistent . Thus , if there is a plan that is a solution , POP j
will find it. So if you accep t the definitio n of solutio n (pag e 349) , you shoul d accep t that POP is J
a soun d and complet e planner .
Sectio n 11.8 . Knowledg e Engineerin g for Plannin g 35 9
11.8_ KNOWLEDG E ENGINEERIN G FOR PLANNING____________ _
The methodolog y for solvin g problem s with the plannin g approac h is very muc h like the genera l
knowledg e engineerin g guidelines,o f Sectio n 8.2:
• Decid e wha t to talk about .
• Decid e on a vocabular y of condition s (literals) , operators , and objects .
• Encod e operator s for the domain .
• Encod e a descriptio n of the specifi c proble m instance .
• Pose problem s to the planne r and get back plans .
We will cove r each of these five steps , demonstratin g them in two domains .
The block s worl d
Wha t to talk about : The main consideratio n is that operator s are so restricte d in what  they can
expres s (althoug h Chapte r 12 relaxe s som e of the restrictions) . In this sectio n we show how to
defin e knowledg e for a classi c plannin g domain : the block s world . This domai n consist s of a set
of cubi c block s sittin g on a table . The block s can be stacked , but only one bloc k can fit directl y
on top of another . A robo t arm can pick up a bloc k and mov e it to anothe r position , eithe r on
the table or on top of anothe r block . The arm can only pick up one bloc k at a time , so it canno t
pick up a bloc k that has anothe r one on it. The goal will alway s be to buil d one or more stack s
of blocks , specifie d in term s of what block s are on top of wha t othe r blocks . For example , a goal
migh t be to mak e two stacks , one with bloc k A on B, and the othe r with C on D.
Vocabulary : The object s in this domai n are the block s and the table . The y are represente d
by constants . We will use On(b,  x) to indicat e that bloc k b is on x, wher e x is eithe r anothe r block
or the table . The operato r for movin g bloc k b from a positio n on top of x to a positio n on top y
will be Move(b,x,y).  Now one of the precondition s on movin g b is that no othe r bloc k is on it.
In first­orde r logic this woul d be ­i3 x On(x,  b) or alternativel y M x ­>On(x,b).  But our languag e
does not allow eithe r of these forms , so we have to thin k of somethin g else. The trick is to inven t
a predicat e to represen t the fact that no bloc k is on b, and then mak e sure  the operator s properl y
maintai n this predicate . We will use Clear(x)  to mea n that nothin g is on x.
Operators : The operato r Move  move s a bloc k b from x to y if both b and y are clear , and
once the mov e is made , x become s clear but y is clear no longer . The forma l descriptio n of Move
is as follows :
Op(ACTlON:Move(b,  x, y),
PRECOND:On(b,x)  A Clear(b)  A Clear(y),
EFFECT : On(b,y)  A Clear(x)  A ­>On(b,x)  A ^Clear(y))
Unfortunately , this operato r does not maintai n Clear  properl y whe n x or y is the table . Whe n
x = Table,  this operato r has the effec t Clear(Table),  but the table shoul d not becom e clear , and
when y = Table,  it has the preconditio n Clear(Table),  but the tabl e does not have to be clea r to
360 Chapte r 11 . Plannin g
move a bloc k onto it. To fix this, we do two things . First , we introduc e anothe r operato r to mov e
a block b from x to the table :
Op(Acnow.MoveToTable(b,  x),
PRECOND:On(b,x)  A Clear(b),
EFFECT : #«(/? , Table)  A Clear(x)  A ~^On(b,x))
Second , we take the interpretatio n of Clear(x)  to be "ther e is a clear spac e on x to hold a block. "
Unde r this interpretation , Clear(Table)  will alway s be part of the initia l situation , and it is prope r
that Move(b,  Table,  y) has the effec t Clear(Table).  The only proble m is that nothin g prevent s
the planne r from usin g Move(b,x,  Table)  instea d of MoveToTable(b,x).  We coul d eithe r live
with this problem—i t will lead to a larger­than­necessar y searc h space , but will not lead to
incorrec t answers—o r we coul d introduc e the predicat e Block  and add Block(b)  A Block(y)  to the
preconditio n of Move.
Finally , ther e is the proble m of spuriou s operation s like Move(B,  C, C),  whic h shoul d be a
no­op , but whic h instea d has contradictor y effects . It is commo n to ignor e problem s like this,
becaus e they tend not to have any effec t on the plan s that are produced . To reall y fix the problem,
we need to be able to put inequalitie s in the precondition : b^x^y.
Shakey' s worl d
The origina l STRIP S progra m was designe d to contro l Shakey,7 a robo t that roame d the halls of
SRI in the early 1970s . It turn s out that mos t of the work on STRIP S involve d simulation s wher e
the action s performe d were just printin g to a terminal , but occasionall y Shake y woul d actuall y
move around , grab , and push things , base d on the plan s create d by STRIPS . Figur e 11.1 5 show s a
versio n of Shakey' s worl d consistin g of four room s line d up alon g a corridor , wher e each room
has a door and a light switch .
Shake y can mov e from plac e to place , push movabl e object s (suc h as boxes) , clim b on
and off of rigid object s (suc h as boxes) , and turn ligh t switche s on and off. We will develo p the j
vocabular y of literal s alon g with the operators :
1. Go from curren t locatio n to locatio n y: Go(y)
This is simila r to the Go operato r used in the shoppin g problem , but somewha t restricted. ;
The preconditio n At(Shakey,x)  establishe s the curren t location , and we will insis t that
and >• be In the same room : In(x,  r) A ln(y,  r). To allow Shake y to plan a route from room |
to room , we will say that the door betwee n two room s is In both of them .
2. Pus h an objec t b from locatio n x to locatio n y: Push(b,  x, y)
Agai n we will insis t that the location s be in the sam e room . We introduc e the predicat e j
Pushable(b),  but otherwis e this is simila r to Go.
3. Clim b up onto a box: Climb(b).
We introduc e the predicat e On and the constan t Floor,  and mak e sure that a precondition |
of Go is On(Shakey,  Floor}.  For Climb(b),  the precondition s are that Shake y is A? the same j
place as b, and b mus t be Climbable.
Shakey' s nam e come s from the fact that its motor s mad e it a little unstabl e whe n it moved .
Sectio n 11.8 . Knowledg e Engineerin g for Plannin g 361
Ls4\
Room 4 DO C
n i.s 3 ,
•"• Roo m 3 DO C
Shake y
Ls2/
Room 2 DO C
Box3 D  „ Ls 1 \Box2jr4
)r3
Corrido r
>r2
ROO m 1 Door l
Box 4 Box 1
Figur e 11. 15 Shakey' s world .
4. Clim b dow n from a box: Down(b).
This just undoe s the effect s of a Climb.
5. Tur n a light switc h on: TurnOn(ls).
Becaus e Shake y is short , this can only be done whe n Shake y is on top of a box that is at
the light switch' s location.8
6. Tur n a ligh t switc h off: TurnOff(ls).
This is simila r to TurnOn.  Not e that it woul d not be possibl e to represen t togglin g a light
switc h as a STRIP S action , becaus e there are no conditional s in the languag e to say that the
light become s on if it was off and off if it was on. (Sectio n 12.4 will add conditional s to
the language. )
In situatio n calculus , we coul d writ e an axio m to say that ever y box is pushabl e and climbable .
But in STRIPS , we have to includ e individua l literal s for each box in the initia l state . We also have
to includ e the complet e map of the worl d in the initia l state , in term s of wha t object s are In whic h
8 Shake y was neve r dextrou s enoug h to clim b on a box or toggl e a switch , but STRIP S was capabl e of findin g plan s usin g
these actions .
362 Chapte r 11 . Plannin g
rooms , and whic h location s they are At. We leav e this, and the specificatio n of the operators , as
an exercise .
In conclusion , it is possibl e to represen t simpl e domain s with STRIP S operators , but it
require s ingenuit y in comin g up with the righ t set of operator s and predicate s in orde r to stay
withi n the syntacti c restriction s that the languag e imposes .
11.9 SUMMAR Y
In this chapter , we hav e define d the plannin g proble m and show n that situatio n calculu s is •
expressiv e enoug h to deal with it. Unfortunately , situatio n calculu s plannin g usin g a general ­
purpos e theore m prove r is very inefficient . Usin g a restricte d languag e and special­purpose ;
algorithms , plannin g system s can solv e quit e comple x problems . Thus , plannin g come s dow n ]
to an exercis e in findin g a languag e that is just expressiv e enoug h for the problem s you wan t to j
solve , but still admit s a reasonabl y efficien t algorithm . The point s to remembe r are as follows :
• Plannin g agent s use lookahea d to come up with action s that will contribut e to goal achieve ­
ment . The y diffe r from problem­solvin g agent s in their use of more flexibl e representation s j
of states , actions , goals , and plans .
• The STRIP S languag e describe s action s in term s of thei r precondition s and effects . It|
capture s muc h of the expressiv e powe r of situatio n calculus , but not all domain s and j
problem s that can be describe d in the STRIP S language .
• It is not feasibl e to searc h throug h the spac e of situation s in comple x domains . Instea d we {
searc h throug h the spac e of plans , startin g with a minima l plan and extendin g it unti l we ^
find a solution . For problem s in whic h mos t subplan s do not interfer e with each other , this j
will be efficient .
• The principl e of least commitmen t says that a planne r (or any searc h algorithm ) shoul d I
avoid makin g decision s unti l ther e is a good reaso n to mak e a choice . Partial­ordering !
constraint s and uninstantiate d variable s allow us to follo w a least­commitmen t approach .
• The causa l link is a usefu l data structur e for recordin g the purpose s for steps . Each causa l •<
link establishe s a protectio n interva l over whic h a conditio n shoul d not be delete d byj
anothe r step. Causa l link s allow early detectio n of unresolvabl e conflict s in a partia l plan ,
thereb y eliminatin g fruitles s search .
• The POP algorith m is a soun d and complet e algorith m for plannin g usin g the STRIP S ;
representation .
• The abilit y to handl e partiall y instantiate d operator s in POP reduce s the need to commi t to j
concret e action s with fixed arguments , thereb y improvin g efficiency .
Sectio n 11.9 . Summar y 363
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
The roots of AI plannin g lie partl y in proble m solvin g throug h state­spac e searc h and associate d
technique s such as proble m reductio n and means­end s analysis , especiall y as embodie d in
Newel l and Simon' s GPS , and partl y in theore m provin g and situatio n calculus , especiall y as
embodie d in the QA3 theore m provin g syste m (Green , 1969b) . Plannin g has also been historicall y
motivate d by the need s of robotics . STRIP S (Fike s and Nilsson , 1971) , the first majo r plannin g
system , illustrate s the interactio n of these three influences . STRIP S was designe d as the plannin g
componen t of the softwar e for the Shake y robo t projec t at SRI International . Its overal l contro l
structur e was modele d on that of GPS, and it used a versio n of QA3 as a subroutin e for establishin g
precondition s for actions . Lifschit z (1986 ) offer s carefu l criticis m and forma l analysi s of the
STRIP S system . Bylande r (1992 ) show s simpl e plannin g in the fashio n of STRIP S to be PSPACE ­
complete . Fike s and Nilsso n (1993 ) give a historica l retrospectiv e on the STRIP S projec t and a
surve y of its relationshi p to mor e recen t plannin g efforts .
For severa l years , terminologica l confusio n has reigne d in the field of planning . Som e
LINEA R author s (Geneseret h and Nilsson , 1987 ) use the term linea r to mean what  we call totall y ordered ,
NONLINEA R an d nonlinea r for partiall y ordered . Sacerdot i (1975) , who originate d the term , used "linear " to
NONINTERLEAVE D refe r to a propert y that we will call noninterleaved . Give n a set of subgoals , a noninterleave d
planne r can find plan s to solv e each subgoal , but then it can only combin e them by placin g all
the steps for one subpla n befor e or after all the steps of the others . Man y early planner s of the
1970s were noninterleaved , and thus were incomplete—the y coul d not alway s find a solutio n
when one exists . This was forcefull y drive n hom e by the Sussma n Anomal y (see Exercis e 11.4) ,
found durin g experimentatio n with the HACKE R syste m (Sussman , 1975) . (Th e anomal y was
actuall y foun d by Alie n Brown , not by Sussma n himself , who though t at the time that assumin g
linearit y to begi n with was often a workabl e approach. ) HACKE R introduce d the idea of protectin g
subgoals , and was also an early exampl e of plan learning .
Goal regressio n planning , in whic h steps in a totall y ordere d plan are reordere d so as to avoid
conflic t betwee n subgoals , was introduce d by Waldinge r (1975 ) and also used by Warren' s (1974 )
WARPLAN . WARPLA N is also notabl e in that it was the first planne r to be writte n usin g a logic
programmin g languag e (Prolog) , and is one of the best example s of the remarkabl e econom y
that can sometime s be gaine d by usin g logi c programming : WARPLA N is only 100 line s of
code, a smal l fractio n of the size of comparabl e planner s of the time . INTERPLA N (Tate , 1975b ;
Tate, 1975a ) also allowe d arbitrar y interleavin g of plan steps to overcom e the Sussma n anomal y
and relate d problems .
The constructio n of partiall y ordere d plan s (then calle d task networks ) was pioneere d by
the NOA H planne r (Sacerdoti , 1975 ; Sacerdoti , 1977) , and thoroughl y investigate d in Tate' s (1977 )
NONLI N system , whic h also retaine d the clear conceptua l structur e of its predecesso r INTERPLAN .
INTERPLA N and NONLI N provid e muc h of the groundin g for the work describe d in this chapte r and
the next , particularl y in the use of causa l link s to detec t potentia l protectio n violations . NONLI N
was also the first planne r to use an explici t algorith m for determinin g the truth or falsit y of
condition s at variou s point s in a partiall y specifie d plan .
TWEA K (Chapman , 1987 ) formalize s a generic , partial­orde r plannin g system . Chapma n
provide s detaile d analysis , includin g proof s of completenes s and intractabilit y (NP­hardnes s and
364 Chapte r 11 . Plannin g
undecidability ) of variou s formulation s of the plannin g proble m and its subcomponents . The
POP algorith m describe d in the chapte r is base d on the SNLP  algorith m (Soderlan d and Weld ,
1991) , whic h is an implementatio n of the planne r describe d by McAlleste r and Rosenblit t (1991) .
Weld contribute d severa l usefu l suggestion s to the presentatio n in this chapter .
A numbe r of importan t paper s on plannin g were presente d at the Timberlin e worksho p in
1986, and its proceeding s (Georgef f and Lansky , 1986 ) are an importan t source . Readings  in
Planning  (Alie n el al., 1990 ) is a comprehensiv e antholog y of man y of the best article s in the ;
field , includin g severa l good surve y articles . Planning  and Control  (Dea n and Wellman , 1991 ) >
is a good genera l introductor y textboo k on planning , and is particularl y remarkabl e becaus e it•'
make s a particula r effor t to integrat e classica l AI plannin g technique s with classica l and modern :
contro l theory , metareasoning , and reactiv e plannin g and executio n monitoring . Wel d (1994) ,
provide s an excellen t surve y of moder n plannin g algorithms .
Plannin g researc h has been centra l to AI sinc e its inception , and paper s on plannin g are :
a stapl e of mainstrea m AI journal s and conferences , but ther e are also specialize d conference s \
devote d exclusivel y to planning , like the Timberlin e workshop , the 1990 DARP A Worksho p on
Innovativ e Approache s to Planning , Scheduling , and Control , or the Internationa l Conference s ;
on AI Plannin g Systems .
EXERCISE S
11.1 Defin e the operato r schemat a for the proble m of puttin g on shoe s and sock s and a hat and |
coat, assumin g that there are no precondition s for puttin g on the hat and coat. Give a partial­orde r j
plan that is a solution , and show that there are 180 differen t linearization s of this solution .
11.2 Le t us conside r a versio n of the milk/banana/dril l shoppin g proble m in whic h mone y is j
included , at least in a simpl e way .
a. Let CC denot e a credi t card that the agen t can use to buy any object . Modif y the description !
of Buy so that the agen t has to have its credi t card in orde r to buy anything .
b. Writ e a PickUp  operato r that enable s the agen t to Have  an objec t if it is portabl e and at thej
same locatio n as the agent .
c. Assum e that the credi t card is at home , but Have(CC)  is initiall y false . Construc t a partiall y j
ordere d plan that achieve s the goal , showin g both orderin g constraint s and causa l links .
d. Explai n in detai l wha t happen s durin g the plannin g proces s whe n the agen t explore s a|
partia l plan in whic h it leave s hom e withou t the card .
11.3 Ther e are man y way s to characteriz e planners . For each of the followin g dichotomies , j
explai n wha t they mean , and how the choic e betwee n them affect s the efficienc y and completenes s j
of a planner .
a. Situatio n spac e vs. plan space .
b. Progressiv e vs. regressive .
Sectio n 11.9 . Summar y 365
c. Refinemen t vs. debugging .
d. Leas t commitmen t vs. more commitment .
e. Boun d variable s vs. unboun d variables .
f. Tota l orde r vs. partia l order .
g. Interleave d vs. noninterleaved .
h. Unambiguou s precondition s vs. ambiguou s preconditions .
i. Systemati c vs. unsystematic .
SUSSMA N ANOMAL Y 11. 4 Figur e 11.1 6 show s a blocks­worl d plannin g proble m know n as the Sussma n anomaly .
The proble m was considere d anomalou s becaus e the noninterleave d planner s of the early 1970 s
could not solv e it. Encod e the proble m usin g STRIP S operators , and use POP to solv e it.
B
Start Stat e Goal Stal e
Figur e 11.1 6 Th e "Sussma n anomaly " blocks­worl d plannin g problem .
11.5 Suppos e that you are the prou d owne r of a bran d new time machine . Tha t mean s that you
can perfor m action s that affec t situation s in the past . Wha t change s woul d you have to mak e to
the planner s in this chapte r to accommodat e such actions ?
11.6 Th e POP algorith m show n in the text is a regressio n planner , becaus e it add s step s
whos e effect s satisf y unsatisfie d condition s in the plan . Progressio n planner s add step s whos e
precondition s are satisfie d by condition s know n to be true in the plan . Modif y POP so that it
work s as a progressio n planner , and compar e its performanc e to the origina l on severa l problem s
of your choosing .
11.7 In this exercise , we will look at plannin g in Shakey' s world .
a. Describ e Shakey' s six action s in situatio n calculu s notation .
b. Translat e them into the STRIP S language .
c. Eithe r manuall y or usin g a partial­orde r planner , construc t a plan for Shake y to get Box2
into Room!  from the startin g configuratio n in Figur e 11.15 .
d. Suppos e Shake y has n boxe s in a room and need s to mov e them all into anothe r room .
Wha t is the complexit y of the plannin g proces s in term s of n?
366 Chapte r 11 . Plannin g
11.8 PO P is a nondeterministi c algorithm , and has a choic e abou t whic h operato r to add to
the plan at each step and how to resolv e each threat . Can you thin k of any domain­independen t
heuristic s for orderin g thes e choice s that are likel y to improv e POP' s efficiency ? Wil l they help
in Shakey' s world ? Are there any additional , domain­dependen t heuristic s that will improv e the
efficienc y still further ?
11.9 In this exercis e we will conside r the monkey­and­banana s problem , in whic h there is a
monke y in a room with some banana s hangin g out of reach from the ceiling , but a box is availabl e
that will enabl e the monke y to reac h the banana s if he climbs  on it. Initially , the monke y is at
A, the banana s at B, and the box at C. The monke y and box have heigh t Low,  but if the monke y
climb s onto the box he will have heigh t High,  the sam e as the bananas . The action s availabl e
to the monke y includ e Go from one place to another , Push  an objec t from one place to another ,
Climb  onto an object , and Grasp  an object . Graspin g result s in holdin g the objec t if the monke y
and objec t are in the same plac e at the same height .
a. Writ e dow n the initia l state descriptio n in predicat e calculus .
b. Writ e dow n STRIPS­styl e definition s of the four actions , providin g at leas t the obviou s
preconditions . :
c. Suppos e the monke y want s to fool the scientists , who are off to tea, by grabbin g the banana s
but leavin g the box in its origina l place . Writ e this as a genera l goal (i.e., not assumin g the '
box is necessaril y at C) in the languag e of situatio n calculus . Can this goal be solve d by a
STRIPS­styl e system ?
d. You r axio m for pushin g is probabl y incorrect , becaus e if the objec t is too heavy , its positio n \
will remai n the same whe n the Push  operato r is applied . Is this an exampl e of the fram e
proble m or the qualificatio n problem ?
12PRACTICA L PLANNIN G
In which  planning  algorithms meet  the real world  and survive,  albeit  with  some
significant  modifications.
12.1 PRACTICA L PLANNER S
Chapte r 11 showe d how a partial­orde r planner' s searc h throug h the spac e of plan s can be more
efficien t than a problem­solver' s searc h throug h the spac e of situations . On the othe r hand , the
POP planne r can only handl e problem s that are state d in the STRIP S language , and its searc h
proces s is so unguide d that it can still only be used for smal l problems . In this chapte r we
begin by surveyin g existin g planner s that operat e in complex , realisti c domains . Thi s will help
to pinpoin t the weaknesse s of POP and sugges t the necessar y extensions . We then show how
the plannin g languag e and algorithm s of Chapte r 11 can be extende d and the searc h focuse d to
handl e domain s like these .
Spacecraf t assembly , integration , and verificatio n
OPTIMUM­AI V is a planne r that is used by the Europea n Spac e Agenc y to help in the assembly ,
integration , and verificatio n (AIV ) of spacecraft . Th e syste m is used both to generat e plan s
and to monito r their execution . Durin g monitoring , the syste m remind s the user of upcomin g
activities , and can sugges t repair s to the plan whe n an activit y is performe d late, cancelled , or
reveal s somethin g unexpected . In fact, the abilit y to quickl y repla n is the principa l objectiv e of
OPTIMUM­AIV . The syste m does not execut e the plans ; that is done by human s with standar d
constructio n and test equipment .
In comple x project s like this, it is commo n to use schedulin g tools from operation s researc h
(OR) such as PER T chart s or the critica l path method . Thes e tool s essentiall y take a hand­
constructed  complet e partial­orde r plan and generat e an optima l schedul e for it. Action s are
367
368 Chapte r 12 . Practica l Plannin g
treate d as object s that take up time and have orderin g constraints ; thei r effect s are ignored .
This avoid s the need for knowledg e engineering , and for one­sho t problem s it may be the
most  appropriat e solution . For mos t practica l applications , however , ther e will be man y relate d
problem s to solve , so it is wort h the effor t to describ e the domai n and then have the plan s
automaticall y generated . This is especiall y importan t durin g the execution  of plans . If a step of
a plan fails , it is ofte n necessar y to repla n quickl y to get the projec t back on track . PER T chart s
do not contai n the causa l links and othe r informatio n neede d to see how to fix a plan , and huma n
replannin g is ofte n too slow .
The succes s of real­worl d AI system s require s integratio n into the environmen t in whic h
they operate . It is vital that a planne r be able to acces s existin g database s of projec t informatio n
in whateve r forma t they migh t have , and that the planner' s inpu t and outpu t representation s be in
a form that is both expressiv e and easil y understoo d by users . The STRIP S languag e is insufficien t
for the AIV domai n becaus e it canno t expres s four key concepts :
1. Hierarchica l plans : Obviously , launchin g a spacecraf t is more complicate d than shoppin g
for groceries . One way to handl e the increase d complexit y is to specif y plan s at varyin g
levels of detail . Th e top­leve l plan migh t be: prepar e booste r rocket , prepar e capsule ,
load cargo , and launch . Ther e migh t be a doze n intermediat e level s befor e we finall y get
down to the leve l of executabl e actions : inser t nut A into hole B and faste n with bolt C.
Addin g the abilit y to represen t hierarchica l plan s can mak e the differenc e betwee n feasibl e
and infeasibl e computation , and it can mak e the resultin g plan easie r to understand . It
also allow s the user to provid e guidanc e to the planne r in the form of a partiall y specified ,
abstrac t plan for whic h the planne r can fill in the details .
2. Comple x conditions : STRIP S operator s are essentiall y prepositional . True , they do allow
variables , but the variable s are used in a very limite d way. For example , there is no universa l
quantification , and withou t it we canno t describ e the fact that the Launch  operato r cause s
all the object s that are in the spacecraf t to go into orbit . Similarly , STRIP S operator s are
unconditional : we canno t expres s the fact that if all system s are go, then the Launch  will
put the spacecraf t into orbit , otherwis e it will put it into the ocean .
3. Time : Becaus e the STRIP S languag e is base d on situatio n calculu s it assume s that all action s
occur instantaneously , and that one actio n follow s anothe r with no break in between . Real ­
world project s need a bette r mode l of time . The y mus t represen t the fact that project s have
deadline s (the spacecraf t mus t be launche d on June 17), action s have duration s (it takes 6
hours to test the XYZ  assembly) , and steps of plan s may have time window s (the machin e
that tests the XYZ  assembl y is availabl e from May 1 to June 1 (excep t weekends) , but it
must be reserve d one wee k ahea d of time) . Indeed , the majo r contributio n of traditiona l
OR technique s is to satisf y time constraint s for a complet e partial­orde r plan .
4. Resources : A projec t normall y has a budge t that canno t be exceeded , so the plan mus t
be constraine d to spen d no mor e mone y than is available . Similarly , ther e are limit s on
the numbe r of worker s that are available , and on the numbe r of assembl y and test stations .
Resourc e limitation s may be place d on the numbe r of thing s that may be used at at one time
(e.g., people ) or on the total amoun t that may be used (e.g. , money) . Actio n description s
must incorporat e resourc e consumptio n and generation , and plannin g algorithm s mus t be
able to handl e constraint s on resource s efficiently .
Sectio n 12.1 . Practica l Planner s 369
OPTIMUM­AI V is base d on the open plannin g architectur e O­PLA N (Curri e and Tate , 1991) .
O­PLA N is simila r to the POP planne r of Chapte r 11, excep t that it is augmente d to accep t a more
expressiv e languag e that can represen t time , resources , and hierarchica l plans . It also accept s
heuristic s for guidin g the searc h and record s its reason s for each choice , whic h make s it easie r
to repla n whe n necessary . O­PLA N has been applie d to a variet y of problems , includin g softwar e
procuremen t plannin g at Price Waterhouse , back axle assembl y proces s plannin g at Jagua r Cars ,
and complet e factor y productio n plannin g at Hitachi .
Job shop schedulin g
The proble m that a factor y solve s is to take in raw material s and components , and assembl e them
into finishe d products . The proble m can be divide d into a plannin g task (decidin g what assembl y
steps are goin g to be performed ) and a schedulin g task (decidin g whe n and wher e each step will
be performed) . In man y moder n factories , the plannin g is done by hand and the schedulin g is
done with an automate d tool.
O­PLA N is bein g used by Hitach i for job shop plannin g and schedulin g in a syste m calle d
TOSCA . A typica l proble m involve s a produc t line of 350 differen t products , 35 assembl y
machines , and over 2000 differen t operations . The planne r come s up with a 30­da y schedul e for
three 8­hou r shift s a day. In general , TOSC A follow s the partial­order , least­commitmen t plannin g
approach . It also allows  for "low­commitment " decisions : choice s that impos e constraint s on
the plan or on a particula r step. For example , the syste m migh t choos e to schedul e an actio n to
be carrie d out on a class of machin e withou t specifyin g any particula r one.
Factorie s with less diversit y of product s often follo w a fixe d plan , but still have a need for
automate d scheduling . The Isis syste m (Fox and Smith , 1984 ) was develope d specificall y for
scheduling . It was first teste d at the Westinghous e turbin e componen t plan t in Winston­Salem ,
NC. The plan t make s thousand s of differen t turbin e blades , and for each one, there are one or
more plans , calle d proces s routings . Whe n an orde r come s in, one of the plan s is chose n and a
time for it is scheduled . The time depend s on the criticalit y of the order : whethe r it is an urgen t
replacemen t for a faile d blad e in service , a schedule d maintenanc e part that has plent y of lead
time but mus t arriv e on time , or just a stock orde r to buil d up the reserves .
Traditiona l schedulin g method s such as PER T are capabl e of findin g a feasibl e orderin g of
steps subjec t to time constraints , but it turn s out that huma n scheduler s usin g PER T spen d 80% to
90% of their time communicatin g with othe r worker s to discove r wha t the real constraint s are.
A successfu l automate d schedule r need s to be able to represen t and reaso n with these additiona l
constraints . Factor s that are importan t includ e the cost of raw material s on hand , the valu e
of finishe d but unshippe d goods , accurat e forecast s of futur e needs , and minima l disruptio n of
existin g procedures . ISIS uses a hierarchical , least­commitmen t searc h to find high­qualit y plan s
that satisf y all of these requirements .
Schedulin g for spac e mission s
Plannin g and schedulin g system s have been used extensivel y in plannin g spac e mission s as well
as in constructin g spacecraft . Ther e are two mai n reason s for this . First , spacecraf t are very
370 Chapte r 12 . Practica l Plannin g
expensiv e and sometime s contai n humans , and any mistak e can be costl y and irrevocable . Second ,
space mission s take plac e in space , whic h does not contai n man y othe r agent s to mes s up the
expecte d effect s of actions . Planner s have been used by the groun d team s for the Bubbl e spac e
telescop e and at least three spacecraft : Voyager , UOSAT­II , and ERS­1 . In each case , the goal is
to orchestrat e the observationa l equipment , signa l transmitters , and attitude ­ and velocity­contro l
mechanisms , in orde r to maximiz e the valu e of the informatio n gaine d from observation s whil e
obeyin g resourc e constraint s on time and energy .
Missio n schedulin g often involve s very comple x tempora l constraints , particularl y those
involvin g periodi c events . Fo r example , ERS­1 , the Europea n Eart h Resourc e Observatio n
satellite , complete s an orbit every 100 minutes , and return s to the same poin t every 72 hours . An
observatio n of a particula r poin t on the earth' s surfac e thus can be made at any one of a numbe r of
times , each separate d by 72 hours , but at no othe r time . Satellite s also have resourc e constraint s
on their powe r output : they canno t excee d a fixe d maximu m output , and they mus t be sure not
to discharg e too muc h powe r over a perio d of time . Othe r than that, a satellit e can be considere d
as a job­sho p schedulin g problem , wher e the telescope s and othe r instrument s are the machines ,
and the observation s are the products . PlanERS­ 1 is a planne r base d on O­PLA N that produce s
observatio n plan s for the ERS­1 .
The Hubbl e spac e telescop e (HST ) is a good exampl e of the need for automate d plannin g
tools . Afte r it was launche d in Apri l 1990 , the primar y mirro r was foun d to be out of focus .
Using Bayesia n technique s for imag e reconstructio n (see Chapte r 24), the groun d team was
able to compensat e for the defec t to a degree , enablin g the HST to delive r nove l and importan t
data on Pluto , a gravitationa l lens , a supernova , and othe r objects . In 1993 , shuttl e astronaut s
repaire d most of the problem s with the primar y mirror , openin g up the possibilit y of a new set of
observations . The groun d team is constantl y learnin g more abou t wha t the HST can and canno t
do, and it woul d be impossibl e to updat e the observatio n plan s to reflec t this ever­increasin g
knowledg e withou t automate d plannin g and schedulin g tools .
Any astronome r can submi t a proposa l to the HST observin g committee . Proposal s are
classifie d as high priorit y (whic h are almos t alway s execute d and take up abou t 70% of the avail ­
able observin g time) , low priorit y (whic h are schedule d as time allows) , or rejected . Proposal s
are receive d at the rate of abou t one per day, whic h mean s there are more proposal s than can be
executed . Eac h proposa l include s a machine­readabl e specificatio n of whic h instrumen t shoul d
be pointe d at whic h celestia l object , and wha t kind of exposur e shoul d be made . Som e observa ­
tions can be done at any time , wherea s other s are dependen t on factor s such as the alignmen t of
planet s and whethe r the HST is in the earth' s shadow . Ther e are some constraint s that are uniqu e
to this domain . For example , an astronome r may reques t periodi c observation s of a quasa r over a
perio d of month s or year s subjec t to the constrain t that each observatio n be taken unde r the same
shado w conditions .
The HST plannin g syste m is split into two parts . A long­ter m scheduler , calle d SPIKE ,
first schedule s observation s into one­wee k segments . Th e heuristi c is to assig n high­priorit y
proposal s so that they can all be execute d withi n the schedule d segment , and then to pack each
segmen t with extra low­priorit y proposal s unti l they are abou t 20% abov e capacity . This is done
a year or more ahea d of time . A multiyea r schedul e with 5000 observation s or so can be create d
in less than an hour , so replannin g is easy . Afte r each segmen t is scheduled , a short­ter m planner ,
SPSS , does the detaile d plannin g of each segment , fillin g in the time betwee n high­priorit y tasks
Sectio n 12.2 . Hierarchica l Decompositio n 37 1
with as man y low­priorit y ones as possible . The syste m also calculate s the command s for the
platfor m attitud e control s so that the observatio n plan can be executed . It can chec k the feasibilit y
of proposal s and detaile d schedule s muc h faste r than huma n experts .
Buildings , aircraf t carriers , and beer factorie s
SIPE (Syste m for Interactiv e Plannin g and Executio n monitoring ) was the first planne r to deal
with the proble m of replanning , and the first to take som e importan t step s towar d expressiv e
operators . It is simila r to O­PLA N in the rang e of its feature s and in its applicability . It is not
in everyda y practica l use, but it has been used in demonstratio n project s in severa l domains ,
includin g plannin g operation s on the fligh t deck of an aircraf t carrie r and job­sho p schedulin g
for an Australia n beer factory . Anothe r stud y used SlPE to plan the constructio n of multistor y
buildings , one of the mos t comple x domain s ever tackle d by a planner .
SIPE allow s deductiv e rule s to operat e over states , so that the user does not have to specif y
all relevan t literal s as effect s of each operator . It allow s for an inheritanc e hierarch y amon g object
classe s and for an expressiv e set of constraints . This mean s it is applicabl e to a wide variet y of
domains , but its generalit y come s at a cost. For example , in the buildin g constructio n domain , it
was foun d that SIPE neede d time 0(«2 5) for an n­stor y building . This suggest s a high degre e of
interactio n betwee n stories , whe n in fact the degre e of interactio n shoul d be muc h lower : if you
remembe r to build from the groun d up and mak e sure that the elevato r shaft s line up, it shoul d be
possibl e to get performanc e muc h close r to O(n).
The example s in this and the precedin g section s give an idea of the state of the art of
plannin g systems . The y are good enoug h to mode l comple x domains , but have not yet gaine d
practica l acceptanc e beyon d a few pilot projects . Clearly , to achiev e the degre e of flexibilit y and
efficienc y neede d to excee d the capabilitie s of huma n planner s arme d with traditiona l schedulin g
tools , we need to go far beyond  the limite d STRIP S language .
12.2 HIERARCHICA L DECOMPOSITIO N
The grocer y shoppin g exampl e of Chapte r 11 produce d solution s at a rathe r high level of abstrac ­
tion. A plan such as
[Go(Supemarket),  Buy(Milk),  Buy(Bananas),  Go(Home)]
is a good high­leve l descriptio n of wha t to do, but it is a long way from the type of instruction s
that can be fed directl y to the agent' s effectors . Thus , it is insufficien t for an agen t that want s to
actuall y do anything . On the othe r hand , a low­leve l plan such as
[Forward(lcm),  Tum(\  deg) , Forward(lcm),...]
woul d have to be man y thousand s of steps long to solve the shoppin g problem . The spac e of plans
of that lengt h is so hug e that the technique s of Chapte r 11 woul d probabl y not find a solutio n in
a reasonabl e amoun t of time .
372 Chapte r 12 . Practica l Plannin g
HIERARCHICA L
DECOMPOSITIO N
ABSTRAC T
OPERATO R
PRIMITIV E
OPERATO RTo resolv e this dilemma , all the practica l planner s we surveye d have adopte d the idea of
hierarchica l decomposition: ' tha t an abstrac t operato r can be decompose d into a grou p of
steps that form s a plan that implement s the operator . Thes e decomposition s can be store d in a
librar y of plan s and retrieve d as needed .
Conside r the proble m of buildin g a fram e house . The abstrac t operato r Build(Hou.se)  can be
decompose d into a plan that consist s of the four steps Obtain  Permit,  Hire  Builder,  Construction,
and Pay  Builder,  as depicte d in Figur e 12.1 . Th e step s of this plan may in turn be furthe r
decompose d into even mor e specifi c plans . We show a decompositio n of the Construction  step.
We coul d continu e decomposin g unti l we finall y get dow n to the leve l of Hammer(Nail).  The
plan is complet e whe n ever y step is a primitiv e operator —on e that can be directl y execute d by
the agent . Hierarchica l decompositio n is mos t usefu l whe n operator s can be decompose d in more
than one way . For example , the Build Walls  operato r can be decompose d into a plan involvin g
wood , bricks , concrete , or vinyl .
To mak e the idea of hierarchica l plannin g work , we have to do two things . (1) Provid e an
extensio n to the STRIP S languag e to allow for nonprimitiv e operators . (2) Modif y the plannin g
algorith m to allow the replacemen t of a nonprimitiv e operato r with its decomposition .
Extendin g the languag e
To incorporat e hierarchica l decomposition , we have to mak e two addition s to the descriptio n of
each proble m domain .
First, we partitio n the set of operator s into primitiv e and nonprimitiv e operators . In our
domain , we migh t defin e Hammer(Nail)  to be primitiv e and Build(House)  to be nonprimitive .
In general , the distinctio n betwee n primitiv e and nonprimitiv e is relativ e to the agen t that will
execut e the plan . For the genera l contractor , an operato r such as Install(FloorBoards)  woul d be
primitive , becaus e all the contracto r has to do is orde r a worke r to do the installation . For the
worker , Install(FloorBoards)  woul d be nonprimitive , and Hammer(Nail)  woul d be primitive .
Second , we add a set of decompositio n methods . Eac h metho d is an expressio n of the
form Decompose(o,  p), whic h mean s that a nonprimitive  operato r that unifie s with o can be
decompose d into a plan p. Her e is decompositio n of Construction  from Figur e 12.1 :
Decompose^  Construction,
Plan(STEPS:{Si  : Build(foundation),  S2 : Build(Frame),
S3 : Build(Roof\S 4 : Build(Walls\
S5 : Build(Interior)}
ORDERiNGS:{5 | ­< S2 ­< 53 ­< S5, 52 ^ 54 ^ S5},
BINDINGS:!} ,
LINKS:{5 , Fou^lim S2,S2 F^Le £3,5­ 2 ^™f S^ ^ S^ ^ 55}))
A decomposition  method  is like a subroutine  or macro  definition  for an operator.  As such , it is
importan t to mak e sure  that the decompositio n is a correc t implementatio n of the operator . We
1 Fo r hierarchica l decomposition , som e author s use the term operato r reductio n (reducin g a high­leve l operato r to
a set of lower­leve l ones) , and som e use operato r expansio n (expandin g a macro­lik e operato r into the structur e that
implement s it). This kind of plannin g is also calle d hierarchica l task networ k planning .
Sectio n 12.2 . Hierarchica l Decompositio n 373
decompose s to
Constructio nPay
Builde r
decompose s to
Build
Foundatio nBuild
Fram e
Figur e 12.1 Hierarchica l decompositio n of the operato r Build  House  into a partial­orde r plan .
say that a plan/ ? correctl y implement s an operato r o if it is a complet e and consisten t plan for the
proble m of achievin g the effect s of o give n the precondition s of o:
1. p mus t be consistent . (Ther e is no contradictio n in the orderin g or variabl e bindin g
constraint s of p.)
2. Ever y effec t of o mus t be asserte d by at leas t one step of p (and is not denie d by som e
other , late r step of p).
3. Ever y preconditio n of the step s in p mus t be achieve d by a step in p or be one of the
precondition s of o.
This guarantee s that it is possibl e to replac e a nonprimitiv e operato r with its decompositio n and
have everythin g hook up properly . Althoug h one will need to chec k for possibl e threat s arisin g
from interaction s betwee n the newl y introduce d step s and condition s and the existin g step s and
conditions , there is no need to worr y abou t interaction s amon g the step s of the decompositio n
itself . Provide d there is not too muc h interactio n between  the differen t parts of the plan , hierar ­
chica l plannin g allow s very comple x plan s to be built up cumulativel y from simple r subplans . It
also allow s plan s to be generated , save d away , and then re­use d in later plannin g problems .
374 Chapte r 12 . Practica l Plannin g
Modifyin g the planne r
We can deriv e a hierarchica l decompositio n planner , whic h we call HD­POP , from the POP
planne r of Figur e 11.13 . The new algorith m in show n in Figur e 12.2 . Ther e are two principa l
changes . First , as well as findin g ways to achiev e unachieve d condition s in the plan , the algorith m
must find a way to decompos e noriprimitiv e operators . Becaus e both kind s of refinement s mus t
be carrie d out to generat e a complete , primitiv e plan , there is no need to introduc e a backtrackin g
choic e of one or the other . HD­PO P simpl y does one of each on each iteration ; more sophisticate d
strategie s can be used to reduc e the branchin g factor . Second , the algorith m takes a plan  as input ,
rathe r than just a goal . We have alread y seen that a goal can be represente d as a Start­Finish
plan, so this is compatibl e with the POP approach . However , it will ofte n be the case that the
user has som e idea of wha t genera l kind s of activitie s are needed , and allowin g more extensiv e
plans as input s mean s that this kind of guidanc e can be provided .
functio n HD­POP(p/a« , operators, methods)  return s plan
inputs : plan,  an abstrac t plan with start and goal step s (and possibl y othe r steps )
loop do
if SOLUTiON?(p/a;i ) then retur n plan
Snmi, C «­ SELECT­SUB­GOAL(p/an )
CHOOSE­OPERATOR(p/a« , operators,  *$„„,; , c)
Snanpri,,,  <— SELECT­NONPRIMITIVE(p/fln )
CHOOSE­DECOMPOSITION(P/OT , methods,  S,m,prim)
RESOLVE­THREATS ( plan)
end
Figur e 12.2 A  hierarchica l decompositio n partial­orde r plannin g algorithm , HD­POP . On
each iteratio n of the loop we first achiev e an unachieve d conditio n (CHOOSE­OPERATOR) , then
decompos e a nonprimitiv e operato r (CHOOSE­DECOMPOS1TION) , then resolv e threats .
To mak e HD­PO P work , we have to chang e SOLUTION ? to chec k that ever y step of the
plan is primitive . The othe r function s from Figur e 11. J 3 remai n unchanged . Ther e are two new
procedures : SELECT­NONPRIMITIV E arbitraril y select s a nonprimitiv e step from the plan . The
functio n CHOOSE­DECOMPOSITIO N pick s a decompositio n metho d for the plan and applie s it. If \
method  is chose n as the decompositio n for the step 5nonpr/m, then the field s of the plan  are altere d
as follows :
• STEPS : Add all the steps of method  to the plan , but remov e Snonpnm.
• BINDINGS : Add all the variabl e bindin g constraint s of method  to the plan . Fai l if this
introduce s a contradiction .
• ORDERINGS : Followin g the principl e of leas t commitment , we replac e each orderin g
constrain t of the form Sa ­< Snonpri,n with constraint(s ) that orde r Sa befor e the latest  step(s )
of method.  Tha t is, if Sm is a step of method,  and ther e is no othe r 5} in method  such that
Sm ­< Sj, then add the constrain t Sa ­< Sm. Similarly , replac e each constrain t of the form {
Sectio n 12.3 . Analysi s of Hierarchica l Decompositio n 375
Snonprim  ­< Sz with constraint(s ) that orde r Sz afte r the earliest  step(s ) of method.  We then
call RESOLVE­THREAT S to add any additiona l orderin g constraint s that may be needed .
• LINKS : It is easie r to matc h up causa l link s with the righ t substep s of method  than it is to
matc h up orderin g constraints . If S, '' , Snonprim was a causa l link in plan,  replac e it by a
set of links S/ c : Sm, wher e each Sm is a step of method  that has c as a precondition , and
there is no earlie r step of method  that has c as a precondition . (If there are severa l such
steps with c as a precondition , then put in a causa l link for each one. If there are none ,
then the causa l link from 5, can be dropped , becaus e c was an unnecessar y preconditio n
of Snonprim .) Similarly , for each link Snonprim c 5} in plan,  replac e it with a set of links
Sm c , Sj, wher e Sm is a step of method  that has c as an effec t and there is no later step of
method  with c as an effect .
In Figur e 12.3 , we show a mor e detaile d diagra m of the decompositio n of a plan step , in the
contex t of a large r plan . Notic e that one of the causa l link s that lead s into the nonprimitiv e step
Build  House  ends up bein g attache d to the first step of the decomposition , but the othe r causa l
link is attache d to a later step.
Buy Land
Get LoanHave  MoneyBuild
Hous eHave(House) Move
In
decompose s to
h Land
———  ^­Obtai n
Permi t
Hire
Builde rNi^
HaConstructio n
ve Money //Pay
Builde rHave(House) Move
In
Figur e 12.3 Detaile d decompositio n of a plan step .
ANALYSI S OF HIERARCHICA L DECOMPOSITIO N
Hierarchica l decompositio n seems  like a good idea , on the sam e ground s that subroutine s or
macro s are a good idea in programming : they allow the programme r (or knowledg e engineer ) to
specif y the proble m in piece s of a reasonabl e size. The piece s can be combine d hierarchicall y to
create large plans , withou t incurrin g the enormou s combinatoria l cost of constructin g large plan s
from primitiv e operators . In this section , we will mak e this intuitiv e idea more precise .
376 Chapte r 12 . Practica l Plannin g
ABSTRAC T
SOLUTIO N
DOWNWAR D
SOLUTIO N
UPWAR D SOLUTIO NSuppos e that there is a way to strin g four abstrac t operator s togethe r to buil d a house—fo r
example , the four operator s show n in Figur e 12.1 . We will call this an abstrac t solution —a plan
that contain s abstrac t operators , but is consisten t and complete . Findin g a smal l abstrac t solution ,
if one exists , shoul d not be too expensive . Continuin g this process , we shoul d be able to obtai n
a primitiv e plan withou t too muc h backtracking .
This assumes , however , that in findin g an abstrac t solution , and in rejectin g othe r abstrac t
plans as inconsistent , one is doin g usefu l work . One woul d like the followin g propertie s to hold :
• If p is an abstrac t solution , then there is a primitiv e solutio n of whic h p is an abstraction .
If this propert y holds , then once an abstrac t solutio n is foun d we can prun e awa y all othe r
abstrac t plan s from the searc h tree. This propert y is the downwar d solutio n property .
• If an abstrac t plan is inconsistent , then ther e is no primitiv e solutio n of whic h it is an
abstraction . If this propert y holds , then we can prun e awa y all the descendant s of any
inconsisten t abstrac t plan . Thi s is calle d the upwar d solutio n propert y becaus e it also
mean s that all complet e abstraction s of primitiv e solution s are abstrac t solutions .
Figur e 12.4 illustrate s these two notion s graphically . Each box represent s an entir e plan (not just
a step) , and each arc represent s a decompositio n step in whic h an abstrac t operato r is expanded .
At the top is a very abstrac t plan , and at the botto m are plan s with only primitiv e steps . The
boxe s with bold outline s are (possibl y abstract ) solutions , and the ones with dotte d outline s are
inconsistent . Plan s marke d with an "X" need not be examine d by the plannin g algorithm . (The
figur e show s only complet e or inconsisten t plans , leavin g out consisten t but incomplet e plan s and
the achievemen t step s that are applie d to them. )
To get a mor e quantitativ e feel for how thes e propertie s affec t the search , we need a
simplifie d mode l of the searc h space . Assum e that ther e is at leas t one solutio n with n primitiv e
steps , and that the time to resolv e threat s and handl e constraint s is negligible : all we will be
concerne d with is the time it take s to choos e the righ t set of steps . Figur e 12.5 define s the searc h
space in term s of the parameter s b, s, and d.
12U I.
(a) Downwar d Solutio n Propert yi'x : i'x ! fx : fx'i f "j \\ fx] LX.:
(b) Upwar d Solutio n Propert y
Figur e 12.4 Th e upwar d and downwar d solutio n propertie s in plan spac e (abstrac t plan s on top,
primitiv e plan s on the bottom) . Bold outline d boxe s are solutions , dotte d outline s are inconsistent ,
and boxe s marke d with an "X" can be prune d awa y in a left­to­righ t search .
Sectio n 12.3 . Analysi s of Hierarchica l Decompositio n 377
b=3 branching  factor:  number  of decomposition  methods  per step
s=4 steps  in a decomposition method
d=3 depth  of the hierarchical  plan
Depth
d=0
d=3
Figur e 12.5 A  portio n of the searc h spac e for hierarchica l decompositio n with dept h d ­ 3;
branchin g facto r b = 3 decomposition s per step;. ? = 4 steps per decompositio n method . A solutio n
will have n = sd steps (n ­ 64 in this case) . We also assum e that l/b decomposition s will lead to
a solution .
A nonhierarchica l planne r woul d have to generat e an «­ste p plan , choosin g amon g b possi ­
bilitie s for each one. (We are also assumin g that the numbe r of decomposition s per nonprimitiv e
step, b, is the sam e as the numbe r of applicabl e new operator s for an open preconditio n of a
primitiv e step. ) Thus , it take s time O(b")  in the wors t case . With  a hierarchical  planner,  we
can adopt  the strategy of  only searching  decompositions  that lead to  abstract  solutions.  (In our
simplifie d model , exactl y 1 of every b decomposition s is a solution . In mor e realisti c models ,
we need to conside r what to do when there are zero or more than one solution. ) The planne r has
to look at sb steps at dept h d = \. At dept h d = 2, it look s at anothe r sb step s for each step it
decomposes , but it only has to decompos e lib of them , for a total of bs1. Thus , the total numbe r
of plan s considere d is
d^ bs1 = O(bsd)
To give you an idea of the difference , for the paramete r value s in Figur e 12.5 , a nonhierarchica l
planne r has to inspec t 3 x 1030 plans , wherea s a hierarchica l planne r look s at only 576.
The upwar d and downwar d solutio n propertie s seem to be enormousl y powerful . At first
glance , it may seem that they are necessar y consequence s of the correctnes s condition s for decom ­
position s (pag e 373) . In fact, neithe r propert y is guarantee d to hold . Withou t these properties , or
some reasonabl e substitute , a hierarchica l planne r does no bette r than a nonhierarchica l planne r
in the wors t case (althoug h it may do bette r in the averag e case) .
378 Chapte r 12 . Practica l Plannin g
UNIQU E MAIN
SUBACTIO NFigur e 12.6 show s an exampl e wher e the upwar d solutio n propert y does not hold . That is,
the abstrac t solutio n is inconsistent , but ther e is a decompositio n that solve s the problem . The
proble m is take n from the O. Henr y story The Gift of the Magi.  A poor coupl e has only two prize d
possessions ; he a gold watc h and she her beautifu l long hair. The y each plan to buy present s to
make the othe r happy . He decide s to trade his watc h to buy a fanc y com b for her hair, and she
decide s to sell her hair to get a gold chai n for his watch . As Figur e 12.6(b ) shows , the resultin g
abstrac t plan is inconsistent . However , it is still possibl e to decompos e this inconsisten t plan
into a consisten t solution , if the righ t decompositio n method s are available . In Figur e 12.6(c )
we decompos e the "Giv e Comb " step with an "installmen t plan " method . In the first step of the
decomposition , the husban d takes possessio n of the comb , and gives it to his wife , whil e agreein g
to delive r the watc h in paymen t at a later date . In the secon d step , the watc h is hande d over and
the obligatio n is fulfilled . A simila r metho d decompose s the "Giv e Chain " step. As long as both
givin g steps are ordere d befor e the deliver y steps , this decompositio n solve s the problem .
One way to guarante e the upwar d solutio n propert y is to make sure that each decompositio n
metho d satisfie s the uniqu e main subactio n condition : that there is one step of the decompose d
plan to whic h all precondition s and effects  of the abstrac t operato r are attached . In the Mag i
example , the uniqu e mai n subactio n conditio n does not hold . It does not hold in Figur e 12.3
either , althoug h it woul d be if Own  Land  were a preconditio n of the Pay Builde r step. Sometime s
Watch  Gj ve
Hairjr Com b
WatchStart ——— —
Hair
(a) Initi a
Watch
Hay
Start /
Ha/r *
WatchHappy(He)
Happy(She)
\ Proble m
Give Com b
(On Credit )
Give Chai n
(On Credit )inish
Comb
Owe(
Happ j
~^~^ _
_r­­— "
Chain
Owe(
Happ jSti
Natch)
'(She)
><
Hair)  —— ——
/(He)art
~^^— —
Watch­ Gi W
Hair Chai n
(b)
Watch
^^^^­^
Hair­Watch
Comb
Happy(She)
^"^ Fii
^*^
Happy(He)
Chain
­Hairiish
Abstrac t Inconsisten t Plan
Delive r
Watc h _
Delive r '
Hair ­­Watch
­Owe(Watch)
^^^ £ Fini
­THair
­Owe(Hair)sh
(c) Decompositio n of (b) into a Consisten t Solutio n
Figur e 12.6 (a ) The Gift of the Magi  problem , (b) The partia l plan is inconsistent , becaus e
there is no way to orde r the two abstrac t steps withou t a conflict , (c) A decompositio n that solve s
the problem . Thi s violate s the upwar d solutio n property , becaus e the inconsisten t plan in (b) now
has a solution .
Sectio n 12.3 . Analysi s of Hierarchica l Decompositio n 379
it is worthwhil e to do som e preprocessin g of the decompositio n method s to put them in a form
that satisfie s the uniqu e main subactio n conditio n so that we can freel y cut off searc h whe n we
hit an inconsisten t abstrac t plan withou t fear of missin g any solutions .
It is importan t to remembe r that even whe n the upwar d solutio n propert y fails to hold , it is
still a reasonabl e heuristi c to prefe r application s of decompositio n to consisten t plan s rathe r than
inconsisten t ones . Similarly , even whe n the downwar d solutio n propert y is violated , it make s
more sens e to pursu e the refinemen t of abstrac t solution s than that of inconsisten t plans , even
thoug h the abstrac t solutio n may not lead to a real solution . (Exercis e 12.4 asks you to find an
exampl e of the violatio n of the downwar d solutio n property. )
CRITIC SDecompositio n and sharin g
In CHOOSE­DECOMPOSITION , we just merg e each step of the decompositio n into the existin g plan .
This is appropriat e for a divide­and­conque r approach—w e solve each subproble m separately ,
and then combin e it into the rest of the solution . But often the only solutio n to a proble m involve s
combinin g the two solution s by sharing  step s rathe r than by joinin g distinc t sets of steps . For
example , conside r the proble m "enjo y a honeymoo n and raise a baby. " A planne r migh t choos e
the decompositio n "get marrie d and go on honeymoon " for the first subproble m and "get marrie d
and have a baby " for the second , but the planne r coul d get into a lot of troubl e if the two "get
married " step s are different . Indeed , if a preconditio n to "get married " is "not married, " and
divorc e is not an option , then there is no way that a planne r can merg e the two subplan s withou t
sharin g steps . Henc e a sharin g mechanis m is require d for a hierarchica l planne r to be complete .
Sharin g can be implemente d by addin g a choic e poin t in CHOOSE­DECOMPOSITIO N for
every operato r in a decomposition : eithe r a new step is create d to instantiat e the operato r or an
existin g step is used . This is exactl y analogou s to the existin g choic e poin t in CHOOSE­OPERATOR .
Althoug h this introduce s a lot of additiona l choic e points , man y of them will have only a singl e
alternativ e if no operator s are availabl e for sharing . Furthermore , it is a reasonabl e heuristi c to
prefe r sharin g to non­sharing .
Many hierarchica l planner s use a differen t mechanis m to handl e this problem : they merg e
decomposition s withou t sharin g but allo w critic s to modif y the resultin g plan . A criti c is a
functio n that take s a plan as inpu t and return s a modifie d plan with som e conflic t or othe r
anomal y corrected . Theoretically , usin g critic s is no more or less powerfu l than puttin g in all the
choic e points , but it can be easie r to manag e the searc h spac e with a well­chose n set of critics .
Note that the choic e of sharin g versu s mergin g step s has an effec t on the efficienc y of
planning , as well as on completeness . An interestin g exampl e of the costs and benefit s of sharin g
occur s in optimizin g compilers . Conside r the proble m of compilin g sin(;t)+cos(X ) for a sequentia l
computer . Mos t compiler s accomplis h this by mergin g two separat e subroutin e calls in a trivia l
way: all the steps of sin com e befor e any of the step s of cos (or vice versa) . If we allowe d sharin g
instea d of merging , we coul d actuall y get a more efficien t solution , becaus e the two computation s
have man y step s in common . Mos t compiler s do not do this becaus e it woul d take too muc h
time to conside r all the possibl e share d plans . Instead , mos t compiler s take the criti c approach :
a peephol e optimize r is just a kind of critic .
380 Chapte r 12 . Practica l Plannin g
APPROXIMATIO N
HIERARCH Y
CRITICALIT Y LEVE LDecompositio n versu s approximatio n
The literatur e on plannin g is confusin g becaus e author s have not agree d on thei r terminology .
Ther e are two completel y separat e idea s that have gone unde r the nam e hierarchica l planning .
Hierarchica l decomposition , as we have seen , is the idea that an abstract , nonprimitiv e operato r
can be decompose d into a mor e comple x networ k of steps . Th e abstractio n here is on the
granularit y with whic h operator s interac t with the world . Second , abstractio n hierarchie s
captur e the idea that a singl e operato r can be planne d with at differen t level s of abstraction . At
the primitiv e level , the operato r has a full set of precondition s and effects ; at highe r levels , the
planne r ignore s som e of these details .
To avoi d confusion , we will use the term approximatio n hierarch y for this kind of
abstraction , becaus e the "abstract " versio n of an operato r is slightl y incorrect , rathe r than merel y
abstract . An approximatio n hierarch y planne r take s an operato r and partition s its precondition s
accordin g to their criticalit y level , for example :
Op(AcnON:Buy(x),
EFFECT: Have(x ) A ­>Have(Money),
PRECOND : 1: Sells(store,  x) A
2:At(store)  A
3: Have(Money))
Condition s labelle d with lowe r number s are considere d mor e critical . In the case of buyin g
something , ther e is not muc h one can do if the stor e does not sell it, so it is vita l to choos e
the righ t store . An approximatio n hierarch y planne r first solve s the proble m usin g only the
precondition s of criticalit y 1. The solutio n woul d be a plan that buy s the righ t thing s at the right
stores , but does not worr y abou t how to trave l betwee n store s or how to pay for the goods . The
idea is that it will be easy to find an abstrac t solutio n like this, becaus e mos t of the pesk y detail s
are ignored . Onc e a solutio n is found , it can be expande d by considerin g the precondition s at
criticalit y leve l 2. The n we get a  plan that take s trave l into consideration , but still does not worr y
abou t paying . We keep expandin g the solutio n in this way unti l all the precondition s are satisfied .
In the framewor k we have presented , we do not reall y need to chang e the languag e or
the planne r to suppor t approximatio n hierarch y planning . All we have to do is provid e the
right decompositio n method s and abstrac t operators . First , we defin e Buy\,  whic h has just the
preconditio n at criticalit y leve l 1, and has a singl e decompositio n metho d to Buy'2,  whic h has
precondition s at criticalit y 1 and 2, and so on. Clearly , any domai n generate d this way has
the uniqu e mai n subactio n property—th e decompositio n has only one step , and it has all the
precondition s and effect s of the abstrac t operator . Therefore , the upwar d solutio n propert y holds .
If we add a heuristi c that cause s the plannin g algorith m to backtrac k to choic e point s involv ­
ing low­numbe r precondition s first , then we get an approximatio n hierarch y planne r usin g our
standar d hierarchica l decompositio n planner . Thus , the criticalit y level s used in approximatio n
hierarch y plannin g can be seen as providin g contro l informatio n to guid e the plannin g search .
In this sense , criticalit y level s are a rathe r crud e tool becaus e they do not allow the criticalit y
to depen d on context . (Fo r example , mone y migh t be a critica l preconditio n if one is buyin g a •
house. ) One promisin g approac h is to replac e criticalit y level s with description s of how likel y
the operato r is to succee d in variou s combination s of circumstances .
Sectio n 12.4 . Mor e Expressiv e Operato r Description s 381
12A MOR E EXPRESSIV E OPERATO R DESCRIPTION S
Hierarchica l plannin g addresse s the proble m of efficiency , but we still need to mak e our repre ­
sentatio n languag e mor e expressiv e in orde r to broade n its applicability . The extension s includ e
allowin g for the effect s of an actio n to depen d on the circumstance s in whic h it is executed ; allow ­
ing disjunctiv e and negate d preconditions ; and allowin g universall y quantifie d precondition s and
effects . We conclud e the sectio n with a plannin g algorithm , POP­DUN C (Partial­Orde r Planne r
with Disjunction , Universa l quantification , Negatio n and Conditiona l effects) .
CONDITIONA L
EFFECT SConditiona l effect s
Operator s with conditiona l effect s have differen t effect s dependin g on wha t the worl d is like
when they are executed . We will retur n to the block s worl d of Sectio n 11.8 for som e of our
examples . Conceptually , the simpl e block s worl d has only one real action—movin g a block from
one plac e to another—bu t we were force d to introduc e two operator s in orde r to maintai n the
Clear  predicat e properly :
<9p(ACTlON:A/0ve(& , x, v),
PRECOND:<9«(6,,v ) A Clear(b)  A Clear(y),
EFFECT : On(b,y)  A Clear(x)  A ­~On(b,x)  A ­>Clear(y))
Op(ACTlON:MoveToTable(b,  x),
PRECOND:On(b,x)  A Clear(b),
EFFECT : On(b,  Table)  A Clear(x)  A ­>On(b,x))
Suppos e that the initia l situatio n include s On(A,B)  and we have the goal Clear(B).  We can
achiev e the goal by movin g A off B, but unfortunatel y we are force d to choos e whethe r we wan t
to mov e A to the table or to somewher e else. This introduce s a prematur e commitmen t and can
lead to inefficiency .
We can eliminat e the prematur e commitmen t by extendin g the operato r languag e to includ e
conditiona l effects . In this case , we can defin e a singl e operato r Move(b,  x, y) with a conditiona l
effec t that says , "if y is not the tabl e then an effec t is ­>Clear(y)r  We will use the synta x "effect
when condition"  to denot e this , wher e effect  and condition  are both literal s or conjunction s of
literals . We plac e this synta x in the EFFEC T slot of an operator , but it is reall y a combinatio n of a
preconditio n and an effect : the effect  part refer s to the situatio n that is the resul t of the operator ,
and the condition  part refer s to the situatio n befor e the operato r is applied . Thus , "Q whe n P"
is not the sam e as the logica l statemen t P => Q', rathe r it is equivalen t to the situatio n calculu s
statemen t P(s)  =£• Q(Result(act,  s)). The conditiona l Move  operato r is writte n as follows :
Op(AcriON:Move(b,  x, y),
PRECOND:On(b,x)  A Clear(b)  A Clear(y),
EFFECT: On(b,y ) A Clear(x)  A ­<On(b,x)
A —>Clear(y)  wheny^7fl£>/e )
382 Chapte r 12 . Practica l Plannin g
CONFRONTATIO NNow we have to incorporat e this new synta x into the planner . Two change s are required . First ,
in SELECT­SUB­GOAL , we have to decid e if a preconditio n c in a conditiona l effec t of the form
e whe n c shoul d be considere d as a candidat e for selection . The answe r is that if the effec t e
supplie s a conditio n that is protecte d by a causa l link , then we shoul d conside r selectin g c, but
not unti l the causa l link is there . This  is becaus e the causa l link mean s that the plan will not
work unles s c is also true . Considerin g the operato r just shown , the planne r woul d usuall y have
no need to establis h y^Table  becaus e ­*Clear(y)  is not usuall y neede d as a preconditio n of some
other step in the plan . Thus , the planne r can usuall y use the table if it need s somewher e to put a
block that does not need to be anywher e special .
Second , we have anothe r possibilit y for RESOLVE­THREAT . An y step that has the effec t
(­ic/ whe n p) is a possibl e threa t to the causa l link S, ­fL, 5, wheneve r c and c' unify . We can
resolv e the threa t by makin g sure that p does not hold . We call this techniqu e confrontation . In
the block s world , if we need a give n bloc k to be clea r in orde r to carry out som e step , then it is
possibl e for the Move(b,  x, y) operato r to threate n this conditio n if y is uninstantiated . However ,
the threa t only occur s ify^Table;  confrontatio n remove s the threa t by settin g y = Table.  A versio n
of RESOLVE­THREAT S that incorporate s confrontatio n appear s in Figur e 12.7 .
procedur e RESOLVE­THREATS(pfon )
for each S, _^ S, in LiNKS(p/a« ) do
for each S,h mlt in SJEfS(plan)  do
for each c' in EFFECT(S, ;,,ra,) do
if SUBST(BlNDINGS(/J/an),c ) = SUBST(BlNDINGS(/7/an),­ i c') then
choos e eithe r
Promotion:  Add S,i, reat ~< 5, to ORDERINGS(/;/an )
Demotion:  Add 5, ­< S,i, ml, to ORDERINGS(p/an )
Confrontation:  if c' is reall y of the form (c' when  p) then
CHOOSE­OPERATOR(p/a/i , operators,  S,h m,t, ~*p)
RESOLVE­THREATS ( plan)
if not CONSiSTENT(p/a« ) then fail
end
end
end
Figur e 12.7 A  versio n of RESOLVE­THREAT S with the confrontatio n techniqu e for resolvin g
conditiona l effects .
Negate d and disjunctiv e goal s
The confrontatio n techniqu e calls CHOOSE­OPERATO R with the goal ­>p. This is somethin g new:
so far we have insiste d that all goal s (preconditions ) be positiv e literals . Dealin g with negate d
literal s as goal s does not add muc h complexity : we still just have to chec k for effect s that matc h .,
Sectio n 12.4 . Mor e Expressiv e Operato r Description s 383
DISJUNCTIV E
EFFECT Sthe goal . We do have to mak e sure that our unificatio n functio n allows/ ? to matc h ­^­<p.  We also
have to treat the initia l state specially : we do not wan t to specif y all the condition s that are false
in the initia l state , so we say that a goal of the form ­ip can be matche d eithe r by an explici t effec t
that unifie s with ­>p or by the initia l state , if it does not contai n p.
Whil e we are at it, it is easy to add disjunctiv e preconditions . In SELECT­SUB­GOAL , if
we choos e a step with a preconditio n of the form/ ? V q, then we nondeterministicall y choos e to
retur n eithe r p or q and reserv e the othe r one as a backtrac k point . Of course , any operato r with
the preconditio n p V q could be replace d with two operators , one with p as a preconditio n and one
with q, but then the planne r woul d have to commi t to one or the other . Keepin g the condition s in
a singl e operato r allow s us to dela y makin g the commitment .
Wherea s disjunctiv e precondition s are easy to handle , disjunctiv e effect s are very diffi ­
cult to incorporate . The y chang e the environmen t from deterministi c to nondeterministic . A
disjunctiv e effec t is used to mode l rando m effects , or effect s that are not determine d by the
precondition s of the operator . For example , the operato r Flip(coin)  woul d have the disjunctiv e
effec t Heads(coin)  V Tails(coin).  In som e cases , a singl e plan can guarante e goal achievemen t
even in the face of disjunctiv e effects . For example , an operato r such as TurnHeadsSideUp(coin)
will coerc e the worl d into a know n state even after a flip. In mos t cases , however , the agen t need s
to develop a  differen t plan to handl e each possibl e outcome . We develo p algorithm s for this kind
of plannin g in Chapte r 13.
UNIVERSALL Y
QUANTIFIE D
PRECONDITION S
UNIVERSALL Y
QUANTIFIE D
EFFECT SUniversa l quantificatio n
In definin g the block s world , we had to introduc e the conditio n Clear(b).  In this section , we
exten d the languag e to allow universall y quantifie d preconditions . Instea d of writin g Clear(b)
as a precondition , we can use \/x Block(x)  => ­>On(x,b)  instead . Not only does this reliev e
us of the burde n of makin g each operato r maintai n the Clear  predicate , but it also allow s us to
handl e mor e comple x domains , such as a block s worl d with differen t size blocks .
We also allo w for universall y quantifie d effects . For example , in the shoppin g domai n
we coul d defin e the operato r Carry(bag,x,y)  so that it has the effec t that all object s that are in
the bag are at y and are no longe r at x. Ther e is no way to defin e this operato r withou t universa l
quantification . Here is the synta x we will use:
Op( AcnON:Carry(bag,  x, y),
PRECOND:Bag(bag)  A At(bag,x),
EFFECT.At(bag,  y), ­iAt(bag,  x) A
V i Item(i)  => (Aid,  y) A ­*At(i,  x)) whe n /«(/, bag))
Althoug h this looks  like full first­orde r logi c with quantifier s and implications , it is not. The
syntax—an d the correspondin g semantics—i s strictl y limited . We will only allow world s with a
finite , static , type d univers e of objects , so that the universall y quantifie d conditio n can be satisfie d
by enumeration . The descriptio n of the initia l state mus t mentio n all the object s and give each
one a type , specifie d as a unar y predicate . For example , Bag(B)  A Item(I\ ) A Item(Ii)  A Item(B).
Notic e that is possibl e for an objec t to have more than one type : B is both a bag and an item . The
static univers e requiremen t mean s that the object s mentione d in the initia l state canno t chang e
type or be destroyed , and no new object s can be created . Tha t is, no operato r excep t Start  can
384 Chapte r 12 . Practica l Plannin g
have Bag(x)  or ­^Bag(x)  as an effect . Wit h this semantic s of object s in mind , we can exten d the
synta x of precondition s and effect s by allowin g the form
VJE T(x)  => C(X >
wher e T is a unar y type predicat e on x, and C is a conditio n involvin g x. The finite , static , type d
univers e mean s that we can alway s expan d this form into an equivalen t conjunctiv e expressio n
with no quantifiers :
MX T(x)  =>• C(jc ) = C(jc,)A...AC(x n)
wher e x\,...  ,xn are the object s in the initia l state that satisf y T(x).  Here is an example :
Initia l State : Bag(B)  A Milk(M l) A Milk(M 2) A Milk(M 3)
Expression : Vx  Milk(x)  => In(x,B)
Expansion : /«(M, , B) A In(M 2, B) A /n(M 3, B)
In our planner , we will expan d universall y quantifie d goal s to eliminat e the quantifier . This can
be inefficien t becaus e it can lead to large conjunction s that need to be satisfied , but there is no
genera l solutio n that does any better .
For universall y quantifie d effects , we are bette r off. We do not need to expan d out the effect
becaus e we may not care abou t man y of the resultin g conjuncts . Instea d we leave the universall y
quantifie d effec t as it is, but mak e sure that RESOLVE­THREAT S can notic e that a universall y
quantifie d effec t is a threa t and that CHOOSE­OPERATO R can notic e whe n a universall y quantifie d
effec t can be used as a causa l link .
Some domain s are dynami c in that object s are create d or destroyed , or chang e their type
over time . Plan s in whic h object s are first made , then used , seem quit e natural . Our insistenc e
on a stati c set of object s migh t seem to mak e it impossibl e to handl e such domains . In fact,
we can ofte n finess e the proble m by specifyin g broa d stati c type s in the universall y quantifie d
expression s and usin g additiona l dynami c unar y predicate s to mak e fine r discriminations . In
particular , we can use the dynami c predicat e to distinguis h betwee n potentia l and actua l objects .
We begi n with a suppl y of non­Actual  objects , and objec t creatio n is handle d by operator s that
make object s Actual.  Suppos e that in the house­buildin g domai n there are two possibl e sites for
the house . In the initia l state we coul d includ e Home(H\ ) A House(H 2), wher e we take House  to
mean that its argumen t is a possibl e house : somethin g that migh t exis t in som e point s of some
plans , but not in others . In the initia l state neithe r Actual(H\ ) nor Actual(H 2) holds , but that
can change : the Build(x)  operato r has Actual(x)  as an effect , and the Demolish(x)  operato r has
­iActual(x)  as an effect .
If there were an infinit e numbe r of possibl e house s (or just a million) , then this approac h
woul d not work . But object s that can potentiall y exis t in large , undifferentiate d quantitie s can
often be treated  as resources , whic h are covere d in the next section .
A planne r for expressiv e operato r description s
We now combin e all the extension s to creat e our POP­DUN C algorithm . Becaus e the top level
of POP­DUN C is identica l to that of POP (Figur e 11.13) , we will not repea t it here . Figur e 12.8
show s the part s of POP­DUN C that diffe r from the original . SELECT­SUB­GOA L is modifie d to
expan d out universall y quantifie d precondition s and to choos e one of two possibl e way s to satisfy
Sectio n 12.4 . Mor e Expressiv e Operato r Description s 385
functio n SELECT­SUB­GOAL(/?fa;i ) returns  plan,  precondition  conjunct
pick a plan step Sneed from STEPS(plan)  with a preconditio n conjunc t c that has not been
achieve d
if c is a universall y quantifie d expressio n then
retur n Sneed, EXPANSION(C )
else if c is a disjunctio n c\ V C2 then
retur n Sneed, choose(ci , ci)
else retur n Smea, c
procedur e CHOOSE­OPERATOR(p/a« , operators,  Sneed, c)
choos e a step Salid from operators  or STEPS(p/a« ) that has cadd as an effec t
such that u = UNIFY(c , cadd, BlNDlNGS( plan) )
if there is no such step then fail
u' <— u withou t the universall y quantifie d variable s of culid
add u' to BiNDiNGS(pfaw )
add Sadd '  : S,, eed tO LlNKS(p/fl« )
add Sadd  ­< 5, 1Prf to ORDERiNGS(p/on )
if £<,</< / is a newl y adde d step from operators  then
add Sa,w to STEPS(plan)
add Start  ­< Salid ­< Finish  to ORDERINGS ( plan)
procedur e RESOLVE­THREATS(p/an )
for each S, _!_> 5y in LiNKS(p/a« ) do
for each S,i, rea in STEPS(pfa« ) do
for each c' in EFFECT(5,A rra() do
if SUBST(BlNDINGS(p/arc) , c) = SUBST(BlNDINGS(pfa/!),­if' ) then
choos e eithe r
Promotion:  Add S,t, m,t ~< St to ORDERlNGS(pton )
Demotion:  Add 5/ ­< S,i, rea/ to ORDERINGS(pfa« )
Confrontation:  if c' is reall y of the form (c' whenp)  then
CHOOSE­OPERATOR(/7/c/« , operators,  S,i, rfat, ­i p)
RESOLVE­THREATS ( p/an)
if not CONSISTENT(/?/a« ) then fail
end
end
end
Figur e 12.8 Th e relevan t component s of POP­DUNC .
a disjunctiv e precondition . CHOOSE­OPERATO R is modifie d only slightly , to handl e universall y
quantifie d variable s properly . RESOLVE­THREAT S is modifie d to includ e confrontatio n as one
metho d of resolvin g a threa t from a conditiona l effect .
386 Chapte r 12 . Practica l Plannin g
12.5 RESOURC E CONSTRAINT S
In Chapte r 11, we tackle d the proble m of shopping , but ignore d money . In this section , we conside r
how to handl e mone y and othe r resource s properly . To do this, we need a languag e in whic h we
can expres s a preconditio n such as Have($  1.89) , and we need a plannin g algorith m that will handl e
this efficiently . The forme r is theoreticall y possibl e with what we alread y have . We can represen t
each coin and bill in the worl d in the initia l state : Dollar(d\)  A Dollar(d­i)  A Quarter(q\)  A ....
We can then add decompositio n method s to enumerat e the way s it is possibl e to have a dollar . We
have to be carefu l abou t inequalit y constraints , becaus e we woul d not wan t the goal Have($2.00 )
to be satisfie d by Have(d\ ) A Have(d\).  The final representatio n woul d be a little unintuitiv e and
extremel y verbose , but we coul d do it.
The proble m is that the representatio n is totall y unsuite d for planning . Let us say we pose
the proble m Have($  1,000,000) , and the best the planne r can come up with is a plan that generate s
$1000 . The planne r woul d then backtrac k lookin g for anothe r plan . For ever y step that achieved ,
say, Have(d\ ), the planne r woul d have to conside r Have(d2)  instead . The planne r woul d end up
generatin g all combination s of all the coin s and bills in the worl d that total $1000 . Clearly , this
is a wast e of searc h time , and it fails to captur e the idea that it is the quantit y of mone y you have f
that is important , not the identit y of the coin s and bills .
Usin g measure s in plannin g
The solutio n is to introduc e numeric­value d measure s (see Chapte r 8). Recal l that a measur e
is an amount  of something , such as mone y or volume . Measure s can be referre d to by logica l
terms such as $( 1.50) or Gallons(6)  or GasLevel.  Measur e function s such as Volume  appl y to
object s such as GasInCar  to yiel d measures : GasLevel  = Volume(GasInCar)  = Gallons(6).  In
plannin g problems , we are usuall y intereste d in amount s that chang e over time . A situatio n :.
calculu s representatio n woul d therefor e includ e a situatio n argumen t (e.g. , GasLevel(s)),  but as
usual in plannin g we will leav e the situatio n implicit . We will call expression s such as GasLevel ;
MEASUR E FLUENT S measur e fluents .
Planner s that use measure s typicall y requir e them to be "declared " up fron t with associate d
range information . For example , in a shoppin g problem , we migh t wan t to state that the amoun t
of mone y the agen t has, Cash,  mus t be nonnegative ; that the amoun t of gas in the tank , GasLevel,
can rang e up to 15 gallons ; that the price of gas range s from $1.0 0 to $1.5 0 per gallon ; and that
the price of milk range s from $ 1.00 to $1.5 0 per quart :
$(0) < Cash
Gallons®)  < GasLevel  < Gallons(\5)
$(1.00 ) < UnitPrice(Gas)  x Gallons(l)  < $(1.50 )
$(1.00 ) < UnitPrice(Milk)  x Quarts(\)  < $(1.50 )
Measure s such as the price of gas are realitie s with whic h the planne r mus t deal, but over whic h it j
RESOURCE S ha s little control . Othe r measures , such as Cash  and GasLevel,  are treate d as resource s that can»
be produce d and consumed . Tha t is, there are operator s such as Drive  that requir e and consum e 1
Sectio n 12.5 . Resourc e Constraint s 387
the GasLevel  resource , and ther e are operator s such as FillUp  that produc e mor e of the GasLevel
resourc e (whil e consumin g some of the Cash  resource) .
To represen t this, we allo w inequalit y tests involvin g measure s in the preconditio n of
an operator . In the effec t of an operator , we allow numeri c assignmen t statements , wher e the
left­han d side is a measur e fluen t and the right­han d side is an arithmeti c expressio n involvin g
measures . This is like an assignmen t and not like an equalit y in that the right­han d side refer s to
value s before  the actio n and the left­han d side refer s to the valu e of the measur e fluen t after  the
action . As usual , the initia l state is describe d by the effect s of the start action :
Op(ACTlON:Start,
EFFECT : Cash  — $(12.50 ) A
GasLevel  <— Gallons(5)  A
The Buy actio n reduce s the amoun t of Cash one has:
Op(ACTlON:Buy(x,  store),
EFFECT : Have(x)  A Cash  <— Cash  — Price(x,  store))
Gettin g gas can be describe d by an abstrac t Fillup  operator :
Op(ACTlON:Fillup(GasLevel),
EFFECT : GasLevel  — Gallons(l5)  A
Cash ^­ Cash  ­ (UnitPrice(Gas)  x (Gallons(\5)  ­ GasLevel)))
The declare d uppe r and lowe r bound s serv e as implici t precondition s for each operator . For
example , Buy(x)  has the implici t preconditio n Cash  > Price(x)  to ensur e that the quantit y will
be withi n rang e after the action . It takes some reasonabl y sophisticate d algebrai c manipulatio n
code to automaticall y generat e thes e preconditions , but it is less error­pron e to do that than to
requir e the user to explicitl y write dow n the preconditions .
These example s shoul d give you an idea of the versatilit y and powe r of usin g measure s
to reaso n abou t consumabl e resources . Althoug h practica l planner s such as SIPE , O­PLAN , and
DEVISE R all have mechanism s for resourc e allocation , the theor y behin d them has not been
formulate d in a clean way , and there is disagreemen t abou t just wha t shoul d be represente d and
how it shoul d be reasone d with . We will sketc h an outlin e of one approach .
It is a good idea to plan for scarc e resource s first . Thi s can be done usin g an abstractio n
hierarch y of preconditions , as in Sectio n 12.3 , or by a specia l resourc e mechanism . Eithe r way, it
is desirabl e to delay the choic e of a causa l link for the resourc e measures . Tha t is, whe n plannin g
to buy a quar t of milk , it is a good idea to chec k if Cash  > (Quarts(l)  x UnitPrice(Milk,  store)),
but it woul d be prematur e to establis h a causa l link to that preconditio n from eithe r the start state
or some othe r step. The idea is to first pick the steps of the plan and do a roug h chec k to see if the
resourc e requirement s are satisfiable . If they are, then the planne r can continu e with the norma l
mechanis m of resolvin g threat s for all the preconditions .
An easy way of doin g the roug h chec k on resource s is to keep track of the minimu m
and maximu m possibl e value s of each quantit y at each step in the plan . For example , if in the
initia l state we have Cash^$(\2.5Q)  and in the descriptio n of measure s we have $(0.50 ) <
UnitPrice(Bananas,store)  x Pounds(l)  < $(1.00 ) and $(1.00 ) < UnitPrice(Milk,store)  x
Quarts(\)  < $(1.50) , then a plan with the step s Buy(Milk)  and Buy(Bananas)  will hav e the
388 Chapte r 12 . Practica l Plannin g
range $(10.00 ) < Cash  < $(11.00 ) at the finish . If we starte d out with less than $(1.50) , then
this approac h woul d lead to failur e quickly , withou t havin g to try all permutation s of Buy steps
at all combination s of stores .
There is a trade­of f in decidin g how muc h of the resourc e quantit y informatio n we wan t to
deal with in the roug h check . We coul d implemen t a full constrain t satisfactio n proble m solve r for
arbitrar y arithmeti c inequalitie s (makin g sure  we tie this proces s in with the norma l unificatio n
of variables) . However , with complicate d domains , we can end up spendin g just as long solvin g
the constrain t satisfactio n proble m as we woul d have spen t resolvin g all the conflicts .
Tempora l constraint s
In mos t ways , time can be treate d like any othe r resource . The initia l state specifie s a start time for
the plan , for example , Time  ­^8:30 . (Her e 8:30 isashorthan d for Minutes(%  x 60 + 30).) We then
can say how muc h time each operatio n consumes . (In the case of time , of course,  consumptio n
mean s adding  to the amount. ) Suppos e it take s 10 second s to pum p each gallo n of gas, and 3
minute s to do the rest of the Fillup  action . The n an effec t of Fillup  is
Time  — Time +  Minutes(3)  + (Seconds(lO)/Gallons(l))  x (Gallons(\5)  ­ GasLevel))
It is hand y to provid e the operato r Wait(x),  whic h has the effec t Time  ^­ Time  + x and no othe r
precondition s or effect s (at least in stati c domains) .
There are two way s in whic h time differ s from othe r resources . First , action s that are
execute d in paralle l consum e the maximu m of their respectiv e time s rathe r than the sum . Second ,
constraint s on the time resourc e have to be consisten t with orderin g constraints . Tha t is, if 51, ­< Sj
is one of the orderin g constraints , then Time  at 5, mus t be less than Time  at Sj.
Anothe r importan t constrain t is that time neve r goe s backward . Thi s implie s that no
operator s generat e time instea d of consumin g it. Thus , if the goal state specifie s a deadlin e (a
maximu m time) , and you have a partia l plan whos e steps requir e more time than is allowed , you
can backtrac k immediately , withou t considerin g any completion s of the plan . (See Exercis e 12.9
for more on this. )
12.6 SUMMAR Y
In this chapte r we have seen severa l way s to exten d the plannin g language—th e representatio n |
of state s and operators—t o allow the planne r to handl e mor e complex , realisti c domains . Eac h
extensio n require s a correspondin g chang e to the plannin g algorithm , and there is ofte n a difficult y
trade­of f betwee n expressivenes s and worst­cas e efficiency . However , whe n the more expressiv e s
representation s are used wisely , they can lead to an increas e in efficiency .
We have tried to presen t the field of plannin g in a way that emphasize s the best aspect s of 1
progres s in AI. The field starte d with a vagu e set of requirement s (to contro l a robot ) and after some !
experimentin g with an initiall y promisin g but ultimatel y intractabl e approac h (situatio n calculus !
and theore m proving ) settle d dow n to a well­understoo d paradig m (STRiPS­styl e planning) . Frorn l
Sectio n 12.6 . Summar y 389
there , progres s was mad e by a serie s of implementations , and formalization s of ever mor e
ambitiou s variation s on the basi c plannin g language .
• The STRIP S languag e is too restricte d for complex , realisti c domains , but can be extende d
in severa l ways .
• Planner s base d on extende d STRlPS­lik e language s and partial­orde r least­commitmen t
algorithm s have prove n capabl e of handlin g comple x domain s such as spacecraf t mission s
and manufacturing .
• Hierarchica l decompositio n allow s nonprimitiv e operator s to be include d in plans , with
a know n decompositio n into more primitiv e steps .
• Hierarchica l decompositio n is mos t effectiv e whe n it serve s to prun e the searc h space .
Prunin g is guarantee d whe n eithe r the downwar d solutio n propert y (ever y abstrac t solutio n
can be decompose d into a primitiv e solution ) or upwar d solutio n propert y (inconsisten t
abstrac t plan s have no primitiv e solutions ) holds .
• We can mak e the plannin g languag e close r to situatio n calculu s by allowin g conditiona l
effect s (the effec t of an operato r depend s on wha t is true whe n it is executed ) and universa l
quantificatio n (the preconditio n and effec t can refe r to all object s of a certai n class) .
• Man y action s consum e resources , such as money , gas, or raw materials . It is convenien t
to trea t thes e as numeri c measure s in a pool rathe r than try to reaso n about , say, each
individua l coin and bill in the world . Action s can generat e and consum e resources , and it
is usuall y chea p and effectiv e to chec k partia l plan s for satisfactio n of resourc e constraint s
befor e attemptin g furthe r refinements .
• Tim e is one of the most importan t resources . Wit h a few exceptions , time can be handle d
with the genera l mechanism s for manipulatin g resourc e measures .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
Abstrac t and hierarchica l plannin g was introduce d in the ABSTRIP S syste m (Sacerdoti , 1974) ,
MACROP S a  varian t of STRIPS . (Actually , the facilit y in STRIP S itsel f for learnin g macrops —"macro ­
operators " consistin g of a sequenc e of bottom­leve l steps—coul d be considere d the first mech ­
anism for hierarchica l planning. ) Hierarch y was also used in the LAWAL Y syste m (Sikloss y
and Dreussi , 1973) . Wilkin s (1986 ) discusse s som e ambiguitie s in the meanin g of the term
"hierarchica l planning. " Yan g (1990 ) explain s the "uniqu e main subaction " propert y in the con­
text of abstractio n planning . Erol , Hendler , and Nau (1994 ) presen t a complet e hierarchica l
decompositio n planne r to whic h our HD­PO P owe s a grea t deal .
Work on decidin g wha t hierarchica l plan s are wort h knowin g abou t and how to adap t
previousl y constructe d plan s to nove l situation s goes unde r the nam e adaptiv e plannin g (Alterman ,
1988 ) or case­base d plannin g (Hammond , 1989) .
Continuou s time was first deal t with by DEVISE R (Vere , 1983) . A mor e recen t plan ­
ner focusin g on the treatmen t of continuou s time constraint s is FORBI N (Dea n et al., 1990) .
NONLIN + (Tat e and Whiter , 1984 ) and SlP E (Wilkins , 1988 ; Wilkins , 1990 ) coul d reaso n
390 Chapte r 12 . Practica l Plannin g
abou t the allocatio n of limite d resource s to variou s plan steps . MOLGE N (Stefik , 1981b ;
Stefik , 198 la) allowe d reasonin g abou t object s that are only partiall y specifie d (via constraints) .
GAR I (Descott e and Latombe , 1985 ) allowe d hierarchica l reasonin g with constraints , in that some
lower­leve l constraint s are give n lowe r prioritie s and may be violate d at som e stage s durin g plan
construction . O­PLA N (Bel l and Tate , 1985 ) had a uniform , genera l representatio n for constraint s
on time and resources .
Classica l situatio n calculu s had allowe d the full predicat e calculu s for describin g the
precondition s and effect s of plan steps . Late r formalism s have tried to get muc h of this expres ­
sivenes s back , withou t sacrificin g the efficienc y gaine d by the use of STRIP S operators . The ADL
plannin g formalis m (Pednault , 1986 ) allow s for multiagen t plannin g and avoid s the problem s
with the STRIP S formalis m that were pointe d out by Lifschit z (1986) . PEDESTA L (McDer ­
mott, 1991 ) was the first (partial ) implementatio n of ADL . UCPO P (Penberth y and Weld , 1992 ;
Barret t et al., 1993 ) is a more complet e implementatio n of ADL that also allow s for partial­orde r
planning . POP­DUN C is base d largel y on this work . Wel d (1994 ) describe s UCPO P and gives a
genera l introductio n to partial­orde r plannin g with action s havin g conditiona l effects . Kambham ­
pati and Nau (1993 ) analyz e the applicabilit y of Chapman' s (1987 ) NP­hardnes s result s to action s
with conditiona l effects . Wolfgan g Bibe l (1986 ) has attempte d to reviv e the use of full predicat e
calculu s for planning , countin g on advance s in theore m provin g to avoi d the intractabilit y that
this led to in the early days of planning .
The improvement s in the richnes s of the representation s used in moder n planner s have
helpe d mak e them mor e nearl y equa l to the challenge s of real­worl d plannin g task s than was
STRIPS . Severa l system s have been used to plan for scientifi c experimentatio n and observation .
MOLGE N was used in the desig n of scientifi c experiment s in molecula r genetics . T­SCHE D
(Drabble , 1990 ) was used to schedul e missio n comman d sequence s for the UOSAT­I I satellite .
PLAN­ERS 1 (Fuch s et al., 1990) , base d on O­PLAN , was used for observatio n plannin g at the
Europea n Spac e Agency ; SPIK E (Johnsto n and Adorf , 1992 ) was used for observatio n plannin g
at NAS A for the Hubbl e spac e telescope . Manufacturin g has been anothe r fertil e application
area for advance d plannin g systems . OPTIMUM­AI V (Aaru p et al., 1994) , base d on O­PLAN ,
has been used in spacecraf t assembl y at the Europea n Spac e Agency . ISI S (Fox et al., 1981; ;
Fox, 1990 ) has been used for job shop schedulin g at Westinghouse . GAR I planne d the machinin g
and constructio n of mechanica l parts . FORBI N was used for factor y control . NONLIN + was used
for nava l logistic s planning . SIPE has had a numbe r of applications , includin g plannin g for aircraf t
carrie r fligh t deck operations .
EXERCISE S
12.1 Giv e decomposition s for the Hire  Builder  and Obtain  Permit  steps in Figur e 12.1 , and
show how the decompose d subplan s connec t into the overal l plan .
12.2 Rewor k the previou s exercis e usin g an approximatio n hierarchy . Tha t is, assig n criticalit y •;
level s to each preconditio n of each step . How did you decid e whic h precondition s get highe r <
criticalit y levels ?
Sectio n 12.6 . Summar y 391
12.3 Giv e an exampl e in the house­buildin g domai n of two abstrac t subplan s that canno t be
merge d into a consisten t plan withou t sharin g steps . (Hint:  Place s wher e two physica l part s of
the hous e com e togethe r are also place s wher e two subplan s tend to interact. )
12.4 Construc t an exampl e of the violatio n of the downwar d solutio n property . Tha t is, find an
abstrac t solutio n such that , whe n one of the steps is decomposed , the plan become s inconsisten t
in that one of its threat s canno t be resolved .
12.5 Prov e that the upwar d solutio n propert y alway s hold s for approximatio n hierarch y plannin g
(see page 380) . You may use Tenenber g (1988 ) for hints .
12.6 Ad d existentia l quantifier s (3) to the plan language , usin g whateve r synta x restriction s you
find reasonable , and exten d the planne r to accommodat e them .
12.7 Writ e operator s for the shoppin g domai n that will enabl e the planne r to achiev e the goal of
havin g three orange s by grabbin g a bag, goin g to the store , grabbin g the oranges , payin g for them ,
and returnin g home . Mode l mone y as a resource . Use universa l quantificatio n in the operators ,
and show that the origina l content s of the bag will still be ther e at the end of the plan .
12.8 We said in Sectio n 11.6 that the SELECT­SUB­GOA L part of the POP algorith m was not a
backtrac k point—tha t we can wor k on subgoal s in any orde r withou t affectin g completenes s (al­
thoug h the choic e certainl y has an effec t on efficiency) . Whe n we chang e the SELECT­SUB­GOA L
part to handl e hierarchica l decomposition , do we need to mak e it a backtrac k point ?
12.9 Som e domain s hav e resource s that are monotonicall y decreasin g or increasing . Fo r
example , tim e is monotonicall y increasing , and if ther e is a Buy  operator , but no Earn,  Beg,
Borrow,  or Steal,  then mone y is monotonicall y decreasing . Knowin g this can cut the searc h
space : if you have a partia l plan whos e steps requir e mor e mone y than is available , then you can
avoid considerin g any of the possibl e completion s of the plan .
a. Explai n how to determin e if a measur e is monotonic , give n a set of operato r descriptions .
b. Desig n an experimen t to analyz e the efficienc y gain s resultin g from the use of monotoni c
resource s in planning .
12.10 Som e of the operation s in standar d programmin g language s can be modelle d as action s
that chang e the state of the world . For example , the assignmen t operatio n change s the content s
of a memor y location ; the prin t operatio n change s the state of the outpu t stream . A progra m
consistin g of thes e operation s can also be considere d as a plan , whos e goal is give n by the
specificatio n of the program . Therefore , plannin g algorithm s can be used to construc t program s
that achiev e a give n specification .
a. Writ e an operato r schem a for the assignmen t operato r (assignin g the valu e of one variabl e
to another) .
b. Sho w how objec t creatio n can be used by a planne r to produc e a plan for exchangin g the
value s of two variable s usin g a temporar y variable .
13PLANNIN G AND ACTIN G
In which  planning  systems  must  face up to the awful  prospect  of actually  having  to
take their  own  advice.
CONDITIONA L
PLANNIN G
CONTINGENC Y
SENSIN G ACTION S
EXECUTIO N
MONITORIN G
REPLANNIN GThe assumption s require d for flawles s plannin g and execution , given the algorithm s in the previou s
chapters , are that the worl d be accessible , static , and deterministic—jus t as for our simpl e searc h
methods . Furthermore , the actio n description s mus t be correc t and complete , describin g all
the consequence s exactly . We describe d a plannin g agen t in this idea l case in Chapte r 11; the
resemblanc e to the simpl e problem­solvin g agen t of Chapte r 3 was no coincidence .
In real­worl d domains , agent s have to deal with both incomplet e and incorrec t information .
Incompletenes s arise s becaus e the worl d is inaccessible ; for example , in the shoppin g world , the
agent may not know wher e the milk is kept unles s it asks . Incorrectnes s arise s becaus e the worl d
does not necessaril y matc h the agent' s mode l of it; for example , the pric e of milk may have
double d overnight , and the agent' s walle t may have been pickpocketed .
There are two differen t way s to deal with the problem s arisin g from incomplet e and
incorrec t information :
<) Conditiona l planning : Also know n as contingenc y planning , conditiona l plannin g deals
with incomplet e informatio n by constructin g a conditiona l plan that account s for each
possibl e situatio n or contingenc y that coul d arise . Th e agen t find s out whic h part of
the plan to execut e by includin g sensin g action s in the plan to test for the appropriat e
conditions . For example , the shoppin g agen t migh t wan t to includ e a sensin g actio n in its •;
shoppin g plan to chec k the price of som e objec t in case it is too expensive . Conditiona l |
plannin g is discusse d in Sectio n 13.1 .
<> Executio n monitoring : The simpl e plannin g agen t describe d in Chapte r 11 execute s its j
plan "wit h its eyes closed"—onc e it has a plan to execute , it does not use its percept s to J
selec t actions . Obviousl y this is a very fragil e strateg y whe n there is a possibilit y that the* ;
agen t is usin g incorrec t informatio n abou t the world . By monitorin g wha t is happenin g j
while it execute s the plan , the agen t can tell whe n thing s go wrong . It can then
replannin g to find a way to achiev e its goal s from the new situation . For example , if the ;
agen t discover s that it does not have enoug h mone y to pay for all the item s it has picke d f
up, it can retur n som e and replac e them with cheape r versions . In Sectio n 13.2 we look j
392
Sectio n 13.1. Conditiona l Plannin g 393
DEFERRIN Gat a simpl e replannin g agen t that implement s this strategy . Sectio n 13.3 elaborate s on this
desig n to provid e a full integratio n of plannin g and execution .
Executio n monitorin g is relate d to conditiona l plannin g in the followin g way . An agen t that
build s a plan and then execute s it whil e watchin g for error s is, in a sense , takin g into accoun t
the possibl e condition s that constitut e executio n errors . Unlik e a conditiona l planner , however ,
the executio n monitorin g agen t is actuall y deferrin g the job of dealin g with thos e condition s
until they actuall y arise . The two approache s of cours e can be combine d by plannin g for some
contingencie s and leavin g other s to be deal t with later if they occur . In Sectio n 13.4 , we will
discus s whe n one migh t prefe r to use one or the othe r approach .
13.1 CONDITIONA L PLANNIN G
We begi n by lookin g at the natur e of conditiona l plan s and how an agen t execute s them . Thi s
will help to clarif y the relationshi p betwee n sensin g action s in the plan and their effect s on the
agent' s knowledg e base . We then explai n how to construc t conditiona l plans .
The natur e of conditiona l plan s
Let us conside r the proble m of fixin g a flat tire. Suppos e we have the followin g three actio n
schemata :
Op(ACT[ON:Remove(x),
PRECOND:On(X) ,
EFFECT.  Off(x)  A ClearHub(x)  A ­>On(x))
Op(ACT[ON:PutOn(x),
PRECOND:Off(x)  A ClearHub(x),
EFFECT : OnW A ­^ClearHub(x)  A ~iOff(x))
Op( ACTIO N :Inflate(x),
PRECOND:Intact(x)  A Flat(x),
EFFECT : Inflated(x)  A ­>Flat(x))
If our goal is to have an inflate d tire on the wheel :
On(x)  A Inflated(x)
and the initia l condition s are
Inflated(Spare)  A Intact(Spare)  A Off (Spare)  A On(Tire\)  A Flat(Tire\)
then any of the standar d planner s describe d in the previou s chapter s woul d be able to com e up
with the followin g plan :
\Remove(Tire\ ), PutOn(Spare)]
If the absenc e of Intact(Tire\)  in the initia l state reall y mean s that the tire is not intac t (as the
standar d planner s assume) , then this is all wel l and good . Bu t suppos e we hav e incomplet e
394 Chapte r 13 . Plannin g and Actin g
knowledg e of the world—th e tire may be flat becaus e it is punctured , or just becaus e it has
not been pumpe d up lately . Becaus e changin g a tire is a dirty and time­consumin g business ,
it woul d be bette r if the agen t coul d execut e a conditiona l plan : if Tire\  is intact , then inflat e
it. If not, remov e it and put on the spare . To expres s this formally , we can exten d our origina l
notatio n for plan step s with a conditiona l step If(<Condition>,  <ThenPart>,  <ElsePart>, ). Thus ,
the tire­fixin g plan now include s the step
If(lntact(Tire\\  [Inflate(Tire\)},  [Remove(Tire\},  PutOn(Spare)])
Thus , the conditiona l plannin g agen t can sometime s do bette r than the standar d plannin g agent s
describe d earlier . Furthermore , ther e are case s wher e a conditiona l plan is the only possibl e
plan. If the agen t does not know if its spare tire is flat or inflated , then the standar d planne r will
fail, wherea s the conditiona l planne r can inser t a secon d conditiona l step that inflate s the spare if
necessary . Lastly , if there is a possibilit y that both tires have holes , then neithe r planne r can come
up with a guarantee d plan . In this case , a conditiona l planne r can plan for all the case s wher e
succes s is possible , and inser t a Fail  actio n on thos e branche s wher e no completio n is possible .
Plans that includ e conditiona l steps are execute d as follows . Whe n the conditiona l step is
executed , the agen t first tests the conditio n agains t its knowledg e base . It then continue s executin g
either the then­par t or the else­part , dependin g on whethe r the conditio n is true or false . The
then­par t and the else­par t can themselve s be plans , allowin g arbitrar y nestin g of conditionals .
The conditiona l plannin g agen t desig n is show n in Figur e 13.1 . Notic e that it deal s with neste d
conditiona l steps by followin g the appropriat e conditiona l branche s unti l it find s a real actio n to
do. (The conditiona l plannin g algorith m itself , CPOP , will be discusse d later. )
The crucia l part of executin g conditiona l plan s is that the agen t must , at the time of
executio n of a conditiona l step, be able to decid e the truth or falsehoo d of the condition—tha t is,
the condition  must be  known  to the agent  at that poin t in the plan . If the agen t does not know
if Tire\  is intac t or not, it canno t execut e the previousl y show n plan . Wha t the agen t know s at
any poin t is of cours e determine d by the sequenc e of percept s up to that point , the sequenc e of
action s carrie d out, and the agent' s initia l knowledge . In this case , the initia l condition s in the
knowledg e base do not say anythin g abou t Intact(Tire\).  Furthermore , the agen t may have no
action s that caus e lntact(Tire\)  to becom e true.1 To ensure  that a conditional  plan  is executable,
the agent  must  insert  actions that  cause  the relevant  conditions  to become  known  by the  agent.
Facts  becom e know n to the agen t throug h its percepts , so wha t we mea n by the previou s
remar k is that the agen t mus t act in such a way as to make sure it receive s the appropriat e percepts .
For example , one way to com e to know that a tire is intac t is to put som e air into it and place
one's listenin g devic e in close proximity . A hissin g percept  then enable s the agen t to infe r that
the tire is not intact.2 Suppos e we use the nam e CheckTire(x)  to refer to an actio n that establishe s
the state of the tire x. This is an exampl e of a sensin g action .
Usin g the situatio n calculu s descriptio n of sensin g action s describe d in Chapte r 8, we
woul d write
Vx,s  Tire(x)  => KnowsWhether("Intact(x)",Result(CheckTire(x),s))
1 Not e that if, for example , a Patch(Tire\)  actio n were available , then a standar d plan coul d be constructed .
2 Agent s withou t soun d percept s can wet the tire. A bubblin g visua l percep t then suggest s the tire is compromised .
Sectio n 13.1 . Conditiona l Plannin g 395
functio n CONDITIONAL­PLANNING ­ AGENT(percept)  return s an action
static : KB,  a knowledg e base (include s actio n descriptions )
p, a plan , initiall y NoPlan
t, a counter , initiall y 0, indicatin g time
G, a goal
TELL(KB,  MAKE­PERCEPT­SENTENCE(/7Wepf , t))
current  <— STATE­DESCR1PTION(#B , f)
itp = NoPlan  then p ^ CPOP(current,  G, KB)
if p = NoPlan  or p is empt y then action  ^ NoOp
else
action'—  FlRST(p )
while CoNDiTiONAL?(ac?wn ) do
if ASK(/ffi , CONDITION­PARTlacft'on] ) then p <­ APPEND(THEN­PART[acft'on],REST(/j) )
else p <­ APFEND(ELSE­PART[acft'on] , REST(p) )
acrio n <— FiRST(/7 )
end
TELL(/fS , MAKE­ACTION­SENTENCE(flCft'0M , 0)
f — f+ 1
retur n action
Figur e 13.1 A  conditiona l plannin g agent .
In our actio n schem a format , we woul d write
Op(ACT:iON:CheckTire(x),
PRECOND:77re(X) ,
EFFECT.KnowsWhether("Intact(x)"y)
Notic e that as well as havin g knowledg e effects , a sensin g actio n can have ordinar y effects . For
example , if the CheckTire  actio n uses the wate r method , then the tire will becom e wet. Sensin g
action s can also have precondition s that need to be established . For example , we migh t need to
fetch the pum p in orde r to put som e air in the tire in orde r to chec k it. A conditional  planner
therefore  will  sometimes  create  plans  that  involve  carrying  out ordinary  actions  for the purpose
of obtaining  some  needed  information.
CONTEX TAn algorith m for generatin g conditiona l plan s
The proces s of generatin g conditiona l plan s is muc h like the plannin g proces s describe d in
Chapte r 11. The main additiona l construc t is the contex t of a step in the plan . A step' s contex t is
simpl y the unio n of the condition s that mus t hold in orde r for the step to be executed—essentially ,
it describe s the "branch " on whic h the step lies. For example , the actio n Inflate(Tire\)  in the
earlie r plan has a contex t Intact(Tire\).  Onc e it is establishe d that a step has a certai n context ,
then subsequen t steps in the plan inheri t that context . Becaus e it canno t be the case that two steps
396 Chapte r 13 . Plannin g and Actin g
with distinc t context s can both be executed , such steps canno t interfer e with each other . Context s
are therefor e essentia l for keepin g track of whic h step s can establis h or violat e the precondition s
of whic h othe r steps . An exampl e will mak e this clear .
The flat­tir e plan begin s with the usua l start and finis h steps (Figur e 13.2) . Notic e that the
finis h step has the contex t True,  indicatin g that no assumption s have been mad e so far.
____ _ On(Tirel)
Start \Flat(Tirel)
Inflated/Spare)On(x)
Finis h
(True)
Figur e 13.2 Initia l plan state for the flat­tir e problem .
CONDITIONA L LINK
CONDITIONA L STEPThere are two open condition s to be resolved : On(x)  and Inflated(x).  The first is satisfie d
by addin g a link from the start step , with the unifie r {xlTire\}.  The secon d is satisfie d by addin g
the step Inflate(Tire\),  whic h has precondition s Flat(Tire\)  and Intact(Tire\)  (see Figur e 13.3) .
| Star t \Flat(Tire1)  —» _
Inflated(Spare)
Figure 13.3 Pla n* Flat(Tirel)
Intact(Tirel)ish
jf (True)
Inflate(Tirel ) ^
(IntactfTirel  ))
after addin g the Inflate(Tire\)  step .
The ope n conditio n Flat(Tire\)  is satisfie d by addin g a link from the start step . Th e
interestin g part is wha t to do with the Intact(Tire\)  condition . In the statemen t of the proble m
there are no action s that can mak e the tire intact—tha t is, no actio n schem a with the effec t
Intact(x).  At this poin t a standar d causal­lin k planne r woul d abando n this plan and try anothe r
way to achiev e the goal . Ther e is, however , an actio n CheckTire(x)  that allow s one to know  the
truth valu e of a propositio n that unifie s with Intact(Tire\).  If (and this is sometime s a big if) the
outcom e of checkin g the tire is that the tire is know n to be intact , then the Inflate(Tire\)  step can
be achieved . We therefore  add the CheckTire  step to the plan with a conditiona l link (show n as a
dotte d arrow in Figur e 13.4 ) to theInflate(Tire\ ) step . The CheckTire  step is calle d a conditiona l
step becaus e it will becom e a branc h poin t in the fina l plan . The inflat e step and the finis h step
now acquir e a contex t labe l statin g that they are assumin g the outcom e lntaci(Tire\}  rathe r than
­<Intact(Tirei).  Becaus e CheckTire  has no precondition s in our simpl e formulation , the plan is
complete  given  the context of  the finish  step.
Obviously , we canno t stop here . We need a plan that work s in both cases . The conditiona l
planne r ensure s this by addin g a secon d copy of the origina l finis h step , labelle d with a contex t
that is the negatio n of the existin g contex t (see Figur e 13.5).3 In this way, the planne r cover s an
3 If the solutio n of this new branc h require s furthe r contex t assumptions , then a third copy of the finis h step will be adde d
whos e contex t is the negatio n of the disjunctio n of the existin g finis h steps . Thi s continue s unti l no mor e assumption s
are needed .
Sectio n 13.1 . Conditiona l Plannin g 397
| Star t~] Flat(Tlre  1) ~
Inflated/Spare) ^  •— . __^ _ „ ,,.. ,,
­preD im^rn,^  Inflate(Tirel ) \
lnl*^_. ­.­^' (lntnrt(Tirt!lH
Check(Tirel ) \"~frrffetfort(Ti>»< ] 1 Flnls h
j^­ (IntactfTirel  ))
Figur e 13.4 Th e plan to inflat e Tire\ will work provide d that it is intact .
On(Tirel)
Start \Flat(Tire1)
Inflated(Spare)On(Tirel)
Inflated/Tire  1)Finis h
Flat(Tire1)
ln,ac,(Tire1)lnflate(Tire1)
(IntactfTirel))<lntact(Tire1»
On(x)  .
Inflated(x)  \Finis h |
(~i/ntact(Tire1))
Figur e 13.5 W e mus t now plan for the case wher e Tire\  is not intact .
exhaustiv e set of possibilities—fo r ever y possibl e outcome , ther e is a correspondin g finis h step ,
and a path to get to the finis h step .
Now we need to solv e the goal whe n Tire\  has a hole in it. Her e the contex t is very useful .
If we wer e to try to add the step Inflate(Tire\)  to the plan , we woul d immediatel y see that the
preconditio n lntact(Tire\ ) is inconsisten t with the contex t ­*Intact(Tire\).  Thus , the only way s to
satisf y the Inflated(x)  conditio n are to link it to the start step with the unifie r {x/spare}  or to add
an Inflate  step for the spare . Becaus e the latte r leads to a dead end (becaus e the spare is not flat) ,
we choos e the former . This leads to the plan state in Figur e 13.6.
The step s Remove(Tire\ ) and PutOn(Spare)  are now adde d to the plan to satisf y the con­
ditio n On(Spare),  usin g standar d causal­lin k addition . Initially , the step s woul d have a True
^^^  On(Tirel)
Start |F/at(T /
/™
tact(Tire1*, IntactfTirel)Inflated(Tirel)Finis h
Flat(Tirel)
Inflale(Tirel )
(IntactfTirel))(Intact(Tirel))
On(Spare)  ,—————
Inflated(Spare)  I Fi"ish
< IntactfTirel
Figur e 13.6 Whe n Tire\  is not intact , we mus t use the spar e instead .
398 Chapte r 13 . Plannin g and Actin g
context , becaus e it has not yet been establishe d that they can only be execute d unde r certai n
circumstances . This mean s that we have to chec k how the steps interac t with othe r steps in the
plan. In particular , the Remove(Tire\)  step threaten s the causa l link protectin g On(Tire\)  in the
first finis h step (the one with the contex t (Intact(Tire\)).  In a standar d causal­lin k planner , the
only solutio n woul d be to promot e or demot e the Remove(Tire\)  step so that it canno t interfere .
CONDITIONIN G I n the conditiona l planner , we can also resolv e the threa t by conditionin g the step so that its
contex t become s incompatibl e with the contex t of the step whos e preconditio n it is threatenin g
(in this case , the first finis h step) . Conditionin g is achieve d by findin g a conditiona l step that has
a possibl e outcom e that woul d mak e the threatenin g step' s contex t incompatibl e with the causa l
link's context . In this case , the CheckTire  step has a possibl e outcom e ^lntact(Tire\).  If we
make a conditiona l link from the CheckTire  step to the Remove(Tire\ ) step , then the remov e step
is no longe r a threat . The new contex t is inherite d by the PutOn(Spare)  step, and the plan is now
complet e (Figur e 13.7) .
On(T
| Star t ~\Flat(­
Inflat
\rire1) —
id(Spare)  ^"""""""""••••••••^ ^
int^ V
Check(Tirel ) \"
. ^­ilntact(Tirel)
^^_ Remove f,^l{r,^ \ Inflate(Tirel ) Y
(Intact(Tirel))
irel) |^­ Puton(Spare )
"V^^ ^ (­<lntact(Tire1))  (­'lntact(Tirei))lnflated(Tire1)nish
^­ (lntact(Tire1»
^^j^On(Spare)  . ————————  ,
Inflated!  Spare]Finish |^ (­ilntact(Tirel))
^^
Figur e 13.7 Th e complet e plan for fixing the tire, showin g causa l and conditiona l links and
context s (in parentheses) .
The algorith m is calle d CPO P (for Conditiona l Partial­Orde r Planner) . It build s on the POP
algorithm , and extend s it by incorporatin g contexts , multipl e finis h step s and the conditionin g
proces s for resolvin g potentia l threats . It is show n in Figur e 13.8 .
PARAMETERIZE D
PLAN SExtendin g the plan languag e
The conditiona l steps we used in the previou s sectio n had only two possibl e outcomes . In some
cases , however , a sensin g actio n can have any numbe r of outcomes . For example , checkin g the
color of som e objec t migh t resul t in a sentenc e of the form Color(x,  c) bein g know n for som e
value of c. Sensin g action s of this type can be used in parameterize d plans , wher e the exac t
action s to be carrie d out will not be know n unti l the plan is executed . For example , suppos e we
have a goal such as
Color(Chair,  c) A Color(Table,  c)
Sectio n 13.1 . Conditiona l Plannin g 399
functio n CPOP(initial,  goals,  operators)  return s plan
plan  <— MAKE­PLAN(;'mYra/ , goals)
loop do
Termination'.
if there are no unsatisfie d precondition s
and the context s of the finis h step s are exhaustiv e
then retur n plan
Alternative  context  generation:
if the plan s for existin g finis h steps are complet e and have context s C\ ... Cn then
add a new finis h step with a contex t ­i (Ci V ... V C n)
this become s the current  context
Subgoal  selection  and addition:
find a plan step Sneeci with an open preconditio n c
Action  selection:
choos e a step Sadd from operators  or STEPS(p/aw ) that adds c or
knowledg e of c and has a contex t compatibl e with the curren t contex t
if there is no such step
then fail
add Sadd '  ; Sneed to LlNKS ( plan)
add Sadd < Swed  to ORDERINGS ( plan)
if Sadd is a newl y adde d step then
add Sadd  to STEPS(plan)
add Start  < Sadd  < FJ'W'S/ Z to ORDERlNGS(p/an )
Threat  resolution:
for each step 5,« r«» that potentiall y threaten s any causa l link 5, c
 ; 5}
with a compatibl e contex t do
choos e one of
Promotion:  AddS,i, rea, < S; to ORDERINGS(pfan )
Demotion:  Add  5/ < 5*™ ; to ORDERINGS(p/a« )
Conditioning:
find a conditiona l step SCOnd possibl y befor e both S,i, reat and 5,­, wher e
1. the contex t of SCOnd is compatibl e with the context s of Sthreat  and S/;
2. the step has outcome s consisten t with S,hreat  and S/, respectivel y
add conditionin g link s for the outcome s from Scnmt to S,/,,­ raf and 5,
augmen t and propagat e the context s of S,/, reai and 5,
if no choic e is consisten t
then fail
end
end
Figur e 13.8 Th e CPO P algorith m for constructin g conditiona l plans .
400 Chapte r 13 . Plannin g and Actin g
RUNTIM E VARIABL E
MAINTENANC E GOA L
AUTOMATI C
PROGRAMMIN G("the chai r and table are the same color") . The chai r is initiall y unpainted , and we have som e
paint s and a paintbrush . The n we migh t use the plan
[SenseColor(Table),  KnowsWhat("Color(Table,  c)"), GetPaint(c),  Paint(Chair,  c)]
The last two steps are parameterized , becaus e unti l executio n the agen t will not know the valu e
of c. We call c a runtim e variable , as distinguishe d from norma l plannin g variable s whos e
value s are know n as soon as the plan is made . The step SenseColor(Table)  will have the effec t of
providin g percept s sufficien t to allow the agen t to deduc e the colo r of the table . The n the actio n
KnowsWhat("Color(Table,c)")  is execute d simpl y by queryin g the knowledg e base to establis h
a valu e for the variabl e c. Thi s valu e can be used in subsequen t steps such as GetPaint(c),  or
in conditiona l steps such as If(c = Green,  [...], [...]) . Sensin g action s are define d just as in the
binary­conditio n case:
Op(AcnON:SenseColor(x),
EFFECT : Knows  What("Color(x,  c)"))
In situatio n calculus , the actio n woul d be describe d by
MX, s 3c KnowsWhat("Color(x,c)",Result(SenseColor(x),s))
The variable s x and c are treated  differentl y by the planner . Logically , c is existentiall y quantifie d
in the situatio n calculu s representation , and thus mus t be treate d as a Skole m functio n of the
objec t bein g sense d and the situatio n in whic h it is sensed . In the planner , runtim e variable s like
c unif y only with ordinar y variables , and not with constant s or with each other . This correspond s
exactl y to wha t woul d happe n with Skole m functions .
When we have the abilit y to discove r that certai n facts are true, as well as the abilit y to
cause them to become  true, then we may wish to have some contro l over whic h facts are change d
and whic h are preserved . For example , the goal of havin g the table and chai r the sam e colo r
can be achieve d by paintin g them both black,  regardles s of wha t colo r the table is at the start .
This can be prevente d by protectin g the table' s colo r so that the agen t has to sens e it, rathe r than
paintin g over it. A maintenanc e goal can be used to specif y this:
Color(Chair,  c) A Color(Table,  c) A Maintain(Color(Table,x))
The Maintain  goal will ensur e that no actio n is inserte d in the plan that has an ordinar y causa l
effec t that changes  the color of the table . This is done in a causal­lin k planne r by addin g a causa l
link from the start step to the finis h step protectin g the table' s initia l color .
Plans with conditional s start to look suspiciousl y like programs . Moreover , executin g such
plans start s to look rathe r like interpretin g a program . The similarit y become s even stronge r when
we includ e loop s in plans . A loop is like a conditional , excep t that whe n the conditio n holds , a
portio n of the plan is repeated . For example , we migh t includ e a loopin g step to mak e sure the
chair is painte d properly :
WMle(KnowsrUnevenColor(Chairy),  [Paint(Chair,  c), CheckColor(Chair)})
Technique s for generatin g plan s with conditional s and loop s are almos t identica l to thos e for
generatin g program s from logica l specification s (so­calle d automati c programming) . Eve n a
standar d planne r can do automati c programmin g of a simpl e kind if we encod e as STRIP S operator s
the action s correspondin g to assignmen t statements , procedur e calls , printing , and so on.
ISectio n 13.2 . A  Simpl e Replannin g Agen t 401
i 3.2 A SIMPL E REPLANNIN G AGEN T
TRIANGL E TABL E
ACTIO N MONITORIN G
BOUNDE D
WDETERMINAC YAs long as the worl d behave s exactl y as the actio n description s describ e it, then executin g a plan
in the ideal or incomplete­informatio n cases will alway s resul t in goal achievement . As each step
is executed , the worl d state will be as predicted—a s long as nothin g goes wrong .
"Somethin g goin g wrong " mean s that the worl d state after an actio n is not as predicted .
More specifically , the remainin g plan segmen t will fail if any of its precondition s is not met. The
precondition s of a plan segment  (as oppose d to an individua l step) are all thos e precondition s of
the steps in the segmen t that are not establishe d by othe r steps in the segment . It is straightforwar d
to annotat e a plan at each step with the precondition s require d for successfu l completio n of the
remainin g steps . In term s of the plan descriptio n adopte d in Chapte r 11, the require d condition s
are just the proposition s protecte d by all the causa l link s beginnin g at or befor e the curren t
step and endin g at or after it. The n we can detec t a potentia l failur e by comparin g the curren t
precondition s with the state descriptio n generate d from the percep t sequence . This is the standar d
mode l of executio n monitoring , first used by the origina l STRIP S planner . STRIP S also introduce d
the triangl e table , an efficien t representatio n for fully annotate d plans .
A secon d approac h is to chec k the precondition s of each actio n as it is executed , rathe r
than checkin g the precondition s of the entir e remainin g plan . This is calle d actio n monitoring .
As well as bein g simple r and avoidin g the need for annotations , this metho d fits in well with
realisti c system s wher e an individua l actio n failur e can be recognized . For example , if a robo t
agent issue s a comman d to the moto r subsyste m to mov e two meter s forward , the subsyste m can
repor t a failur e if the robo t bump s into an obstacl e that materialize d unexpectedly . On the othe r
hand , actio n monitorin g is less effectiv e than executio n monitoring , becaus e it does not look
ahead to see that an unexpecte d curren t state will caus e an actio n failur e some time in the future .
For example , the obstacl e that the robo t bumpe d into migh t have been knocke d off the table by
acciden t muc h earlie r in the plan . An agen t usin g executio n monitorin g coul d have realize d the
proble m and picke d it up again .
Actio n monitorin g is also usefu l whe n a goal is serendipitousl y achieved . Tha t is, if
someon e or somethin g else has alread y change d the worl d so that the goal is achieved , actio n
monitorin g notice s this and avoid s wastin g time by goin g throug h the rest of the plan .
Thes e form s of monitorin g requir e that the percept s provid e enoug h informatio n to tell
if a plan or actio n is abou t to fail. In an inaccessibl e worl d wher e the relevan t condition s are
not perceivable , mor e complicate d strategie s are neede d to cope with undetecte d but potentiall y
seriou s deviation s from expectations . This issue is beyon d the scop e of the curren t chapter .
We can divid e the cause s of plan failur e into two kinds , dependin g on whethe r it is possibl e
to anticipat e the possibl e contingencies :
0 Bounde d indeterminacy : In this case, action s can have unexpecte d effects , but the possibl e
effect s can be enumerate d and describe d as part of the actio n descriptio n axiom . For
example , the resul t of openin g a can of pain t can be describe d as the disjunctio n of havin g
paint available , havin g an empt y can, or spillin g the paint . Usin g a combinatio n of CPO P
and the "D" (disjunctive ) part of POP­DUN C we can generat e conditiona l plan s to deal
with this kind of indeterminacy .
402 Chapte r 13 . Plannin g and Actin g
UNBOUNDE D
INDETERMINAC Y<> Unbounde d indeterminacy : In this case , the set of possibl e unexpecte d outcome s is too
large to be completel y enumerated . This woul d be the case in very comple x and/o r dynami c
domain s such as driving , economi c planning , and militar y strategy . In such cases , we can
plan for at mos t a limite d numbe r of contingencies , and mus t be able to replan  whe n realit y
does not behav e as expected .
The next subsectio n describe s a simpl e metho d for replannin g base d on tryin g to get the plan
"back on track " as quickl y as possible . Sectio n 13.3 describe s a more comprehensiv e approac h
that deal s with unexpecte d condition s as an integra l part of the decision­makin g process .
Simpl e replannin g with executio n monitorin g
One approac h to replannin g base d on executio n monitorin g is show n in Figur e 13.9 . The simpl e
plannin g agen t is modifie d so that it keep s trac k of both the remainin g plan segmen t p and
the complet e plan q. Befor e carryin g out the firs t actio n of p, it check s to see whethe r the
precondition s of the/? are met. If not, it calls CHOOSE­BEST­CONTINUATIO N to choos e som e poin t
in the complet e plan q such that the plan p' from that poin t to the end of q is easies t to achiev e
from the curren t state . The new plan is to first achiev e the precondition s of/?' and then execut e it.
Conside r how REPLANNlNG­AcEN T will perfor m the task of paintin g the chai r to matc h
the table . Suppos e that the moto r subsyste m responsibl e for the paintin g actio n is imperfec t
and sometime s leave s smal l area s unpainted . The n afte r the Paint(Chair,c)  actio n is done , the
execution­monitorin g part will chec k the precondition s for the rest of the plan ; the precondition s
functio n REPLANNING­AGBNT(  percept)  return s an action
static : KB,  a knowledg e base (include s actio n descriptions )
p, an annotate d plan , initiall y NoPlan
q, an annotate d plan , initiall y NoPlan
G, a goal
TELL(/TS , MAKE­PERCEPT­SENTENCE ( percept,  t))
current*—  STATE­DESCRIPTION(XB , t)
ifp = NoPlan  then
p <— PLANNER(current,  G, KB)
q — p
if p = NoPlan  or p is empt y then retur n NoOp
if PRECONDITIONS^ ) not currentl y true in KB then
p' <— CHOOSE­BEST­CONT!NUATION(cwrr«;f , q)
p <­ APPEND(PLANNER(c«m'nf , PRECONDITIONS^') , KB),p')
q^­p
action  <— FlRST(p )
p^REST(p)
retur n action
Figur e 13.9 A n agen t that does executio n monitorin g and replanning .
Sectio n 13.3 . Full y Integrate d Plannin g and Executio n 403
are just the goal condition s becaus e the remainin g plan p is now empty , and the agen t will detec t
that the chai r is not all the sam e colo r as the table . Lookin g at the origina l plan q, the curren t
state is identica l to the preconditio n befor e the chair­paintin g step , so the agen t will now try to
paint over the bare spots . This behavio r will cycl e unti l the chai r is completel y painted .
Suppos e instea d that the agen t runs out of pain t durin g the paintin g process . Thi s is not
envisage d by the actio n descriptio n for Paint,  but it will be detecte d becaus e the chair will again
not be completel y painted . At this point , the curren t state matche s the preconditio n of the plan
beginnin g with GetPaint,  so the agen t will go off and get a new can of pain t befor e continuing .
Conside r agai n the agent' s behavio r in the first case , as it paint s and repaint s the chair .
Notic e that the behavior  is identica l to that of a conditiona l plannin g agen t runnin g the loopin g
plan show n earlier . The  difference lies  in the  time  at which  the computation  is done and  the
information  is available  to the computation  process.  Th e conditiona l plannin g agen t reason s
explicitl y abou t the possibilit y of uneve n paint , and prepare s for it even thoug h it may not occur .
The loopin g behavio r result s from a loopin g plan . The replannin g agen t assume s at plannin g
time that paintin g succeeds , but durin g executio n check s on the result s and plan s just for thos e
contingencie s that actuall y arise . The loopin g behavio r result s not from a loopin g plan but from
the interactio n betwee n actio n failure s and a persisten t replanner .
We shoul d mentio n the questio n of learnin g in respons e to faile d expectation s abou t the
result s of actions . Conside r a plan for the paintin g agen t that include s an actio n to open a door
(perhap s to the pain t store) . If the door stick s a little , the replannin g agen t will try agai n unti l the
door opens . But if the door is locked , the agen t has a problem . Of course , if the agen t alread y
know s abou t locke d doors , then Unlocked  will be a preconditio n of openin g the door , and the
agen t will have inserte d a ChecklfLocked  actio n that observe s the state of the door , and perhap s
a conditiona l branc h to fetch the key. But if the agen t does not know abou t locke d doors , it will
continu e pullin g on the door indefinitely . Wha t we woul d like to happe n is for the agen t to learn
that its actio n descriptio n is wrong ; in this case, ther e is a missin g precondition . We will see how
this kind of learnin g can take plac e in Chapte r 21.
I33FULL Y INTEGRATE D PLANNIN G AND EXECUTIO N
SITUATE D PLANNIN G
"btNTIn this section , we describ e a mor e comprehensiv e approac h to plan execution , in whic h the
plannin g and executio n processe s are full y integrated . Rathe r than thinkin g of the planne r and
executio n monito r as separat e processes , one of whic h passe s its result s to the other , we can
think of them as a singl e proces s in a situate d plannin g agent.4 The agen t is though t of as
alway s bein g part  of the way through  executin g a plan—th e gran d plan of livin g its life. Its
activitie s includ e executin g som e step s of the plan that are read y to be executed ; refinin g the plan
to resolv e any of the standar d deficiencie s (ope n conditions , potentia l clobbering , and so on);
refinin g the plan in the ligh t of additiona l informatio n obtaine d durin g execution ; and fixin g the
4 Th e wor d "situated, " whic h becam e popula r in AI in the late 1980s , is intende d to emphasiz e that the proces s of
deliberatio n take s plac e in an agen t that is directl y connecte d to an environment . In this book all the agent s are "situated, "
but the situate d plannin g agen t integrate s deliberatio n and actio n to a greate r exten t than som e of the othe r designs .
404 Chapte r 13 . Plannin g and Actin g
plan in the ligh t of unexpecte d change s in the environment , whic h migh t includ e recoverin g from
executio n error s or removin g steps that have been mad e redundan t by serendipitou s occurrences .
Obviously , whe n it first gets a new goal the agen t will have no action s read y to execute , so it
will spen d a whil e generatin g a partia l plan . It is quit e possible , however , for the agen t to begi n
executio n befor e the plan is complete , especiall y whe n it has independen t subgoal s to achieve .
The situate d agen t continuousl y monitor s the world , updatin g its worl d mode l from new percept s
even if its deliberation s are still continuing .
As in the discussio n of the conditiona l planner , we will first go throug h an exampl e and
then give the plannin g algorithm . We will keep to the formulatio n of step s and plan s used by
the partial­orde r planne r POP , rathe r than the more expressiv e language s used in Chapte r 12. It
is, of course , possibl e to incorporat e more expressiv e languages , as well as conditiona l plannin g
techniques , into a situate d planner .
The exampl e we will use is a versio n of the block s world . The star t state is show n in
Figur e 13.10(a) , and the goal is On(C,D)  A On(D,B).  The actio n we will need is Move(x,y),
whic h move s bloc k x onto bloc k y, provide d both are clear . Its actio n schem a is
Op(ACTlON:Move(x,  y),
PRECOND'.Clear(x)  A Clear(y)  A On(x,z),
EFFECT: On(x,y ) A Clear(z)  A ­^On(x,z)f\  jClear(y))
Atmt
(b) (c)
Figur e 13.1 0 Th e sequenc e of state s as the situate d plannin g agen t tries to reac h the goal state
On(C,  D) A On(D,  B) as show n in (d). The start state is (a). At (b), anothe r agen t has interfered ,
puttin g DonB.  At (c), the agen t has execute d Move(C,  D) but has failed , droppin g C on A instead .
It retrie s Move(C,D),  reachin g the goal state (d).
The agen t firs t construct s the plan show n in Figur e 13.11 . Notic e that althoug h the
precondition s of both action s are satisfie d by the initia l state , ther e is an orderin g constrain t
puttin g Move(D,B)  befor e Move(C,D).  Thi s is neede d to protec t the conditio n Clear(D)  unti l
Move(D,  B) is completed .
At this point , the plan is read y to be executed , but natur e intervenes . An externa l agen t
move s D onto B (perhap s the agent' s teache r gettin g impatient) , and the worl d is now in the state
show n in Figur e 13.10(b) . No w Clear(B)  and On(D,G)  are no longe r true in the initia l state ,
whic h is update d from the new percept . The causa l link s that were supplyin g the precondition s
Clear(B)  and On(D,  G) for the Move(D,  B) actio n becom e invalid , and mus t be remove d from the
plan. The new plan is show n in Figur e 13.12 . Notic e that two of the precondition s for Move(D,  B)
ISectio n 13.3 . Full y Integrate d Plannin g and Executio n 405
Figur e 13.1 1 Th e initia l plan constructe d by the situate d plannin g agent . The plan is indistin ­
guishable , so far, from that produce d by a norma l partial­orde r planner .
Figur e 13.1 2 Afte r someon e else moves  D onto B, the unsupporte d link s protectin g Clear(B)
and On(D,  G) are dropped , producin g this plan .
are now open , and the preconditio n On(D,y}  is now uninstantiate d becaus e ther e is no reaso n to
assum e the mov e will be from G any more .
Now the agen t can take advantag e of the "helpful " interferenc e by noticin g that the causa l
link protectin g On(D,  B) and supplie d by Move(D,  B) can be replace d by a direc t link from START .
EXTENDIN G Thi s proces s is calle d extendin g a causa l link , and is done wheneve r a conditio n can be supplie d
by an earlie r step instea d of a later one withou t causin g a new threat .
Once the old link from Move(D,  B) is removed , the step no longe r supplie s any causa l link s
REDUNDAN T STEP a t all. It is now a redundan t step . All redundan t step s are droppe d from the plan , alon g with
any link s supplyin g them . This give s us the plan show n in Figur e 13.13 .
Figur e 13.1 3 Th e link supplie d by Move(D,B)  has been replace d by one from START , and the
now­redundan t step Move(D,  B) has been dropped .
Now the step Move(C,  D) is read y to be executed , becaus e all of its precondition s are
satisfie d by the STAR T step , no othe r steps are necessaril y befor e it, and it does not threate n any
other link in the plan . The step is remove d from the plan and executed . Unfortunately , the agen t
is clums y and drop s C onto A instea d of D, givin g the state show n in Figur e 13.10(c) . The new
406 Chapte r 13 . Plannin g and Actin g
plan state is show n in Figur e 13.14 . Notic e that althoug h there are now no action s in the plan ,
there is still an open conditio n for the FINIS H step .
StartOntable(A)
On(B.E)
I On(C,A)
OnfD.B)  —
Clear(F)
Clear(C)
Clear(D)
Clear(G)OnfC.D)Finis h
Figur e 13.1 4 Afte r Move(C,D)  is execute d and remove d from the plan , the effect s of the
STAR T step reflec t the fact that C ende d up on A instea d of the intende d D. The goal preconditio n
On(C,  D) is still open .
The agen t now does the sam e plannin g operation s as a norma l planner , addin g a new
step to satisf y the open condition . Onc e again , Move(C,D)  will satisf y the goal condition . Its
precondition s are satisfie d in turn by new causa l link s from the STAR T step. The new plan appear s
in Figur e 13.15 .
StartOntable(A)
On(B,E)
I' On(C,A)
On(D.B)
Clear(F)
Clear(C)
Clear(D)
Clear(G)On(C,A)
Clear(C)
Clear(D)Move(C.D )
OnfC.D)  Finis h­OnfD.B)  rmis n
Figur e 13.15 Th e open conditio n is resolve d by addin g Move(C,D)  back in. Notic e the new
binding s for the preconditions .
Once again , Move(C,D)  is read y for execution . Thi s time it works , resultin g in the goal
state show n in Figur e 13.10(d) . Onc e the step is droppe d from the plan , the goal conditio n
On(C,  D) become s open again . Becaus e the STAR T step is update d to reflec t the new worl d state ,
however , the goal conditio n can be satisfie d immediatel y by a link from the STAR T step. Thi s
is the norma l cours e of event s whe n an actio n is successful . The fina l plan state is show n in
Figur e 13.16 . Becaus e all the goal condition s are satisfie d by the STAR T step and there are no
remainin g actions , the agen t reset s the plan and look s for somethin g else to do.
The complet e agen t desig n is show n in Figur e 13.1 7 in muc h the sam e form as used for
POP and CPOP , althoug h we abbreviat e the part in commo n (resolvin g standar d flaws) . On e
significan t structura l differenc e is that plannin g and actin g are the same "loop " as implemente d by
the couplin g betwee n agen t and environment . Afte r each plan modification , an actio n is returne d
(even if it is a NoOp)  and the worl d mode l is update d from the new percept . We assum e that each
actio n finishe s executin g befor e the next percep t arrives . To allow for extende d executio n with a .
completio n signal , an execute d actio n mus t remai n in the plan unti l it is completed .
Sectio n 13.4 . Discussio n and Extension s 407
StartOntable(A)
On(B,E)
On(C.D)  —
On(D.B)  —
Clear(F)
Clear(C)
Clear(A)
Clear(G)^On(C.D)
­On(D,B) Finis h
Figur e 13.1 6 Afte r Move(C,D)  is execute d and droppe d from the plan , the remainin g open
conditio n On(C,D)  is resolve d by addin g a causa l link from the new STAR T step. The plan is now
completed .
13.4 DISCUSSIO N AND EXTENSION S
We have arrive d at an agen t desig n that addresse s man y of the issue s arisin g in real domains :
• The agen t can use explici t domai n description s and goal s to contro l its behavior .
• By usin g partial­orde r planning , it can take advantag e of proble m decompositio n to deal
with comple x domain s withou t necessaril y sufferin g exponentia l complexity .
• By usin g the technique s describe d in Chapte r 12, it can handl e domain s involvin g condi ­
tiona l effects , universall y quantifie d effects , objec t creatio n and deletion , and ramifications .
It can also use "canne d plans " to achiev e subgoals .
• It can deal with error s in its domai n description , and, by incorporatin g conditiona l planning ,
it can plan to obtai n informatio n whe n more is needed .
• It can deal with a dynamicall y changin g worl d by incrementall y fixin g its plan as it detect s
error s and unfulfille d preconditions .
Clearly , this is progress . It is, however , a good idea to examin e each advanc e in capabilitie s and
try to see wher e it break s down .
Comparin g conditiona l plannin g and replannin g
Lookin g at conditiona l planning , we see that almos t all action s in the real worl d have a variet y of
possibl e outcome s beside s the expecte d outcome . The numbe r of possibl e condition s that mus t
be planne d for grow s exponentiall y with the numbe r of steps in the plan . Give n that only one set
of condition s will actuall y occur , this seem s rathe r wastefu l as wel l as impractical . Man y of the
event s bein g planne d for have only an infinitesima l chanc e of occurring .
Lookin g at replanning , we see that the planne r is basicall y assumin g no actio n failures , and
then fixin g problem s as they arise durin g execution . This too has its drawbacks . The planne r may
produc e very "fragile " plans , whic h are very hard to fix if anythin g goes wrong . For example ,
the entir e existenc e of "spar e tires " is a resul t of conditiona l plannin g rathe r than replanning . If
the agen t does not plan for a puncture , then it will not see the need for a spar e tire. Unfortunately ,
withou t a spar e tire, even the mos t determine d replannin g agen t migh t be face d with a long walk .
408 Chapte r 13 . Plannin g and Actin g
functio n SiTUATED­PLANNiNG­AGENT(percepf ) return s an action
static : KB,  a knowledg e base (include s actio n descriptions )
p, a plan , initiall y NoPlan
t, a counter , initiall y 0, indicatin g time
G, a goal
, MAKE­PERCEPT­SENTENCE(percep/ ,
current'—  STATE­DESCRIPTION(A:B , f)
EFFECTS(START(P) ) — current
ifp­ NoPlan  then
G <— ASK(A"B , MAKE­GOAL­QUERY(f) )
p «­ MAKE­PLAN(«/rrenf , G, KB)
action  <— NoOp  (the default )
if there are no open precondition s and p has no step s othe r than STAR T and FTNIS H then
p <— NoPlan  and skip remainin g steps
Resolving  standard  flaws:
resolv e any open conditio n by addin g a causa l link from any existin g
possibl y prior step or a new step
resolv e potentia l threat s by promotio n or demotio n
Remove  unsupported  causal  links:
if there is a causa l link STAR T c
 ; S protectin g a propositio n c
that no longe r hold s in STAR T then
remov e the link and any associate d binding s
Extend  causal  links  back  to earliest  possible  step:
if there is a causa l link 5, _ ^ S/, such that
anothe r step 5, exist s with 5, < S, and the link 5, '' , SA is safe then
replac e 5) _L, Sk with S, _1+ S*
Remove  redundant  actions:
remov e any step S that supplie s no causa l links
Execute  actions  when  ready  for execution:
if a step S in the plan othe r than FINIS H satisfie s the following :
(a) all precondition s satisfie d by START ;
(b) no othe r steps necessaril y betwee n STAR T and S; and
(c) S does not threate n any causa l link in p then
add orderin g constraint s to force all othe r steps after S
remov e S from p, and all causa l links to and from S
action  <— the actio n in 5
TELL(/fB , MAKE­ACTION­SENTENCE(acft'on , ?))
t — t+ 1
retur n action
Figur e 13.1 7 A  situate d plannin g agent .
Sectio n 13.4 . Discussio n and Extension s 409
Conditiona l plannin g and replannin g are reall y two extreme s of a continuou s spectrum .
One way to construc t intermediat e system s is to specif y disjunctiv e outcome s for action s wher e
more than one outcom e is reasonabl y likely . The n the agen t can inser t a sensin g actio n to see
whic h outcom e occurre d and construc t a conditiona l plan accordingly . Othe r contingencie s are
dealt with by replanning . Althoug h this approac h has its merits , it require s the agen t designe r
to decid e whic h outcome s need tb be considered . Thi s also mean s that the decisio n mus t be
made once for each actio n schema , rathe r than dependin g on the particula r contex t of the action .
In the case of the provisio n of spar e tires , for example , it is clear that the decisio n as to whic h
contingencie s to plan for depend s not just on the likelihoo d of occurrence—afte r all, puncture s are
quite rare—bu t also on the cost of an actio n failure . An unlikel y conditio n need s to be take n into
accoun t if it woul d resul t in catastroph e (e.g. , a punctur e whe n drivin g acros s a remot e desert) .
Even if a conditiona l plan can be constructed , it migh t be bette r to plan aroun d the suspec t actio n
altogethe r (e.g. , by bringin g two spar e tires or crossin g the deser t by camel) .
What all this suggest s is that whe n face d with a comple x domai n and incomplet e and
incorrec t information , the agen t need s a way to asses s the likelihood s and cost s of variou s
outcomes . Give n this information , it shoul d construc t a plan that maximize s the probabilit y of
succes s and minimize s costs , whil e ignorin g contingencie s that are unlikel y or are easy to deal
with . Part V of this book deal s with these issue s in depth .
Coercio n and abstractio n
Althoug h incomplet e and incorrec t informatio n is the norma l situatio n in real domains , there are
technique s that still allow an agen t to mak e quit e complex , long­rang e plan s withou t requirin g
the full apparatu s of reasonin g abou t likelihoods .
COERCIO N Th e first metho d an agen t can appl y is coercion , whic h reduce s uncertaint y abou t the worl d
by forcin g it into a know n state regardles s of the initia l state . A simpl e exampl e is provide d by
the table­paintin g problem . Suppos e that som e aspect s of the worl d are permanentl y inaccessibl e
to the agent' s senses—fo r example , it may have only a black and whit e camera . In this case, the
agent can pick up a can of pain t and pain t both the chai r and the table from the same can. Thi s
achieve s the goal and reduce s uncertainty . Furthermore , if the agen t can read the labe l on the
can, it will even know the colo r of the chai r and table .
A secon d techniqu e is abstraction . Althoug h we have discusse d abstractio n as a tool for
handlin g complexit y (see Chapte r 12), it also allow s the agen t to ignor e detail s of a proble m abou t
whic h it may not have exac t and complet e knowledge . For example , if the agen t is currentl y in
Londo n and plan s to spen d a wee k in Paris , it has a choic e as to whethe r to plan the trip at an
abstrac t level (fly out on Sunday , retur n the followin g Saturday ) or a detaile d level (take fligh t B A
216 and then taxi numbe r 1347 1 via the Boulevar d Peripherique) . At the abstrac t level , the agen t
has action s such as Fly(London,  Paris)  that are reasonabl y certai n to work . Eve n with delays ,
oversol d flights , and so on, the agen t will still get to Paris . At the detaile d level , there is missin g
informatio n (fligh t schedules , whic h taxi will turn up, Paris traffi c conditions ) and the possibilit y
of unexpecte d situation s developin g that woul d lead to a particula r fligh t bein g missed .
AGGREGATIO N Aggregatio n is anothe r usefu l form of abstractio n for dealin g with larg e number s of
objects . For example , in plannin g its cash flows , the U.S. Governmen t assume s a certai n numbe r
410 Chapte r 13 . Plannin g and Actin g
of taxpayer s will send in their tax return s by any give n date . At the level of individua l taxpayers ,
there is almos t complet e uncertainty , but at the aggregat e level , the syste m is reliable . Similarly ,
in tryin g to pour wate r from one bottl e into anothe r usin g a funnel , it woul d be hopeles s to plan
the path of each wate r molecul e becaus e of uncertaint y as well as complexity , yet the pourin g as
an aggregate d actio n is very reliable .
The discussio n of abstractio n leads naturall y to the issue of the connectio n betwee n plan s
and physica l actions . Our agen t design s have assume d that the action s returne d by the planne r
(whic h are concret e instance s of the action s describe d in the knowledg e base ) are directl y
executabl e in the environment . A mor e sophisticate d desig n migh t allo w plannin g at a highe r
level of abstractio n and incorporat e a "moto r subsystem " that can take an actio n from the plan and
generat e a sequenc e of primitiv e action s to be carrie d out. The subsyste m might , for example ,
generat e a speec h signa l from an utteranc e descriptio n returne d by the planner ; or it migh t
generat e stepper­moto r command s to turn the wheel s of a robo t to carry out a "move " actio n in a
motio n plan . We discus s the connectio n betwee n plannin g and moto r program s in more detai l in
Chapte r 25. Not e that the subsyste m migh t itsel f be a planne r of some sort; its goal is to find a
sequenc e of lower­leve l action s that achiev e the effect s of the higher­leve l actio n specifie d in the
plan. In any case , the action s generate d by the higher­leve l planne r mus t be capabl e of executio n
independentl y of each other , becaus e the lower­leve l moto r syste m canno t allow for interaction s
or interleaving s amon g the subplan s that implemen t differen t actions .
13.5 SUMMAR Y
The worl d is not a tidy place . Whe n the unexpecte d or unknow n occurs , an agen t need s to do
somethin g to get back on track . Thi s chapte r show s how conditiona l plannin g and replannin g
can help an agen t recover .
• Standar d plannin g algorithm s assum e complet e and correc t information . Man y domain s
violat e this assumption .
• Incomplet e informatio n can be deal t with usin g sensin g action s to obtai n the informatio n
needed.  Conditiona l plan s includ e differen t subplan s in differen t contexts , dependin g on
the informatio n obtained .
• Incorrec t informatio n result s in unsatisfie d precondition s for action s and plans . Executio n
monitorin g detect s violation s of the precondition s for successfu l completio n of the plan .
Actio n monitorin g detect s action s that fail.
• A simpl e replannin g agen t uses executio n monitorin g and splice s in subplan s as needed .
• A mor e comprehensiv e approac h to plan executio n involve s incrementa l modification s to
the plan , includin g executio n of steps , as condition s in the environmen t evolve .
• Abstractio n and coercio n can overcom e the uncertaint y inheren t in mos t real domains .
Sectio n 13.5 . Summar y 41 1
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
Early planners , whic h lacke d conditional s and loops , sometime s resorte d to a coerciv e style in
respons e to environmenta l uncertainty . Sacerdoti' s NOA H used coercio n in its solutio n to the
"keys and boxes " problem , a plannin g challeng e proble m in whic h the planne r know s little abou t
the initia l state . Maso n (1993 ) argue d that sensin g ofte n can and shoul d be dispense d with in
roboti c planning , and describe s a sensorles s plan that can mov e a tool into a specifi c positio n on
a table by a sequenc e of tiltin g action s regardless  of the initia l position .
WARPLAN­ C (Warren , 1976) , a varian t of WARPLAN , was one of the earlies t planner s to use
conditiona l actions . Olawsk i and Gini (1990 ) lay out the majo r issue s involve d in conditiona l
planning . Recen t system s for partial­orde r conditiona l plannin g includ e UW L (Etzion i et ai,
1992) and CNL P (Peo t and Smith , 1992) , on whic h CPO P is based) . C­BuRiDA N (Drape r et al.,
1994) handle s conditiona l plannin g for action s with probabilisti c outcomes , thereb y connectin g
to the work on Marko v decisio n problem s describe d in Chapte r 17.
There is a clos e relatio n betwee n conditiona l plannin g and automate d progra m synthesis ,
for whic h there are a numbe r of reference s in Chapte r 10. The two field s have usuall y been
pursue d separatel y becaus e of the enormou s differenc e in typica l cost betwee n executio n of
machin e instruction s and executio n of action s by robo t vehicle s or manipulators . Linde n (1991 )
attempt s explici t cross­fertilizatio n betwee n the two fields .
The earlies t majo r treatmen t of executio n monitorin g was PLANE X (Fike s et al., 1972) ,
whic h worke d with the STRIP S planne r to contro l the robo t Shakey . PLANE X used triangl e table s
to allow recover y from partia l executio n failur e withou t complet e replanning . Shakey' s mode l of
executio n is discusse d furthe r in Chapte r 25. The NAS L planne r (McDermott , 1978a ) treate d a
plannin g proble m simpl y as a specificatio n for carryin g out a comple x action , so that executio n
and plannin g were completel y unified . It used theore m provin g to reaso n abou t these comple x
actions . IPE M (Integrate d Planning , Execution , and Monitoring ) (Ambros­Ingerso n and Steel ,
1988) , whic h was the first syste m to smoothl y integrat e partial­orde r plannin g and plannin g
execution , form s the basis for the discussio n in this chapter .
A syste m that contain s an explicitl y represente d agen t function , whethe r implemente d as
a table or a set of condition­actio n rules , need not worr y abou t unexpecte d development s in the
environment . All it has to do is to execut e whateve r actio n its functio n recommend s for the
state in whic h it find s itsel f (or in the case of inaccessibl e environments , the percep t sequenc e to
REACTIV E PLANNIN G date) . The field of reactiv e plannin g aims to take advantag e of this fact, thereb y avoidin g the
complexitie s of plannin g in dynamic , inaccessibl e environments . "Universa l plans " (Schoppers ,
1987) were develope d as a schem e for reactiv e planning , but turne d out to be a rediscover y of
the idea of policie s in Marko v decisio n processes . Brooks' s (1986 ) subsumptio n architectur e
(also discusse d in Chapte r 25) uses a layere d finit e state machin e to represen t the agen t function ,
and stresse s the use of minima l interna l state . Anothe r importan t manifestatio n of the reactiv e
plannin g paradig m is Peng i (Agr e and Chapman , 1987) , designe d as a respons e to the criticis m
of classica l AI plannin g in Chapma n (1987) . Ginsber g (1989 ) mad e a spirite d attac k on reactiv e
planning , includin g intractabilit y result s for some formulation s of the reactiv e plannin g problem .
For an equall y spirite d response , see Schopper s (1989) .
412 Chapte r 13 . Plannin g and Actin g
EXERCISE S
13.1 Conside r how one migh t use a plannin g syste m to play chess .
a. Writ e actio n schemat a for legal moves . Mak e sure to includ e in the state descriptio n som e
way to indicat e whos e mov e it is. Wil l basic STRIP S action s suffice ?
b. Explai n how the opponent' s move s can be handle d by conditiona l steps .
c. Explai n how the planne r woul d represen t and achiev e the goal of winnin g the game .
d. How migh t we use the planne r to do a finite­horizo n lookahea d and pick the best move ,
rathe r than plannin g for outrigh t victory ?
e. How woul d a replannin g approac h to ches s work ? Wha t migh t be an appropriat e way to
combin e conditiona l plannin g and replannin g for chess ?
13.2 Discus s the applicatio n of conditiona l plannin g and replannin g technique s to the vacuu m
world and wumpu s world .
13.3 Represen t the action s for the flat­tir e domai n in the appropriat e format , formulat e the
initia l and goal state descriptions , and use the POP algorith m to solv e the problem .
13.4 Thi s exercis e involve s the use of POP to actually  fix a flat tire (in simulation) .
a. Buil d an environmen t simulato r for the flat­tir e world . You r simulato r shoul d be able to
updat e the state of the environmen t accordin g to the action s take n by the agent . The easies t
way to do this is to take the postcondition s directl y from the operato r description s and use
TELL and RETRAC T to updat e a logica l knowledg e base representin g the worl d state .
b. Implemen t a plannin g agen t for your environment , and show that it fixes the tire.
13.5 In this exercise , we will add nondeterminis m to the environmen t from Exercis e 13.4 .
a. Modif y you r environmen t so that with probabilit y 0.1, an actio n fails—tha t is, one of the
effect s does not occur . Sho w an exampl e of a plan not workin g becaus e of an actio n failure .
b. Modif y you r plannin g agen t to includ e a simpl e replannin g capability . It shoul d call POP
to construc t a repai r plan to get back to the desire d state alon g the solutio n path , execut e
the repai r plan (callin g itsel f recursively , of course , if the repai r plan fails) , and then
continu e executin g the origina l plan from there . (Yo u may wish to start by havin g faile d
action s do nothin g at all, so that this recursiv e repai r metho d automaticall y result s in a
"loop­until­success " behavior ; this will probabl y be easie r to debug! )
c. Sho w that your agen t can fix the tire in this new environment .
13.6 Softbot s construc t and execut e plan s in softwar e environments . On e typica l task for
softbot s is to find copie s of technica l report s that have been publishe d at som e othe r institution .
Suppos e that the softbo t is give n the task "Ge t me the mos t recen t repor t by X on topi c Y.
Relevan t action s includ e loggin g on to a librar y informatio n syste m and issuin g queries , usin g
an Interne t director y to find X's institution , sendin g emai l to X; connectin g to X's institutio n
by ftp , and so on. Writ e dow n forma l representation s for a representativ e set of actions , and
discus s wha t sort of plannin g and executio n algorithm s woul d be needed .
PartV
UNCERTAI N KNOWLEDG E AND
REASONIN G
Parts III and IV covere d the logica l agen t approac h to AI. We used first­orde r
logic as the languag e to represen t facts , and we showe d how standar d inferenc e
procedure s and plannin g algorithm s can deriv e new belief s and henc e identif y
desirabl e actions . In Part V, we reexamin e the very foundatio n of the logica l
approach , describin g how it mus t be change d to deal with the ofte n unavoidabl e
proble m of uncertai n information . Probabilit y theor y provide s the basis for our
treatmen t of system s that reaso n unde r uncertainty . Also , becaus e action s are
no longe r certai n to achiev e goals , agent s will need way s of weighin g up the
desirabilit y of goals and the likelihoo d of achievin g them . For this, we use utilit y
theory . Probabilit y theor y and utilit y theor y togethe r constitut e decisio n theory ,
whic h allow s us to build rationa l agent s for uncertai n worlds .
Chapte r 14 cover s the basic s of probabilit y theory , includin g the repre ­
sentatio n languag e for uncertai n beliefs . Belie f networks , a powerfu l tool for
representin g and reasonin g with uncertai n knowledge , are describe d in detai l
in Chapte r 15, alon g with severa l othe r formalism s for handlin g uncertainty .
Chapte r 16 develop s utilit y theor y and decisio n theor y in som e depth . Finally ,
Chapte r 17 describe s the full decision­theoreti c agen t desig n for uncertai n envi ­
ronments , thereb y generalizin g the plannin g method s of Part IV.
14UNCERTAINT Y
In which  we see  what  an agent  should  do when  not all is crystal  clear.
14.1 ACTIN G UNDE R UNCERTAINT Y
• ,44: On e proble m with first­orde r logic , and thus with the logical­agen t approach , is that agents
'**•" almost  never  have access  to the whole  truth about  their environment.  Som e sentence s can
be ascertaine d directl y from the agent' s percepts , and other s can be inferre d from curren t and
previou s percept s togethe r with knowledg e abou t the propertie s of the environment . In almos t
every case , however , even in world s as simpl e as the wumpu s worl d in Chapte r 6, ther e will
be importan t question s to whic h the agen t canno t find a categorica l answer . The agen t mus t
UNCERTAINT Y therefor e act unde r uncertainty . For example , a wumpu s agen t often will find itsel f unabl e to
discove r whic h of two square s contain s a pit. If those square s are en rout e to the gold , then the
agent migh t have to take a chanc e and ente r one of the two squares .
Uncertaint y can also arise becaus e of incompletenes s and incorrectnes s in the agent' s
understandin g of the propertie s of the environment . The qualificatio n problem , mentione d in
Chapte r 7, says that man y rule s abou t the domai n will be incomplete , becaus e ther e are too
many condition s to be explicitl y enumerated , or becaus e som e of the condition s are unknown .
Suppose , for example , that the agen t want s to driv e someon e to the airpor t to catch a flight , and
is considerin g a plan A90 that involve s leavin g hom e 90 minute s befor e the fligh t depart s and
drivin g at a reasonabl e speed . Eve n thoug h the airpor t is only abou t 15 mile s away , the agen t will
not be able to reach a definit e conclusio n such as "Pla n Ago will get us to the airpor t in time, " but
rathe r only the weake r conclusio n "Plan Ayo will get us to the airpor t in time , as long as my car
doesn' t brea k dow n or run out of gas, and I don' t get into an accident , and there are no accident s
on the bridge , and the plan e doesn' t leav e early , and there' s no earthquake , ... ."' A logica l
1 Conditiona l plannin g can overcom e uncertaint y to som e extent , but only if the agent' s sensin g action s can obtai n the
require d information , and if there are not too man y differen t contingencies .
416 Chapte r 14 . Uncertaint y
agent therefor e will not believ e that plan A9o will necessaril y achiev e the goal , and that make s it
difficul t for the logica l agen t to conclud e that plan A90 is the right thing to do.
Nonetheless , let us suppos e that A90 is in fact the right thing to do. Wha t do we mea n by
sayin g this? As we discusse d in Chapte r 2, we mean that out of all the possibl e plans that could be
executed , Ago is expecte d to maximiz e the agent' s performanc e measure , give n the informatio n it
has abou t the environment . The performanc e measur e include s gettin g to the airpor t in time for
the flight , avoidin g a long , unproductiv e wait at the airport , and avoidin g speedin g ticket s alon g
the way . The informatio n the agen t has canno t guarante e any of these outcomes  for A90, but it
can provid e some degre e of belie f that they will be achieved . Othe r plans , such as A no, migh t
increas e the agent' s belie f that it will get to the airpor t on time , but also increas e the likelihoo d of
a long wait . The  right  thing  to do, the rational  decision,  therefore,  depends  on both  the relative
importance  of various  goals  and the likelihood  that,  and  degree  to which,  they  will be  achieved.
The remainde r of this sectio n sharpen s up thes e ideas , in preparatio n for the developmen t of
the genera l theorie s of uncertai n reasonin g and rationa l decision s that we presen t in this and
subsequen t chapters .
LAZINES S
THEORETICA L
IGNORANC EHandlin g uncertai n knowledg e
In this section , we look mor e closel y at the nature  of uncertai n knowledge . We will use a
simpl e diagnosi s exampl e to illustrat e the concept s involved . Diagnosis—whethe r for medicine ,
automobil e repair , or whatever—i s a task that almos t alway s involve s uncertainty . If we tried to
build a denta l diagnosi s syste m usin g first­orde r logic , we migh t propos e rule s such as
Vp Symptom(p,  Toothache}  => Disease(p,  Cavity)
The proble m is that this rule is wrong . Not all patient s with toothache s have cavities ; som e of
them may have gum disease , or impacte d wisdo m teeth , or one of severa l othe r problems :
V p Symptom(p,  Toothache)  =>
Disease(p,  Cavity)  V Disease(p,  GumDisease)  V Disease(p,  ImpactedWisdom)...
Unfortunately , in orde r to mak e the rule true , we have to add an almos t unlimite d list of possibl e
causes . We coul d try turnin g the rule into a causa l rule :
V/? Disease(p,  Cavity)  => Symptom(p,  Toothache)
But this rule is not righ t either ; not all cavitie s caus e pain . The only way to fix the rule is to
make it logicall y exhaustive : to exten d the left­han d side to cove r all possibl e reason s why a
cavit y migh t or migh t not caus e a toothache . Eve n then , for the purpose s of diagnosis , one mus t
also take into accoun t the possibilit y that the patien t may have a toothach e and a cavit y that are
unconnected .
Tryin g to use first­orde r logi c to cope with a domai n like medica l diagnosi s thus fails for
three mai n reasons :
<> Laziness : It is too muc h work to list the complet e set of antecedent s or consequent s neede d
to ensur e an exceptionles s rule , and too hard to use the enormou s rule s that result .
0 Theoretica l ignorance : Medica l scienc e has no complet e theor y for the domain .
Sectio n 14.1 . Actin g unde r Uncertaint y 417
DEGRE E OF BELIE F
PROBABILIT Y
THEOR Y
EVIDENC E<C> Practica l ignorance : Eve n if we know all the rules , we may be uncertai n abou t a particula r
patien t becaus e all the necessar y tests have not or canno t be run.
The connectio n betwee n toothache s and cavitie s is just not a logica l consequenc e in eithe r
direction . This is typica l of the medica l domain , as well as mos t othe r judgmenta l domains : law,
business , design , automobil e repair , gardening , dating , and so on. The agent' s knowledg e can at
best provid e only a degre e of belie f in the relevan t sentences . Our main tool for dealin g with
degree s of belie f will be probabilit y theory , whic h assign s a numerica l degre e of belie f betwee n
0 and 1 to sentences.2
Probability  provides  a way  of summarizing  the uncertainty  that  comes  from  our laziness
and ignorance.  We may not know for sure wha t afflict s a particula r patient , but we believ e that
there is, say, an 80% chance—tha t is, a probabilit y of 0.8—tha t the patien t has a cavit y if he or she
has a toothache . Thi s probabilit y could be derive d from statistica l data—80 % of the toothach e
patient s seen so far have had cavities—o r from som e genera l rules , or from a combinatio n of
evidenc e sources . The 80% summarize s thos e cases in whic h all the factor s neede d for a cavit y to
cause a toothach e are present , as well as othe r cases in whic h the patien t has both toothach e and
cavit y but the two are unconnected . The missin g 20% summarize s all the othe r possibl e cause s
of toothach e that we are too lazy or ignoran t to confir m or deny .
A probabilit y of 0 for a give n sentenc e correspond s to an unequivoca l belie f that the
sentenc e is false , whil e a probabilit y of 1 correspond s to an unequivoca l belie f that the sentenc e is
true. Probabilitie s betwee n 0 and 1 correspon d to intermediat e degree s of belie f in the truth of the
sentence . The sentenc e itsel f is in fact eithe r true or false . It is importan t to note that a degre e of
belief is differen t from a degre e of truth . A probabilit y of 0.8 does not mean "80% true" but rathe r
an 80% degre e of belief—tha t is, a fairl y stron g expectation . If an agen t assign s a probabilit y of
0.8 to a sentence , then the agen t expect s that in 80% of cases that are indistinguishabl e from the
curren t situatio n as far as the agent' s knowledg e goes , the sentenc e will turn out to be actuall y
true. Thus , probabilit y theor y make s the sam e ontologica l commitmen t as logic , namely , that
facts eithe r do or do not hold in the world . Degre e of truth , as oppose d to degre e of belief , is the
subjec t of fuzz y logic , whic h is covere d in Sectio n 15.6 .
Befor e we plung e into the detail s of probability , let us paus e to conside r the statu s of prob ­
abilit y statement s such as "The probabilit y that the patien t has a cavit y is 0.8." In propositiona l
and first­orde r logic , a sentenc e is true or false dependin g on the interpretatio n and the world ; it
is true just whe n the fact it refer s to is the case . Probabilit y statement s do not have quit e the sam e
kind of semantics.3 This is becaus e the probabilit y that an agen t assign s to a propositio n depend s
on the percept s that it has receive d to date . In discussin g uncertai n reasoning , we call this the
evidence . For example , suppos e that the agen t has draw n a card from a shuffle d pack . Befor e
lookin g at the card , the agen t migh t assig n a probabilit y of 1/52 to its bein g the ace of spades .
After lookin g at the card , an appropriat e probabilit y for the sam e propositio n woul d be 0 or 1.
Thus , an assignmen t of probabilit y to a propositio n is analogou s to sayin g whethe r or not a give n
logica l sentenc e (or its negation ) is entaile d by the knowledg e base , rathe r than whethe r or not it
2 Unti l recently , it was though t that probabilit y theor y was too unwield y for genera l use in AI, and man y approximation s
and alternative s to probabilit y theor y were proposed . Som e of these will be covere d in Sectio n 15.6 .
3 Th e objectivist  view of probability , however , claim s that probabilit y statement s are true or false in the sam e way as
logica l sentences . In Sectio n 14.5 , we discus s this claim further .
418 Chapte r 14 . Uncertaint y
is true . Jus t as entailmen t statu s can chang e whe n mor e sentence s are adde d to the knowledg e
base, probabilitie s can chang e whe n mor e evidenc e is acquired.4
All probabilit y statement s mus t therefor e indicat e the evidenc e with respec t to whic h the
probabilit y is bein g assessed . As the agen t receive s new percepts , its probabilit y assessment s
are update d to reflec t the new evidence . Befor e the evidenc e is obtained , we talk abou t prio r or
unconditiona l probability ; after the evidenc e is obtained , we talk abou t posterio r or conditiona l
probability . In mos t cases , an agen t will hav e som e evidenc e from its percepts , and will be
intereste d in computin g the conditiona l probabilitie s of the outcome s it care s abou t give n the
evidenc e it has. In som e cases , it will also need to comput e conditiona l probabilitie s with respec t
to the evidenc e it has plus the evidenc e it expect s to obtai n durin g the cours e of executin g some
sequenc e of actions .
PREFERENCE S
UTILIT Y THEOR YUncertaint y and rationa l decision s
The presenc e of uncertaint y change s radicall y the way in whic h an agen t make s decisions . A
logica l agen t typicall y has a singl e (possibl y conjunctive ) goal , and execute s any plan that is
guarantee d to achiev e it. An actio n can be selecte d or rejecte d on the basi s of whethe r or not it
achieve s the goal , regardles s of wha t othe r action s achieve . Whe n uncertaint y enter s the picture ,
this is no longe r the case . Conside r agai n the Ago plan for gettin g to the airport . Suppos e it has a
95% chanc e of succeeding . Doe s this mea n it is a rationa l choice ? Obviously , the answe r is "Not
necessarily. " Ther e migh t be othe r plans , such as A120, with highe r probabilitie s of success . If it
is vita l not to miss the flight , then it migh t be wort h riskin g the longe r wait at the airport . Wha t
abou t A1440 , a plan that involve s leavin g hom e 24 hour s in advance ? In mos t circumstances , this
is not a good choice , becaus e althoug h it almos t guarantee s gettin g ther e on time , it involve s an
intolerabl e wait .
To mak e such choices , an agen t mus t first have preference s betwee n the differen t possibl e
outcome s of the variou s plans . A particula r outcom e is a completel y specifie d state , includin g
such factor s as whethe r or not the agen t arrive s in time , and the lengt h of the wait at the airport .
We will be usin g utilit y theor y to represen t and reaso n with preferences . The term utilit y is
used here in the sens e of "the qualit y of bein g useful, " not in the sens e of the electri c compan y
or wate r works . Utilit y theor y says that ever y state has a degre e of usefulness , or utility , to an
agent , and that the agen t will prefe r state s with highe r utility .
The utilit y of a state is relativ e to the agen t whos e preference s the utilit y functio n is suppose d
to represent . For example , the payof f function s for game s in Chapte r 5 are utilit y functions . The
utilit y of a state in whic h Whit e has won a gam e of ches s is obviousl y high for the agen t playin g
White , but low for the agen t playin g Black . Or again , som e player s (includin g the authors ) migh t
be happ y with a draw agains t the worl d champion , wherea s othe r player s (includin g the forme r
worl d champion ) migh t not. Ther e is no accountin g for taste or preferences : you migh t think
that an agen t who prefer s jalapefio­bubble­gu m ice crea m to chocolate­chocolate­chi p is odd or
even misguided , but you coul d not say the agen t is irrational .
4 Thi s is quit e differen t from a sentenc e becomin g true or fals e as the worl d changes . Handlin g a changin g worl d
usin g probabilitie s require s the sam e kind s of mechanisms—situations , interval s and events—a s we used in Chapte r o
for logica l representations .
Sectio n 14.1 . Actin g unde r Uncertaint y 419
It is also interestin g that utilit y theor y allow s for altruism . It is perfectl y consisten t for
an agen t to assig n high utilit y to a state wher e the agen t itsel f suffer s a concret e loss but other s
profit . Here , "concret e loss" mus t denot e a reductio n in "persona l welfare " of the kind normall y
associate d with altruis m or selfishness—wealth , prestige , comfort , and so on—rathe r than a loss
of utilit y per se. Therefore , utilit y theor y is necessaril y "selfish " only if one equate s a preferenc e
for the welfar e of other s with selfishness ; conversely , altruis m is only inconsisten t with the
principl e of utilit y maximizatio n if one's goals do not includ e the welfar e of others .
Preferences , as expresse d by utilities , are combine d with probabilitie s in the genera l theor y
DECISIO N THEOR Y o f rationa l decision s calle d decisio n theory :
Decisio n theor y = probabilit y theor y + utilit y theor y
The fundamenta l idea of decisio n theor y is that an agent is  rational  if and only  if it chooses
\ t: the  action  that  yields  the highest  expected  utility,  averaged  over  all the possible  outcomes of
the action.  Thi s is calle d the principl e of Maximu m Expecte d Utilit y (MEU) . Probabilitie s
and utilitie s are therefor e combine d in the evaluatio n of an actio n by weightin g the utilit y of a
particula r outcom e by the probabilit y that it occurs . We saw this principl e in actio n in Chapte r 5,
wher e we examine d optima l decision s in backgammon . We will see that it is in fact a completel y
genera l principle .
Desig n for a decision­theoreti c agen t
The structur e of an agen t that uses decisio n theor y to selec t action s is identical , at an abstrac t
level , to that of the logica l agen t describe d in Chapte r 6. Figur e 14.1 show s wha t need s to be
done . In this chapte r and the next , we will concentrat e on the task of computin g probabilitie s for
curren t state s and for the variou s possibl e outcome s of actions . Chapte r 16 cover s utilit y theor y
in more depth , and Chapte r 17 fleshe s out the complet e agen t architecture .
functio n DT­AGENT( 'percept ) return s an action
static : a set probabilisti c belief s abou t the state of the worl d
calculat e update d probabilitie s for curren t state base d on
availabl e evidenc e includin g curren t percep t and previou s actio n
calculat e outcom e probabilitie s for actions ,
given  actio n description s and probabilitie s of curren t state s
selec t action  with highes t expecte d utilit y
given probabilitie s of outcome s and utilit y informatio n
retur n action
Figur e 14.1 A  decision­theoreti c agen t that select s rationa l actions . The step s will be fleshe d
out in the next four chapters .
420 Chapte r 14 . Uncertaint y
14.2 BASI C PROBABILIT Y NOTATIO N
Now that we have set up the genera l framewor k for a rationa l agent , we will need a forma l languag e
for representin g and reasonin g with uncertai n knowledge . Any notatio n for describin g degree s of
belie f must be able to deal with two main issues : the natur e of the sentence s to whic h degree s of
belie f are assigned , and the dependenc e of the degre e of belie f on the agent' s state of knowledge .
The versio n of probabilit y theor y we presen t uses an extensio n of propositiona l logi c for its
sentences . The dependenc e on experienc e is reflecte d in the syntacti c distinctio n betwee n prior
probabilit y statements , whic h appl y befor e any evidenc e is obtained , and conditiona l probabilit y
statements , whic h includ e the evidenc e explicitly .
UNCONDITIONA L
PRIO R PROBABILIT Y
RANDO M VARIABLE S
DOMAI NPrior probabilit y
We will use the notatio n P(A)  for the unconditiona l or prio r probabilit y that the propositio n A
is true . For example , if Cavity  denote s the propositio n that a particula r patien t has a cavity ,
P(Cavity)  = O.I
mean s that in the absence  of any other  information,  the agen t will assig n a probabilit y of 0.1
(a 10% chance ) to the even t of the patient's  havin g a cavity . It is importan t to remembe r that
P(A) can only be used whe n ther e is no othe r information . As soon  as som e new informatio n
B is known , we have to reaso n with the conditiona l probabilit y of A give n B instea d of P(A).
Conditiona l probabilitie s are covere d in the next section .
The propositio n that is the subjec t of a probabilit y statemen t can be represente d by a
propositio n symbol , as in the P(A)  example . Proposition s can also includ e equalitie s involvin g
so­calle d rando m variables . Fo r example , if we are concerne d abou t the rando m variabl e
Weather,  we migh t have
P( Weather  = Sunny)  = 0.7
P(Weather  = Rain)  = 0.2
P(Weather=  Cloudy)  = 0.08
P(Weather  = Snow)  = 0.02
Each rando m variabl e X has a domai n of possibl e value s (x\,...,x n) that it can take on.5 We will
usuall y deal with discret e sets of values , althoug h continuou s rando m variable s will be discusse d
briefl y in Chapte r 15. We can view propositio n symbol s as rando m variable s as well , if we
assum e that they have a domai n [true,false).  Thus , the expressio n P(Cavity)  can be viewe d as
shorthan d for P(Cavity  = true).  Similarly , P(­>Cavity)  is shorthan d for P(Cavity  =false).  Usually ,
we will use the letter s A, B, and so on for Boolea n rando m variables , and the letter s X, Y, and so
on for multivalue d variables .
Sometimes , we will wan t to talk abou t the probabilitie s of all the possibl e value s of a
rando m variable . In this case , we will use an expressio n such as P(Weather),  whic h denote s a
5 In probability , the variable s are capitalized , whil e the value s are lowercase . This is unfortunatel y the revers e of logica l
notation , but it is the tradition .
Sectio n 14.2 . Basi c Probabilit y Notatio n 421
PROBABILIT Y
DISTRIBUTIO Nvector  of value s for the probabilitie s of each individua l state of the weather . Give n the precedin g
values , for example , we woul d write
P(Weather)  = (0.7,0.2,0.08,0.02 )
This statemen t define s a probabilit y distributio n for the rando m variabl e Weather.
We will also use expressions ' such as P(Weather,  Cavity)  to denot e the probabilitie s of all
combination s of the value s of a set of rando m variables . In this case , P(Weather,  Cavity)  denote s
a 4 x 2 table of probabilities . We will see that this notatio n simplifie s man y equations .
We can also use logica l connective s to make more comple x sentence s and assig n probabil ­
ities to them . For example ,
P(Cavity  A ­^Insured)  ­ 0.06
says there is an 6% chanc e that a patien t has a cavit y and has no insurance .
CONDITIONA L
POSTERIO R
PRODUC T RUL EConditiona l probabilit y
Once the agen t has obtaine d som e evidenc e concernin g the previousl y unknow n proposition s
makin g up the domain , prior probabilitie s are no longe r applicable . Instead , we use conditiona l
or posterio r probabilities , with the notatio n P(A\B).  This is read as "the probabilit y of A give n
that all we  know  is B." For example ,
P(Cavity\Toothache)  ­ 0.8
indicate s that if a patien t is observe d to have a toothache , and no othe r informatio n is yet available ,
then the probabilit y of the patien t havin g a cavit y will be 0.8. It is importan t to remembe r that
P(A\B)  can only be used whe n all we know is B. As soon as we know C, then we mus t comput e
P(A\B  A C) instea d of P(A\B).  A prior probabilit y P(A)  can be though t of as a specia l case of
conditiona l probabilit y P(A\ ), wher e the probabilit y is conditione d on no evidence .
We can also use the P notatio n with conditiona l probabilities . P(X\  Y) is a two­dimensiona l
table givin g the value s of P(X=x,\Y  = yj) for each possibl e /, j. Conditiona l probabilitie s can be
define d in term s of unconditiona l probabilities . The equatio n
P(A A B)P(A\B)  =P(B)(14.1 )
holds wheneve r P(B)  > 0. This equatio n can also be writte n as
P(A A B) = P(A\B}P(B)
whic h is calle d the produc t rule . The produc t rule is perhap s easie r to remember : it come s from
the fact that for A and B to be true, we need B to be true , and then A to be true give n B. We can
also have it the othe r way around :
P(A /\B)  = P(B\A)P(A)
In som e cases , it is easie r to reaso n in term s of prio r probabilitie s of conjunctions , but for the
most part, we will use conditiona l probabilitie s as our vehicl e for probabilisti c inference .
We can also exten d our P notatio n to handl e equation s like these , providin g a welcom e
degre e of conciseness . For example , we migh t write
P(X,Y)  = P(X\Y)P(Y)
422 Chapte r 14 . Uncertaint y
whic h denote s a set of equation s relatin g the correspondin g individua l entrie s in the table s (not  a
matri x multiplicatio n of the tables) . Thus , one of the equation s migh t be
In general , if we are intereste d in the probabilit y of a propositio n A, and we have accu ­
mulate d evidenc e B, then the quantit y we mus t calculat e is P(A\B).  Sometime s we will not
have this conditiona l probabilit y availabl e directl y in the knowledg e base , and we mus t resor t to
probabilisti c inference , whic h we describ e in later sections .
As we have alread y said, probabilisti c inferenc e does not work like logica l inference . It
is temptin g to interpre t the statemen t P(A\B)  ­ 0.8 to mean  "wheneve r B is true , conclud e that
P(A) is 0.8." Thi s is wron g on two counts : first , P(A)  alway s denote s the prio r probabilit y of
A, not the posterio r give n som e evidence ; second , the statemen t P(A\B)  = 0.8 is only applicabl e
when B is the only availabl e evidence . Whe n additiona l informatio n C is available , we mus t
calculat e P(A\B  A C), whic h may bear little relatio n to P(A\B).  In the extrem e case , C migh t
tell us directl y whethe r A is true or false . If we examin e a patien t who complain s of toothache ,
and discove r a cavity , then we have additiona l evidenc e Cavity,  and we conclud e (trivially ) that
P(Cavity\Toothache  A Cavity)  = 1 .0.
14.3 TH E AXIOM S OF PROBABILIT Y
In orde r to defin e properl y the semantic s of statement s in probabilit y theory , we will need to
describ e how probabilitie s and logica l connective s interact . We take as give n the propertie s of
the connective s themselves , as define d in Chapte r 6. As for probabilities , it is norma l to use a
small set of axiom s that constrai n the probabilit y assignment s that an agen t can mak e to a set of
propositions . The followin g axiom s are in fact sufficient :
1. All probabilitie s are betwee n 0 and 1.
0 < P(A)  < 1
2. Necessaril y true (i.e. , valid ) proposition s have probabilit y 1, and necessaril y false (i.e.,
unsatisfiable ) proposition s have probabilit y 0.
P(True)  = 1 P(False)  = 0
3. The probabilit y of a disjunctio n is give n by
P(A V 5) = P(A)  + P(B)  ­ P(A A B)
The firs t two axiom s serv e to defin e the probabilit y scale . Th e third is best remembere d by
referenc e to the Ven n diagra m show n in Figur e 14.2 . The figur e depict s each propositio n as a
set, whic h can be though t of as the set of all possibl e world s in whic h the propositio n is true . The
total probabilit y of A V B is seen to be the sum of the probabilitie s assigne d to A and B, but with
P(A A B) subtracte d out so that thos e case s are not counte d twice .
From thes e three axioms , we can deriv e all othe r propertie s of probabilities . For example ,
if we let B be ­iA in the last axiom , we obtai n an expressio n for the probabilit y of the negatio n of
Sectio n 14.3 . Th e Axiom s of Probabilit y 423
True
A A  /\ B
™B
|
P
Figur e 14.2 A  Venn diagra m showin g the proposition s A, B, A V B (the unio n of A and B), and
A A B (the intersectio n of A and B) as sets of possibl e worlds .
a propositio n in term s of the probabilit y of the propositio n itself :
P(AV­.A ) = P(A ) + P(­.A ) ­ P(A A ­­A) (by 3 with B = ­.A)
P(True}  = P(A ) + P(­Vl ) ­ P(False)  (b y logica l equivalence )
1 = P(A)  + P(­.A ) (b y 2)
P(­Vl ) = 1 ­ P(A) (b y algebra )
Why the axiom s of probabilit y are reasonabl e
The axiom s of probabilit y can be seen as restrictin g the set of probabilisti c belief s that an agen t can
hold. This is somewha t analogou s to the logica l case, wher e a logica l agen t canno t simultaneousl y
believ e A, B, and ­i(A A B), for example . Ther e is, however , an additiona l complication . In the
logica l case , the semanti c definitio n of conjunctio n mean s that at least one of the three belief s
just mentione d must be  false  in the  world,  so it is unreasonabl e for an agen t to believ e all three .
With probabilities , on the othe r hand , statement s refer not to the worl d directly , but to the agent' s
own state of knowledge . Why , then , can an agen t not hold the followin g set of beliefs , give n that
these probabilit y assignment s clearl y violat e the third axiom ?
P(A)
P(B)
P(A A B)
P(A V B)0.4
0.3
0.0
0.8(14.2 )
This kind of questio n has been the subjec t of decade s of intens e debat e betwee n thos e who
advocat e the use of probabilitie s as the only legitimat e form for degree s of belief , and thos e who
advocat e alternativ e approaches . Here , we give one argumen t for the axiom s of probability , first
stated in 1931 by Brun o de Finetti .
424 Chapte r 14 . Uncertaint y
The key to de Finetti' s argumen t is the connectio n betwee n degre e of belie f and actions .
The idea is that if an agen t has som e degre e of belie f in a proposition , A, then the agen t shoul d
be able to state odds at whic h it is indifferen t to a bet for or agains t A. Thin k of it as a gam e
betwee n two agents : Agen t 1 state s "my degre e of belie f in even t A is 0.4." Agen t 2 is then free
to choos e whethe r to bet for or agains t A, at stake s that are consisten t with the state d degre e of
belief . Tha t is, Agen t 2 coul d choos e to bet that A will occur , bettin g $4 agains t Agen t 1's $6.
Or Agen t 2 coul d bet $6 agains t $4 that A will not occur.6 If an agent' s degree s of belie f do not
accuratel y reflec t the world , then you woul d expec t it woul d tend to lose mone y over the long
run, dependin g on the skill of the opposin g agent .
But de Finett i prove d somethin g muc h stronger : if Agent  1 expresses  a set of degrees  of
belief  that violate the axioms of probability theory then there is a betting strategy for Agent 2 that
guarantee s that Agent  1 will lose money.  So if you accep t the idea that an agen t shoul d be willin g
to "put its mone y wher e its probabilitie s are," then you shoul d accep t that it is irrationa l to have
belief s that violat e the axiom s of probability .
One migh t think that this bettin g game is rathe r contrived . For example , wha t if one refuse s
to bet? Doe s that scuppe r the whol e argument ? The answe r is that the bettin g gam e is an abstrac t
mode l for the decision­makin g situatio n in whic h ever y agen t is unavoidably  involve d at ever y
moment . Ever y actio n (includin g inaction ) is a kind of bet, and ever y outcom e can be seen as a
payof f of the bet. One can no more refus e to bet than one can refus e to allow time to pass .
We will not provid e the proo f of de Finetti' s theore m (see Exercis e 14.15) , but we will
show an example . Suppos e that Agen t 1 has the set of degree s of belie f from Equatio n (14.2) . If
Agen t 2 choose s to bet $4 on A, $3 on B, and $2 on ­i(A V B), then Figur e 14.3 show s that Agen t
1 alway s loses money , regardles s of the outcome s for A and B.
Agen t
Propositio n
A
B
AV B1
Belie f
0.4
0.3
0.8Bet
A
B
­>(A VAgen t 2
Stake s
4 to 6
3 to 7
B) 2  to 8Outcom e
A A B A  A ­.B
­6
­7
2
­11­6
3
2
­1for Agen t 1
­.A AS ­
4
­7
2
­1
Figur e 14.3 Becaus e Agen t 1 has inconsisten t beliefs , Agen t 2 is able to devis e a set
that guarantee s a loss for Agen t 1 , no matte r wha t the outcom e of A and B.­nAA­i B
4
3
­8
­1
of bets
Othe r stron g philosophica l argument s have been put forwar d for the use of probabilities ,
most notabl y thos e of Cox (1946 ) and Carna p (1950) . The worl d bein g the way it is, however ,
practica l demonstration s sometime s spea k loude r than proofs . The succes s of reasonin g system s
based on probabilit y theor y has been muc h mor e effectiv e in makin g converts . We now look at
how the axiom s can be deploye d to mak e inferences .
6 On e migh t argu e that the agent' s preference s for differen t bank balance s are such that the possibilit y of losin g $1 is
not counterbalance d by an equa l possibilit y of winnin g $1. We can mak e the bet amount s smal l enoug h to avoi d this
problem , or we can use the mor e sophisticate d treatmen t due to Savag e (1954 ) to circumven t this issu e altogether .
Sectio n 14.3 . Th e Axiom s of Probabilit y 425
JOINT PROBABILIT Y
DISTRIBUTIO N
ATOMI C EVEN TThe joint probabilit y distributio n
In this section , we defin e the joint probabilit y distributio n (or "joint " for short) , whic h com ­
pletel y specifie s an agent' s probabilit y assignment s to all proposition s in the domai n (both simple
and complex) .
A probabilisti c mode l of a'domai n consist s of a set of rando m variable s that can take on
particula r value s with certai n probabilities . Let the variable s be X\ ... Xn. An atomi c even t is an
assignmen t of particula r value s to all the variables—i n othe r words , a complet e specificatio n of
the state of the domain .
The join t probabilit y distributio n P(X],.. . ,Xn) assign s probabilitie s to all possibl e atomi c
events . Recal l that P(X, ) is a one­dimensiona l vecto r of probabilitie s for the possibl e value s of
the variabl e X,­. The n the join t is an w­dimensiona l table with a valu e in ever y cell givin g the
probabilit y of that specifi c state occurring . Her e is a join t probabilit y distributio n for the trivia l
medica l domai n consistin g of the two Boolea n variable s Toothache  and Cavity:
Cavity
^CavityToothache
0.04
0.01^Toothache
0.06
0.89
Becaus e the atomi c event s are mutuall y exclusive , any conjunctio n of atomi c event s is necessaril y
false. Becaus e they are collectivel y exhaustive , their disjunctio n is necessaril y true. Hence , from
the secon d and third axiom s of probability , the entrie s in the table sum to 1. In the sam e way ,
the join t probabilit y distributio n can be used to comput e any probabilisti c statemen t we care to
know abou t the domain , by expressin g the statemen t as a disjunctio n of atomi c event s and addin g
up their probabilities . Addin g acros s a row  or colum n give s the unconditiona l probabilit y of a
variable , for example , P(Cavity)  = 0.06 + 0.04 = 0.10 . As anothe r example :
P(Cavity  V Toothache) =  0.04 + 0.01 + 0.06 = 0.11
Recal l that we can mak e inference s abou t the probabilitie s of an unknow n propositio n
A, give n evidenc e B, by calculatin g P(A\B).  A quer y to a probabilisti c reasonin g syste m will
therefor e ask for the valu e of a particula r conditiona l probability . Conditiona l probabilitie s can
be foun d from the join t usin g Equatio n (14.1) :
P(Cavity  A Toothache)  0.0 4P(Cavity\Toothache)  = = 0.80P(Toothache)  0.0 4 + 0.01
Of course , in a realisti c problem , there migh t be hundred s or thousand s of rando m variable s
to consider , not just two . In genera l it is not practica l to defin e all the 2" entrie s for the join t
probabilit y distributio n over « Boolea n variables , but it is importan t to remembe r that if we coul d
defin e all the numbers , then we coul d read off any probabilit y we were intereste d in.
Moder n probabilisti c reasonin g system s sideste p the join t and wor k directl y with condi ­
tiona l probabilities , whic h are after all the value s that we are intereste d in. In the next section ,
we introduc e a basic tool for this task .
426 Chapte r 14 . Uncertaint y
14.4 BAYES ' RUL E AND ITS USE
Recal l the two form s of the produc t rule:
P(Af\B)=P(A\B)P(B)  *
P(A f\B)=  P(B\A)P(A)
Equatin g the two right­han d sides and dividin g by P(A),  we get
w  = «p  (14 ,)
BAYE S RULE Thi s equatio n is know n as Bayes ' rule (also Bayes ' law or Bayes ' theorem).7 Thi s simpl e
equatio n underlie s all moder n AI system s for probabilisti c inference . The more genera l case of
multivalue d variable s can be writte n usin g the P notatio n as follows :
P(X\Y)P(Y)P(Y\X)  =
P(X)
wher e again this is to be taken as representin g a set of equation s relatin g correspondin g element s
of the tables . We will also have occasio n to use a more genera l versio n conditionalize d on some
backgroun d evidenc e E:
P(X\Y,E)P(Y\E)  (M4 )
The proo f of this form is left as an exercise .
Applyin g Bayes ' rule : The simpl e case
On the surface , Bayes ' rule does not seem very useful . It require s three terms— a conditiona l
probabilit y and two unconditiona l probabilities—jus t to comput e one conditiona l probability .
Bayes ' rule is usefu l in practic e becaus e ther e are man y case s wher e we do have good
probabilit y estimate s for these three number s and need to comput e the fourth . In a task such
as medica l diagnosis , we ofte n have conditiona l probabilitie s on causa l relationship s and wan t
to deriv e a diagnosis . A docto r know s that the diseas e meningiti s cause s the patien t to have a
stiff neck , say, 50% of the time . The docto r also know s som e unconditiona l facts : the prior
probabilit y of a patien t havin g meningiti s is 1/50,000 , and the prio r probabilit y of any patien t
havin g a stiff neck is 1/20 . Lettin g S be the propositio n that the patien t has a stiff neck and M be
the propositio n that the patien t has meningitis , we have
P(S\M)  = 0.5
P(M) = 1/5000 0
P(S) = 1/20
P(M\S)  =P(S\M)P(M)  0. 5 x 1/5000 0
P(S) 1/20= 0.000 2
7 Accordin g to rule 1 on page 1 of Strun k and White' s The  Elements  of Style,  it shoul d be Bayes' s rathe r than Bayes' .
The latte r is, however , more commonl y used .
ISectio n 14.4 . Bayes ' Rule and Its Use 427
That is, we expec t only one in 5000 patient s with a stiff neck to have meningitis . Notic e that
even thoug h a stiff neck is strongl y indicate d by meningiti s (probabilit y 0.5), the probabilit y of
meningiti s in the patien t remain s small . Thi s is becaus e the prior on stiff neck s is muc h highe r
than that for meningitis .
One obviou s questio n to ask is why one migh t have availabl e the conditiona l probabilit y
in one directio n but not the other . In the meningiti s case, perhap s the docto r know s that 1 out
of 5000 patient s with stiff neck s has meningitis , and therefor e has no need to use Bayes ' rule .
Unfortunately , diagnostic  knowledge  is often more  tenuous than  causal  knowledge.  If there is a
sudde n epidemi c of meningitis , the unconditiona l probabilit y of meningitis , P(M),  will go up.
The docto r who derive d P(M\S)  from statistica l observatio n of patient s befor e the epidemi c will
have no idea how to updat e the value , but the docto r who compute s P(M\S)  from the othe r three
value s will see that P(M\S)  shoul d go up proportionatel y to P(M}.  Mos t importantly , the causa l
informatio n P(S\M)  is unaffected  by the epidemic , becaus e it simpl y reflect s the way meningiti s
works . The use of this kind of direc t causa l or model­base d knowledg e provide s the crucia l
robustnes s neede d to mak e probabilisti c system s feasibl e in the real world .
RELATIV E
LIKELIHOO DNormalizatio n
Conside r agai n the equatio n for calculatin g the probabilit y of meningiti s give n a stiff neck :
P(S\M)P(M)P(M\S)  =
P(S)
Suppos e we are also concerne d with the possibilit y that the patien t is sufferin g from whiplas h W
given a stiff neck :
P(S\W)P(W)P(W\S)  =
P(S)
Comparin g thes e two equations , we see that in order  to comput e the relativ e likelihoo d of
meningiti s and whiplash , give n a stiff neck , we need not asses s the prior probabilit y P(S)  of a stiff
neck. To put number s on the equations , suppos e that P(S\W)  = 0.8 and P(W)  = 1/1000 . Then
P(M\S)  _ P(S\M)P(M)  _ 0.5 x 1/5000 0 _ J_
P(W\S)  ~ P(S\W)P(W)  ~ 0.8 x 1/100 0 ~ 80
That is, whiplas h is 80 time s more likel y than meningitis , given a stiff neck .
In som e cases , relativ e likelihoo d is sufficien t for decisio n making , but when , as in this
case, the two possibilitie s yield radicall y differen t utilitie s for variou s treatmen t actions , one need s
exact value s in orde r to mak e rationa l decisions . It is still possibl e to avoi d direc t assessmen t of
the prior probabilit y of the "symptoms, " by considerin g an exhaustiv e set of cases . For example ,
we can writ e equation s for M and for ­iM:
P(S\M)P(M)P(M\S)  =P(S)
P(S)
Addin g thes e two equations , and usin g the fact that P(M\S)  + P(­^M\S)  = 1, we obtai n
P(S) = P(S\M)P(M)  ­
428 Chapte r 14 . Uncertaint y
Substitutin g into the equatio n for P(M\S),  we have
P(S\M)P(M)P(M\S)  =P(S\M)P(M)
NORMALIZATIO N Thi s proces s is calle d normalization , becaus e it treat s 1/P(S)  as a normalizin g constan t that
allow s the conditiona l term s to sum to 1. Thus , in retur n for assessin g the conditiona l probabilit y
P(S\­<M),  we can avoi d assessin g P(S)  and still obtai n exac t probabilitie s from Bayes ' rule . In
the general , multivalue d case, we obtai n the followin g form for Bayes ' rule :
P(Y\X)  = aP(X\Y)P(Y)
wher e a is the normalizatio n constan t neede d to mak e the entrie s in the table P(Y|X ) sum to 1.
The norma l way to use normalizatio n is to calculat e the unnormalize d values , and then scale them
all so that they add to 1 (Exercis e 14.7) .
BAYESIA N UPDATIN GUsing Bayes ' rule : Combinin g evidenc e
Suppos e we have two conditiona l probabilitie s relatin g to cavities :
P(Cavity\Toothache)  = 0.8
P(Cavity\  Catch)  = 0.95
whic h migh t perhap s have been compute d usin g Bayes ' rule . Wha t can a dentis t conclud e if her
nasty steel prob e catche s in the achin g tooth of a patient ? If we knew the whol e joint distribution ,
it woul d be easy to read off P(Cavity\Toothache  A Catch).  Alternatively , we coul d use Bayes '
rule to reformulat e the problem :
„ , P(Toothache/\  Catch\Cavity)P(Cavity)P(Cavity  Toothache  A Catch) =  ————————————!——J—­————^' P(Toothache  A Catch)
For this to work , we need to know the conditiona l probabilitie s of the pair Toothache  A Catch
given Cavity.  Althoug h it seem s feasibl e to estimat e conditiona l probabilitie s (give n Cavity)  for
n differen t individua l variables , it is a dauntin g task to com e up with number s for n2 pair s of
variables . To mak e matter s worse , a diagnosi s may depen d on dozen s of variables , not just two.
That mean s we need an exponentia l numbe r of probabilit y value s to complet e the diagnosis —
we migh t as wel l go back to usin g the joint . Thi s is wha t first led researcher s awa y from
probabilit y theor y towar d approximat e method s for evidenc e combinatio n that , whil e givin g
incorrec t answers , requir e fewe r number s to give any answe r at all.
In man y domains , however , the applicatio n of Bayes ' rule can be simplifie d to a form
that require s fewe r probabilitie s in orde r to produc e a result . The first step is to take a slightl y
differen t view of the proces s of incorporatin g multipl e piece s of evidence . Th e proces s of
Bayesia n updatin g incorporate s evidenc e one piec e at a time , modifyin g the previousl y held
belief in the unknow n variable . Beginnin g with Toothache,  we have (writin g Bayes ' rule in such
a way as to revea l the updatin g process) :
P(Toothache\  Cavity)P(Cavity\Toothache)  = P(Cavity)­P(Toothache)
Sectio n 14.4 . Bayes ' Rul e and Its Use 429
CONDITIONA L
INDEPENDENC EWhe n Catch  is observed , we can appl y Bayes ' rule with Toothache  as the constan t conditionin g
contex t (see Exercis e 14.5) :
„._, ,  .  P(Catch\Toothache  A Cavity)P(Cavity\Toothache  A Catch)  = P(Cavity\Toothache)­  '
= P(Cavity)P(Catch\Toothache)
P(Toothache\Cavity)  P(Catch\Toothache  A Cavity)
P(Toothache)  P(Catch\Toothache)
Thus , in Bayesia n updating , as each new piece of evidenc e is observed , the belie f in the unknow n
variabl e is multiplie d by a facto r that depend s on the new evidence . Exercis e 14.8 asks you to
prove that this proces s is order­independent , as we woul d hope .
So far we are not out of the woods , becaus e the multiplicatio n facto r depend s not just
on the new evidence , but also on the evidenc e alread y obtained. . Findin g a valu e for the nu­
merator , P(Catch\Toothache  A Cavity),  is not necessaril y any easie r than findin g a valu e for
P(Toothache  A Catch\Cavity).  We will need to mak e a substantiv e assumptio n in orde r to sim­
plify our expressions . The key observation , in the cavit y case , is that the cavit y is the direct  cause
of both the toothach e and the prob e catchin g in the tooth . Onc e we know the patien t has a cavity ,
we do not expec t the probabilit y of the prob e catchin g to depen d on the presenc e of a toothache ;
similarly , the prob e catchin g is not goin g to chang e the probabilit y that the cavit y is causin g a
toothache . Mathematically , thes e propertie s are writte n as
P(Catch\Cavity/\  Toothache)  = P(Catch\Cavity)
P(Toothache\Cavity  A Catch)  ­ P(Toothache\Cavity)
These equation s expres s the conditiona l independenc e of Toothache  and Catch  give n Cavity.
Give n conditiona l independence , we can simplif y the equatio n for updating :
P(Toothache\Cavity)  P(Catch\Cavity)
P(CaVity\Toothacher,  Catch)  = P(Cav lty) ^J ^  P(Catch \Toothache}
There is still the term P(Catch\Toothache),  whic h migh t seem to involv e considerin g all pair s
(triples , etc. ) of symptoms , but in fact this term goes away . Notic e that the produc t of the
denominator s is P(Catch\Toothache)P(Toothache),  or P(Toothache  A Catch).  W e can elimi ­
nate this term by normalization , as before , provide d we also asses s P(Toothache\^Cavity)  and
P(Catch\­>Cavity).  Thus , we are back wher e we were with a singl e piec e of evidence : we just
need to evaluat e the prior for the cause , and the conditiona l probabilitie s of each of its effects .
We can also use conditiona l independenc e in the multivalue d case . To say that X and Y are
independen t give n Z, we writ e
P(X\Y,Z)  = P(X\Z)
whic h represent s a set of individua l conditiona l independenc e statements . The correspondin g
simplificatio n of Bayes ' rule for multipl e evidenc e is
P(Z\X,  Y) = aP(Z)P(X\Z)P(Y\Z)
wher e a is a normalizatio n constan t such that the entrie s in P(Z|X , Y) sum to 1.
It is importan t to remembe r that this simplifie d form of Bayesia n updatin g only work s
when the conditiona l independenc e relationship s hold . Conditiona l independenc e informatio n
therefor e is crucia l to makin g probabilisti c system s wor k effectively . In Chapte r 15, we show
how it can be represente d and manipulate d in a systemati c fashion .
430 Chapte r 14 . Uncertaint y
14.5 WHER E Do PROBABILITIE S COM E FROM ?
FREQUENTIS T Ther e has been endles s debat e over the sourc e and statu s of probabilit y numbers . The frequentis t
positio n is that the number s can eom e only from experiments:  if we test 100 peopl e and find that
10 of them  have a cavity , then we can say the probabilit y of a cavit y is approximatel y 0.1. A
OBJECTIVIS T grea t deal of work has gone into makin g such statistica l assessment s reliable . The objectivis t
view is that probabilitie s are real aspect s of the universe—propensitie s of object s to behav e in
certai n ways—rathe r than bein g just description s of an observer' s degre e of belief . In this view ,
SUBJECTIVIS T frequentis t measurement s are attempt s to observ e the real probabilit y value . The subjectivis t
view describe s probabilitie s as a way of characterizin g an agent' s beliefs , rathe r than havin g any
externa l physica l significance . Thi s allow s the docto r or analys t to mak e thes e number s up, to
say, "In my opinion , I expec t the probabilit y of a cavit y to be abou t 0.1." Severa l mor e reliabl e
techniques , such as the bettin g system s describe d earlier , have also been develope d for elicitin g
probabilit y assessment s from humans .
In the end, even a stric t frequentis t positio n involve s subjectiv e analysis , so the differenc e
probabl y has little practica l importance . Conside r the probabilit y that the sun will still exist
tomorro w (a questio n first raise d by Hume' s Inquiry).  Ther e are severa l way s to comput e this:
• The probabilit y is undefined , becaus e there has neve r been an experimen t that teste d the
existenc e of the sun tomorrow.
• The probabilit y is 1, becaus e in all the experiment s that have been done (on past days ) the
sun has existed .
• The probabilit y is 1 — e, wher e e is the proportio n of stars in the univers e that go supernov a
and explod e per day.
• The probabilit y is (d + 1 )/(d +  2), wher e d is the numbe r of days that the sun has existe d so
far. (Thi s formul a is due to Laplace. )
• The probabilit y can be derive d from the type , age, size , and temperatur e of the sun, even
thoug h we have neve r observe d anothe r star with thos e exac t properties .
The first three of these method s are frequentist , wherea s the last two are subjective . But even if
you prefe r not to allow subjectiv e methods , the choic e of whic h of the first three experiment s to
REFERENC E CLAS S use  is a subjectiv e choic e know n as the referenc e clas s problem . It is the  same proble m faced
by the docto r who want s to know the chance s that a patien t has a particula r disease . The docto r
want s to conside r othe r patient s who are simila r in importan t ways—age , symptoms , perhap s
sex—an d see wha t proportio n of them had the disease . But if the docto r considere d everythin g
that is know n abou t the patient—weigh t to the neares t gram , hair color , materna l grandmothe r s
maide n name—th e resul t woul d be that there are no othe r patient s who are exactl y the same , and
thus no referenc e class from whic h to collec t experimenta l data . This has been a vexin g proble m
in the philosoph y of science . Carna p (alon g with othe r philosophers ) tried in vain to find a way
of reducin g theorie s to objectiv e truth—t o show how a serie s of experiment s necessaril y lead s
to one theor y and not another . The approac h we will take in the next chapte r is to minimiz e the
numbe r of probabilitie s that need to be assessed , and to maximiz e the numbe r of cases availabl e
for each assessment , by takin g advantag e of independenc e relationship s in the domain .
Sectio n 14.6 . Summar y 431
j_4.6 SUMMAR Y ___________________________ _
This chapte r show s that probabilit y is the righ t way to reaso n abou t uncertainty .
• Uncertaint y arise s becaus e of both lazines s and ignorance . It is inescapabl e in complex ,
dynamic , or inaccessibl e worlds .
• Uncertaint y mean s that man y of the simplification s that are possibl e with deductiv e infer ­
ence are no longe r valid .
• Probabilitie s expres s the agent' s inabilit y to reach a definit e decisio n regardin g the truth of
a sentence , and summariz e the agent' s beliefs .
• Basi c probabilit y statement s includ e prio r probabilitie s and conditiona l probabilitie s
over simpl e and comple x propositions .
• The axiom s of probabilit y specif y constraint s on reasonabl e assignment s of probabili ­
ties to propositions . An agen t that violate s the axiom s will behav e irrationall y in som e
circumstances .
• The joint probabilit y distributio n specifie s the probabilit y of each complet e assignmen t
of value s to rando m variables . It is usuall y far too large to creat e or use.
• Bayes ' rule allows  unknow n probabilitie s to be compute d from known , stabl e ones .
• In the genera l case , combinin g man y piece s of evidenc e may requir e assessin g a large
numbe r of conditiona l probabilities .
• Conditiona l independenc e brough t abou t by direc t causa l relationship s in the domai n
allow s Bayesia n updatin g to work effectivel y even with multipl e piece s of evidence .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
Althoug h game s of chanc e date back at least to aroun d 300 B.C. , the mathematica l analysi s of
odds and probabilit y appear s to be muc h later . Som e wor k done by Mahaviracary a in Indi a
is date d to roughl y the nint h centur y A.D . In Europe , the first attempt s date only to the Italia n
Renaissance , beginnin g aroun d 1500 A.D. The first significan t systemati c analyse s were produce d
by Girolam o Cardan o aroun d 1565 , but they remaine d unpublishe d unti l 1663 . By that time , the
discover y by Blais e Pasca l (in correspondenc e with Pierr e Ferma t in 1654 ) of a systemati c way
of calculatin g probabilitie s had for the first time establishe d probabilit y as a widel y and fruitfull y
studie d mathematica l discipline . The first publishe d textboo k on probabilit y was De Ratiociniis
in Ludo  Aleae  (Huygens , 1657) . Pasca l also introduce d conditiona l probability , whic h is covere d
in Huygens' s textbook . The Rev. Thoma s Baye s (1702­1761 ) introduce d the rule for reasonin g
abou t conditiona l probabilitie s that was name d after him. It was publishe d posthumousl y (Bayes ,
1763) . Kolmogoro v (1950 , firs t publishe d in Germa n in 1933 ) presente d probabilit y theor y
in a rigorousl y axiomati c framewor k for the first time . Reny i (1970 ) later gave an axiomati c
presentatio n that took conditiona l probability , rathe r than absolut e probability , as primitive .
432 Chapte r 14 . Uncertaint y
CONFIRMATIO N
INDUCTIV E LOGI CPasca l used probabilit y in way s that require d both the objectiv e interpretation , as a propert y
of the worl d base d on symmetr y or relativ e frequency , and the subjectiv e interpretatio n as degre e
of belief : the forme r in his analyse s of probabilitie s in game s of chance , the latte r in the famou s
"Pascal' s wager " argumen t abou t the possibl e existenc e of God . However , Pasca l did not clearl y
realiz e the distinctio n betwee n thes e two interpretations . The distinctio n was first draw n clearl y
by Jame s Bernoull i (1654­1705) .
Leibni z introduce d the "classical " notio n of probabilit y as a proportio n of enumerated ,
equall y probabl e cases , whic h was also used by Bernoulli , althoug h it was brough t to prominenc e
by Laplac e (1749­1827) . Thi s notio n is ambiguou s betwee n the frequenc y interpretatio n and
the subjectiv e interpretation . The case s can be though t to be equall y probabl e eithe r becaus e of
a natural , physica l symmetr y betwee n them , or simpl y becaus e we do not have any knowledg e
that woul d lead us to conside r one more probabl e than another . The use of this latter , subjectiv e
consideratio n to justif y assignin g equa l probabilitie s is know n as the principle  of indifference
(Keynes , 1921) .
The debat e betwee n objectiv e and subjectiv e interpretation s of probabilit y becam e sharpe r
in the twentiet h century . Kolmogoro v (1963) , R. A. Fishe r (1922) , and Richar d von Mises(1928 )
were advocate s of the relativ e frequenc y interpretation . Kar l Popper' s (1959 , first publishe d in
Germa n in 1934 ) "propensity " interpretatio n trace s relativ e frequencie s to an underlyin g physica l
symmetry . Fran k Ramse y (1931) , Brun o de Finett i (1937) , R. T. Cox (1946) , Leonar d Savag e
(1954) , and Richar d Jeffre y (1983 ) interprete d probabilitie s as the degree s of belie f of specifi c
individuals . Thei r analyse s of degre e of belie f were closel y tied to utilitie s and to behavior ,
specificall y to the willingnes s to plac e bets . Rudol f Carnap , followin g Leibni z and Laplace ,
offere d a differen t kind of subjectiv e interpretatio n of probability : not as any actua l individual' s
degre e of belief , but as the degre e of belie f that an idealize d individua l should  have in a particula r
proposition/ ? give n a particula r body of evidenc e E. Carna p attempted  to go furthe r than Leibni z
or Laplac e by makin g this notio n of degre e of confirmatio n mathematicall y precise , as a logica l
relatio n betwee n p and E. The stud y of this relatio n was intende d to constitut e a mathematica l
disciplin e calle d inductiv e logic , analogou s to ordinar y deductiv e logic (Carnap , 1948 ; Carnap ,
1950) . Carna p was not able to exten d his inductiv e logic muc h beyon d the prepositiona l case,
and Putna m (1963 ) showe d that som e fundamenta l difficultie s woul d preven t a stric t extensio n
to language s capabl e of expressin g arithmetic .
The questio n of referenc e classe s is closel y tied to the attemp t to find an inductiv e logic . The
approac h of choosin g the "mos t specific " referenc e class of sufficien t size was formall y propose d
by Reichenbac h (1949) . Variou s attempt s have been mad e to formulat e mor e sophisticate d
policie s in orde r to avoi d som e obviou s fallacie s that arise with Reichenbach' s rule , notabl y by
Henr y Kybur g (1977 ; 1983) , but such approache s remai n somewha t ad hoc.  Mor e recen t work
by Bacchus , Grove , Halpern , and Rolle r (1992 ) extend s Carnap' s method s to first­orde r theorie s
on finit e domains , thereb y avoidin g man y of the difficultie s associate d with referenc e classes .
Bayesia n probabilisti c reasonin g has been used in AI since the 1960s , especiall y in medica l
diagnosis . It was used not only to mak e a diagnosi s from availabl e evidence , but also to selec t
furthe r question s and tests whe n availabl e evidenc e was inconclusiv e (Gorry , 1968 ; Gorr y et
al, 1973) , usin g the theor y of informatio n valu e (Sectio n 16.6) . On e syste m outperforme d
huma n expert s in the diagnosi s of acut e abdomina l illnesse s (de Domba l et al., 1974) . Thes e
early Bayesia n system s suffere d from a numbe r of problems , however . Becaus e they lacke d any
Sectio n 14.6 . Summar y 433
theoretica l mode l of the condition s they were diagnosing , they were vulnerabl e to unrepresentativ e
data occurrin g in situation s for whic h only a smal l sampl e was availabl e (de Domba l et al., 1981) .
Even mor e fundamentally , becaus e they lacke d a concis e formalis m (suc h as the one to be
describe d in Chapte r 15) for representin g and usin g conditiona l independenc e information , they
depende d on the acquisition , storage , and processin g of enormou s amount s of probabilisti c data.
De Dombal' s system , for example , was built by gatherin g and analyzin g enoug h clinica l case s
to provid e meaningfu l data for ever y entr y in a large joint probabilit y table . Becaus e of these
difficulties , probabilisti c method s for copin g with uncertaint y fell out of favo r in AI from the
1970s to the mid­1980s . In Chapte r 15, we will examin e the alternativ e approache s that were
taken and the reaso n for the resurgenc e of probabilisti c method s in the late 1980s .
There are man y good introductor y textbook s on probabilit y theory , includin g thos e by
Chun g (1979 ) and Ros s (1988) . Morri s DeGroo t (1989 ) offer s a combine d introductio n to
probabilit y and statistic s from a Bayesia n standpoint , as well as a more advance d text (1970) .
Richar d Hamming' s (1991 ) textboo k give s a mathematicall y sophisticate d introductio n to prob ­
abilit y theor y from the standpoin t of a propensit y interpretatio n base d on physica l symmetry .
Hackin g (1975 ) and Hald (1990 ) cove r the early histor y of the concep t of probability .
EXERCISE S
14.1 Sho w from first principle s that
P(A|BAA) = 1
14.2 Conside r the domai n of dealin g five­car d poke r hand s from a standar d deck of 52 cards ,
unde r the assumptio n that the deale r is fair.
a. How man y atomi c event s are ther e in the join t probabilit y distributio n (i.e., how man y
five­car d hand s are there) ?
b. Wha t is the probabilit y of each atomi c event ?
c. Wha t is the probabilit y of being deal t a roya l straigh t flush (the ace, king , queen , jack and
ten of the same suit) ?
d. Wha t is the probabilit y of four of a kind ?
14.3 Afte r your yearl y checkup , the docto r has bad new s and good news . The bad new s is that
you teste d positiv e for a seriou s disease , and that the test is 99% accurat e (i.e., the probabilit y of
testin g positiv e give n that you have the diseas e is 0.99 , as is the probabilit y of testin g negativ e
given that you don' t have the disease) . The good new s is that this is a rare disease , strikin g only
one in 10,00 0 people . Wh y is it good new s that the diseas e is rare? Wha t are the chance s that
you actuall y have the disease ?
14.4 Woul d it be rationa l for an agen t to hold the thre e belief s P(A)  = OA, P(B)  = 0.3, and
P(A V B) = 0.5?  If so, wha t rang e of probabilitie s woul d be rationa l for the agen t to hold for
A A Bl Mak e up a tabl e like the one in Figur e 14.3 and show how it support s you r argumen t
434 Chapte r 14 . Uncertaint y
abou t rationality . The n draw anothe r versio n of the table wher e P(A V B) = 0.7. Explai n why it
is rationa l to have this probability , even thoug h the table show s one case that is a loss and three
that just brea k even . (Hint:  wha t is Agen t 1 committe d to abou t the probabilit y of each of the
four cases , especiall y the case that is a loss? )
14.5 It is quit e ofte n usefu l to conside r the effec t of some specifi c proposition s in the contex t
of some genera l backgroun d evidenc e that remain s fixed , rathe r than in the complet e absenc e of
information . The followin g question s ask you to prov e more genera l version s of the produc t rule
and Bayes ' rule , with respec t to som e backgroun d evidenc e E:
a. Prov e the conditionalize d versio n of the genera l produc t rule:
P(A,B\E)  = P(A\B,E)P(B\E)
b. Prov e the conditionalize d versio n of Bayes ' rule :
P(5|A,C)P(A|C )P(A|B,C ) =P(B\C)
14.6 Sho w that the statemen t
P(A,B\C)  = P(A\C)P(B\C)
is equivalen t to the statemen t
P(A|fi,C ) = P(A|C )
and also to
P(fi|A,C ) = P(fi|C )
14.7 In this exercise , you will complet e the normalizatio n calculatio n for the meningiti s exam ­
ple. First , mak e up a suitabl e valu e for P(S|­>A/) , and use it to calculat e unnormalize d value s for
P(M\S)  and P(­iM|S ) (i.e., ignorin g the P(S)  term in the Bayes ' rule expression) . Now normaliz e
these value s so that they add to 1.
14.8 Sho w that the degre e of belie f after applyin g the Bayesia n updatin g proces s is independen t
of the orde r in whic h the piece s of evidenc e arrive . Tha t is, show that P(A\B,  C) = P(A\C,B)
using the Bayesia n updatin g rule.
14.9 Thi s exercis e investigate s the way in whic h conditiona l independenc e relationship s affec t
the amoun t of informatio n neede d for probabilisti c calculations .
a. Suppos e we wish to calculat e P(H\E\,E 2), and we have no conditiona l independenc e
information . Whic h of the followin g sets of number s are sufficien t for the calculation ?
(i) P(£i ,Ei), P(H),  P(£, \H), P(E 2\H)
(ii) P(£ , ,E 2), P(H),  P(£, , E2 \H)
(iii) P(H),P(E,\H),P(E 2\H)
b. Suppos e we know that P(E } \H,E 2) = P(£, \H) for all value s of H, E\, E2. Now whic h of
the abov e thre e sets are sufficient ?
Sectio n 14.6 . Summar y 43 5
14.10 Expres s the statemen t that X and Y are conditionall y independen t give n Z as a constrain t
on the joint distributio n entrie s for P(X,  Y, Z).
14.11 (Adapte d from Pear l (1988). ) Yo u are a witnes s of a night­tim e hit­and­ru n acciden t
involvin g a taxi in Athens . All taxi s in Athen s are blue or green . You swear , unde r oath , that
the taxi was blue. Extensiv e testin g show s that unde r the dim lightin g conditions , discriminatio n
betwee n blue and gree n is 75% reliable . Is it possibl e to calculat e the mos t likel y colo r for the
taxi? (Hint : distinguis h carefull y betwee n the propositio n that the taxi is blue and the propositio n
that it appears  blue. )
What now , give n that 9 out of 10 Athenia n taxis are green ?
14.12 (Adapte d from Pear l (1988). ) Thre e prisoners , A, B, and C, are locke d in their cells . It
is commo n knowledg e that one of them will be execute d the next day and the other s pardoned .
Only the governo r know s whic h one will be executed . Prisone r A asks the guar d a favor : "Pleas e
ask the governo r who will be executed , and then take a messag e to one of my friend s B and C to
let him know that he will be pardone d in the morning. " The guar d agrees , and come s back later
and tells A that he gave the pardo n messag e to B.
Wha t are As chance s of bein g executed , give n this information ? (Answe r this mathemati­
cally,  not by energeti c wavin g of hands. )
14.13 Thi s exercis e concern s Bayesia n updatin g in the meningiti s example . Startin g with a
patien t abou t who m we know nothing , show how the probabilit y of havin g meningitis , P(M),  is
update d after we find the patien t has a stiff neck . Next , show how P(M)  is update d agai n whe n
we find the patien t has a fever . (Say wha t probabilitie s you need to comput e this, and mak e up
value s for them. )
14.14 In previou s chapters , we foun d the techniqu e of reiflcatio n usefu l in creatin g represen ­
tation s in first­orde r logic . For example , we handle d chang e by reifyin g situations , and belie f by
reifyin g sentences . Suppos e we try to do this for uncertai n reasonin g by reifyin g probabilities ,
thus embeddin g probabilit y entirel y within  first­orde r logic . Whic h of the followin g are true ?
a. Thi s woul d not work .
b. Thi s woul d work fine ; in fact, it is just anothe r way of describin g probabilit y theory .
c. Thi s woul d work fine; it woul d be an alternativ e to probabilit y theory .
14.15 Prov e that the thre e axiom s of probabilit y are necessar y for rationa l behavio r in bettin g
situations , as show n by de Finetti .
15PROBABILISTI C
REASONIN G SYSTEM S
In which  we explain how  to build  reasoning systems  that  use network  models  to
reason  with uncertainty according to the laws of probability theory.
Chapte r 14 gav e the synta x and semantic s of probabilit y theory . Thi s chapte r introduce s an
inferenc e mechanism , thus givin g us everythin g we need to build an uncertain­reasonin g system .
The mai n advantag e of probabilisti c reasonin g over logica l reasonin g is in allowin g the
agent to reach rationa l decision s even whe n there is not enoug h informatio n to prov e that any
given actio n will work . We begi n by showin g how to captur e uncertai n knowledg e in a natura l
and efficien t way. We then show how probabilisti c inference , althoug h exponentiall y hard in the
wors t case, can be done efficientl y in man y practica l situations . We conclud e the chapte r with a
discussio n of knowledg e engineerin g technique s for buildin g probabilisti c reasonin g systems , a
case stud y of one successfu l system , and a surve y of alternat e approaches .
15.1 REPRESENTIN G KNOWLEDG E IN AN UNCERTAI N DOMAI N
In Chapte r 14, we saw that the join t probabilit y distributio n can answe r any questio n abou t
the domain , but can becom e intractabl y large as the numbe r of variable s grows . Furthermore ,
specifyin g probabilitie s for atomi c event s is rathe r unnatura l and may be very difficul t unles s a
large amoun t of data is availabl e from whic h to gathe r statistica l estimates .
We also saw that, in the contex t of using Bayes ' rule, conditiona l independenc e relationship s
amon g variable s can simplif y the computatio n of quer y result s and greatl y reduc e the numbe r
of conditiona l probabilitie s that need to be specified . We use a data structur e calle d a belie f
BELIE F NETWOR K network ' to represen t the dependenc e betwee n variable s and to give a concis e specificatio n of
the joint probabilit y distribution . A belie f networ k is a grap h in whic h the followin g holds :
1 Thi s is the mos t commo n name , but ther e are man y others , includin g Bayesia n network , probabilisti c network ,
causa l network , and knowledg e map . An extensio n of belie f network s calle d a decisio n networ k or influenc e diagra m
will be covere d in Chapte r 16.
436
Sectio n 15.1 . Representin g Knowledg e in an Uncertai n Domai n 437
1. A set of rando m variable s make s up the node s of the network .
2. A set of directe d link s or arrow s connect s pair s of nodes . The intuitiv e meanin g of an
arrow from node X to node Y is that X has a direct  influence  on Y.
3. Eac h nod e has a conditiona l probabilit y tabl e that quantifie s the effect s that the parent s
have on the node . The parent s of a node are all thos e node s that have arrow s pointin g to it.
4. The grap h has no directe d cycle s (henc e is a directed , acycli c graph , or DAG) .
It is usuall y easy for a domai n exper t to decid e wha t direc t conditiona l dependenc e relationship s
hold in the domain—muc h easier , in fact, than actuall y specifyin g the probabilitie s themselves .
Once the topolog y of the belie f networ k is specified , we need only specif y conditiona l probabil ­
ities for the node s that participat e in direc t dependencies , and use thos e to comput e any othe r
probabilit y values .
Conside r the followin g situation . You have a new burgla r alarm installe d at home . It is
fairly reliabl e at detectin g a burglary , but also respond s on occasio n to mino r earthquakes . (Thi s
exampl e is due to Jude a Pearl , a residen t of Los Angeles ; henc e the acut e interes t in earthquakes. )
You also have two neighbors , John and Mary , who have promise d to call you at work whe n they
hear the alarm . John alway s calls whe n he hear s the alarm , but sometime s confuse s the telephon e
ringin g with the alarm and calls then , too. Mary , on the othe r hand , likes rathe r loud musi c and
sometime s misse s the alarm altogether . Give n the evidenc e of who has or has not called , we
woul d like to estimat e the probabilit y of a burglary . This simpl e domai n is describe d by the belie f
networ k in Figur e 15.1 .
The topolog y of the networ k can be though t of as an abstrac t knowledg e base that hold s
in a wid e variet y of differen t settings , becaus e it represent s the genera l structur e of the causa l
processe s in the domai n rathe r than any detail s of the populatio n of individuals . In the case of the
burglar y network , the topolog y show s that burglar y and earthquake s directl y affec t the probabilit y
of the alarm goin g off, but whethe r or not John and Mar y call depend s only on the alarm—th e
networ k thus represent s our assumptio n that they do not perceiv e any burglarie s directly , and they
do not feel the mino r earthquakes .
Figur e 15.1 A  typica l belie f network .
438 Chapte r 15 . Probabilisti c Reasonin g System s
CONDITIONA L
PROBABILIT Y TABL E
CONDITIONIN G CAS ENotic e that the networ k does not have node s correspondin g to Mar y currentl y listenin g to
loud music , or to the telephon e ringin g and confusin g John . Thes e factor s are summarize d in
the uncertaint y associate d with the link s from Alarm  to JohnCalls  and MaryCalls.  Thi s show s
both lazines s and ignoranc e in operation : it woul d be a lot of work to determin e any reaso n why
those factor s woul d be more or less likel y in any particula r case, and we have no reasonabl e way
to obtai n the relevan t informatio n anyway . The probabilitie s actuall y summariz e a potentially
infinite  set of possibl e circumstance s in whic h the alarm migh t fail to go off (hig h humidity ,
powe r failure , dead battery , cut wires , dead mous e stuc k insid e bell,... ) or John or Mary migh t
fail to call and repor t it (out to lunch , on vacation , temporaril y deaf , passin g helicopter , ...). In
this way , a smal l agen t can cope with a very large world , at least approximately . The degre e of
approximatio n can be improve d if we introduc e additiona l relevan t information .
Once we have specifie d the topology , we need to specif y the conditiona l probabilit y table
or CPT for each node . Eac h row in the table contain s the conditiona l probabilit y of each node
value for a conditionin g case . A conditionin g case is just a possibl e combinatio n of value s for
the paren t node s (a miniatur e atomi c event , if you like) . For example , the conditiona l probabilit y
table for the rando m variabl e Alarm  migh t look like this:
Burglar y Earthquak e
True True
True False
False  True
False  FalseP(Alarm\Burglary,  Earthquake)
True False
0.950 0.05 0
0.950 0.05 0
0.290 0.71 0
0.001 0.99 9
Each row in a conditiona l probabilit y table mus t sum to 1, becaus e the entrie s represen t an
exhaustiv e set of cases for the variable . Henc e only one of the two number s in each row show n
above is independentl y specifiable . In general , a table for a Boolea n variabl e with n Boolea n
parent s contain s 2" independentl y specifiabl e probabilities . A node with no parent s has only one
row, representin g the prior probabilitie s of each possibl e valu e of the variable .
The complet e networ k for the burglar y exampl e is show n in Figur e 15.2 , wher e we show
just the conditiona l probabilit y for the True  case of each variable .
15.2 TH E SEMANTIC S OF BEEIE F NETWORK S
The previou s sectio n describe d wha t a networ k is, but not wha t it means . Ther e are two way s
in whic h one can understan d the semantic s of belie f networks . The first is to see the networ k as
a representatio n of the join t probabilit y distribution . The secon d is to view it as an encodin g of
a collectio n of conditiona l independenc e statements . The two view s are equivalent , but the first
turns out to be helpfu l in understandin g how to construct  networks , wherea s the secon d is helpfu l
in designin g inferenc e procedures .
Sectio n 15.2 . Th e Semantic s of Belie f Network s 439
Figur e 15.2 A  typica l belie f networ k with conditiona l probabilities . The letters  B, E, A, J, and
M stand for Burglary,  Earthquake,  Alarm,  JohnCalls,  and Mary  Calls,  respectively . All variable s
(nodes ) are Boolean , so the probabilit y of, say, ­>P(A)  in any row of its table is 1 — P(A).
Representin g the joint probabilit y distributio n
A belie f networ k provide s a complet e descriptio n of the domain . Ever y entr y in the join t
probabilit y distributio n can be calculate d from the informatio n in the network . A generi c entry
in the join t is the probabilit y of a conjunctio n of particula r assignment s to each variable , such
as P(X] =x\ A ... A Xn =*„) . We use the notatio n P(x\, ..., xn) as an abbreviatio n for this. The
value of this entry is give n by the followin g formula :
P(x }, (15.1 ) xn) = [[P(x,\Parents(Xiy)
i=i
Thus , each entr y in the join t is represente d by the produc t of the appropriat e element s of the
conditiona l probabilit y table s (CPTs ) in the belie f network . Th e CPT s therefor e provid e a
decompose d representatio n of the joint .
To illustrat e this, we can calculat e the probabilit y of the even t that the alarm has sounde d
but neithe r a burglar y nor an earthquak e has occurred , and both John and Mar y call. We use
single­lette r name s for the variables :
P(J A M A A A ­iB A ­.£)
= P(J\A)P(M\A)P(A\^B  A ­i£)P(­nB)P(­.£ )
= 0.90 x 0.70 x 0.00 1 x 0.99 9 x 0.99 8 = 0.0006 2
Sectio n 14.3 explaine d that the join t distributio n can be used to answe r any quer y abou t
the domain . If a belie f networ k is a representatio n of the joint , then it too can be used to answe r
any query . Trivially , this can be done by first computin g all the joint entries . We will see belo w
that there are muc h bette r methods .
440 Chapte r 15 . Probabilisti c Reasonin g System s
A metho d for constructin g belie f network s
Equatio n (15.1 ) define s wha t a given belie f networ k means . It does not, however , explai n how to
construct  a belie f networ k such that the resultin g joint distributio n is a good representatio n of a
given domain . We will now show that Equatio n (15.1 ) implie s certai n conditiona l independenc e
relationship s that can be used to guid e the knowledg e enginee r in constructin g the topolog y of
the network . First , we rewrit e the joint in term s of a conditiona l probabilit y usin g the definitio n
of conditiona l probability :
P(xt, ...,x n) = P(x n\xn­i, ...,xi)P(x n_\, ...,x\)
Then we repea t this process , reducin g each conjunctiv e probabilit y to a conditiona l probabilit y
and a smalle r conjunction . We end up with one big product :
P(x\ , = P(x n P(x 2\xi)P(x\)
Comparin g this with Equatio n (15.1) , we see that the specificatio n of the joint is equivalen t to the
genera l assertio n that
= P(X i\Parents(X i)) (15.2 )
provide d that Parents(Xf)  C {*,_] , ..., x\}.  Thi s last conditio n is easil y satisfie d by labellin g
the node s in any orde r that is consisten t with the partia l orde r implici t in the grap h structure .
What the precedin g equatio n says is that the belie f networ k is a correc t representatio n of the
domai n only if each node is conditionall y independen t of its predecessor s in the node ordering ,
given its parents . Hence , in orde r to construc t a belie f networ k with the correc t structur e for the
domain , we need to choos e parent s for each node such that this propert y holds . Intuitively , the
parent s of node X,­ shoul d contai n all thos e node s in X\, ..., X/_ i that directl y influenc e X/. For
example , suppos e we have complete d the networ k in Figur e 15.1 excep t for the choic e of parent s
for MaryCalls.  MaryCalls  is certainl y influence d by whethe r or not ther e is a Burglary  or an
Earthquake,  but it is not directly  influenced . Intuitively , our knowledg e of the domai n tells us that
these event s only influenc e Mary' s callin g behavio r throug h their effec t on the alarm . Also , given
the state of the alarm , whethe r or not John calls has no influenc e on Mary' s calling . Formall y
speaking , we believ e that the followin g conditiona l independenc e statemen t holds :
P(MaryCalls\  JohnCalls,  Alarm,  Earthquake, Burglary)  = P(MaryCalls\Alarm)
The genera l procedur e for incrementa l networ k constructio n is as follows :
1. Choos e the set of relevan t variable s X, that describ e the domain .
2. Choos e an orderin g for the variables .
3. Whil e there are variable s left:
(a) Pick a variabl e X,­ and add a node to the networ k for it.
(b) Set Parents(Xi)  to som e minima l set of node s alread y in the net such that the condi ­
tiona l independenc e propert y (15.2 ) is satisfied .
(c) Defin e the conditiona l probabilit y table for X,­.
Sectio n 15.2 . Th e Semantic s of Belie f Network s 441
Becaus e each node is only connecte d to earlie r nodes , this constructio n metho d guarantee s that
the networ k is acyclic . Anothe r importan t propert y of belie f network s is that they contai n no
redundan t probabilit y values , excep t perhap s for one entr y in each row of each conditiona l
probabilit y table . Thi s mean s that it is impossible  for the knowledge engineer  or domain  expert
to create a  belief  network  that violates  the axioms of  probability.  We will see example s of the
applicatio n of the constructio n metho d in the next section .
LOCALL Y
STRUCTURE D
SPARS ECompactnes s and node orderin g
As well as bein g a complet e and nonredundan t representatio n of the domain , a belie f networ k can
often be far more compact  than the full joint . This propert y is what  make s it feasibl e to handl e
a large numbe r of piece s of evidenc e withou t the exponentia l growt h in conditiona l probabilit y
value s that we saw in the discussio n of Bayesia n updatin g in Sectio n 14.4 .
The compactnes s of belie f network s is an exampl e of a very genera l propert y of locall y
structure d (also calle d sparse ) systems . In a locall y structure d system , each subcomponen t
interact s directl y with only a bounde d numbe r of othe r components , regardles s of the tota l
numbe r of components . Loca l structur e is usuall y associate d with linea r rathe r than exponentia l
growt h in complexity . In the case of belie f networks , it is reasonabl e to suppos e that in mos t
domain s each rando m variabl e is directl y influence d by at mos t k others , for some constan t k. If
we assum e Boolea n variable s for simplicity , then the amoun t of informatio n neede d to specif y the
conditiona l probabilit y table for a node will be at mos t 2k numbers , so the complet e networ k can
be specifie d by n2k numbers . In contrast , the joint contain s 2" numbers . To mak e this concrete ,
suppos e we have 20 node s (n = 20) and each has at mos t 5 parent s (k = 5). The n the belie f networ k
require s 640 numbers , but the full joint require s over a million .
There are domain s in whic h each variabl e can be influence d directl y by all the others , so
that the networ k is fully connected.  The n specifyin g the conditiona l probabilit y table s require s
the same amoun t of informatio n as specifyin g the joint . The reductio n in informatio n that occur s
in practic e come s abou t becaus e real domain s have a lot of structure , whic h network s are very
good at capturing . In som e domains , ther e will be sligh t dependencie s that shoul d strictl y be
include d by addin g a new link . But if these dependencie s are very tenuous , then it may not be
wort h the additiona l complexit y in the networ k for the smal l gain in accuracy . For example , one
migh t objec t to our burglar y networ k on the ground s that if there is an earthquake , then John
and Mary woul d not call even if they hear d the alarm , becaus e they assum e the earthquak e is the
cause . Whethe r to add the link from Earthquake  to JohnCalls  and MaryCalls  (and thus enlarg e
the tables ) depend s on the importanc e of gettin g more accurat e probabilitie s compare d to the cost
of specifyin g the extra information .
Even in a locall y structure d domain , constructin g a locall y structure d belie f networ k is
not a trivia l problem . We requir e not only that each variabl e is directl y influence d by only a
few others , but also that the networ k topolog y actuall y reflect s thos e direc t influence s with the
appropriat e set of parents . Becaus e of the way that the constructio n procedur e works , the "direc t
influencers " will have to be adde d to the networ k first if they are to becom e parent s of the node
they influence . Therefore , the correct order  to add nodes  is to add  the "root  causes"  first,  then
the variables  they  influence,  and so on unti l we reac h the "leaves, " whic h have no direc t causa l
influenc e on the othe r variables .
442 Chapte r 15 . Probabilisti c Reasonin g System s
Figur e 15.3 Networ k structur e depend s on orde r of introduction .
What happen s if we happe n to choos e the wron g order ? Let us conside r the burglar y
exampl e again . Suppos e we decid e to add the node s in the orde r MaryCalls,  JohnCalls,  Alarm,
Burglary,  Earthquake.  We get a somewha t mor e complicate d networ k (Figur e 15.3 , left) . The
proces s goes as follows :
• Addin g MaryCalls:  no parents .
• Addin g JohnCalls:  if Mar y calls , that probabl y mean s the alarm has gon e off, whic h of
cours e woul d mak e it more likel y that John calls . Therefore , there is a dependence :
P(JohnCalls\MaryCalls)  j P(JohnCalls)
Hence , JohnCalls  need s MaryCalls  as a parent .
• Addin g Alarm:  clearly , if both call, it is mor e likel y that the alarm has gone off than if just
one or neithe r call, so we need both MaryCalls  and JohnCalls  as parents .
• Addin g Burglary:  if we know the alarm state , then the call (or lack of it) from John or
Mary migh t tell us abou t whethe r our telephon e is ringin g or whethe r Mary' s musi c is on
loud, but it does not give us furthe r informatio n abou t a burglary . Tha t is,
P(Burglary\Alarm,  JohnCalls, MaryCalls)  = P(Burglary\Alarm)
Henc e we need just Alarm  as parent .
• Addin g Earthquake:  if the alarm is on, it is mor e likel y that there has been an earthquak e
(becaus e the alarm is an earthquak e detecto r of sorts) . But if we know ther e has been a
burglary , then that account s for the alar m and the probabilit y of an earthquak e woul d be
only slightl y abov e normal . Henc e we need both Alarm  and Burglary  as parents :
P(Earthquake\Burglary,Alarm,  JohnCalls, MaryCalls)  = P(Earthquake\Burglary,Alarm)
The resultin g networ k has two mor e link s than the origina l networ k in Figur e 15.1 , and require s
three mor e probabilitie s to be specified . Bu t the wors t part is that som e of the link s represen t
Sectio n 15.2 . Th e Semantic s of Belie f Network s 443
tenuou s relationship s that requir e difficul t and unnatura l probabilit y judgments , such as assessin g
the probabilit y of Earthquake  give n Burglary  and Alarm.  Thi s phenomeno n is quit e general .
If we try to buil d a diagnosti c mode l with link s from symptom s to cause s (as from MaryCalls
to Alarm,  or Alarm  to Burglary),  we end up havin g to specif y additiona l dependencie s betwee n
otherwis e independen t causes , and often betwee n separatel y occurrin g symptom s as well . If we
stick to a causal  model,  we end­up  having  to specify  fewer  numbers,  and  the numbers will  often
be easier  to come  up with.  In the domai n of medicine , for example , it has been show n by Tversk y
and Kahnema n (1982 ) that exper t physician s prefe r to give probabilit y judgment s for causa l rules
rathe r than for diagnosti c ones .
The right­han d side of Figur e 15.3 show s a reall y bad orderin g of nodes : MaryCalls,
JohnCalls,  Earthquake,  Burglary,  Alarm.  Thi s networ k require s 31 distinc t probabilitie s to be
specified—exactl y the same as the full joint distribution . It is importan t to realize , however , that
any of the three network s can represen t exactly  the same  joint  distribution.  The last two version s
simpl y fail to represen t all the conditiona l independenc e relationships , and end up specifyin g a
lot of unnecessar y number s instead .
CANONICA L
DISTRIBUTION S
DETERMINISTI C
NODE S
NOISY­O R
LEAK NOD ERepresentatio n of conditiona l probabilit y table s
Even with a fairl y smal l numbe r of parents , a node' s conditiona l probabilit y table still require s
a lot of numbers . Fillin g in the table woul d appea r to requir e a good deal of time and also a
lot of experienc e with all the possibl e conditionin g cases . In fact, this is a worst­cas e scenario ,
wher e the relationshi p betwee n the parent s and the child is completel y arbitrary . Usually , such
relationship s fall into one of severa l categorie s that have canonica l distributions —tha t is, they
fit some standar d pattern . In such cases , the complet e table can be specifie d by namin g the patter n
and perhap s supplyin g a few parameters .
The simples t exampl e is provide d by deterministi c nodes . A deterministi c node has its
value specifie d exactl y by the value s of its parents , with no uncertainty . The relationshi p can
be a logica l one—fo r example , the relationshi p betwee n paren t node s Canadian,  US, Mexican
and the child node NorthAmerican  is simpl y that the child is the disjunctio n of the parents . The
relationshi p can also be numerical—fo r example , if the paren t node s are the price s of a particula r
mode l of car at severa l dealers , and the child node is the price that a bargai n hunte r ends up
paying , then the child node is the minimu m of the paren t values ; or if the paren t node s are the
inflow s (rivers , runoff , precipitation ) into a lake and the outflow s (rivers , evaporation , seepage )
from the lake and the child is the chang e in lake level , then the child is the differenc e betwee n
the the inflo w parent s and the outflo w parents .
Uncertai n relationship s can ofte n be characterize d by so­calle d "noisy " logica l relation ­
ships . Th e standar d exampl e is the so­calle d noisy­O R relation , whic h is a generalizatio n of
the logica l OR. In propositiona l logic , we migh t say Fever  is true if and only if Cold,  Flu,  or
Malaria  is true . The noisy­O R mode l adds som e uncertaint y to this stric t logica l approach . The
mode l make s thre e assumptions . First , it assume s that each caus e has an independen t chanc e
of causin g the effect . Second , it assume s that all the possibl e cause s are listed . (Thi s is not as
strict as it seems , becaus e we can alway s add a so­calle d leak node that cover s "miscellaneou s
causes" ) Third , it assume s that whateve r inhibits , say, Cold  from causin g a feve r is independen t
of whateve r inhibit s Flu from causin g a fever . Thes e inhibitor s are not represente d as node s but
444 Chapte r 15 . Probabilisti c Reasonin g System s
rathe r are summarize d as "nois e parameters. " If P(Fever\Cold)  = 0.4, P(Fever\Flu)  = 0.8, and
P(Fever\Malaria)  = 0.9, then the nois e parameter s are 0.6, 0.2, and 0.1, respectively . If no paren t
node is true , then the outpu t node is false with 100% certainty . If exactl y one paren t is true , then
the outpu t is fals e with probabilit y equa l to the nois e paramete r for that node . In general , the
probabilit y that the outpu t node is False  is just the produc t of the nois e parameter s for all the
inpu t node s that are true . For this example , we have the following :
Cold
F
F
F
F
T
T
T
TFlu
F
F
T
T
F
F
T
TMalaria
F
T
F
T
F
T
F
TP(Fever)
0.0
0.9
0.8
0.98
0.4
0.94
0.88
0.988P(­~Fever)
1.0
0.1
0.2
0.02 = 0.2 x
0.6
0.06 = 0.6 x
0.12 = 0.6 x
0.012 = 0.6)0.1
0.1
0.2
< 0.2 x 0.1
In general , nois y logica l relationship s in whic h a variabl e depend s on k parent s can be describe d
using O(K)  parameter s instea d of 0(2* ) for the full conditiona l probabilit y table . Thi s make s
assessmen t and learnin g muc h easier . For example , the CPS C networ k (Pradha n et al., 1994 )
uses noisy­O R and noisy­MAX , and require s "only " 8,25 4 value s instea d of 133,931,43 0 for a
networ k with full CPTs .
Conditiona l independenc e relation s in belie f network s
The precedin g analysi s show s that a belie f networ k expresse s the conditiona l independenc e of a
node and its predecessors , give n its parents , and uses this independenc e to desig n a constructio n
metho d for networks . If we wan t to desig n inferenc e algorithms , however , we will need to know
whethe r mor e genera l conditiona l independence s hold . If we are give n a network , is it possibl e
to "read off" whethe r a set of node s X is independen t of anothe r set Y, give n a set of evidenc e
node s El The answe r is yes, and the metho d is provide d by the notio n of direction­dependen t
D­SEPARATIO N separatio n or d­separation .
,,:4ip" First , we will say wha t d­separatio n is good for. If every  undirected  path2 from  a node in
''"•§?•• ' X  to a  node in  Y is d­separated  by E, then  X and Y are  conditionally  independent given  E. The
definitio n of d­separatio n is somewha t complicated . We will need to appea l to it severa l time s in
constructin g our inferenc e algorithms . Onc e this is done , however , the proces s of constructin g
and usin g belie f network s does not involv e any uses of d­separation .
A set of node s E d­separate s two sets of node s X and Y if ever y undirecte d path from a
BLOCKE D nod e in X to a node in Y is blocke d give n E. A path is blocke d give n a set of node s E if there is
a node Z on the path for whic h one of three condition s holds :
1. Z is in £ and Z has one arro w on the path leadin g in and one arro w out.
2 An undirecte d path is a path throug h the networ k that ignore s the directio n of the arrows .
Sectio n 15.3 . Inferenc e in Belie f Network s 445
2. Z is in E and Z has both path arrow s leadin g out.
3. Neithe r Z nor any descendan t of Z is in E, and both path arrow s lead in to Z.
Figur e 15.4 show s these three cases . The proo f that d­separate d node s are conditionall y indepen ­
dent is also complicated . We will use Figur e 15.5 to give example s of the three cases :
1. Whethe r ther e is Gas  in the car and whethe r the car Radio  play s are independen t give n
evidenc e abou t whethe r the SparkPlugs  fire.
2. Gas  and Radio  are independen t if it is know n if the Battery  works .
3. Gas  and Radio  are independen t give n no evidenc e at all. Bu t they are dependen t give n
evidenc e abou t whethe r the car Starts.  For example , if the car does not start , then the radio
playin g is increase d evidenc e that we are out of gas. Gas  and Radio  are also dependen t
given evidenc e abou t whethe r the car Moves,  becaus e that is enable d by the car starting .
Figur e 15.4 Thre e way s in whic h a path from X to Y can be blocked , give n the evidenc e E. If
every path from X to Y is blocked , then we say that E d­separate s X and Y.
15.3 INFERENC E IN BELIE F NETWORK S
The basi c task for any probabilisti c inferenc e syste m is to comput e the posterio r probabilit y
distributio n for a set of quer y variables , give n exac t value s for some evidenc e variables . Tha t
is, the syste m compute s P(Query\Evidence).  In the alarm example , Burglary  is an obviou s quer y
variable , and JohnCalls  and MaryCalls  coul d serv e as evidenc e variables . Of course , belie f
network s are flexibl e enoug h so that any node can serv e as eithe r a quer y or an evidenc e variable .
There is nothin g to stop us from askin g P(Alarm\  JohnCalls,Earthquake),  althoug h it woul d be
somewha t unusual . In general , an agent  gets  values  for evidence  variables  from  its percepts  (or
from  other  reasoning),  and  asks  about  the possible  values  of other  variables  so that  it can decide
446 Chapte r 15 . Probabilisti c Reasonin g System s
Figur e 15.5 A  belie f networ k describin g some feature s of a car's electrica l syste m and engine .
what  action  to take.  The two function s we need are BELIEF­NET­TELL , for addin g evidenc e to the
network , and BELIEF­NET­ASK , for computin g the posterio r probabilit y distributio n for a give n
query variable .
The natur e of probabilisti c inference s
Befor e plungin g into the detail s of the inferenc e algorithms , it is worthwhil e to examin e the kind s
of thing s such algorithm s can achieve . We will see that a singl e mechanis m can accoun t for a
very wide variet y of plausibl e inference s unde r uncertainty .
Conside r the proble m of computin g P(Burglary\JohnCalls\  the probabilit y that ther e is
a burglar y give n that John calls . Thi s task is quite trick y for humans , and therefor e for man y
reasonin g system s that attemp t to encod e huma n judgment . The difficult y is not the complexit y
of the problem , but keepin g the reasonin g straight . An incorrec t but all­too­commo n line of
reasonin g start s by observin g that whe n the alarm goes off, JohnCalls  will be true 90% of the
time. The alarm is fairl y accurat e at reflectin g burglaries , so P(Burglary\JohnCalls)  shoul d also
be abou t 0.9, or mayb e 0.8 at worst . The proble m is that this line of reasonin g ignore s the prior
probabilit y of John calling . Ove r the cours e of 1000 days , we expec t one burglary , for whic h
John is very likel y to call. However , John also calls with probabilit y 0.05 whe n there actuall y is
no alarm—abou t 50 time s over 1000 days . Thus , we expec t to receiv e abou t 50 false alarm s from
John for ever y 1 burglary , so P(Burglary\JohnCalls)  is abou t 0.02 . In fact , if we carr y out the
exact computation , we find that the true valu e is 0.016 . It is less than our 0.02 estimat e becaus e
the alarm is not perfect .
Now suppos e that as soon as we get off the phon e with John , Mar y calls . We are now inter ­
ested in incrementall y updatin g our networ k to give P(Burglary\JohnCalls  A MaryCalls).  Again ,
human s ofte n overestimat e this value ; the correc t answe r is only 0.29 . We can also determin e
that P(Alarm\JohnCalls/\  MaryCalls)  is 0.76 and P(Earthquake\JohnCalls  A MaryCalls)  is 0.18 .
ISectio n 15.3 . Inferenc e in Belie f Network s 447
DIAGNOSTI C
INFERENCE S
INFERENCE S
INTERCAUSA L
INFERENCE S
EXPLAININ G AWA Y
MIXE D INFERENCE SIn both of these problems , the reasonin g is diagnostic . But belie f network s are not limite d
to diagnosti c reasonin g and in fact can mak e four distinc t kind s of inference :
0 Diagnosti c inference s (fro m effect s to causes) .
Give n that JohnCalls,  infe r that P(Burglary\JohnCalls)  = 0.016 .
0 Causa l inference s (fro m cause s to effects) .
Give n Burglary,  P(JohnCalls\Burglary)  = 0.86 and P(MaryCalls\Burglary)  = 0.67 .
<) Intercausa l inference s (betwee n cause s of a commo n effect) .
Given Alarm,  we have P(Burglary\Alarm)  = 0.376 . Bu t if we add the evidenc e that
Earthquake  is true , then P(Burglary\Alarm  A Earthquake)  goes dow n to 0.003 . Eve n
thoug h burglarie s and earthquake s are independent , the presenc e of one make s the othe r
less likely . This patter n of reasonin g is also know n as explainin g away.3
0 Mixe d inference s (combinin g two or more of the above) .
Settin g the effec t JohnCalls  to true and the caus e Earthquake  to false give s
P(Alarm\JohnCalls  A ­^Earthquake)  = 0.03
This is a simultaneou s use of diagnosti c and causa l inference . Also ,
P(Burglary\JohnCalls  A ­^Earthquake)  ­ 0.01 7
This is a combinatio n of intercausa l and diagnosti c inference .
These four pattern s are depicte d in Figur e 15.6 . Beside s calculatin g the belie f in quer y variable s
given definit e value s for evidenc e variables , belie f network s can also be used for the following :
• Makin g decision s base d on probabilitie s in the networ k and on the agent' s utilities .
• Decidin g whic h additiona l evidenc e variable s shoul d be observe d in orde r to gain usefu l
information .
• Performin g sensitivit y analysi s to understan d whic h aspect s of the mode l have the greates t
impac t on the probabilitie s of the quer y variable s (and therefor e mus t be accurate) .
• Explainin g the result s of probabilisti c inferenc e to the user.
These tasks are discusse d furthe r in Chapte r 16. In this chapter , we focu s on computin g posterio r
probabilitie s of quer y variables . In Chapte r 18, we show how belie f network s can be learne d
from exampl e cases .SENSITIVIT Y
ANALYSI S
SINGL Y CONNECTE DAn algorith m for answerin g querie s
In this section , we will deriv e an algorith m for BELIEF­NET­ASK . Thi s is a rathe r technica l
section , and some of the mathematic s and notatio n are unavoidabl y intricate . The algorith m itself ,
however , is very simple . Our versio n will work rathe r like the backward­chainin g algorith m in
Chapte r 9, in that it begin s at the quer y variabl e and chain s alon g the path s from that node unti l
it reache s evidenc e nodes . Becaus e of the complication s that may arise whe n two differen t path s
converg e on the sam e node , we will deriv e an algorith m that work s only on singl y connecte d
3 To get this effect , we do not have to know  that Earthquake  is true , we just need som e evidenc e for it. Whe n one of the
author s first move d to California , he lived in a ricket y hous e that shoo k when large truck s wen t by. Afte r one particularl y
large shake , he turne d on the radio , hear d the Carol e King song / Feel the  Earth  Move,  and considere d this stron g evidenc e
that a large truc k had not gone by.
448 Chapte r 15 . Probabilisti c Reasonin g System s
( E )
Diagnosti cV Q )
Causa l(Explainin g Away )
Intercausa l( E )
Mixe d
Figur e 15.6 Simpl e example s of four pattern s of reasonin g that can be handle d by belie f
networks . E represent s an evidenc e variabl e and Q is a quer y variable .
POLYTREE S
CAUSA L SUPPOR T
EVIDENTIA L
SUPPOR Tnetworks , also know n as polytrees . In such networks , ther e is at mos t one undirecte d path
betwee n any two node s in the network . Algorithm s for genera l network s (Sectio n 15.4 ) will use
the polytre e algorithm s as their main subroutine .
Figur e 15.7 show s a generi c singl y connecte d network . Nod e X has parent s V = U\ ... Um,
and childre n Y = Y\ ... Yn. For each child and paren t we have draw n a box that contain s all the
node' s descendant s and ancestor s (excep t for X). The singl y connecte d propert y mean s that all
the boxe s are disjoin t and have no link s connectin g them . We are assumin g that X is the quer y
variable , and that ther e is som e set of evidenc e variable s E. The aim is to comput e P(X\E).
(Obviously , if X is itsel f an evidenc e variabl e in E, then calculatin g PX\E  is trivial . We will
assum e that X is not in E.)
In orde r to deriv e the algorithm , it will be helpfu l to be able to refer to differen t portion s of
the total evidence . The first distinctio n we will need is the following :
EX is the the causa l suppor t for X—the evidenc e variable s "above " X that are connecte d to X
throug h its parents .
EX is the evidentia l suppor t for X—the evidenc e variable s "below " X that are connecte d to X
throug h its children .
Sometime s we will need to exclud e certai n path s whe n considerin g evidenc e connecte d to a
certai n variable . For example , we will use EU^X  to refe r to all the evidenc e connecte d to node
Ui except  via the path from X. Similarly , Ey, x mean s all the evidenc e connecte d to 7,­ through
its parent s except  for X. Notic e that the total evidenc e E can be writte n as Ex (all the evidenc e
connecte d to X) and as Ex\ (all the evidenc e connecte d to X with no exceptions) .
Now we are read y to comput e P(X\E).  The genera l strateg y is roughl y the following :
• Expres s P(X\E)  in term s of the contribution s of £J and E% •
• Comput e the contributio n of EJ by computin g its effec t on the parent s of X, and then
passin g that effec t on to X. Notic e that computin g the effec t on each paren t of X is a
recursiv e instanc e of the proble m of computin g the effec t on X.
• Comput e the contributio n of E% by computin g its effec t on the childre n of X, and then
passin g that effec t on to X. Notic e that computin g the effec t on each child of X is a recursiv e
instanc e of the proble m of computin g the effec t on X.
Sectio n 15.3 . Inferenc e in Belie f Network s 449
Figur e 15.7 A  generi c singl y connecte d network . The networ k is show n partitione d accordin g
to the parent s and childre n of the quer y variabl e X.
Our derivatio n will work by applyin g Bayes ' rule and othe r standar d method s for manipulatin g
probabilit y expressions , unti l we have massage d the formula s into somethin g that look s like a
recursiv e instanc e of the origina l problems . Alon g the way, we will use simplification s sanctione d
by the conditiona l independenc e relationship s inheren t in the networ k structure .
The total evidenc e E consist s of the evidenc e abov e X and the evidenc e belo w X, sinc e we
are assumin g that X itsel f is not in E. Hence , we have
To separat e the contribution s of £J and E% , we appl y the conditionalize d versio n of Bayes ' rule
(Equatio n (14.4) ) keepin g E% as fixe d backgroun d evidence :
Becaus e X d­separate s £J from E% in the network , we can use conditiona l independenc e to
simplif y the first term in the numerator . Also , we can treat 1/P(£ ^ £J) as a normalizin g constant ,
givin g us
P(X\E)  = aP(E^  |X)P(X|£J )
So now we just need to comput e the two term s P(E%  \X) and P(X|£J) . The latte r is easier , so we
shall look at it first .
450 Chapte r 15 . Probabilisti c Reasonin g System s
We comput e P(X\E%)  by considerin g all the possibl e configuration s of the parents  of X,
and how likel y they are give n EJ. Give n each configuration , we kno w the probabilit y of X
directl y from the CPT ; we then averag e thos e probabilities , weighte d by the likelihoo d of each
configuration . To say this formally , let U be the vecto r of parent s U\, ...,  Um, and let u be an
assignmen t of value s to them.4 The n we have
Now U d­separate s X from EJ, so we can simplif y the first term to P(X|u) . We can simplif y the
secon d term by notin g that EJ d­separate s each [7, from the others , and by rememberin g that the
probabilit y of a conjunctio n of independen t variable s is equa l to the produc t of their individua l
probabilities . This give s us
The final step is to simplif y the last term of this equatio n by partitionin g EX into E(y,\x , • • • , EUm\x
(the separat e boxe s in Figur e 15.7 ) and usin g the fact that E(/,\ x d­separate s t/, from all the othe r
evidenc e in £J. So that give s us
P(X|EJ ) =
(15.3 )EU,\X)
u /
whic h we can plug into our earlie r equatio n to yield
P(X|E ) = aP(E x |X) ^ P(X|u) "
Finally , this is startin g to look like an algorithm : P(X|u ) is a looku p in the conditiona l probabilit y
table of X, and P((//|E(/ ;\x) is a recursiv e instanc e of the origina l problem , whic h was to comput e
P(X|E ) — that is, P(X|£x\) ­ Note that the set of evidenc e variable s in the recursiv e call is a prope r
subse t of those in the origina l call — a good sign that the algorith m will terminate .
Now we retur n to P(E X |X), with an eye towar d comin g up with anothe r recursiv e solution .
In this case we averag e over the value s of K,, the childre n of X, but we will also need to includ e
YI'S parents . We let Z, be the parent s of Y, othe r than X, and let z, be an assignmen t of value s to
the parents . The derivatio n is simila r to the previou s one. First , becaus e the evidenc e in each F;
box is conditionall y independen t of the other s give n X, we get
Averagin g over 7, and z/ yield s
|X) = n E E  V(E r,\x IX yi, z,­)P(y,­ , z,­ |X)
Breakin g EY :\X into the two independen t component s Ey. and EJ.\ X:
|X) = n  E E P(^ \X,yi,  z/)P(£^ Ax|X, v,, z,­)P(v ;, z;|X)
4 Fo r example , if ther e are two Boolea n parents , U\ and 1/2, then u range s over four possibl e assignments , of whic h
[true,false}  is one .
ISectio n 15.3 . Inferenc e in Belie f Network s 451
Ey. is independen t of X and z, give n y,, and Ey, x is independen t of X and y,. We can also pull a
term with no z, out of the z, summation :
P(E^  \X) = n E P(Ey,  I
i >' i
Appl y Bayes ' rule to P(£ y, x|z,­):
Rewritin g the conjunctio n F/, z/:
r (Z,
­P(y 1­|X,z,­)P(z,­|X )
Now P(z/|X ) = P(ZJ) , becaus e Z and X are d­separated , so we can cance l them out. We can also
replac e P(Ey* x) by a normalizin g constan t /?,:
X, z,­) |X) = [I E ^  ?/) E A^(
/ V, ' Z ;
Finally , the parent s of K,­ (the Zy­) are independen t of each other , so we can multipl y them together ,
just as we did with the {/, parent s previously . We also combin e the /?,• into one big normalizin g
constan t 3:
i*' z
<) n p(z
<>(15.4 )
Notic e that each of the term s in the final expressio n is easil y evaluated :
• P(Ey  yj) is a recursiv e instanc e of P(£J \X).
• P(yi  xzi) is a conditiona l probabilit y table entry for Yj.
• Pfaj  EZij\Yi) is a recursiv e instanc e of the P(X\E)  calculation—tha t is, P(X\E X\)
It is now a simpl e matte r to turn all this into an algorithm . We will need two basi c routines .
SUPPORT­EXCEPT(X,V ) compute s P(X\Ex\v),  usin g a sligh t generalizatio n of Equatio n (15.3 ) to
handl e the "except " variabl e V. EVIDENCE­EXCEPT(X,V ) compute s P(E~> V\X), usin g a general ­
izatio n of Equatio n (15.4) . The algorithm s are show n in Figur e 15.8 .
The computatio n involve s recursiv e calls that sprea d out from X alon g all path s in the
network . The recursio n terminate s on evidenc e nodes , root node s (whic h have no parents) , and
leaf node s (whic h have no children) . Eac h recursiv e call exclude s the node from whic h it was
called , so each node in the tree is covere d only once . Henc e the algorith m is linea r in the numbe r
of node s in the network . Remembe r that this only work s becaus e the networ k is a polytree.  If
there were mor e than one path betwee n a pair of nodes , then our recursion s woul d eithe r coun t
the sam e evidenc e mor e than once or they woul d fail to terminate .
We have chose n to presen t a "backward­chaining " algorith m becaus e it is the simples t
algorith m for polytrees . On e drawbac k is that it compute s the probabilit y distributio n for just
one variable . If we wante d the posterio r distribution s for all the non­evidenc e variables , we
woul d have to run the algorith m once for each . Thi s woul d give a quadrati c runtime , and
452 Chapte r 15 . Probabilisti c Reasonin g System s
functio n BELIEF­NET­ASK(X ) return s a probabilit y distributio n over the value s of X
inputs : X, a rando m variabl e
SUPPORT­EXCEPT(X , null)
functio n SUPPORT­EXCEPT(A" , V) return s P(X\E X\ v)
if EviDENCE?(X ) then retur n observe d poin t distributio n forX
else
calculat e P(E~\  V\X) = EVIDENCE­EXCEPT(X , V)
U — PARENT S [X]
if U is empt y
then retur n a P(E~\  V\X) P(X)
else
for each U; in U
calculat e and store P(Ui\E Vj \x) = SUPPORT­ExcEPT(t/,,X )
' retur n aV\X) £ P(X|u )
functio n EviDENCE­ExcEPT(X , V) return s P(E~\  V\X)
Y—CHILDREN[X ] ­ V
if Y is empt y
then retur n a unifor m distributio n
else
for each Y, in Y do
calculat e P(E~  | v,) = EviDENCE­ExcEPT(7, , null)
Z, — PARENTS ' [7,] ­ X
for each Zy­ in Z,
calculat e P(Zjj\E z..\ Yl) = SUPPORT­EXCEPT(Z,> , 7,)
retur n 13 fl £ P(Ey :\yi) E PCv,­|X, Z/) fl P(^\E ZiA r.)
Figur e 15.8 A  backward­chainin g algorith m for solvin g probabilisti c querie s on a polytree .
To simplif y the presentation , we have assume d that the networ k is fixe d and alread y prime d with
evidence , and that evidenc e variable s satisf y the predicat e EVIDENCE? . The probabilitie s P(X|U) ,
wher e U denote s the parent s of X, are availabl e from the CPT for X. Calculatin g the expression s
«... and ^ ... is done by normalization .
woul d involv e man y repeate d calculations . A bette r way to arrang e thing s is to "memoize " the
computation s by forward­chainin g from the evidenc e variables . Give n carefu l bookkeeping , the
entir e computatio n can be done in linea r time . It is interestin g to note that the forward­chainin g
versio n can be viewe d as consistin g of "messages " bein g "propagated " throug h the network . This
leads to a simpl e implementatio n on paralle l computers , and an intriguin g analog y to messag e
propagatio n amon g neuron s in the brai n (see Chapte r 19).
Sectio n 15.4 . Inferenc e in Multipl y Connecte d Belie f Network s 453
L 5.4 INFERENC E IN MULTIPL Y CONNECTE D BELIE F NETWORK S
CONNECTE D
CLUSTERIN G
CONDITIONIN G
STOCHASTI C
SIMULATIO NA multipl y connecte d grap h is one in whic h two node s are connecte d by more than one path .
One way this happen s is whe n there are two or more possibl e cause s for some variable , and the
cause s shar e a commo n ancestor . Alternatively , one can thin k of multipl y connecte d network s
as representin g situation s in whic h one variabl e can influenc e anothe r throug h mor e than one
causa l mechanism . For example , Figur e 15.9 show s a situatio n in whic h whethe r it is cloud y has
a causa l link to whethe r it rains , and also a causa l link to whethe r the lawn sprinkler s are turne d
on (becaus e a gardene r who observe s cloud s is less likel y to turn the sprinkler s on). Bot h rain
and sprinkler s have an effec t on whethe r the gras s gets wet.
There are three basi c classe s of algorithm s for evaluatin g multipl y connecte d networks ,
each with its own area s of applicability :
0 Clusterin g method s transfor m the networ k into a probabilisticall y equivalen t (but topo ­
logicall y different ) polytre e by mergin g offendin g nodes .
0 Conditionin g method s do the transformatio n by instantiatin g variable s to definit e values ,
and then evaluatin g a polytre e for each possibl e instantiation .
<> Stochasti c simulatio n method s use the networ k to generat e a large numbe r of concret e
model s of the domai n that are consisten t with the networ k distribution . The y give an
approximatio n of the exac t evaluation .
In the genera l case , exac t inferenc e in belie f network s is know n to be NP­hard . It is fairl y
straightforwar d to prov e this, becaus e a genera l belie f networ k can represen t any propositiona l
logic proble m (if all the probabilitie s are 1 or 0) and propositiona l logic problem s are know n to
be NP­complete . For very large networks , approximatio n usin g stochasti c simulatio n is currentl y
the metho d of choice . The proble m of approximatin g the posterio r probabilitie s to withi n an
arbitrar y toleranc e is itsel f NP­hard , but for event s that are not too unlikel y the calculation s are
usuall y feasible .
Clusterin g method s
One way of evaluatin g the networ k in Figur e 15.9 is to transfor m it into a polytre e by combinin g
MEGANOD E th e Sprinkler  and Rain  node into a meganod e calle d Sprinkler+Rain,  as show n in Figur e 15.10 .
The two Boolea n node s are replace d by a meganod e that take s on four possibl e values : TT, TF,
FT, and FF. The meganod e has only one parent , the Boolea n variabl e Cloudy,  so there are two
conditionin g cases . Onc e the networ k has been converte d to a polytree , a linear­tim e algorith m
can be applie d to answe r queries . Querie s on variable s that have been clustere d can be answere d
by averagin g over the value s of the othe r variable s in the cluster .
Althoug h clusterin g make s it possibl e to use a linear­tim e algorithm , the NP­hardnes s of
the proble m does not go away . In the wors t case , the size of the networ k increase s exponen ­
tially , becaus e the conditiona l probabilit y table s for the cluster s involv e the cross­produc t of the
domain s of the variables . In Figur e 15.10 , there are six independentl y specifiabl e number s in
Sprinkler+Rain,  as oppose d to four total in Sprinkler  and Rain.  (On e of the number s in each row
454 Chapte r 15 . Probabilisti c Reasonin g System s
is not independent , becaus e the row mus t sum to 1. In Figur e 15.9 , we droppe d one of the two
columns , but here we show all four. )
The trick y part abou t clusterin g is choosin g the righ t meganodes . Ther e are severa l way s
to mak e this choice , but all of them ultimatel y produc e meganode s with large probabilit y tables .
Despit e this problem , clusterin g method s are currentl y the mos t effectiv e approac h for exac t
evaluatio n of multipl y connecte d networks .
CUTSE T
CONDITIONIN GCutse t conditionin g method s
The cutse t conditionin g metho d take s the opposit e approach . Instea d of transformin g the
networ k into one comple x polytree , this metho d transform s the networ k into severa l simple r
Figur e 15.9 A  multipl y connecte d networ k with  conditiona l probabilit y tables .
S+R
T T
T F
F T
F FP(W)
.99
.90
.90
.00P(C) = .5
P(S+R=x )
C T T TF FT FF
T .0 8 .02 .72 .18
F .4 0 .10 .40 .10
Figur e 15.10 A  clustere d equivalen t of the multipl y connecte d network .
Sectio n 15.4 . Inferenc e in Multipl y Connecte d Belie f Network s 455
CUTSE T
BOUNDE D CUTSE T
CONDITIONIN Gpolytrees . Eac h simpl e networ k has one or more variable s instantiated  to a definit e value . P(X\E)
is compute d as a weighte d averag e over the value s compute d by each polytree .
A set of variable s that can be instantiate d to yield polytree s is calle d a cutset . In Fig­
ure 15.11 , the cutse t is just {Cloudy},  and becaus e it is a Boolea n variable , ther e are just two
resultin g polytrees . In general , the numbe r of resultin g polytree s is exponentia l in the size of the
cutset , so we wan t to find a smalTcutse t if possible .
Cutse t conditionin g can be approximate d by evaluatin g only som e of the resultin g polytrees .
The error in the approximatio n is bounde d by the total probabilit y weigh t of the polytree s not yet
evaluated . The obviou s approac h is to evaluat e the mos t likel y polytree s first . For example , if we
need an answe r accurat e to withi n 0.1, we evaluat e trees in decreasin g orde r of likelihoo d unti l
the total probabilit y exceed s 0.9. This techniqu e is calle d bounde d cutse t conditioning , and is
usefu l in system s that need to mak e approximatel y correc t decision s quickly .
Figur e 15.1 1 Network s create d by instantiation .
Stochasti c simulatio n method s
LOGIC SAMPLIN G I n the stochasti c simulatio n metho d know n as logi c sampling , we run repeate d simulation s of
the worl d describe d by the belie f network , and estimat e the probabilit y we are intereste d in by
countin g the frequencie s with whic h relevan t event s occur . Eac h roun d of the simulatio n start s by
randoml y choosin g a valu e for each root node of the network , weightin g the choic e by the prior
probabilities . If the prio r P(Cloudy)  = 0.5, then we woul d pick Cloudy  = True  half the time , and
Cloudy  = False  half the time . Whateve r valu e is chosen , we can then choos e value s randoml y for
Sprinkler  and Rain,  usin g the conditiona l probabilitie s of thos e node s give n the know n valu e of
Cloudy.  Finally , we do the same for WetGrass,  and the first roun d is done .
To estimat e P(WetGrass\Cloudy)  (or in genera l P(X\E)),  we repea t the proces s man y times ,
and then comput e the ratio of the numbe r of runs wher e WetGrass  and Cloudy  are true to the
numbe r of runs wher e just Cloudy  is true . This will alway s converg e to the righ t value , althoug h
it may take man y runs .
The main proble m is whe n we are intereste d in som e assignmen t of value s to E that rarel y
occurs . For example , suppos e we wante d to know
P(WetGrass\Sprinkler  A Rain)
456 Chapte r 15 . Probabilisti c Reasonin g System s
LIKELIHOO D
WEIGHTIN GBecaus e Sprinkler  A Rain  is rare in the world , mos t of the simulatio n round s woul d end up with
differen t value s for these evidenc e variables , and we woul d have to discar d thos e runs . The
fractio n of usefu l runs decrease s exponentiall y with the numbe r of evidenc e variables .
We can get aroun d this proble m with an approac h calle d likelihoo d weighting . The idea is
that ever y time we reach an evidenc e variable , instea d of randoml y choosin g a valu e (accordin g
to the conditiona l probabilities) , we take the give n valu e for the evidenc e variable , but use the
conditiona l probabilitie s to see how likel y that is. For example , to comput e P(WetGrass\Rairi)
we woul d do the following :
1. Choos e a valu e for Cloudy  with prior P(Cloudy)  ­ 0.5. Assum e we choos e Cloudy  = False.
2. Choos e a valu e for Sprinkler.  We see that P(Sprinkler  ­^Cloudy)  = 0.5, so we randoml y
choos e a valu e give n that distribution . Assum e we choos e Sprinkler  ­ True.
3. Loo k at Rain.  This is an evidenc e variabl e that has been set to True,  so we look at the table
to see that P(Rain\­>Cloudy)  = 0.2. This run therefor e count s as 0.2 of a complet e run.
4. Loo k at WetGrass.  Choos e randoml y with P(WetGrass\Sprinkler  A Rain)  = 0.99 ; assum e
we choos e WetGrass  = True.
5. We now have complete d a run with likelihoodO. 2 that says Wetgrass  = True  give n Rain.  The
next run will resul t in a differen t likelihood , and (possibly ) a differen t valu e for WetGrass.
We continu e until we have accumulate d enoug h runs , and then add up the evidenc e for
each value , weighte d by the likelihoo d score .
Likelihoo d weightin g usuall y converge s considerabl y faste r than logic sampling , and can handl e
very large networks . In the CPS C projec t (Pradha n et al., 1994) , for example , a belie f networ k has
been constructe d for interna l medicin e that contain s 448 nodes , 906 link s and 8,25 4 conditiona l
probabilit y values . (The fron t cove r show s a smal l portio n of the network. ) Likelihoo d weightin g
typicall y obtain s accurat e value s in aroun d 35 minute s on this network .
The mai n difficult y with likelihoo d weighting , and indee d with any stochasti c samplin g
method , is that it takes a long time to reach accurat e probabilitie s for unlikel y events . In general ,
the runtim e necessar y to reach a give n level of accurac y is inversel y proportiona l to the probabilit y
of the event . Event s such as the meltdow n of a nuclea r reacto r on a particula r day are extremely
unlikely , but there is a very big differenc e betwee n 10~5 and 10~I0, so we still need to get
accurat e values . Researcher s are currentl y workin g to find way s aroun d this problem .
[5.5 KNOWLEDG E ENGINEERIN G FOR UNCERTAI N REASONIN G
The approac h to knowledg e engineerin g for probabilisti c reasonin g system s is very muc h like
the approac h for logica l reasonin g system s outline d in Sectio n 8.2:
• Decide  what  to talk about.  Thi s remain s a difficul t step . It is importan t to decid e whic h ,
factor s will be modelled , and whic h will just be summarize d by probabilit y statements . In.;
an exper t syste m of all denta l knowledge , we will certainl y want  to talk abou t toothaches ,
gum disease , and cavities . We may wan t to know if a patient' s parent s have a histor y of|
gum disease , but we probabl y do not need to talk abou t the patient' s third cousins . Onc e we I
Sectio n 15.5 . Knowledg e Engineerin g for Uncertai n Reasonin g 457
have an initia l set of factors , we can exten d the mode l by asking , "Wha t directl y influence s
this factor? " and "Wha t does this facto r directl y influence? "
Decide  on a vocabulary  of random  variables.  Determin e the variable s you want  to use,
and wha t value s they can take on. Sometime s it is usefu l to quantiz e a continuous­value d
variabl e into discret e ranges .
Encode  general  knowledge  about  the dependence  between variables.  Here there is a qual ­
itativ e part, wher e we say wha t variable s are dependen t on wha t others , and a quantitativ e
part, wher e we specif y probabilit y values . The value s can com e eithe r from the knowledg e
engineer' s (or expert's ) subjectiv e experience , or from measurement s of frequencie s in a
databas e of past experiences , or from som e combinatio n of the two.
Encode  a description  of the specific  problem  instance.  Fo r example , we say that the
particula r patien t we are intereste d in is a 34­year­ol d femal e with moderat e to sever e pain
in the lowe r jaw.
Pose  queries to  the inference  procedure  and get answers.  The mos t commo n quer y is to
ask for the valu e of some hypothesi s variable . For example , give n the patient' s symptoms ,
what is the probabilit y of gum disease , or of any othe r disorder . It is also commo n to use
sensitivit y analysi s to determin e how robus t the answer s are with respec t to perturbation s
in the conditiona l probabilit y table values .
SIMILARIT Y
NETWORK SCase study : The Pathfinde r syste m
PATHFINDE R is a diagnosti c exper t syste m for lymph­nod e diseases , buil t by members  of the
Stanfor d Medica l Compute r Scienc e progra m durin g the 1980 s (see Heckerma n (1991 ) for a
discussion) . The syste m deal s with over 60 disease s and over 100 diseas e finding s (symptom s
and test results) . Fou r version s of the syste m hav e been built , and the histor y is instructiv e
becaus e it show s a trend towar d increasin g sophisticatio n in reasonin g with uncertainty .
PATHFINDE R I was a rule­base d syste m writte n with the logica l metareasonin g syste m MRS .
It did not do any uncertai n reasoning .
PATHFINDE R II experimente d with severa l method s for uncertai n reasoning , includin g
certaint y factor s and the Dempster­Shafer  theor y (see Sectio n 15.6) . The result s showe d that
a simplifie d Bayesia n mode l (in whic h all diseas e finding s were assume d to be independent )
outperforme d the othe r methods . One interestin g resul t of the experimen t was that 10% of cases
were diagnose d incorrectl y becaus e the exper t had give n a probabilit y of zero to an unlikel y but
possibl e event .
PATHFINDE R III used the same simplifie d Bayesia n model , but with a reassessmen t of the
probabilitie s usin g a differen t protocol  and payin g attentio n to low­probabilit y events .
PATHFINDE R IV used a belie f networ k to represen t the dependencie s that coul d not be
handle d in the simplifie d Bayesia n model . The autho r of the syste m sat dow n with an exper t
physicia n and followe d the knowledg e engineerin g approac h describe d earlier . Decidin g on a
vocabular y took 8 hours , devisin g the topolog y of the networ k took 35 hours , and makin g the
14,00 0 probabilit y assessment s took anothe r 40 hours . The physicia n reportedl y foun d it easy to
think in term s of probabilitie s on causa l links , and a concep t calle d similarit y network s mad e
it easie r for the exper t to asses s a large numbe r of probabilities . The networ k constructe d in this
458 Chapte r 15 . Probabilisti c Reasonin g System s
proces s cover s one of the 32 majo r area s of pathology . The plan is to cove r all of patholog y
throug h consultation s with leadin g expert s in each area.
An evaluatio n of PATHFINDE R III and IV used 53 actua l cases of patient s who were referre d
to a lymph­nod e specialist . As referrals , these cases were probabl y of above­averag e difficulty . In
a blind evaluation , exper t analysi s of the diagnose s showe d that PATHFINDE R III score d an averag e
7.9 out of 10, and PATHFINDE R IV score d 8.9, significantl y better . The differenc e amount s to
savin g one life ever y thousan d cases or so. A recen t compariso n showe d that Pathfinde r is now
outperformin g the expert s who were consulte d durin g its creation—thos e expert s bein g som e of
the world' s leadin g pathologists .
15.6 OTHE R APPROACHE S TO UNCERTAI N REASONIN G
Othe r science s (e.g. , physics , genetics , economics ) have long favore d probabilit y as a mode l for
uncertainty . Pierr e Laplac e said in 181 9 that "Probabilit y theor y is nothin g but commo n sens e
reduce d to calculation. " Jame s Maxwel l said in 1850 that "the true logic for this worl d is the cal­
culus of Probabilities , whic h take s accoun t of the magnitud e of the probabilit y whic h is, or ough t
to be, in a reasonabl e man' s mind. " Stephe n Jay Goul d (1994 ) claime d that "misunderstandin g
of probabilit y may be the greates t of all genera l impediment s to scientifi c literacy. "
Give n this long tradition , it is perhap s surprisin g that AI has considere d man y alternative s
to probability . The earlies t exper t system s of the 1970 s ignore d uncertaint y and used strict logica l
reasoning , but soon  it becam e clea r that this was impractica l for mos t real­worl d domains . The
next generatio n of exper t system s (especiall y in medica l domains ) used probabilisti c techniques .
Initia l result s wer e promising , but they did not scale up becaus e of the exponentia l numbe r of
probabilitie s require d in the full join t distribution . (Belie f net algorithm s were not know n then. )
As a result , probabilisti c approache s fell out of favo r from roughl y 1975 to 1988 , and a variet y
of alternative s were tried for a variet y of reasons :
• One commo n view is that probabilit y theor y is essentiall y numerical , wherea s huma n
judgmenta l reasonin g is mor e "qualitative. " Certainly , we are not consciousl y awar e
of doin g numerica l calculation s of degree s of belief . (O n the othe r hand , it migh t be
that we have som e kind of numerica l degree s of belie f encode d directl y in strength s of
connection s and activation s in our neurons . In that case , the difficult y of consciou s acces s
to thos e strength s is only to be expected. ) One shoul d also note that qualitativ e reasonin g
mechanism s can be buil t directl y on top of probabilit y theory , so that the "no numbers "
argumen t agains t probabilit y has little force . Nonetheless , som e qualitativ e scheme s have a
good deal of appea l in their own right . One of the mos t well­studie d is defaul t reasoning ,
whic h treat s conclusion s not as "believe d to a certai n degree, " but as "believe d unti l a bette r
reaso n is foun d to believ e somethin g else. "
• Rule­base d approache s to uncertaint y also have been tried . Suc h approache s hope to build
on the succes s of logica l rule­base d systems , but add a sort of "fudg e factor " to each rule to
accommodat e uncertainty . Thes e method s were develope d in the mid­1970s , and forme d
the basi s for a large numbe r of exper t system s in medicin e and othe r areas .
Sectio n 15.6 . Othe r Approache s to Uncertai n Reasonin g 459
• One area that we have not addresse d so far is the questio n of ignorance , as oppose d to
uncertainty . Conside r flippin g a coin . If we know the coin to be fair, then a probabilit y
of 0.5 for heads  is reasonable . If we kno w the coin is biased , but we do not kno w
whic h way , then 0.5 is the only reasonabl e probability . Obviously , the two case s are
different , yet probabilit y seem s not to distinguis h them . The Dempster­Shafe r theor y
uses interval­value d degree s of belie f to represen t an agent' s knowledg e of the probabilit y
of a proposition . Othe r method s usin g second­orde r probabilitie s are also discussed .
• Probabilit y make s the same ontologica l commitmen t as logic : that event s are true or false
in the world , even if the agen t is uncertai n as to whic h is the case . Researcher s in fuzz y
logic have propose d an ontolog y that allow s vagueness : that an even t can be "sort of" true .
Vaguenes s and uncertaint y are in fact orthogona l issues , as we will see.
The followin g section s treat each of these approache s in slightl y more depth . We will not provid e
detaile d technica l material , but we provid e reference s for furthe r study .
NONMONOTONICIT Y
MONOTONICIT Y
DEFAUL T LOGI C
CIRCUMSCRIPTIO N
SPECIFICIT Y
PREFERENC EDefaul t reasonin g
Commonsens e reasonin g is ofte n said to involv e "jumpin g to conclusions. " For example , whe n
one sees a car parke d on the street , one woul d normall y be willin g to accep t that it has four wheel s
even thoug h only three are visible . (If you feel that the existenc e of the fourt h whee l is dubious ,
conside r also the questio n as to whethe r the three visibl e wheel s are real or merel y cardboar d
facsimiles. ) Probabilit y theor y can certainl y provid e a conclusio n that the fourt h whee l exist s
with high probability . On the othe r hand , introspectio n suggest s that the possibilit y of the car
not havin g four wheel s does not even arise unless  some  new evidence  presents  itself.  Thus , it
seem s that the four­whee l conclusio n is reache d by default,  in the absenc e of any reaso n to doub t
it. If new evidenc e arrives—fo r example , if one sees the owne r carryin g a whee l and notice s
that the car is jacke d up—the n the conclusio n can be retracted . This kind of reasonin g is said to
exhibi t nonmonotonicity , becaus e the set of belief s does not grow monotonicall y over time as
new evidenc e arrives . First­orde r logic , on the othe r hand , exhibit s stric t monotonicity .
Reasonin g scheme s such as defaul t logic (Reiter , 1980) , nonmonotoni c logic (McDermot t
and Doyle , 1980 ) and circumscriptio n (McCarthy , 1980 ) are designe d to handl e reasonin g with
defaul t rules and retractio n of beliefs . Althoug h the technica l detail s of these system s are quite
different , they share a numbe r of problemati c issue s that arise with defaul t reasoning :
• Wha t is the semantic  statu s of defaul t rules ? If "Car s have four wheels " is false , wha t does
it mea n to have it in one's knowledg e base ? Wha t is a good set of defaul t rules to have ?
Withou t a good answe r to these questions , defaul t reasonin g system s will be nonmodular ,
and it will be hard to develo p a good knowledg e engineerin g methodology .
• Wha t happen s whe n the evidenc e matche s the premise s of two defaul t rules with conflict ­
ing conclusions ? We saw example s of this in the discussio n of multipl e inheritanc e in
Sectio n 10.6 . In som e schemes , one can expres s prioritie s betwee n rule s so that one rule
takes precedence . Specificit y preferenc e is a commonl y used form of priorit y in whic h
a special­cas e rule take s precedenc e over the genera l case . For example , "Three­wheele d
cars have thre e wheels " take s precedenc e over "Car s have four wheels. "
460 Chapte r 15 . Probabilisti c Reasonin g System s
• Sometime s a syste m may draw a numbe r of conclusion s on the basi s of a belie f that is
later retracted . Ho w can a syste m keep track of whic h conclusion s need to be retracte d
as a result ? Conclusion s that have multipl e justifications , only som e of whic h have been
abandoned , shoul d be retained ; wherea s thos e with no remainin g justification s shoul d be
dropped . Thes e problem s have been addresse d by truth maintenanc e systems , whic h are
discusse d in Sectio n 10.8 .
• How can belief s that have defaul t statu s be used to mak e decisions ? This is probabl y the
hardes t issue for defaul t reasoning . Decision s ofte n involv e trade­offs , and one therefor e
needs to compar e the strength  of belie f in the outcome s of differen t actions . In case s
wher e the same kind s of decision s are bein g mad e repeatedly , it is possibl e to interpre t
defaul t rule s as "threshol d probability " statements . For example , the defaul t rule "My
brake s are alway s OK" reall y mean s "The probabilit y that my brake s are OK, give n no
other information , is sufficientl y high that the optima l decisio n is for me to drive withou t
checkin g them. " Whe n the decisio n contex t changes—fo r example , whe n one is drivin g
a heavil y lade n truc k dow n a stee p mountai n road—th e defaul t rule suddenl y become s
inappropriate , even thoug h there is no new evidenc e to sugges t that the brake s are faulty .
To date, no defaul t reasonin g syste m has successfull y addresse d all of these issues . Furthermore ,
most system s are formall y undecidable , and very slow in practice . Ther e have been severa l
attempt s to subsum e defaul t reasonin g in a probabilisti c system , usin g the idea that a defaul t rule
is basicall y a conditiona l probabilit y of 1 — f. For reason s alread y mentioned , such an approac h
is likel y to requir e a full integratio n of decisio n makin g befor e it full y capture s the desirabl e
feature s of defaul t reasoning .
LOCALIT Y
DETACHMEN T
TRUTH ­
FUNCTIONALIT YRule­base d method s for uncertai n reasonin g
In additio n to monotonicity , logica l reasonin g system s have three othe r importan t propertie s that
probabilisti c reasoner s lack:
0 Locality : In logica l systems , wheneve r we have a rule of the form A => B, we can conclud e
B give n evidenc e A, without  worrying  about  any other  rules.  In probabilisti c systems , we
need to conside r all of the availabl e evidence .
<C> Detachment : Onc e a logica l proo f is foun d for a propositio n B, the propositio n can be
used regardles s of how it was derived . Tha t is, it can be detache d from its justification .
In dealin g with probabilities , on the othe r hand , the sourc e of the evidenc e for a belie f is |
importan t for subsequen t reasoning .
0 Truth­functionality : In logic , the truth of comple x sentence s can be compute d from the
truth of the components . Probabilit y combinatio n does not work this way , excep t unde r
stron g independenc e assumptions .
These propertie s confe r obviou s computationa l advantages . Ther e have been severa l attempt s to
devis e uncertai n reasonin g scheme s by attachin g degree s of belie f to proposition s and rules in
what is essentiall y a logica l system . Thi s mean s treatin g degre e of belie f as a generalize d truth
value , in orde r to retai n truth­functionality . Tha t is, each propositio n is assigne d a degre e of
belief , and the degre e of belie f in, say, A V B is a functio n of the belie f in A and the belie f in B.
Sectio n 15.6 . Othe r Approache s to Uncertai n Reasonin g 461
The bad new s for truth­functiona l system s is that the propertie s of locality,  detachment,
and truth­functionality  are simply  not appropriate  for uncertain  reasoning.  Let us look at truth ­
functionalit y first . Let H\ be the even t of a coin comin g up head s on a fair flip , let T\ be the
event of the coin comin g up tails on that sam e flip , and let H2 be the even t of the coin comin g
up head s on a secon d flip . Clearly , all thre e event s have the sam e probability , 0.5, and so a
truth­functiona l syste m mus t assig n the same belie f to the conjunctio n of any two of them . But
we can see that the probabilit y of the conjunctio n depend s on the event s themselves , and not just
on their probabilities :
CERTAINT Y FACTOR SP(A)
/>(//, ) = 0.5P(B)
P(//, ) = 0.5
p(r,) = o.5
P(H 2) = 0.5P(A V B)
P(H { V //,) = 0.50
P(H } vr,) = 1.00
P(Hi V H2) = 0.75
It gets wors e whe n we chai n evidenc e together . Truth­functiona l system s have rule s of the form
A H­> B, whic h allow us to comput e the belie f in B as a functio n of the belie f in the rule and the
belie f in A. Bot h forward ­ and backward­chainin g system s can be devised . The belie f in the
rule is assume d to be constant , and is usuall y specifie d by the knowledg e engineer , for example ,
A H­o. 9 B.
Conside r the wet­gras s situatio n from Sectio n 15.4 . If we wante d to be able to do both
causa l and diagnosti c reasoning , we woul d need the two rules :
Rain  i— WetGrass and WetGrass  H­*• Rain
If we are not careful , these two rules will act in a feedbac k loop so that evidenc e for Rain  increase s
the belie f in WetGrass,  whic h in turn increase s the belie f in Rain  even more . Clearly , uncertai n
reasonin g system s need to keep track of the path s alon g whic h evidenc e is propagated .
Intercausa l reasonin g (or explainin g away ) is also tricky . Conside r what happen s whe n we
have the two rules :
Sprinkler  i— WetGrass and WetGrass  i—» Rain
Suppos e we see that the sprinkle r is on. Chainin g forwar d throug h our rules , this increase s the
belie f that the gras s will be wet, whic h in turn increase s the belie f that it is raining . Bu t this
is ridiculous : the fact that the sprinkle r is on explain s awa y the wet grass , and shoul d reduce
the belie f in rain . In a truth­functiona l system , the transitivel y derive d rule Sprinkler  t—> Rain  is
unavoidable .
Give n these difficulties , how is it possibl e that truth­functiona l system s were ever consid ­
ered useful ? The answe r lies in restrictin g the tasks require d of them , and in carefull y engineerin g
the rule base so that undesirabl e interaction s do not occur . The mos t famou s exampl e of a truth ­
functiona l syste m for uncertai n reasonin g is the certaint y factor s model , whic h was develope d
for the MYCI N medical  diagnosi s progra m and widel y used in exper t system s of the late 1970 s and
1980s . Almos t all uses of certaint y factor s involve d rule sets that were eithe r purel y diagnosti c
(as in MYCIN ) or purel y causal . Furthermore , evidenc e was only entere d at the "roots " of the rule
set, and mos t rule sets were singl y connected . Heckerma n (1986 ) has show n that unde r thes e
circumstances , a mino r variatio n on certainty­facto r inferenc e was exactl y equivalen t to Bayesia n
462 Chapte r 15 . Probabilisti c Reasonin g System s
inferenc e on polytrees . In othe r circumstances , certaint y factor s coul d yield disastrousl y incorrec t
degree s of belie f throug h overcountin g of evidence . Detail s of the metho d can be foun d unde r
the "Certaint y Factors " entr y in Encyclopedia  ofAI,  but the use of certaint y factor s is no longe r
recommende d (eve n by one of its inventors—se e the forewor d to Heckerma n (1991)) .
DEMPSTER­SHAFE R
BELIE F FUNCTIO NRepresentin g ignorance : Dempster­Shafe r theor y
The Dempster­Shafe r theor y is designed  to deal with the distinctio n betwee n uncertaint y and
ignorance . Rathe r than computin g the probabilit y of a proposition , it compute s the probabilit y
that the evidenc e support s the proposition . Thi s measur e of belie f is calle d a belie f function ,
writte n Bel(X).
We retur n to coin flippin g for an exampl e of belie f functions . Suppos e a shad y characte r
come s up to you and offer s to bet you $10 that his coin will come up head s on the next flip. Give n
that the coin may or may not be fair, wha t belie f shoul d you ascrib e to the even t of it comin g up
heads ? Dempster­Shafe r theor y says that becaus e you have no evidenc e eithe r way, you have to
say that the belie f Bel(Heads)  = 0, and also that Bel(­^Heads)  = 0. This make s Dempster­Shafe r
reasonin g system s skeptica l in a way that has som e intuitiv e appeal . Now suppos e you have an
exper t at you r disposa l who testifie s with 90% certaint y that the coin is fair (i.e., he is 90% sure
that P(Heads)  = 0.5) . The n Dempster­Shafe r theor y give s Bel(Heads)  = 0.9 x 0.5 = 0.45 and
likewis e Bel(­<Heads)  = 0.45 . Ther e is still a 0.1 "gap " that is not accounte d for by the evidence .
"Dempster' s rule" (Dempster , 1968 ) show s how to combin e evidenc e to give new value s for Bel,
and Shafer' s work extend s this into a complet e computationa l model .
As with defaul t reasoning , there is a proble m in connectin g belief s to actions . With prob ­
abilities , decisio n theor y says that if P(Heads)  = P(­^Heads)  ­ 0.5 then (assumin g that winnin g
$10 and losin g $10 are considere d equa l opposites ) the reasone r will be indifferen t betwee n the
actio n of acceptin g and declinin g the bet. A Dempster­Shafe r reasone r has Bel(­^Heads)  ­ 0,
and thus no reaso n to accep t the bet, but then it also has Bel(Heads)  ­ 0, and thus no reaso n to
declin e it. Thus , it seem s that the Dempster­Shafe r reasone r come s to the same conclusio n abou t
how to act in this case . Unfortunately , Dempster­Shafe r theor y allow s no definit e decisio n in
many othe r case s wher e probabilisti c inferenc e does yield a specifi c choice . In fact, the notio n
of utilit y in the Dempster­Shafe r mode l is not yet well­understood , partl y becaus e the semantic s
of Bel is not define d precisel y with respec t to decisio n making .
One interpretatio n of Dempster­Shafe r theor y is that it define s a probabilit y interval—th e
interva l for Heads  is [0,1 ] befor e our exper t testimony , and [0.45,0.55 ] after . The widt h of the
interva l can be a good aid in decidin g whe n we need to acquir e mor e evidence : it can tell you
that the expert' s testimon y will help you if you do not know whethe r the coin is fair, but will
not help you if you have alread y determine d that the coin is fair. In the Bayesia n approach , this
kind of reasonin g can be don e easil y by examinin g how muc h one' s belie f woul d chang e if one
were to acquir e mor e evidence . For example , knowin g whethe r the coin is fair woul d have a
significan t impac t on the belie f that it will com e up heads . A Bayesia n probabilit y therefore  has
an "implicit " uncertaint y associate d with the variou s possibl e change s that it migh t underg o as a
resul t of futur e observations .
Sectio n 15.6 . Othe r Approache s to Uncertai n Reasonin g 463
Representin g vagueness : Fuzz y sets and fuzz y logic
FUZZY SET THEOR Y Fuzz y set theor y is a mean s of specifyin g how well an objec t satisfie s a vagu e description . For
example , conside r the propositio n "Nat e is tall." Is this true , give n that Nate is 5' 10"? Mos t
peopl e woul d hesitat e to answe r "true " or "false, " preferrin g to say, "sort of." Not e that this is not
a questio n of uncertaint y abou t the externa l worl d — we are sure of Nate' s height . Rathe r it is a
case of vaguenes s or uncertaint y abou t the meanin g of the linguisti c term "tall. " So most  authors
I ?­ say  that fuzzy  set theory  is not a method  for uncertain  reasoning  at all.
Anothe r way to thin k of this is as similarit y to a prototyp e — how clos e is Nate to our
prototyp e for tall person ? We coul d expres s this in logica l notatio n by makin g TallPerson  a
constan t and havin g a functio n Similarity  that compare s thing s to it:
Similarity(Nate,  TallPerson)  = SortOf
Fuzz y set theor y take s a slightl y differen t approach : it treat s TallPerson  as a fuzz y predicat e and
says that the truth valu e of TallPerson(Nate)  is a numbe r betwee n 0 and 1 , rathe r than bein g just
True or False.  The nam e "fuzz y set" derive s from the interpretatio n of the predicat e as implicitl y
definin g a set of its members , a set that does not have shar p boundaries . Conside r a kenne l
with 60 rottweiler s and 40 retrievers . If we pick a dog at random , the probabilit y of it bein g a
rottweile r is 0.6; this is uncertainty . The uncertaint y can be resolve d by lookin g at the dog. But if
a rottweile r and a retrieve r were to mate , then the puppie s woul d be half rottweiler , half retriever .
There is no uncertaint y here , no additiona l evidenc e we can gathe r to determin e the bree d of the
puppies . Rather , this is a case of fuzzines s at the boundar y betwee n two breeds .
FUZZY LOGI C Fuzz y logi c take s a comple x sentenc e such as TallPerson(Nate)  V Smart(Nate)  and deter ­
mine s its truth valu e as a functio n of the truth value s of its components . The rule s for evaluatin g
the fuzz y truth , T, of a comple x sentenc e are
T(A/\B)  = min(T(A),T(B))
T(AVB)  = max(TXA) ,
Fuzz y logi c is therefor e a truth­functiona l system , and is thus subjec t to all the usua l problems .
It has the additiona l proble m that it is inconsisten t with norma l propositiona l or first­orde r logic .
We woul d like any logic to ensur e that standar d equivalence s such as A V ­iA ­o­ True  hold , but
in fuzz y logic , T(A V ­^A)^T(True).
Despit e these seriou s semanti c difficulties , fuzz y logic has been very successfu l in commer ­
cial applications , particularl y in contro l system s for product s such as automati c transmissions ,
trains , vide o cameras , and electri c shavers . Elka n (1993 ) argue s that these application s are suc­
cessfu l becaus e they have smal l rule bases , hav e no chainin g of inferences , and have tunabl e
parameter s that can be adjuste d to improv e the system' s performanc e (ofte n by learnin g tech ­
niques) . The fact that they are implemente d with fuzz y operator s is incidenta l to their success .
Elkan predict s that whe n mor e comple x application s are tackle d with fuzz y logic , they will run
into the sam e kind s of problem s that plagued  knowledge­base d system s usin g certaint y factor s
and othe r approaches . As one migh t expect , the debat e continues .
464 Chapte r 15 . Probabilisti c Reasonin g System s
15.7 SUMMAR Y
Both Chapte r 9 and this chapte r are concerne d with reasonin g properly , but ther e is a differenc e
in just wha t that means . In first­orde r logic , prope r reasonin g mean s that conclusion s follo w from
premises—tha t if the initia l knowledg e base faithfull y represent s the world , then the inference s
also faithfull y represen t the world . In probability , we are dealin g with beliefs , not with the state
of the world , so "prope r reasoning " mean s havin g belief s that allow an agen t to act rationally .
In Chapte r 10, we saw a variet y of approache s to the proble m of implementin g logica l
reasonin g systems . In this chapter , we see muc h mor e of a consensus : efficien t reasonin g
with probabilit y is so new that there is one main approach—belie f networks—wit h a few mino r
variations . The main point s are as follows :
• Conditiona l independenc e informatio n is a vital and robus t way to structur e informatio n
abou t an uncertai n domain .
• Belie f network s are a natura l way to represen t conditiona l independenc e information . The
links betwee n node s represen t the qualitativ e aspect s of the domain , and the conditiona l
probabilit y table s represen t the quantitativ e aspects .
• A belie f networ k is a complet e representatio n for the join t probabilit y distributio n for the
domain , but is ofte n exponentiall y smalle r in size.
• Inferenc e in belie f network s mean s computin g the probabilit y distributio n of a set of quer y
variables , give n a set of evidenc e variables .
• Belie f network s can reaso n causally , diagnostically , in mixe d mode , or intercausally . No
other uncertai n reasonin g mechanis m can handl e all these modes .
• The complexit y of belie f networ k inferenc e depend s on the networ k structure . In polytree s
(singl y connecte d networks) , the computatio n time is linea r in the size of the network .
• Ther e are variou s inferenc e technique s for genera l belie f networks , all of whic h have
exponentia l complexit y in the wors t case . In real domains , the loca l structur e tend s to
make thing s more feasible , but care is neede d to construc t a tractabl e networ k with more
than a hundre d nodes .
• It is also possibl e to use approximatio n techniques , includin g stochasti c simulation , to get
an estimat e of the true probabilitie s with less computation .
• Variou s alternativ e system s for reasonin g with uncertaint y have been suggested . All the
truth­functiona l system s have seriou s problem s with mixe d or intercausa l reasoning .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
The use of network s to represen t probabilisti c informatio n bega n early in the twentiet h century ,
with the wor k of Sewal l Wrigh t on the probabilisti c analysi s of geneti c inheritanc e and anima l
growt h factor s (Wright , 1921 ; Wright , 1934) . One of his network s appear s on the cove r of this
Sectio n 15.7 . Summar y 465
QUALITATIV Ebook . I. J. Goo d (1961 ) investigate d the use of Bayesia n inferenc e in belie f networks.5 The in­
fluenc e diagra m or decisio n networ k representatio n for decisio n problems , whic h incorporate d
a DAG representatio n for rando m variables , was used in decisio n analysi s in the late 1970 s (see
Chapte r 16), but no interestin g evaluatio n algorithm s were develope d for decisio n network s unti l
1986 (Shachter , 1986) . Jude a Pear l (1982a ) develope d the message­passin g metho d for carryin g
out inferenc e in network s that had the form of trees . Jin Kim (Kir n and Pearl , 1983 ) extende d
the metho d to all singl y connecte d networks . The backward­chainin g algorith m give n in the text
was derive d by the authors , but has muc h in commo n with the method s of Pear l (1988 ) and of
Shachte r et al. (1990) . Gregor y Coope r (1990 ) showe d that the genera l proble m of inferenc e in
unconstraine d belie f network s is NP­hard , and Paul Dagu m and Mik e Lub y (1993 ) showe d the
correspondin g approximatio n proble m to be NP­hard .
David Spiegelhalte r and Steffe n Lauritze n pioneere d the use of clusterin g for inferenc e
in multipl y connecte d network s (Spiegelhalter , 1986 ; Lauritze n and Spiegelhalter , 1988) . Finn
Jense n and colleague s (1990 ) develope d an object­oriente d computationa l schem e for clustering .
The schem e is implemente d in the HUGI N system , an efficien t and widel y used tool for uncertai n
reasonin g (Anderse n et al., 1989) . Eric Horvitz , H. Jacque s Suermondt , and Gregor y Coope r
(1989 ) develope d bounde d cutse t conditioning , buildin g on earlie r work on cutset s (Pearl , 1986) .
The logic samplin g metho d is due to Max Henrio n (1988) , and likelihoo d weightin g was develope d
by Fung and Chan g (1989 ) and Shachte r and Peot (1989) . A large­scal e applicatio n of likelihoo d
weightin g to medica l diagnosi s appear s in Shw e and Coope r (1991) . Th e latte r pape r also
incorporates  element s of the "Marko v blanket " simulatio n schem e develope d by Pearl (1987) . The
first exper t syste m using belie f network s was CONVINC E (Kim , 1983 ; Kim and Pearl , 1987) . Mor e
recen t system s includ e the MUNI N syste m for diagnosin g neuromuscula r disorder s (Anderse n et
al., 1989 ) and the PATHFINDE R syste m for patholog y (Heckerman , 1991) .
The extensio n of belie f network s to handl e continuou s rando m variable s is an importan t
topic of curren t research . The basic technica l proble m is that the distribution s mus t be repre ­
sentabl e by a finit e numbe r of parameters , and mus t com e from a famil y of distribution s that
is close d unde r belie f net updating . To date , only Gaussia n distribution s have been used . Net ­
work s with continuou s variable s but no discret e variable s have been considere d (Pearl , 1988 ;
Shachte r and Kenley , 1989) . Suc h network s can be evaluate d in polynomia l time , regard ­
less of topology . The inclusio n of discret e variable s has been investigate d by Lauritze n and
Wermut h (1989) , and implemente d in the cHUGI N syste m (Olesen , 1993) . Currently , exac t
algorithm s are know n only for the case in whic h discret e variable s are ancestors , not descendants ,
of continuou s variables . Stochasti c simulatio n algorithms , on the othe r hand , can handl e arbitrar y
distribution s and topologies .
Qualitativ e probabilisti c network s (Wellman , 1990 ) provid e a purel y qualitativ e abstrac ­
tion of belie f networks , usin g the notio n of positiv e and negativ e influence s betwee n variables .
Wellma n show s that in man y case s such informatio n is sufficien t for optima l decisio n makin g
withou t the need for precis e specificatio n of probabilit y values . Wor k by Adna n Darwich e and
Matt Ginsber g (1992 ) extract s the basi c propertie s of conditionin g and evidenc e combinatio n
from probabilit y theor y and show s that they can also be applie d in logica l and defaul t reasoning .
5 I. J. Goo d was chie f statisticia n for Turing' s codebreakin g team in Worl d II. In 2001:  A Space  Odyssey,  Arthu r
C. Clark e credite d Goo d and Minsk y with makin g the breakthroug h that led to the developmen t of the HAL computer .
466 Chapte r 15 . Probabilisti c Reasonin g System s
POSSIBILIT Y THEOR YThe literatur e on defaul t and nonmonotoni c reasonin g is very large . Earl y wor k is col­
lected in Readings  in Nonmonotonic  Reasoning  (Ginsberg , 1987) . Shoha m (1988 ) discusse s
the nonmonotoni c natur e of reasonin g abou t time and change , and propose s a semantic s base d
on genera l preference s betwee n possibl e model s of a defaul t knowledg e base . Geffne r (1992 )
provide s an excellen t introductio n to defaul t reasoning , includin g early philosophica l wor k on
defaul t conditionals . Som e informativ e computationa l complexit y result s on defaul t reasonin g
have been derive d by Kaut z and Selma n (1991) . Bain and Muggleto n (1991 ) provid e method s for
learnin g defaul t rules , suggestin g a role for them as compac t representation s of data containin g
regularitie s with exceptions . Autoepistemi c logic (Moore , 1985b ) attempt s to explai n nonmono ­
tonicit y as arisin g from the agent' s reflectio n on its own state s of knowledge . A comprehensiv e
retrospectiv e and summar y of work in this area is give n by Moor e (1993) .
Application s of nonmonotoni c reasonin g have been largel y limite d to improvin g the the­
oretica l underpinning s of logic programming . The use of negatio n as failur e in Prolo g can be
viewe d as sourc e of nonmonotonicit y becaus e addin g new fact s will rule out som e potentia l
derivation s (Clark , 1978) . An importan t subclas s of the so­calle d defaul t theorie s in the defaul t
logic of (Reiter , 1980 ) can be show n to be equivalen t to logi c program s in a languag e in whic h
both negatio n as failur e and classica l logica l negatio n are availabl e (Gelfon d and Lifschitz , 1991) .
The same is true of circumscriptiv e theorie s (Gelfon d and Lifschitz , 1988) . Autoepistemi c logic
can, in turn , be closel y imitate d by circumscriptio n (Lifschitz , 1989) .
Certaint y factor s wer e invente d for use in the medica l exper t syste m MYCI N (Shortliffe ,
1976) , whic h was intende d both as an engineerin g solutio n and as a mode l of huma n judgmen t
unde r uncertainty . The collectio n Rule­Based  Expert  Systems  (Buchana n and Shortliffe , 1984 )
provide s a complet e overvie w of MYCI N and its descendants . Davi d Heckerma n (1986 ) showe d
that a slightl y modifie d versio n of certaint y factor s can be analyze d as a disguise d form of reason ­
ing with standar d probabilit y theory . The PROSPECTO R exper t syste m (Dud a et al., 1979 ) used a
rule­base d approac h in whic h the rules were justifie d by a globa l independenc e assumption . Mor e
recently , Brya n Tod d has show n that a rule­base d syste m can perfor m correc t Bayesia n infer ­
ence, provide d that the structur e of the rule set exactl y reflect s a set of conditiona l independenc e
statement s that is equivalen t to the topolog y of a belie f networ k (Tod d et al., 1993) .
Dempster­Shafe r theor y originate s with a pape r by Arthu r Dempste r (1968 ) proposin g a
generalizatio n of probabilit y to interva l values , and a combinatio n rule for usin g them . Late r work
by Glen n Shafe r (1976 ) led to the Dempster­Shafe r theor y bein g viewe d as a competin g approac h
to probability . Ruspin i et al. (1992 ) analyz e the relationshi p betwee n the Dempster­Shafe r theor y
and standar d probabilit y theor y from an Al standpoint . Sheno y (1989 ) has propose d a metho d
for decisio n makin g with Dempster­Shafe r belie f functions .
Fuzz y sets wer e develope d by Lotf i Zade h (1965 ) in respons e to the perceive d difficult y
of providin g exac t input s for intelligen t systems . The text by Zimmerman n (1991 ) provide s a
thoroug h introductio n to fuzz y set theory . As we mentione d in the text , fuzz y logi c has ofte n
been perceive d incorrectl y as a direc t competito r to probabilit y theor y wherea s in fact it addresse s
a differen t set of issues . Possibilit y theor y (Zadeh , 1978 ) was introduce d to handl e uncertaint y
in fuzz y systems , and has muc h in commo n with probability . Duboi s and Prad e (1994 ) provid e
a thoroug h surve y of the connection s betwee n possibilit y theor y and probabilit y theory . An
interestin g interview/debat e with Lotf i Zade h (the founde r of fuzz y logic ) and Willia m Kaha n
(the Turin g awar d winner ) appear s in Woeh r (1994) .
Sectio n 15.7 . Summar y 467
The thre e approache s to quantitativ e reasonin g abou t uncertaint y just surveyed—fuzz y
logic , certaint y factors , and Dempster­Shafe r theory—wer e the three main alternative s to whic h
AI researcher s resorte d after early probabilisti c system s fell out of favo r in the early 1970s , as de­
scribe d in Chapte r 14. The later developmen t of nonmonotoni c logic s in the early 1980 s was also,
in part, a respons e to the need for an effectiv e metho d for handlin g uncertainty . The resurgenc e
of probabilit y depende d mainl y on the discover y of belie f network s as a metho d for representin g
and usin g conditiona l independenc e information . This resurgenc e did not com e withou t a fight —
probabilisti c method s were attacke d both by logicists , who believe d that numerica l approache s
to AI wer e both unnecessar y and introspectivel y implausible , and by supporter s of the othe r
quantitativ e approache s to uncertainty . Pete r Cheeseman' s (1985 ) pugnaciou s "In Defens e of
Probability, " and his later articl e "An Inquir y into Compute r Understanding " (Cheeseman , 1988 ,
with commentaries ) give somethin g of the flavo r of the debate .
It is fair to say that certaint y factor s are now of historica l interes t only (for reasons  note d
above) , and althoug h fuzz y logi c is a healthy , ongoin g enterprise , it is increasingl y perceive d
as a way of handlin g continuous­value d variable s rathe r than uncertainty . Nonmonotoni c logic s
continu e to be of interes t as a purel y qualitativ e mechanism , althoug h there is a good deal of work
aimed at showin g how nonmonotoni c inferenc e is best viewe d as a specia l case of probabilisti c
reasonin g with probabilitie s close to 0 or 1 (Goldszmid t et al., 1990) . Dempster­Shafe r theor y
is still perceive d by its supporter s as a viabl e alternativ e to Bayesia n systems . Nonprobabilist s
continu e to be irritate d by the dogmati c "Bayesianity " of the probabilists .
Probabilistic  Reasoning  in Intelligent  Systems:  Networks  of Plausible  Inference  (Pearl ,
1988) is a comprehensiv e textboo k and referenc e on belie f networks , by one of the majo r contrib ­
utors to the field . Probabilistic  Reasoning  in Expert Systems:  Theory  and Algorithms  (Neapolitan ,
1990 ) give s a relativel y readabl e introductio n to belie f network s from the standpoin t of their use
in exper t systems . Readings  in Uncertain  Reasoning  (Shafe r and Pearl , 1990 ) is a voluminou s
collectio n of importan t paper s abou t probability , belie f networks , and othe r formalism s for rea­
sonin g abou t uncertainty , and their application s in AI. Uncertainty  in Artificial Intelligence  (Kana l
and Lemmer , 1986 ) is an antholog y containin g a numbe r of importan t early paper s abou t belie f
network s and othe r matter s havin g to do with reasonin g unde r uncertainty . Althoug h this volum e
is not numbered , a serie s of numbere d anthologie s with the same title have also been published .
New researc h on probabilisti c reasonin g appear s both in mainstrea m AI journal s such as Artificial
Intelligence  and in mor e specialize d journal s such as the International  Journal  of Approximate
Reasoning.  The proceeding s of the Conference s on Uncertaint y in Artificia l Intelligenc e (know n
as UAI ) are an excellen t sourc e for curren t research .
EXERCISE S
15.1 Conside r the proble m of dealin g with a car that will not start , as diagramme d in Figur e 15.5 .
a. Exten d the networ k with the Boolea n variable s IcyWeather  and StarterMotorWorking.
b. Giv e reasonabl e conditiona l probabilit y table s for all the nodes .
468 Chapte r 15 . Probabilisti c Reasonin g System s
c. How man y independen t value s are containe d in the join t probabilit y distributio n for eigh t
Boolea n nodes , assumin g no conditiona l independenc e relation s hold amon g them ?
d. How man y independen t probabilit y value s do your networ k table s contain ?
e. The conditiona l probabilit y table for Starts  is a canonica l one. Describe , in English , wha t
its structur e will be in general , as more possibl e cause s for not startin g are added .
15.2 In you r loca l nuclea r powe r station , ther e is an alar m that sense s whe n a temperatur e
gaug e exceed s a give n threshold . Th e gaug e measure s the core temperature . Conside r the
Boolea n variable s A (alar m sounds) , FA (alar m is faulty) , and FC (gaug e is faulty) , and the
multivalue d node s G (gaug e reading ) and T (actua l core temperature) .
a. Dra w a belie f net for this domain , give n that the gaug e is more likel y to fail whe n the core
temperatur e gets too high .
b. Is your networ k a polytree ?
c. Suppos e ther e are just two possibl e actua l and measure d temperatures , Norma l and High ;
and that the gaug e give s the incorrec t temperatur e x% of the time whe n it is working , but
v% of the time whe n it is faulty . Giv e the conditiona l probabilit y table associate d with G.
d. Suppos e the alar m work s unles s it is faulty , in whic h case it neve r goes off. Giv e the
conditiona l probabilit y table associate d with A.
e. Suppos e the alarm and gaug e are working , and the alarm sounds . Calculat e the probabilit y
that the core temperatur e is too high .
f. In a give n time period , the probabilit y that the temperatur e exceeds  threshol d is p. The cost
of shuttin g dow n the reacto r is c.s; the cost of not shuttin g it dow n whe n the temperatur e is
in fact too high is cm (m is for meltdown) . Assumin g the gaug e and alarm to be workin g
normally , calculat e the maximu m valu e for x for whic h the gaug e is of any use (i.e., if x is
any highe r than this, we have to shut dow n the reacto r all the time) .
g. Suppos e we add a secon d temperatur e gaug e H, connecte d so that the alarm goes off when
eithe r gaug e read s High . Wher e do H and FH (the even t of H failing ) go in the network ?
h. Are ther e circumstance s unde r whic h addin g a secon d gaug e woul d mea n that we woul d
need more  accurat e (i.e. , mor e likel y to give the correc t temperature ) gauges ? Why (not) ?
15.3 Tw o astronomers , in differen t part s of the world , mak e measurement s M\ and MI of the
numbe r of stars N in som e smal l regio n of the sky, usin g thei r telescopes . Normally , ther e is
a smal l possibilit y of error by up to one star. Eac h telescop e can also (wit h a slightl y smalle r
probability ) be badl y out of focu s (event s F\ and ^2), in whic h case the scientis t will undercoun t
by three or mor e stars . Conside r the three network s show n in Figur e 15.12 .
a. Whic h of thes e belie f network s correctl y (but not necessaril y efficiently ) represen t the
above information ?
b. Whic h is the best network ?
c. Giv e a reasonabl e conditiona l probabilit y table for the value s of P(M\  \N~). (For simplicity ,
conside r only the possibl e value s 1, 2, and 3 in this part. )
d. Suppos e MI = 1 and M3 = 3. Wha t are the possibl e number s of stars ?
e. Of these,  whic h is the mos t likel y number ?
Sectio n 15.7 . Summar y 469
Figur e 15.1 2 Thre e possibl e network s for the telescop e problem .
15.4 Yo u are an Al consultan t for a credi t card company , and your task is to construc t a belie f
net that will allow the compan y to determin e whethe r or not to gran t a perso n a card .
a. Wha t are the evidenc e variables ? Thes e are the variable s for whic h you can obtai n
information , on the basi s of whic h it is lega l to mak e decisions , and that are relevan t to
the decision . Thus , Age migh t be one of your variables , but VotingRecord  and HairColor
woul d not.
b. Wha t is the outpu t variabl e (i.e. , wha t propositio n is the compan y goin g to examin e the
probabilitie s of in orde r to determin e whethe r to gran t a card) ? This shoul d not be Decision
with value s \es and no becaus e the compan y has contro l over the valu e of this variable .
c. Construc t your networ k by incrementall y addin g variable s in causa l order , as describe d in
the chapter . You may wish to add intermediat e node s such as Reliability  and Futurelncome.
(Remembe r that the compan y care s abou t wha t will happe n in the future , not abou t the past
per se.) Set the conditiona l probabilitie s for each node . Writ e commentar y to describ e
your reasonin g in choosin g your variable s and links . If you find that a node has too man y
predecessor s (so that the conditiona l probabilit y table become s too big) , that is a good hint
that you need som e intermediat e variables .
d. Buil d a file of test data correspondin g to your evidenc e variables . As far as possibl e thes e
shoul d be real people ! Do som e interviewin g to get real data if you can; try to get a  wide
variet y of cases , from deadbeat s to trust­fun d babies . Run the data throug h your networ k
to see if the net's result s correspon d to your own judgements . Examin e not only the outpu t
variable , but the valu e of othe r intermediat e variables . You may need to go throug h severa l
iteration s of steps (c) and (d).
e. Writ e a repor t showin g variou s test case s and explainin g the advantage s of your approach .
Your repor t shoul d be enoug h to convinc e the compan y to adop t your product .
15.5 Yo u are an AI consultan t for an auto insuranc e company . You r task is construc t a belie f
networ k that will allo w the compan y to decid e how muc h financia l risk they run from variou s
polic y holders , give n certai n data abou t the polic y holders . (In case you thin k all clas s project s
470 Chapte r 15 . Probabilisti c Reasonin g System s
are just toys to amus e sadisti c instructors , bear in mind that a 1 % improvemen t in risk assessmen t
is wort h well over a billio n dollar s a year. )
In orde r to desig n a belie f network , you need output  variables  and evidence  variables.
The outpu t variable s are thos e for whic h the insuranc e compan y is tryin g to get a probabilit y
distributio n for their possibl e values . The evidenc e variable s are thos e variable s for whic h you
can obtai n information , on the basi s of whic h it is lega l to mak e decisions , and that are relevan t
to the decision .
Outpu t variable s represen t the costs of variou s catastrophi c event s that the insuranc e com­
pany migh t have to reimburse . (W e do not conside r the way s in whic h the compan y tries to
avoid payment. ) In the automobil e insuranc e domain , the majo r outpu t variable s are medica l cost
(MedCost),  propert y cost (PropCost),  and intangibl e liabilit y cost (ILiCost).  Medica l and prop ­
erty cost s are thos e incurre d by all individual s involve d in an accident ; auto thef t or vandalis m
migh t also incu r propert y costs . Intangibl e liabilit y costs are legal penaltie s for thing s like "pain
and suffering, " punitiv e damages , and so forth , that a drive r migh t incu r in an acciden t in whic h
he or she is at fault .
Evidenc e variable s for this domai n includ e the driver' s age and record ; whethe r or not he
or she own s anothe r car; how far he or she drive s per year ; the vehicle' s make , mode l and year ;
whethe r it has safet y equipmen t such as an airba g and antiloc k brakes ; wher e it is garage d and
whethe r it has an antithef t device .
Build a networ k for this problem . Yo u will need to decid e on suitabl e domain s for the
variables , bearin g in min d the need to discretiz e (unles s you plan to use a stochasti c simulatio n al­
gorithm) . You will also need to add intermediat e node s such as DrivingSkill  andAutoRuggedness.
Writ e commentar y to describ e you r reasonin g in choosin g you r domains , variables , links , and
inferenc e algorithm . If you find that a node has too man y predecessor s (so that the conditiona l
probabilit y table become s too big), that is a good hint that you need some intermediat e variables .
Generat e a few reasonable­lookin g test case s to get a feelin g for wha t you r networ k does with
them . How woul d you convinc e the insuranc e compan y to adop t your product ?
15.6 Is probabilisti c reasonin g monotoni c or nonmonotonic ? Do thes e concept s even appl y to
probabilities ?
16MAKIN G SIMPL E
DECISION S
In which  we see how  an agent  should  make  decisions  so that it gets what it  wants—on
average,  at least.
In this chapter , we retur n to the idea of utilit y theor y that was introduce d in Chapte r 14 and show
how it is combine d with probabilit y theor y to yield a decision­theoreti c agent—on e that can mak e
rationa l decision s base d on wha t it believe s and wha t it wants . Suc h an agen t can mak e decision s
in context s wher e uncertaint y and conflictin g goal s leav e a logica l agen t with no way to decide .
Sectio n 16.1 introduce s the basic principl e of decisio n theory : the maximizatio n of expecte d
utility . Sectio n 16.2 show s that the behavio r of any rationa l agen t can be capture d by supposin g a
utilit y functio n that is bein g maximized . Sectio n 16.3 discusse s the natur e of utilit y function s in
more detail , and in particula r their relatio n to individua l quantitie s such as money . Sectio n 16.4
show s how to handl e utilit y function s that depen d on severa l quantities . In Sectio n 16.5 , we
describ e the implementatio n of decision­makin g systems . In particular , we introduc e a formalis m
called decisio n network s (also know n as influenc e diagrams ) that extend s belie f network s by
incorporatin g action s and utilities . The remainde r of the chapte r discusse s issue s that arise in
application s of decisio n theor y to exper t systems .
16.1 COMBININ G BELIEF S AND DESIRE S UNDE R UNCERTAINT Y
In the Port­Royal  Logic,  writte n in 1662 , the Frenc h philosophe r Arnaul d state d that
To judg e wha t one mus t do to obtai n a good or avoi d an evil , it is necessar y to conside r not
only the good and the evil in itself , but also the probabilit y that it happen s or does not happen ;
and to view geometricall y the proportio n that all thes e thing s have together .
These days , it is mor e commo n in scientifi c texts to talk of utilit y rathe r than good and evil, but
the principl e is exactl y the same . An agent' s preference s betwee n worl d state s are capture d by a
utilit y function , whic h assign s a singl e numbe r to expres s the desirabilit y of a state . Utilitie s are
combine d with the outcom e probabilitie s for action s to give an expecte d utilit y for each action.
471
472 Chapte r 16 . Makin g Simpl e Decision s
EXPECTE D UTILIT Y
MAXIMU M EXPECTE D
UTILIT Y
ONE­SHO T
DECISION SWe will use the notatio n U(S)  to denot e the utilit y of state S accordin g to the agen t that
is makin g the decisions . For now , we will conside r state s as complet e snapshot s of the world ,
simila r to the situation s of Chapte r 7. This will simplif y our initia l discussions , but it can becom e
rathe r cumbersom e to specif y the utilit y of each possibl e state separately . In Sectio n 16.4 , we
will see how state s can be decompose d unde r som e circumstance s for the purpos e of utilit y
assignment .
An nondeterministi c actio n A will hav e possibl e outcom e state s Resulti(A),  wher e i
range s over the differen t outcomes . Prio r to executio n of A, the agen t assign s probabilit y
P(Resulti(A)\Do(A),E)  to each outcome , wher e E summarize s the agent' s availabl e evidenc e
abou t the world , and Do(A)  is the propositio n that actio n A is execute d in the curren t state .
Then we can calculat e the expecte d utilit y of the actio n give n the evidence , EU(A\E),  usin g the
followin g formula :
EU(A\E)  = '^2 lP(Result i(A)\E,Do(A})  U(Result t(A)) (16.1 )
The principl e of maximu m expecte d utilit y (MEU ) says that a rationa l agen t shoul d choos e an
action that maximize s the agent' s expecte d utility .
In a sense , the MEU principl e could be seen as definin g all of AI.1 All an intelligen t agen t
has to do is calculat e the variou s quantities , maximiz e over its actions , and away it goes . But this
does not mea n that the AI proble m is solved  by the definition !
Althoug h the MEU principl e define s the righ t actio n to take in any decisio n problem ,
the computation s involve d can be prohibitive , and sometime s it is difficul t even to formulat e
the proble m completely . Knowin g the initia l state of the worl d require s perception , learning ,
knowledg e representation , and inference . Computin g P(Result,(A)  \Do(A),  E) require s a complet e
causa l mode l of the worl d and, as we saw in Chapte r 15, NP­complet e updatin g of belie f nets.
Computin g the utilit y of each state , U (Result  i(A)),  ofte n require s searc h or planning , becaus e an
agent does not know how good a state is unti l it know s wher e it can get to from that state . So,
decisio n theor y is not a panace a that solve s the AI problem . But it does provid e a framewor k in
whic h we can see wher e all the component s of an AI syste m fit in.
The MEU principl e has a clear relatio n to the idea of performanc e measure s introduce d in
Chapte r 2. The basic idea is very simple . Conside r the possibl e environment s that coul d lead to
an agen t havin g a give n percep t history , and conside r the differen t possibl e agents  that we could
design . If an agent  maximizes  a utility  function  that  correctly  reflects the  performance  measure
by which  its behavior  is being  judged,  then  it will achieve  the highest  possible  performance  score,
if we average  over  the possible  environments  in which  the agent  could be  placed.  Thi s is the
centra l justificatio n for the MEU principl e itself .
In this chapter , we will only be concerne d with singl e or one­sho t decisions , wherea s
Chapte r 2 define d performanc e measure s over environmen t histories , whic h usuall y involv e
many decisions . In the next chapter , whic h cover s the case of sequentia l decisions , we will show
how these two view s can be reconciled .
1 Actually , it is not quite true that AI is, even in principle , a field that attempt s to build agent s that maximiz e their expecte d
utility . We believe , however , that understandin g such agent s is a good plac e to start . This is a difficul t methodologica l
question , to whic h we retur n in Chapte r 27.
Sectio n 16.2 . Th e Basi s of Utilit y Theor y 47 3
16.2 TH E BASI S OF UTILIT Y THEORY__________________ _
Intuitively , the principl e of Maximu m Expecte d Utilit y (MEU ) seem s like a reasonabl e way to
make decisions , but it is by no mean s obviou s that it is the only  rationa l way . Afte r all, why
shoul d maximizin g the average  utilit y be so special—wh y not try to maximiz e the sum of the
cubes of the possibl e utilities , or try to minimiz e the wors t possibl e loss? Also , couldn' t an agen t
act rationall y just by expressin g preference s betwee n state s withou t givin g them numeri c values ?
Finally , why shoul d a utilit y functio n with the require d propertie s exis t at all? Perhap s a rationa l
agent can have a preferenc e structur e that is too comple x to be capture d by somethin g as simpl e
as a singl e real numbe r for each state .
Constraint s on rationa l preference s
Thes e question s can be answere d by writin g dow n som e constraint s on the preference s that a
rationa l agen t shoul d have , and then showin g that the MEU principl e can be derive d from the
constraints . Writin g dow n thes e constraint s is a way of definin g the semantic s of preferences .
The idea is that, give n some preference s on individua l atomi c states , the theor y shoul d allow one
to deriv e result s abou t preference s for comple x decision­makin g scenarios . Thi s is analogou s
to the way that the truth valu e of a comple x logica l sentenc e is derive d from the truth valu e of
its componen t propositions , and the way the probabilit y of a comple x even t is derive d from the
probabilit y of atomi c events .
LOTTERIE S I n the languag e of utilit y theory , the comple x scenario s are calle d lotterie s to emphasiz e
the idea that the differen t attainabl e outcome s are like differen t prizes , and that the outcom e is
determine d by chance . The lotter y L, in whic h there are two possibl e outcomes—stat e A with
probability/ ) and state B with the remainin g probability—i s writte n
L=[p,A;  l­p,B]
In general , a lotter y can have any numbe r of outcomes . Eac h outcom e can be eithe r an atomi c
state or anothe r lottery . A lotter y with only one outcom e can be writte n eithe r as A or [1, A]. It
is the decision­maker' s job to choos e amon g lotteries . Preference s betwee n prize s are used to
determin e preference s betwee n lotteries . The followin g notatio n is used to expres s preference s
or the lack of a preferenc e betwee n lotterie s or states :
A >­ B A is preferre d to B
A ~ B th e agen t is indifferen t betwee n A and B
A ~ B th e agen t prefer s A to B or is indifferen t betwee n them
Now we impos e reasonabl e constraint s on the preferenc e relation , muc h as we impose d rationalit y
constraint s on degree s of belie f in orde r to obtai n the axiom s of probabilit y in Chapte r 14. One
reasonabl e constrain t is that preferenc e shoul d be transitive , that is, if A >­ B and B >­ C, then
we woul d expec t A >­ C. We argu e for transitivit y by showin g that an agen t whos e preference s
do not respec t transitivit y woul d behav e irrationally . Suppose , for example , that an agen t has
the nontransitiv e preference s A >­ B >­ C >­ A, wher e A, B, and C are good s that can be freel y
474 Chapte r 16 . Makin g Simpl e Decision s
exchanged . If the agen t currentl y has A, then we coul d offe r to trade C for A and some cash . The
agent prefer s C, and so woul d be willin g to give up som e amoun t of cash to mak e this trade . We
could then offe r to trade B for C, extractin g mor e cash , and finall y trade A for B. This bring s us
back wher e we starte d from , excep t that the agen t has less money . It seem s reasonabl e to claim
that in this case the agen t has not acted rationally .
The followin g six constraint s are know n as the axiom s of utilit y theory . The y specif y the
most obviou s semanti c constraint s on preference s and lotteries .
ORDERABILIT Y < ) Ordcrability : Give n any two states , a rationa l agen t mus t eithe r prefe r one to the othe r or
else rate the two as equall y preferable . Tha t is, an agen t shoul d know wha t it wants .
(A >­ B) V (B >­ A) V (A ~ 5)
TRANSITIVIT Y 0  Transitivity : Give n any thre e states , if an agen t prefer s A to B and prefer s B to C, then the
agen t mus t prefe r A to C.
(A >­ B) A (B >­ C) => (A >­ C)
CONTINUIT Y 0  Continuity : If some state B is betwee n A and C in preference , then there is some probabilit y
p for whic h the rationa l agen t will be indifferen t betwee n gettin g B for sure and the lotter y
that yield s A with probability/ ? and C with probabilit y 1 — p.
A >­ B y C => 3p  [/>, A; 1 — p, C] ~ fi
SUBSTITUTABILIT Y < > Substitutability : If an agen t is indifferen t betwee n two lotteries , A and B, then the agen t is
indifferen t betwee n two more comple x lotterie s that are the same excep t that B is substitute d
for A in one of them . This hold s regardles s of the probabilitie s and the othe r outcome(s ) in
the lotteries .
A~B => lp,A;  l­p,C]~  \p,B;l  ­p,C]
MONOTONICIT Y < ) Monotonicity : Suppos e there are two lotterie s that have the sam e two outcomes , A and B.
If an agen t prefer s A to B, then the agen t mus t prefe r the lotter y that has a highe r probabilit y
for A (and vice versa) .
A>B  => (p>q  O [p,A;  l~p,B]t[q,A;  1 ­q,B])
DECOMPOSABILIT Y < > Dccomposability : Compoun d lotterie s can be reduce d to simple r ones usin g the laws of
probability . Thi s has been calle d the "no fun in gambling " rule becaus e it says that an
agen t shoul d not prefe r (or disprefer ) one lotter y just becaus e it has mor e choic e point s
than another.2
[p, A; 1 — p, [q,B;  1 — q,C]\  ~ [p,A;  (1 — p)q,B;  (1 — p)(l — q),C]
... and then ther e was Utilit y
Notic e that the axiom s of utilit y theor y do not say anythin g abou t utility . The y only talk abou t
preferences . Preferenc e is assume d to be a basi c propert y of rationa l agents . The existenc e of a
utilit y function  follows  from the axiom s of utility :
2 It is still possibl e to accoun t for an agen t who enjoy s gamblin g by reifyin g the act of gamblin g itself : just includ e in
the appropriat e outcom e state s the propositio n "participate d in a gamble, " and base utilitie s in part on that proposition .
Sectio n 16.3 . Utilit y Function s 47 5
1 . Utilit y principl e
If an agent' s preference s obey the axiom s of utility , then there exist s a real­value d functio n
U that operate s on state s such that U(A)  > U(B)  if and only if A is preferre d to B, and
U(A) ­ U(B)  if and only if the agen t is indifferen t betwee n A and B.
U(A) > U(B) o A X B
U(A) = U(B) ^ A ~ B
2. Maximu m Expecte d Utilit y principl e
The utilit y of a lotter y is the sum of the probabilitie s of each outcom e time s the utilit y of
that outcome .
In othe r words , once the probabilitie s and utilitie s of the possibl e outcom e state s are specified ,
the utilit y of a compoun d lotter y involvin g these state s can be computed . U(\p\  , S\ ; p2, S^; • • • 1)
is completel y determine d by U(Si)  and the probabilit y values .
It is importan t to remembe r that the existenc e of a utilit y functio n that describe s an agent' s
preferenc e behavio r does not necessaril y mea n that the agen t is explicitly  maximizin g that utilit y
functio n in its own deliberations . As we showe d in Chapte r 2, rationa l behavio r can be generate d
in any numbe r of ways , som e of whic h are mor e efficien t than explici t utilit y maximization . By
observin g an agent' s preferences , however , it is possibl e to construc t the utilit y functio n that
represent s wha t it is that the agent' s action s are tryin g to achieve .
16.3 UTILIT Y FUNCTION S
Utilit y is a functio n that map s from state s to real numbers . Is that all we can say abou t utilit y
functions ? Strictl y speaking , that is it. Beyon d the constraint s liste d earlier , an agen t can have
any preference s it likes . For example , an agen t migh t prefe r to have a prim e numbe r of dollar s
in its bank account ; in whic h case , if it had $16 it woul d give awa y $3. It migh t prefe r a dente d
1973 Ford Pint o to a shin y new Mercedes . Preference s can also interact : for example , it migh t
only prefe r prim e number s of dollar s whe n it own s the Pinto , but whe n it own s the Mercedes , it
migh t prefe r more dollar s to less.
If all utilit y function s wer e as arbitrar y as this , however , then utilit y theor y woul d not
be of muc h help becaus e we woul d have to observ e the agent' s preference s in ever y possibl e
combinatio n of circumstance s befor e bein g able to mak e any prediction s abou t its behavior .
Fortunately , the preference s of real agents  are usuall y mor e systematic . Conversely , ther e are
systemati c way s of designin g utilit y function s that, whe n installe d in an artificia l agent , caus e it
to generat e the kind s of behavio r we want .
476 Chapte r 16 . Makin g Simpl e Decision s
MONOTONI C
PREFERENC E
VALU E FUNCTIO N
ORDINA L UTILIT Y
EXPECTE D
MONETAR Y VALU EThe utilit y of mone y
Utilit y theor y has its root s in economics , and economic s provide s one obviou s candidat e for a
utilit y measure : mone y (or mor e specifically , an agent' s total net assets) . The almos t universa l
exchangeabilit y of mone y for all kind s of good s and service s suggest s that mone y play s a
significan t role in huma n utilit y functions.3
If we restric t our attentio n to action s that only affec t the amoun t of mone y that an agen t has,
then it will usuall y be the case that the agen t prefer s mor e mone y to less, all othe r thing s bein g
equal . We say that the agen t exhibit s a monotoni c preferenc e for money . Thi s is not, however ,
sufficien t to guarante e that mone y behave s as a utilit y function . Technicall y speaking , mone y
behave s as a valu e functio n or ordina l utilit y measure , meanin g just that the agen t prefer s to
have mor e rathe r than less whe n considerin g definite amounts.  To understan d monetar y decisio n
makin g unde r uncertainty , we also need to examin e the agent' s preference s betwee n lotterie s
involvin g money .
Suppos e you have triumphe d over the othe r competitor s in a televisio n gam e show . The
host now offer s you a choice : you can eithe r take the $ 1 ,000,00 0 prize or you can gambl e it on
the flip of a coin . If the coin come s up heads , you end up with nothing , but if it come s up tails ,
you get $3,000,000 . If you'r e like mos t people , you woul d declin e the gambl e and pocke t the
million . Are you bein g irrational ?
Assumin g you believ e that the coin is fair, the expecte d monetar y valu e (EMV ) of the
gambl e is ±($0 ) + ^($3,000,000 ) = $1,500,000 , and the EMV of takin g the origina l prize is of
cours e $1 ,000,000 , whic h is less. But that does not necessaril y mea n that acceptin g the gambl e
is a bette r decision . Suppos e we use S,, to denot e the state of possessin g total wealt h $n, and
that your curren t wealt h is $k. The n the expecte d utilitie s of the two action s of acceptin g and
declinin g the gambl e are
EU(Accept)  = £
EU (Decline)  = 1,000.000 )
To determin e wha t to do, we need to assig n utilitie s to the outcom e states . Utilit y is not directl y
proportiona l to monetar y valu e becaus e the utilit y — the positiv e chang e in lifestyl e — for your
first millio n is very high (or so we are told) , wherea s the utilit y for additiona l million s is muc h
smaller . Suppos e you assig n a utilit y of 5 to your curren t financia l statu s (S^), a 10 to the state
S/t+3.ooo.ooo > and an 8 to the state S&+ 1,000.000 ­ The n the rationa l actio n woul d be to decline , becaus e
the expecte d utilit y of acceptin g is only 7.5 (less than the 8 for declining) . On the othe r hand ,
suppos e that you happe n to have $500,000,00 0 in the bank alread y (and appea r on gam e show s
just for fun, one assumes) . In this case , the gambl e is probabl y acceptable , provide d that you
prefe r mor e mone y to less, becaus e the additiona l benefi t of the 503r d millio n is probabl y abou t
the same as that of the 501s t million .
In 1738 , long befor e televisio n gam e shows , Bernoull i cam e up with an even mor e com ­
pellin g example , know n as the St. Petersbur g paradox . Suppos e you are offere d a chanc e to play
a gam e in whic h a fair coin is tosse d repeatedl y unti l it come s up heads . If the first heads  appear s
on the nth toss , you win 2" dollars . How muc h woul d you pay for a chanc e to play this game ?
This despit e the assurance s of a well­know n song to the effec t that the best thing s in life are free.
I
Sectio n 16.3 . Utilit y Function s 477
The probabilit y of the first head showin g up on the nth toss is 1/2" , so the expecte d monetar y
value (EMV ) of this game is
EMV(St.P.)  = P(Headsi)MV(Head Si) = ^­2' ' = ­ + ­ + ­ + ••• = DO
Thus , an agen t who is tryin g to maximiz e its expecte d monetar y gain shoul d be willin g to pay
any finit e sum for a chanc e to play this game . Someho w this does not seem rationa l to mos t
people . Bernoull i resolve d the allege d parado x by positin g that the utilit y of mone y is measure d
on a logarithmi c scale (at least for positiv e amounts) :
U(S k+n) = log 2n (forn>0 )
With this utilit y function , the expecte d utilit y of the gam e is 2:
1 17 3
EU(St.P.)  = P(Heads,)U(Head Si) = 7  Iog 2 2' = +  ++"' = 2
whic h mean s that a rationa l agen t with the give n utilit y scale shoul d be willin g to pay up to $4
for a chanc e to play the game , becaus e U(S/t+4)  = Iog 2 4 = 2.
Bernoull i chos e Iog 2 as the utilit y functio n just to mak e this proble m work out. However ,
in a pioneerin g stud y of actua l utilit y functions , Gray son ( 1 960) foun d an almos t perfec t fit to the
logarithmi c form . One particula r curve , for a certai n Mr. Beard , is show n in Figur e 16.1 (a). The
data obtaine d for Mr. Beard' s preference s are consisten t with a utilit y functio n
U(S k+n) = ­263.3 1 + 22.0 9 log(n + 150,000 )
for the rang e betwee n n = ­$150 , 000 and n = $800 , 000.
We shoul d not assum e that this is the definitiv e utilit y functio n for monetar y value , but
it is likel y that mos t peopl e have a utilit y functio n that is concav e for positiv e wealth . Goin g
into debt is usuall y considere d disastrous , but preference s betwee n differen t level s of debt can
displa y a reversa l of the concavit y associate d with positiv e wealth . For example , someon e alread y
+u +u
­150.00 0+$
800,00 0
(a) (b)
Figur e 16.1 Th e utilit y of money , (a) Empirica l data for Mr. Bear d over a limite d range , (b)
A typica l curv e for the full range .
478 Chapte r 16 . Makin g Simpl e Decision s
RISK­AVERS E
RISK­SEEKIN G
CERTAINT Y
EQUIVALEN T
INSURANC E
PREMIU M
RISK­NEUTRA L$ 10,000,00 0 in debt migh t well accep t a gambl e on a fair coin with a gain of $ 10,000,00 0 for head s
and a loss of $20,000,00 0 for tails.4 This yield s the S­shape d curv e show n in Figur e 16.1(b) .
If we restric t our attentio n to the positiv e part of the curves , wher e the slop e is decreasing ,
then for any lotter y L, the utilit y of bein g face d with that lotter y is less than the utilit y of being
hande d the expecte d monetar y valu e of the lotter y as a sure thing :
U(S L) < U(S EMV(L))
That is, agent s with curve s of this shap e are risk­averse : they prefe r a sure thin g with a payof f
that is less than the expecte d monetar y valu e of a gamble . On the othe r hand , in the "desperate "
regio n at large negativ e wealt h in Figur e 16.1 (b), the behavio r is risk­seeking . The valu e an agen t
will accep t in lieu of a lotter y is calle d the certaint y equivalen t of the lottery . Studie s have show n
that mos t peopl e will accep t abou t $400 in lieu of a gambl e that give s $100 0 half the time and
$0 the othe r half—tha t is, the certaint y equivalen t of the lotter y is $400 . The differenc e betwee n
the expecte d monetar y valu e of a lotter y and its certaint y equivalen t is calle d the insuranc e
premium . Risk aversio n is the basis for the insuranc e industry , becaus e it mean s that insuranc e
premium s are positive . Peopl e woul d rathe r pay a smal l insuranc e premiu m than gambl e with the
price of their hous e agains t the possibilit y of a fire. From the insuranc e company' s poin t of view ,
the risk is very low becaus e the law of large number s make s it almos t certai n that the claim s it
pays will be substantiall y less than the premium s it receives .
Notic e that for small  change s in wealt h relativ e to the curren t wealth , almos t any curv e
will be approximatel y linear . An agen t that has a linea r curv e is said to be risk­neutral . For
gamble s with smal l sums , therefore , we expec t risk neutrality . In a sense , this justifie s the
simplifie d procedur e that propose d smal l gamble s to asses s probabilitie s and to justif y the axiom s
of probabilit y in Chapte r 14.
NORMALIZE D
UTILITIE S
STANDAR D LOTTER YUtilit y scale s and utilit y assessmen t
The axiom s of utilit y do not specif y a uniqu e utilit y functio n for an agent , give n its preferenc e
behavior . It is simpl e to see that an agen t usin g a utilit y functio n
U'(S) = k]+k2U(S),
wher e k\ is a constan t and k^ is any positive  constant , will behav e identicall y to an agen t usin g
U(S),  provide d they have the same beliefs.5 The scale of utilitie s is therefor e somewha t arbitrary .
One commo n procedur e for assessin g utilitie s is to establis h a scale with a "best possibl e
prize " at U(S)  = M T and a "wors t possibl e catastrophe " at £7(5 ) ­ MJ_ . Normalize d utilitie s
use a scale with u± _ = 0 and MT = 1­ Utilitie s of intermediat e outcome s are assesse d by askin g
the agen t to indicat e a preferenc e betwee n the give n outcom e state S and a standar d lotter y
[p, MT ; (1 — P\ u j_ ]. The probabilit y p is adjuste d until the agen t is indifferen t betwee n S and the
standar d lottery . Assumin g normalize d utilities , the utilit y of S is give n by p.
In medical , transportation , and environmenta l decisio n problems , amon g others , people' s
lives are at stake . In such cases , MJ_ is the valu e assigne d to immediat e deat h (or perhap s man y
4 Suc h behavio r migh t be calle d desperate , but it is nonetheles s perfectl y rationa l if one is alread y in a desperat e situation .
5 In Chapte r 5, we saw that mov e choic e in deterministic  game s is unchange d by any monotoni c transformatio n of the
evaluatio n function , but in backgammon , wher e position s are lotteries , only linea r transformation s preserv e mov e choice .
Sectio n 16.3 . Utilit y Function s 47 9
HUMA N JUDGMEN T AND FALLIBILIT Y
Decisio n theor y is a normativ e theory : it describe s how a rationa l agen t should
act. The applicatio n of economi c theor y woul d be greatl y enhance d if it were also a
descriptiv e theor y of actua l huma n decisio n making . However , there is experimenta l
evidenc e indicatin g that peopl e systematicall y violat e the axiom s of utilit y theory .
An exampl e is give n by the psychologist s Tversk y and Kahnema n (1982) , base d on
an exampl e by the economis t Allai s (1953) . Subject s in this experimen t are give n a
choic e betwee n lotterie s A and B, and then betwee n C and D:
A : 80 % chanc e of $400 0 C  : 20% chanc e of $400 0
B : 100 % chanc e of $300 0 D  : 25% chanc e of $300 0
The majorit y of subject s choos e B over A and C over D. But if we assig n £/($0 ) = 0,
then the first of these choice s implie s that 0.8f/($4000 ) < t/($3000) , wherea s the
secon d choic e implie s exactl y the reverse . In othe r words , there seem s to be no utilit y
functio n that is consisten t with these choices . One possibl e conclusio n is that human s
are simpl y irrationa l by the standard s of our utilit y axioms . An alternativ e view is that
the analysi s does not take into accoun t regret —the feelin g that human s kno w they
woul d experienc e if they gave up a certai n rewar d (B) for an 80% chanc e at a highe r
reward , and then lost. In othe r words , if A is chosen , ther e is a 20% chanc e of gettin g
no mone y and feelin g like a complet e idiot .
Kahnema n and Tversk y go on to develop a  descriptiv e theor y that explain s how
peopl e are risk­avers e with high­probabilit y events , but are willin g to take more risks
with unlikel y payoffs . The connectio n betwee n this findin g and AI is that the choice s
our agent s can mak e are only as good as the preference s they are base d on. If our
huma n informant s insis t on contradictor y preferenc e judgments , ther e is nothin g our
agen t can do to be consisten t with them .
Fortunately , preferenc e judgment s mad e by human s are ofte n open to revisio n
in the ligh t of furthe r consideration . In earl y wor k at Harvar d Busines s Schoo l on
assessin g the utilit y of money , Keene y and Raiff a (1976 , p. 210) foun d the foil owing :
A grea t deal of empirica l investigatio n has show n that there is a seriou s deficienc y
in the assessmen t protocol . Subject s tend to be too risk­avers e in the smal l and
therefor e ... the fitte d utilit y function s exhibi t unacceptabl y large risk premium s
for lotterie s with  a large spread . ... Mos t of the subjects , however , can reconcil e
their inconsistencie s and feel that they have learne d an importan t lesso n abou t how
they wan t to behave . As a consequence , som e subject s cance l thei r automobil e
collisio n insuranc e and take out more term insuranc e on their lives .
Even today , huma n (ir)rationalit y is the subjec t of intensiv e investigation .
480 Chapte r 16 . Makin g Simpl e Decision s
MICROMOR T
QALYdeaths) . Although  nobody  feels  comfortable  with  putting  a value  on human  life,  it is a fact that
trade­offs  are made all  the time.  Aircraf t are give n a complet e overhau l at interval s determine d
by trips and mile s flown , rathe r than after ever y trip. Car bodie s are mad e with relativel y thin
sheet meta l to reduc e costs , despit e the decreas e in acciden t surviva l rates . Leade d fuel is still
widel y used even thoug h it has know n healt h hazards . Paradoxically , a refusa l to "put a monetar y
value on life" mean s that life is ofte n undervalued.  Ros s Shachte r relate s an experienc e with a
governmen t agenc y that commissione d a stud y on removin g asbesto s from schools . The stud y
assume d a particula r dolla r valu e for the life of a school­ag e child , and argue d that the rationa l
choic e unde r that assumptio n was to remov e the asbestos . Th e governmen t agency , morall y
outraged , rejecte d the repor t out of hand . It then decide d agains t asbesto s removal .
Some attempt s have been mad e to find out the valu e that peopl e plac e on their own lives .
Two commo n "currencies"6 used in medica l and safet y analysi s are the micromor t (a one in a
millio n chanc e of death ) and the QALY , or quality­adjuste d life year (equivalen t to a year in good
healt h with no infirmities) . A numbe r of studie s acros s a wide rang e of individual s have show n
that a micromor t is wort h abou t $20 (198 0 dollars) . We have alread y seen that utilit y function s
need not be linear , so this does not impl y that a decisio n make r woul d kill himsel f for $20 million .
Again , the loca l linearit y of any utilit y curv e mean s that micromor t and QAL Y value s are usefu l
for smal l incrementa l risks and rewards , but not necessaril y for large risks .
16.4 MULTIATTRIBUT E UTILIT Y FUNCTION S
MULTIATTRIBUT E
UTILIT Y THEOR Y
REPRESENTATIO N
THEOREM SDecisio n makin g in the field of publi c polic y involve s both million s of dollar s and life and death .
For example , in decidin g wha t level s of a carcinogeni c substanc e to allo w into the environment ,
polic y maker s mus t weig h the preventio n of death s agains t the economi c hardshi p that migh t
resul t from the eliminatio n of certai n product s and processes . Sitin g a new airpor t require s
consideratio n of the disruptio n cause d by construction ; the cost of land ; the distanc e from center s
of population ; the nois e of fligh t operations ; safet y issue s arisin g from loca l topograph y and
weathe r conditions ; and so on. Problem s like these , in whic h outcome s are characterize d by two
or more attributes , are handle d by multiattribut e utilit y theory , or MAUT .
We will call the attribute s X\, X2, and so on; thei r value s will be x\,X2 , and so on; and a
complet e vecto r of attribut e value s will be x = (x\,x 2, ...}. Eac h attribut e is generall y assume d
to have discret e or continuou s scala r values . For simplicity , we will assum e that each attribut e is
define d in such a way that, all othe r thing s bein g equal , highe r value s of the attribut e correspon d to
highe r utilities . For example , if we choos e as an attribut e in the airpor t proble m AbsenceOfNoise,
then the greate r its value , the bette r the solution . In som e cases , it may be necessar y to subdivid e
the rang e of value s so that utilit y varie s monotonicall y withi n each range .
The basi c approac h adopte d in multiattribut e utilit y theor y is to identif y regularitie s in the
preferenc e behavio r we woul d expec t to see, and to use wha t are calle d representatio n theorem s
(l We use quotatio n mark s becaus e these measure s are definitel y not currencie s in the standar d sense of bein g exchange ­
able for good s at constan t rates regardles s of the curren t "wealth " of the agen t makin g the exchange .
Sectio n 16.4 . Multiattribut e utilit y function s 481
to show that an agen t with a certai n kind of preferenc e structur e has a utilit y functio n
U(xi,...,x a)=f\fi(xi),...,f n(xn)]
where / is, we hope , a simpl e functio n such as addition . Notic e the similarit y to the use of belie f
network s to decompos e the joint probabilit y of severa l rando m variables .
Dominanc e
Suppos e that airpor t site Si costs less, generate s less noise pollution , and is safer than site S2. One
STRIC T DOMINANC E woul d not hesitat e to rejec t S2. We say that there is stric t dominanc e of Si over S2. In general ,
if an optio n is of lowe r valu e on all attribute s than som e othe r option , it need not be considere d
further . Stric t dominanc e is often very usefu l in narrowin g dow n the field of choice s to the real
contenders , althoug h it seldo m yield s a uniqu e choice . Figur e 16.2(a ) show s a schemati c diagra m
for the two­attribut e case.
X2 X2
l Thi s regio n
1 dominate s A
°D
­*­x.
(a) (b)
Figur e 16.2 Stric t dominance , (a) Deterministic : Optio n A is strictl y dominate d by B but not
by C or D. (b) Uncertain : A is strictl y dominate d by B but not by C.
STOCHASTI C
DOMINANC EThat is fine for the deterministi c case , in whic h the attribut e value s are know n for sure .
What abou t the genera l case, wher e the actio n outcome s are uncertain ? A direc t analogu e of strict
dominanc e can be constructed , where , despit e the uncertainty , all possibl e concret e outcome s for
Si strictl y dominat e all possibl e outcome s for S2. (See Figur e 16.2(b ) for a schemati c depiction. )
Of course , this will probabl y occu r even less ofte n than in the deterministi c case .
Fortunately , ther e is a mor e usefu l generalizatio n calle d stochasti c dominance , whic h
occur s very frequentl y in real problems . Stochasti c dominanc e is easies t to understan d in the
contex t of a singl e attribute . Suppos e that we believ e the cost of sitin g the airpor t at Si to be
normall y distribute d aroun d $3.7 billion , with standar d deviatio n $0.4 billion ; and the cost at $2 to
be normall y distribute d aroun d $4.0 billion , with standar d deviatio n $0.35 billion . Figur e 16.3(a )
show s thes e distributions , with cost plotte d as a negativ e value . Then , give n only the informatio n
482 Chapte r 16 . Makin g Simpl e Decision s
1.7
1
>­ 0­8
1 0.6
1 0.4
0.2
0• \ s i; / • , \ 5 2 ­­­­­­ _
' / ' • \
6 .5. 5 .5 .4, 5 .4 .3. 5 .3 .2. 5 ­21
0.8
£ 0.6
.£
t 0.4
0.2
0
­' ' '  '  . . • *,,­'  '  '/'
/
/ / S I ——
/ /' S 2
6 ­5. 5 ­5 ­4. 5 ­4 ­3. 5 ­3 ­2. 5 ­2
Negativ e cost Negativ e cost
(a) (b )
Figur e 16.3 Stochasti c dominance , (a) Si stochasticall y dominate s 82 on cost , (b) Cumulativ e
distribution s for the negativ e cost of Si and 82.
that utilit y decrease s with cost, we can say that Si stochasticall y dominate s 82 — that is, £2 can be
discarded . It is importan t to note that this does not follo w from comparin g the expecte d costs .
For example , if we knew the cost of S\ to be exactly  $3.7 billion , then we woul d be unable  to
make a decisio n withou t additiona l informatio n on the utilit y of money.7
The exac t relationshi p betwee n the attribut e distribution s neede d to establis h stochasti c
dominanc e is best seen by examinin g the cumulative  distributions , show n in Figur e 16.3(b) . The
cumulativ e distributio n measure s the probabilit y that the cost is less than or equa l to any give n
amoun t — that is, it integrate s the origina l distribution . If the cumulativ e distributio n for S\ is
alway s to the righ t of the cumulativ e distributio n for 52, then stochasticall y speakin g 5i is cheape r
than ST. Formally , if two action s A i and A 2 lead to probabilit y distribution s /7i(X ) and pi(x)  on
attribut e A", then A \ stochasticall y dominate s A 2 on X if
p\(x')dx>  p 2(x')dx
If an actio n is stochasticall y dominate d by anothe r actio n on all attributes , then it can be discarded .
The stochasti c dominanc e conditio n migh t seem rathe r technical , and perhap s not so easy
to determin e withou t extensiv e probabilit y calculations . In fact, it can be decide d very easil y in
many cases . Suppose , for example , that the constructio n cost depend s on the distanc e to center s
of population . The cost itsel f is uncertain , but the greate r the distance , the greate r the cost. If
5i is less remot e than 52, then Si will dominat e S2 on cost . Althoug h we will not presen t them
here, there exist algorithm s for propagatin g this kind of qualitativ e informatio n amon g uncertai n
variable s in qualitativ e probabilisti c networks , enablin g a syste m to mak e rationa l decision s
based on stochasti c dominanc e withou t ever needin g to use numerica l probabilitie s or utilities .
7 It migh t seem odd that more  informatio n on the cost of Si coul d mak e the agen t less able to decide . The parado x is
resolve d by notin g that the decisio n reache d in the absenc e of exac t cost informatio n is less likel y to be correct .
Sectio n 16.4 . Multiattribut e utilit y function s 483
Preferenc e structur e and multiattribut e utilit y
Suppos e we have n attributes , each of whic h has m distinc t possibl e values . This gives a set of
possibl e outcome s of size m". In the wors t case, the agent' s utilit y functio n yield s an arbitrar y set
of preference s over these mn state s with no regularitie s beyond  those implie d by the basic axiom s
of utility . Multiattribut e utilit y theor y is base d on the suppositio n that mos t utilit y function s have
much mor e structur e than that, allowin g us to use simplifie d decisio n procedures .
PREFERENC E
INDEPENDENC E
MUTUA L
PREFERENTIA L
INDEPENDENC E
.Preference s withou t uncertaint y
Let us begi n by considerin g the case in whic h there is no uncertaint y in the outcome s of actions ,
and we are just considerin g preference s betwee n concret e outcomes . In this case , the basi c
regularit y in preferenc e structur e is calle d preferenc e independence . Tw o attribute s X\ and
Xi are preferentiall y independen t of a third attribut e XT, if the preferenc e betwee n outcome s
(x\,X2,x­})  and (x>
},x>
2,x^) does not depen d on the particula r valu e .Q for attribut e X^.
Goin g back to the airpor t example , wher e we have (amon g othe r attributes ) Noise,  Cost,
and Deaths  to consider , one may propos e that Noise  and Cost  are preferentiall y independen t of
Deaths.  For example , if we prefe r a state with 20,00 0 peopl e residin g in the fligh t path and a
constructio n cost of $4 billio n to a state with 70,00 0 peopl e residin g in the fligh t path and a cost of
$3.7 billio n when the safet y level is 0.06 death s per millio n passenge r mile s in both cases , then we
woul d have the same preferenc e whe n the safet y leve l is 0.13 or 0.01 ; and the same independenc e
woul d hold for preference s betwee n any othe r pair of value s for Noise  and Cost.  It is also
apparen t that Cost  and Deaths  are preferentiall y independen t of Noise,  and that Noise  and Deaths
are preferentiall y independen t of Cost.  We say that the set of attribute s {Noise,  Cost,  Deaths}
exhibit s mutua l preferentia l independenc e (MPI) . MPI says that wherea s each attribut e may
be important , it does not affec t the way in whic h one trade s off the othe r attribute s agains t each
other .
Mutua l preferentia l independenc e is somethin g of a mouthful , but thank s to a remarkabl e
theore m due to the economis t Debre u (1960) , we can deriv e from it a very simpl e form for the
agent' s valu e function : Ifattributes  X\, ...,  Xn are mutually  preferentially  independent,  then  the
agent's  preference  behavior  can be described  as maximizing  the function
V(S)  =
where  each  V,­ is a value  function  referring only to  the attribute  X,. For example , it migh t well be
the case that the airpor t decisio n can be mad e usin g a valu e functio n
V(S) ­ ­Noise  x 104 ­ Cost  ­ Deaths  x 1012
A valu e functio n of this type is calle d an additiv e valu e function . Additiv e function s are an
extremel y natura l way to describ e an agent' s valu e function , and are vali d in man y real­worl d
situations . Eve n whe n MPI does not strictl y hold , as migh t be the case at extrem e value s of
the attributes , an additiv e valu e functio n can still provid e a good approximatio n to the agent' s
preferences . Thi s is especially  true whe n the violation s of MPI occu r in portion s of the attribut e
range s that are unlikel y to occu r in practice .
484 Chapte r 16 . Makin g Simpl e Decision s
UTILIT Y
INDEPENDENC E
MUTUALL Y UTILITY ­
INDEPENDEN T
MULTIPLICATIV E
UTILIT Y FUNCTIO NPreference s with uncertaint y
Whe n uncertaint y is presen t in the domain , we will also need to conside r the structur e of
preference s betwee n lotterie s and to understan d the resultin g propertie s of utilit y functions ,
rathe r than just valu e functions . The mathematic s of this proble m can becom e quit e complicated ,
so we will give just one of the main result s to give a flavo r of wha t can be done . The reade r is
referre d to Keene y and Raiff a (1976 ) for a thoroug h surve y of the field .
The basi c notio n of utilit y independenc e extend s preferenc e independenc e to cove r lot­
teries : a set of attribute s X is utility­independen t of a set of attribute s Y if preference s betwee n
lotterie s on the attribute s in X are independen t of the particula r value s of the attribute s in Y. A set
of attribute s is mutuall y utility­independen t (MUI ) if each subse t is utility­independen t of the
remainin g attributes . Again , it seem s reasonabl e to propos e that the airpor t attribute s are MUI .
MUI implie s that the agent' s behavio r can be describe d usin g a multiplicativ e utilit y
functio n (Keeney , 1974) . The genera l form of a multiplicativ e utilit y functio n is best seen by
lookin g at the case for three attributes . For simplicity , we will use t/, to mean [/,­(X,­(.s)) :
U = k}U}+k2U2+kiUi  + klk2 U\ U2 + k2k3 U2 U3 + k?,k\ U3 U\ + k\ k2k3 U{ U2 U3
Althoug h this does not look very simple , it contain s thre e single­attribut e utilit y function s and
just three constants . In general , an w­attribut e proble m exhibitin g MU I can be modelle d usin g
n single­attribut e utilitie s and n constants . Eac h of the single­attribut e utilit y function s can
be develope d independentl y of the othe r attributes , and this combinatio n will be guarantee d to
generat e the correc t overal l preferences . Additiona l assumption s are require d to obtai n a purel y
additiv e utilit y function .
16.5 DECISIO N NETWORK S
In this section , we will look at a genera l mechanis m for makin g rationa l decisions . The notatio n
INFLUENC E DIAGRA M is ofte n calle d a influenc e diagra m (Howar d and Matheson , 1984) , but we will use the mor e
DECISIO N NETWOR K descriptiv e term decisio n network . Decisio n network s combin e belie f network s with additiona l
node type s for action s and utilities . We will use the airpor t sitin g proble m as an example .
CHANC E NODE SRepresentin g a decisio n proble m usin g decisio n network s
In its mos t genera l form , a decisio n networ k represent s informatio n abou t the agent' s curren t
state, its possibl e actions , the state that will resul t from the agent' s action , and the utilit y of that
state. It therefor e provide s a substrat e for implementin g utility­base d agent s of the type first
introduce d in Sectio n 2.3. Figur e 16.4 show s a decisio n networ k for the airpor t sitin g problem .
It illustrate s the three type s of node s used :
0 Chanc e node s (ovals ) represen t rando m variables , just as they do in belie f nets. The agen t
may be uncertai n abou t the constructio n cost , the leve l of air traffi c and the potentia l for
litigation , as well as the Deaths, Noise,  and total Cost  variables , each of whic h also depend s
Sectio n 16.5 . Decisio n Network s 485
DECISIO N NODE S
UTILIT Y NODE SFigur e 16.4 A  simpl e decisio n networ k for the airport­sitin g problem .
ACTON­UTILIT Yon the site chosen . Each chanc e node has associate d with it a conditiona l probabilit y table
(CPT ) that is indexe d by the state of the paren t nodes . In decisio n networks , the paren t
nodes can includ e decisio n node s as well as chanc e nodes . Not e that each of the current ­
state chanc e node s coul d be part of a large belie f networ k for assessin g constructio n costs ,
air traffi c levels , or litigatio n potential . For simplicity , these are omitted .
0 Decisio n node s (rectangles ) represen t point s wher e the decision­make r has a choic e of
actions . In this case, the AirportSite  actio n can take on a differen t valu e for each site unde r
consideration . The choic e influence s the cost , safety , and nois e that will result . In this
chapter , we will assum e that we are dealin g with a singl e decisio n node . Chapte r 17 deal s
with cases wher e more than one decisio n mus t be made .
<) Utilit y node s (diamonds ) represen t the agent' s utilit y function.8 The utilit y node has as
parent s all those variable s describin g the outcom e state that directl y affec t utility . The table
associate d with a utilit y node is thus a straightforwar d tabulatio n of the agent' s utilit y as a
functio n of the attribute s that determin e it. As with canonica l CPTs , multiattribut e utilit y
function s can be represente d by a structure d descriptio n rathe r than a simpl e tabulation .
A simplifie d form is also used in man y cases . Th e notatio n remain s identical , but the
chanc e node s describin g the outcom e state are omitted . Instead , the utilit y node is connecte d
directl y to the current­stat e node s and the decisio n node . In this case , rathe r than representin g
a utilit y functio n on states , the tabl e associate d with the utilit y nod e represent s the expected
utilit y associate d with each action , as define d in Equatio n (16.1) . We therefor e call such table s
action­utilit y tables . Figur e 16.5 show s the action­utilit y representatio n of the airpor t problem .
Notic e that becaus e the Noise, Deaths,  and Cost  chanc e node s in Figur e 16.4 refer to futur e
states , they can neve r have thei r value s set as evidenc e variables . Thus , the simplifie d versio n
8 Thes e node s are ofte n calle d valu e node s in the literature . We prefe r to maintai n the distinctio n betwee n utilit y and
value functions , as discusse d earlier , becaus e the outcom e state may represen t a lottery .
486 Chapte r 16 . Makin g Simpl e Decision s
Figur e 16.5 A  simplifie d representatio n of the airport­sitin g problem . Chanc e node s corre ­
spondin g to outcom e state s have been factore d out.
that omit s thes e node s can be used wheneve r the mor e genera l form can be used . Althoug h the
simplifie d form contain s fewe r nodes , the omissio n of an explici t descriptio n of the outcom e of
the sitin g decisio n mean s that it is less flexibl e with respec t to change s in circumstances . For
example , in Figur e 16.4 , a chang e in aircraf t nois e level s can be reflecte d by a chang e in the
conditiona l probabilit y tabl e associate d with the Noise  node , wherea s a chang e in the weigh t
accorde d to nois e pollutio n in the utilit y functio n can be reflecte d by a chang e in the utilit y table .
In the action­utilit y diagram , Figur e 16.5 , on the othe r hand , all such change s have to be reflecte d
by changes  to the action­utilit y table . Essentially , the action­utilit y formulatio n is a compiled
versio n of the origina l formulation .
Evaluatin g decisio n network s
Action s are selecte d by evaluatin g the decisio n networ k for each possibl e settin g of the decisio n
node . Onc e the decisio n node is set, it behave s exactl y like a chanc e node that has been set as an
evidenc e variable . The algorith m for evaluatin g decisio n network s is the following :
1. Set the evidenc e variable s for the curren t state .
2. For each possibl e valu e of the decisio n node :
(a) Set the decisio n node to that value .
(b) Calculat e the posterio r probabilitie s for the paren t node s of the utilit y node , usin g a
standar d probabilisti c inferenc e algorithm .
(c) Calculat e the resultin g utilit y for the action .
3. Retur n the actio n with the highes t utility .
This is a straightforwar d extensio n of the belie f networ k algorithm , and can be incorporate d
directl y into the agen t desig n give n in Figur e 14.1 . We will see in Chapte r 17 that the possibility :
of executin g severa l action s in sequenc e make s the proble m muc h mor e interesting .
Sectio n 16.6 . Th e Valu e of Informatio n 48 7
16.6 TH E VALU E OF INFORMATION___________________ _
In the precedin g analysis , we have assume d that all relevan t information , or at least  all availabl e
information , is provide d to the agen t befor e it make s its decision . In practice , this is hardl y ever
the case. One  of the most  important  parts  of decision  making is  knowing  what  questions  to ask.
I :i Fo r example , a docto r canno t expec t to be provide d with the result s of all possible  diagnosti c tests
and question s at the time a patien t first enter s the consultin g room.9 Test s are often expensiv e and
sometime s hazardou s (both directl y and becaus e of associate d delays) . Thei r importanc e depend s
on two factors : whethe r the differen t possibl e outcome s woul d mak e a significan t differenc e to
the optima l cours e of action , and the likelihoo d of the variou s outcomes .
TNHFE°oRRYATION VALU E Thi s sectio n describe s informatio n valu e theory , whic h enable s an agen t to choos e wha t
informatio n to acquire . Th e acquisitio n of informatio n is achieve d by sensin g actions , as
describe d in Chapte r 13. Becaus e the agent' s utilit y functio n seldo m refer s to the content s of the
agent' s interna l state , wherea s the whol e purpos e of sensin g action s is to affec t the interna l state ,
we mus t evaluat e sensin g action s by thei r effec t on the agent' s subsequen t actions . Informatio n
value theor y is therefor e a specia l kind of sequentia l decisio n making .
A simpl e exampl e
Suppos e an oil compan y is hopin g to buy one of n indistinguishabl e block s of ocea n drillin g
rights . Let us assum e furthe r that exactl y one of the block s contain s oil wort h C dollars , and that
the price of each bloc k is C/n dollars . If the compan y is risk­neutral , then it will be indifferen t
betwee n buyin g a bloc k or not.
Now suppos e that a seismologis t offer s the compan y the result s of a surve y of block numbe r
3, whic h indicate s definitivel y whethe r the bloc k contain s oil. How muc h shoul d the compan y
be willin g to pay for the information ? The way to answe r this questio n is to examin e wha t the
compan y woul d do if it had the information :
• Wit h probabilit y 1/n, the surve y will show that bloc k 3 contain s the oil. In this case , the
compan y will buy bloc k 3 for C/n dollars , and mak e a profi t of C — C/n =  (n — l)C/ «
dollars .
• Wit h probabilit y (n — \ )/«, the surve y will show that the bloc k contain s no oil, in whic h
case the compan y will buy a differen t block . Now the probabilit y of findin g oil in one of
the othe r block s change s from l/« to \/(n ­ 1), so the compan y make s an expecte d profi t
of Cl(n  ­ 1) ­ C/n = C/n(n  ­ 1) dollars .
Now we can calculat e the expecte d profi t give n the surve y information :
1 (n­\)C  n­1  C
­ x ———— — + —— — x ——— ­ = C/nn n  n  n(n  — 1)
Therefore , the compan y shoul d be willin g to pay the seismologis t up to C/n  dollar s for the
information : the informatio n is wort h as muc h as the bloc k itself .
9 In the Unite d States , the only questio n that is alway s aske d beforehan d is whethe r the patien t has insurance .
488 Chapte r 16 . Makin g Simpl e Decision s
The valu e of informatio n derive s from the fact that with  the information , one' s cours e of
actio n may chang e to becom e mor e appropriat e to the actua l situation . One can discriminat e
accordin g to the situation , wherea s withou t the informatio n one has to do what' s best on averag e
over the possibl e situations . In general , the valu e of a give n piece of informatio n is define d to be
the differenc e in expecte d valu e betwee n best action s befor e and after informatio n is obtained .
VALU E OF PERFEC T
INFORMATIO NA genera l formul a
It is simpl e to deriv e a genera l mathematica l formul a for the valu e of information . Usually , we
assum e that exac t evidenc e is obtaine d abou t the valu e of some rando m variabl e Ej, so the phras e
value of perfec t informatio n (VPI ) is used . Let the agent' s curren t knowledg e be E. The n the
value of the curren t best actio n a is define d by
EU(a\E)  = max U(Result t(A)) P(Resulti(A)\E,Do(A))
and the valu e of the new best actio n (afte r the new evidenc e Ej is obtained ) will be
EU(a Ej = max U(Resulti(A))  P(Result i(A)\E,Do(A),E j)
But Ej is a rando m variabl e whos e valu e is currently  unknown , so we mus t averag e over all
possibl e value s ejk that we migh t discove r for Ej, usin g our current  belief s abou t its value . The
value of discoverin g Ej is then denne d as
VPI E(Ej) = j ^P(E j = ejk\E)EU(a ejt\E,Ej  = ejk) ] ­ EU(a\E)
\ k I
In orde r to get som e intuitio n for this formula , conside r the simpl e case wher e there are
only two action s A i and A2 from whic h to choose . Thei r curren t expecte d utilitie s are U\ and U2.
The informatio n Ej will yield som e new expecte d utilit y U( and U' 2 for the actions , but befor e
we obtai n Ej, we will have som e probabilit y distribution s over the possibl e value s of V( and U2
(whic h we will assum e are independent) .
Suppos e that A\ and A2 represen t two differen t route s throug h a mountai n rang e in winter .
A i is a nice , straigh t highwa y throug h a low pass , and A2 is a windin g dirt road over the top. Just
given this information , A i is clearl y preferable , becaus e it is quite likel y that the secon d route is
blocke d by avalanches , wherea s it is quit e unlikel y that the first rout e is blocke d by traffic . U\ is
therefor e clearl y highe r than U2. It is possibl e to obtai n satellit e report s Ej on the actua l state of
each road , whic h woul d give new expectation s U{ and U2 for the two crossings . The distribution s
for thes e expectation s are show n in Figur e 16.6(a) . Obviously , in this case , it is not wort h the
expens e of obtainin g satellit e reports , becaus e it is so unlikel y that they will caus e a chang e of
plan. Wit h no chang e of plan , informatio n has no value .
Now suppos e that we are choosin g betwee n two differen t windin g dirt road s of slightl y
differen t lengths , and we are carryin g a seriousl y injure d passenger . Then , althoug h U\ and 1/2
may be quite close , the distribution s of U( and U2 are very broad . Ther e is a significan t possibilit y
that the secon d rout e will turn out to be clear wherea s the first is blocked , and in this case the
differenc e in utilitie s will be very high . The VPI formul a indicate s that it migh t be wort h gettin g
the satellit e reports . This situatio n is show n in Figur e 16.6(b) .
Sectio n 16.6 . Th e Valu e of Informatio n 489
Now suppos e that we are choosin g betwee n the two dirt road s in summertime , whe n
blockag e by avalanche s is unlikely . In this case, satellit e report s migh t show one route to be more
sceni c than the othe r becaus e of flowerin g alpin e meadows , or perhap s wette r becaus e of erran t
streams . It is therefor e quite likel y that we woul d chang e our plan if we had the information . But
in this case , the differenc e in valu e betwee n the two route s is still likel y to be very small , so we
will not bothe r to obtai n the reports . This situatio n is show n in Figur e 16.6(c) .
In summary , we can say that information  has value to the  extent  that it is likely  to cause a
change  of plan, and to the extent that the new plan will be significantly better than the old plan.
P(UIE, ) P(UIEj ) PtUIE J
(a)U2 U,
(b)
Figur e 16.6 Thre e generi c case s for the valu e of information . In (a), A\ will almos t certainl y
remai n superio r to AI, so the informatio n is not needed . In (b), the choic e is unclea r and the
informatio n is crucial . In (c), the choic e is unclea r but becaus e it make s little difference , the
informatio n is less valuable .
Propertie s of the valu e of informatio n
One migh t ask if it is possibl e for informatio n to be deleterious—ca n it actuall y have negativ e
expecte d value ? Intuitively , one shoul d expec t this to be impossible . Afte r all, one coul d in the
wors t case just ignor e the informatio n and preten d one has neve r receive d it. This is confirme d by
the followin g theorem , whic h applie s to any decision­theoreti c agent : The  value  of information
is nonnegative:
Vj,E VPI E(Ej)>0
proof as an exercis e (Exercis e 16.11) . It is importan t to remembe r that VPI depend s on the
curren t state of information , whic h is why it is subscripted . It can chang e as mor e informatio n is
acquired . In the extrem e case , it will becom e zero if the variabl e in questio n alread y has a know n
490 Chapte r 16 . Makin g Simpl e Decision s
value . Thus , VPI is not additive . Tha t is,
VPI E(Ej, Ek) 4 VPI E(Ej) + VPl E(Ek) (i n general )
It is, however , order­independent , whic h shoul d be intuitivel y obvious . Tha t is,
VPI E(Ej,E k) = VPI E(Ej) + VPI E.Ej(Ek) = VPI K(Ek) + VPI EA(Ej)
Orde r independenc e distinguishe s sensin g action s from ordinar y actions , and simplifie s the
proble m of calculatin g the valu e of a sequenc e of sensin g actions .
Implementin g an information­gatherin g agen t
As we mentione d earlier , a sensibl e agen t shoul d ask question s of the user in a reasonabl e order ,
shoul d avoi d askin g question s that are irrelevant , shoul d take into accoun t the importanc e of each
piece of informatio n in relatio n to its cost , and shoul d stop askin g question s whe n appropriate .
All of these capabilitie s can be achieve d by usin g the valu e of informatio n as a guide .
Figur e 16.7 show s the overal l desig n of an agen t that can gathe r informatio n intelligentl y
befor e acting . For now , we will assum e that with each observabl e evidenc e variabl e £,­, ther e
is an associate d cost , Cost(Ef),  whic h reflect s the cost of obtainin g the evidenc e throug h tests ,
consultants , questions , or whatever . The agen t request s wha t appear s to be the mos t valuabl e
piece of information , compare d to its cost . We assum e that the resul t of the actio n Request(Ej)  is
that the next percep t provide s the valu e of £}. If no observatio n is wort h its cost, the agen t select s
a non­information­gatherin g action .
MYOPI Cfunctio n INFORMATION­GATHERING­ AGENT ( percept)  return s an action
static : D, a decisio n networ k
integrat e percept  into D
j <— the valu e that maximize s VPI(Ej)  — Cost(Ej)
ifVPf(Ej)  >  Cost(Ej)
then retur n REQUEST(£, )
else retur n the best actio n from D
Figur e 16.7 Desig n of a simpl e information­gatherin g agent . The agen t work s by repeatedl y
selectin g the observatio n with the highes t informatio n value , unti l the cost s of observin g are
greate r than the benefits .
The agen t algorith m we have describe d implement s a form of informatio n gatherin g that
is calle d myopic . Thi s is becaus e it uses the VPI formul a short­sightedly , calculatin g the valu e
of informatio n assumin g that only a singl e evidenc e variabl e will be acquired . If ther e is no
singl e evidenc e variabl e that will help a lot, a myopi c agen t may hastil y take an actio n whe n
it woul d have been bette r to reques t two or more variable s first , and then take action . Myopi c
contro l is base d on the sam e heuristi c idea as greed y search , and ofte n work s wel l in practice .
(For example , it has been show n to outperfor m exper t physician s in selectin g diagnosti c tests. )
Sectio n 16.7 . Decision­Theoreti c Exper t System s 491
However , a perfectl y rationa l information­gatherin g agen t shoul d conside r all possibl e sequence s
of informatio n request s terminatin g in an externa l action . Becaus e VPI is order­independent ,
this is somewha t simplifie d by the fact that any permutation s of a give n sequenc e of informatio n
request s has the sam e value . Thus , one need conside r only subset s of the possibl e informatio n
requests , withou t worryin g abou t ordering .
16.7 DECISION­THEORETI C EXPER T SYSTEM S
DECISIO N ANALYSI S Th e field of decisio n analysis , whic h evolve d in the 1950 s and 1960s , studie s the applicatio n of
decisio n theor y to actua l decisio n problems . It is used to help make rationa l decision s in importan t
domain s wher e the stake s are high , such as business , government , law, militar y strategy , medica l
diagnosi s and publi c health , engineerin g design , and resourc e management . The proces s involve s
a carefu l stud y of the possibl e action s and outcome s as well as the preference s place d on each
DECISIO N MAKE R outcome . It is traditiona l in decisio n analysi s to talk abou t two roles : the decisio n make r state s
DECISIO N ANALYS T preference s betwee n outcomes , and the decisio n analys t enumerate s the possibl e action s and
outcome s and elicit s preference s from the decisio n make r to determin e the best cours e of action .
Until the early 1980s , the use of computer s in decisio n analysi s was quit e limited , and the main
purpos e of analysi s was seen as helpin g human s to mak e decision s that actuall y reflec t their own
preferences .
As we discusse d in Chapte r 15, earl y exper t syste m researc h concentrate d on answerin g
questions , rathe r than makin g decisions . Thos e system s that did recommen d action s rathe r
than providin g opinion s on matter s of fact generall y did so usin g condition­actio n rules , rathe r
than with explici t representation s of outcome s and preferences . Th e eventua l emergenc e of
belief network s mad e it possibl e to build large­scal e system s that generate d soun d probabilisti c
inference s from evidence . The additio n of decisio n network s mean s that exper t system s can be
develope d that recommen d optima l decisions , reflectin g the preference s of the user as well as the
availabl e evidence .
There are man y advantage s that accru e from the inclusio n of explici t utilit y model s and
calculation s in the exper t syste m framework . The exper t benefit s from the proces s of makin g
his or her (or the client's ) preference s explicit , and the syste m can automat e the actio n selectio n
proces s as well as the proces s of drawin g conclusion s from evidence . A syste m that incorporate s
utilitie s can avoi d one of the mos t commo n pitfall s associate d with the consultatio n process :
confusin g likelihoo d and importance . A commo n strateg y in earl y medica l exper t systems ,
for example , was to rank possibl e diagnose s in orde r of likelihood , and repor t the mos t likely .
Unfortunately , this can be disastrous , becaus e it will miss case s of relativel y rare, but treatable ,
condition s that are easil y confuse d with mor e commo n diseases . The confusio n of Hodgkin' s
diseas e (a form of cancer ) with mononucleosi s (a mild and very commo n vira l infection ) is a
classi c case in point . (For the majorit y of patient s in genera l practice , moreover , the mos t likely
diagnosi s is "There' s nothin g wron g with you." ) Obviously , a testin g or treatmen t plan shoul d
depen d both on probabilitie s and utilities . Finally , the availabilit y of utilit y informatio n help s in
the knowledg e engineerin g and consultatio n process , as we now explain .
492 Chapte r 16 . Makin g Simpl e Decision s
The knowledg e engineerin g proces s require d for buildin g and usin g decision­theoreti c
exper t syste m is as follows :
• Determin e the scop e of the problem . Determin e wha t are the possibl e actions , outcomes ,
and evidenc e to consider . Normally , the analys t will have to intervie w one or more expert s
in the domai n to discove r the importan t factors . Not e that we recommende d the same sort
of determinatio n as the first step of knowledg e engineerin g in Sectio n 8.2.
• Lay out the topology . Onc e all the relevan t factor s are determined , we need to know
whic h ones are influence d by whic h others . It is particularl y importan t to understan d
whic h aspect s of the outcom e state determin e its utility .
• Assig n probabilities . In decisio n networks , the conditiona l probabilitie s reflec t not only
causa l influence s betwee n rando m variables , but also the effect s of actions .
• Assig n utilities . A utilit y functio n is ofte n assesse d usin g the technique s describe d earlier .
Compute r program s exist that automat e the task of extractin g preference s for variou s lotter ­
ies and constructin g a utilit y function . Identifyin g the preferenc e structur e of multiattribut e
utilit y function s is also vital in reducin g the dimensionalit y of the assessmen t problem . It
can reduc e the numbe r of question s exponentially .
• Ente r availabl e evidence . For each specifi c case in whic h the syste m is used , there may
be some initia l evidenc e available .
• Evaluat e the diagram . Calculat e the optima l actio n accordin g to the existin g evidence .
• Obtai n new evidence . Calculat e the valu e of information , comparin g it with the costs of
acquisition , and perfor m the appropriat e observations , if any. Notic e that purel y inferentia l
exper t systems , withou t utilities , canno t decid e wha t new evidenc e to acquire .
• Perfor m sensitivit y analysis . Thi s importan t step check s to see if the best decisio n is
sensitiv e to smal l change s in the assigne d probabilitie s and utilitie s by systematicall y
varyin g thes e parameter s and runnin g the evaluatio n again . If smal l change s lead to
significantl y differen t decisions , then it may be worthwhil e to spen d mor e resource s to
collec t bette r data . If all variation s lead to the same decision , then the user will have more
confidenc e that it is the righ t decision .
Sensitivit y analysi s is particularl y important , becaus e one of the main criticism s of probabilisti c |
approache s to exper t system s is that it is too difficul t to asses s the numerica l probabilitie s
required . Sensitivit y analysi s ofte n reveal s that man y of the number s need only be specifie d very
approximately—within , say, 0.2 of the valu e that migh t be obtaine d from an exhaustiv e analysis .
Some system s allow probabilitie s to be specifie d as ranges . This leads to range s for the utilitie s of
actions . If the rang e of one actio n dominate s the range s of all others , then no furthe r probabilit y .
assessmen t need occur . For example , it migh t be the case that the optima l sitin g of an airpor t is
insensitiv e to the predicte d air traffi c over a large rang e of values , give n the system' s belief s abou t
the othe r relevan t factors , so that the user can remai n unruffle d by his or her lack of expertis e on |
air traffi c prediction .
Sectio n 16.8 . Summar y 49 3
16.8 SUMMARY_____________________________ _
This chapte r show s how to combin e utilit y theor y with probabilit y to enabl e an agen t to selec t
action s that will maximiz e its expecte d performance .
• Probabilit y theor y describe s wha t an agen t shoul d believ e on the basis of evidence , utilit y
theor y describe s wha t an agen t wants , and decisio n theor y puts the two togethe r to describ e
what an agen t shoul d do.
• We can use decisio n theor y to buil d a syste m that make s decision s by considerin g all
possibl e action s and choosin g the one that lead s to the best expecte d outcome . Suc h a
syste m is know n as a rationa l agent .
• Utilit y theor y show s that an agen t whos e preference s betwee n lotterie s are consisten t with
a set of simpl e axiom s can be describe d as possessin g a utilit y function ; furthermore , the
agen t select s action s as if maximizin g its expecte d utility .
• Multiattribut e utilit y theor y deal s with utilitie s that depen d on severa l distinc t attribute s
of states . Stochasti c dominanc e is a particularl y usefu l techniqu e for makin g unambiguou s
decision s even withou t precis e utilit y value s for attributes .
• Decisio n network s provid e a simpl e formalis m for expressin g and solvin g decisio n prob ­
lems . The y are a natura l extensio n of belie f networks , containin g decisio n and utilit y node s
in additio n to chanc e nodes .
• Sometime s solvin g a proble m involve s findin g more informatio n befor e makin g a decision .
The valu e of informatio n is define d as the expecte d improvemen t in utilit y compare d to
makin g a decisio n withou t the information .
• Exper t system s that incorporat e utilit y informatio n have additiona l capabilitie s compare d
to pure inferenc e systems . In additio n to bein g able to mak e decisions , they can decid e
to acquir e informatio n base d on its value , and they can calculat e the sensitivit y of their
decision s to smal l change s in probabilit y and utilit y assessments .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
One of the earlies t application s of the principl e of maximu m expecte d utilit y (althoug h a devian t
one involvin g infinit e utilities ) was Pascal' s Wager , first publishe d as part of the Port­Royal  Logic
(Arnauld , 1662) . The derivatio n of numerica l utilitie s from preferenc e (utilit y ordering ) was first
carrie d out by Ramse y (1931) ; the axiom s for preferenc e in the presen t text are close r in form to
those rediscovere d in Theory  of Games  and Economic  Behavior  (Von Neuman n and Morgenstern ,
1944) . A good presentatio n of these axioms , in the cours e of a discussio n on risk preference , is
given by Howar d (1977) . Ramse y had derive d subjectiv e probabilitie s (not just utilities ) from an
agent' s preferences ; Savag e (1954 ) and Jeffre y (1983 ) carry out more recen t construction s of this
kind . Von Winterfeld t and Edward s (1986 ) provid e a moder n perspectiv e on decisio n analysi s
and its relationshi p to huma n preferenc e structures .
494 Chapte r 16 . Makin g Simpl e Decision s
The St. Petersbur g parado x was first presente d by Bernoull i (1738) . Jeffre y (1983 ) present s
a resolutio n of the parado x base d not on logarithmi c utilit y functions , but on denyin g that playin g
the gam e is a real possibilit y (becaus e no one coul d have a bank from whic h reward s of arbitraril y
high utilit y coul d be paid out) .
The micromor t utilit y measur e is discusse d by Howar d (1989) . QALY s are muc h mor e
widel y used in medica l and socia l polic y decision­makin g than are micromorts ; see (Russell ,
1990) for a typica l exampl e of an argumen t for a majo r chang e in publi c healt h polic y on ground s
of increase d expecte d utilit y measure d in QALYs .
The boo k Decisions  with  Multiple  Objectives:  Preferences  and  Value  Trade­Offs  (Keene y
and Raiffa , 1976 ) give s a thoroug h introductio n to multiattribut e utilit y theory . It describe s early
compute r implementation s of method s for elicitin g the necessar y parameter s for a multiattribut e
utilit y function , and include s extensiv e account s of real application s of the theory . In AI, the
principa l referenc e for MAU T is Wellman' s (1985 ) paper , whic h include s a syste m calle d URP
(Utilit y Reasonin g Package ) that can use a collectio n of statement s abou t preferenc e indepen ­
denc e and conditiona l independenc e to analyz e the structur e of decisio n problems . The use of
stochasti c dominanc e togethe r with qualitativ e probabilit y model s was investigate d extensivel y
by Wellma n (1988 ; 1990) . Wellma n and Doyl e (1992 ) provid e a preliminar y sketc h of how a
comple x set of utility­independenc e relationship s migh t be used to provid e a structure d mode l of
a utilit y function , in muc h the same way that belie f network s provid e a structure d mode l of joint
probabilit y distributions .
Decisio n theor y has been a standar d tool in economics , finance , and managemen t scienc e
since the 1950s . Unti l the 1980s , decisio n trees were the main tool used for representin g simpl e
decisio n problems . Decisio n network s or influenc e diagram s were introduce d by Howar d and
Matheso n (1984) , althoug h the underlyin g concept s wer e develope d muc h earlie r by a grou p
(includin g Howar d and Matheson ) at SRI (Mille r et al., 1976) . Howar d and Matheson' s metho d
involve d the derivatio n of a decisio n tree from a decisio n network , but in genera l the tree is of
exponentia l size. Shachte r (1986 ) develope d a metho d for makin g decision s base d directl y on a
decisio n network , withou t the creatio n of an intermediat e decisio n tree. The collectio n by Olive r
and Smit h (1990 ) has a numbe r of usefu l article s on decisio n networks , as does the 1990 specia l
issue of the journa l Networks.  Paper s on decisio n network s and utilit y modellin g also appea r
regularl y in the journa l Management  Science.
Informatio n valu e theor y was first analyze d by Ron Howar d (1966) . His pape r ends with
the followin g remark :
If informatio n valu e theor y and associate d decisio n theoreti c structure s do not in the futur e
occup y a larg e part of the educatio n of engineers , then the engineerin g professio n will find
that its traditiona l role of managin g scientifi c and economi c resource s for the benefi t of man
has been forfeite d to anothe r profession .
To date , the implie d revolutio n in manageria l method s has not occurred , althoug h as the use of
informatio n valu e theor y in system s such as Pathfinde r become s mor e widespread , it may yet
becom e part of ever y decision­maker' s armory .
Surprisingl y few AI researcher s adopte d decision­theoreti c tools after the early application s
in medica l decisio n makin g describe d in Chapte r 14. On e of the few exception s was Jerr y
Feldman , who applie d decisio n theor y to problem s in visio n (Feldma n and Yakimovsky , 1974 ) and
Sectio n 16.8 . Summar y 495
EXERCISE Splannin g (Feldma n and Sproull , 1977) . Afte r the resurgenc e of interes t in probabilisti c method s
in AI in the 1980s , decision­theoreti c exper t system s gaine d widesprea d acceptanc e (Horvit z et
al., 1988)—i n fact, from 1991 onward , the cove r desig n of the journa l Artificial  Intelligence  has
depicte d a decisio n network , althoug h som e artisti c licens e appear s to have been take n with the
directio n of the arrows .
16.1 (Adapte d from Davi d Heckerman. ) This exercis e concern s the Almana c Game , whic h is
used by decisio n analyst s to calibrat e numeri c estimations . For each of the question s below , give
your best gues s of the answer , that is, a numbe r that you thin k is as likel y to be too high as it is
to be too low. Also give your gues s at a 25th percentil e estimate , that is, a numbe r that you thin k
has a 25% chanc e of bein g too high , and a 75% chanc e of bein g too low. Do the sam e for the
75th percentile . (Thus , you shoul d give three estimate s in all—low , median , and high—fo r each
question. )
a. Numbe r of passenger s who flew betwee n New York and Los Angele s in 1989 .
b. Populatio n of Warsa w in 1992 .
c. Yea r in whic h Coronad o discovere d the Mississipp i River .
d. Numbe r of vote s receive d by Jimm y Carte r in the 1976 presidentia l election ,
e. Numbe r of newspaper s in the U.S. in 1990 .
f. Heigh t of Hoove r Dam in feet,
g. Numbe r of eggs produce d in Orego n in 1985 .
h. Numbe r of Buddhist s in the worl d in 1992 .
i. Numbe r of death s due to AID S in the U.S. in 1981 .
j. Numbe r of U.S. patent s grante d in 1901 .
The correc t answer s appea r afte r the last exercis e for this chapter . Fro m the poin t of view of
decisio n analysis , the interestin g thin g is not how clos e you r media n guesse s cam e to the real
answers , but rathe r how often the real answe r came withi n your 25% and 75% bounds . If it was
abou t half the time , then your bound s are accurate . But if you'r e like mos t people , you will be
more sure  of yoursel f than you shoul d be, and fewe r than half the answer s will fall withi n the
bounds . Wit h practice , you can calibrat e yoursel f to give realisti c bounds , and thus be mor e
usefu l in supplyin g informatio n for decisio n making . Try this secon d set of question s and see if
there is any improvement :
a. Yea r of birth of Zsa Zsa Gabor .
b. Maximu m distanc e from Mar s to the sun in miles .
c. Valu e in dollar s of export s of whea t from the U.S. in 1992 .
d. Ton s handle d by the port of Honolul u in 1991 .
e. Annua l salar y in dollar s of the governo r of Californi a in 1993 .
496 Chapte r 16 . Makin g Simpl e Decision s
LEXICOGRAPHI C
PREFERENC Ef. Populatio n of San Dieg o in 1990 .
g. Yea r in whic h Roge r William s founde d Providence , R.I.
h. Heigh t of Mt. Kilimanjar o in feet.
i. Lengt h of the Brookly n Bridg e in feet.
j. Numbe r of death s due to automobil e accident s in the U.S. in 1992 .
16.2 Ticket s to the state lotter y cost $1. Ther e are two possibl e prizes : a $10 payof f with
probabilit y 1/50 , and a $1,000,00 0 payof f with probabilit y 1/2,000,000 . Wha t is the expecte d
monetar y valu e of a lotter y ticket ? Whe n (if ever) is it rationa l to buy a ticket ? Be precise—sho w
an equatio n involvin g utilities . You may assum e that t/($10 ) = 10 x £/($!) , but you may not
make any assumption s abou t {/($! , 000,000) . Sociologica l studie s show that peopl e with lowe r
incom e buy a disproportionat e numbe r of lotter y tickets . Do you think this is becaus e they are
wors e decisio n maker s or becaus e they have a differen t utilit y function ?
16.3 Asses s your own utilit y for differen t incrementa l amount s of money . Do this by runnin g
a serie s of preferenc e tests betwee n som e definit e amoun t M\ and a lotter y [p,M2',  (1 — p), 0].
Choos e differen t value s of M\ 1 and A/2, and vary p unti l you are indifferen t betwee n the two
choices . Plot the resultin g utilit y function .
16.4 Writ e a compute r progra m to automat e the proces s in Exercis e 16.3 . Try your progra m out
on severa l peopl e of differen t net wort h and politica l outlook . Commen t on the consistenc y of your
results , both acros s individual s and withi n the set of choice s mad e by a singl e individual .
16.5 It has sometime s been suggeste d that lexicographi c preferenc e is a form of rationa l
behavio r that is not capture d by utilit y theory . Lexicographi c preference s rank attribute s in some
order Xi, ...,  Xn, and treat each attribut e as infinitel y more importan t than attribute s later in the
order . In choosin g betwee n two prizes , the valu e of attribut e X, only matter s if the prize s have
the same value s for X\, ...,  X,_ i. In a lottery , an infinitesima l probabilit y of a tiny improvemen t
in a more importan t attribut e is considere d bette r than a dead certaint y of a huge improvemen t in
a less importan t attribute . For example , in the airport­sitin g problem , it migh t be propose d that
preservin g huma n life is of paramoun t importance , and therefor e if one site is mor e dangerou s
than another , it shoul d be ruled out immediately , withou t considerin g the othe r attributes . Only
if two sites are equall y safe shoul d they be compare d on othe r attribute s such as cost.
a. Giv e a precis e definitio n of lexicographi c preferenc e betwee n deterministi c outcomes .
b. Giv e a precis e definitio n of lexicographi c preferenc e betwee n lotteries .
c. Doe s lexicographi c preferenc e violat e any of the axiom s of utilit y theory ? If so, give an
example . (Hint:  conside r pair­wis e preferenc e comparison s of three differen t possibilities. )
d. Sugges t a set of attribute s for whic h you migh t exhibi t lexicographi c preferences .
16.6 Sho w that if X{ and X2 are preferentiall y independen t of X3, and X2 and X3 are preferentiall y
independen t of X|, then it follow s that X3 and Xi are preferentiall y independen t of X2.
16.7 Encod e the airport­sitin g proble m as show n in Figur e 16.4, provid e reasonabl e probabilitie s
and utilities , and solv e the proble m for the case of choosin g amon g thre e sites . Wha t happen s
Sectio n 16.8 . Summar y 49 7
if change s in technolog y mea n that each aircraf t generate s half as muc h noise ? Wha t if nois e
avoidanc e become s three time s more important ?
16.8 Repea t Exercis e 16.7 , usin g the action­utilit y representatio n show n in Figur e 16.5 .
16.9 Fo r eithe r of the airport­sitin g diagram s constructe d in Exercise s 16.7 and 16.8 , to whic h
conditiona l probabilit y table entry is the utilit y most sensitive , give n the availabl e evidence ?
16.10 (Adapte d from Pear l (1988). ) A used­ca r buye r can decid e to carry out variou s tests with
variou s cost s (e.g. , kick the tires , take the car to a qualifie d mechanic) , and then , dependin g on
the outcom e of the tests , decid e whic h car to buy . We will assum e that the buye r is decidin g
whethe r to buy car c\ , that there is time to carry out at mos t one test, and that t [ is the test of c\
and costs $50.
A car can be in good shap e (qualit y q+) or bad shap e (qualit y q~),  and the tests may help
to indicat e wha t shap e the car is in. Car c\ cost s $1,500 , and its marke t valu e is $2,00 0 if it is in
good shape ; if not, $700 in repair s will be neede d to mak e it in good shape . The buyer' s estimat e
is that c\ has a 70% chanc e of being in good shape.
a. Calculat e the expecte d net gain from buyin g c\ , give n no test.
b. Test s can be describe d by the probabilit y that the car will pass or fail give n that the car is
in good or bad shape . We have the followin g information :
Use Bayes ' theore m to calculat e the probabilit y that the car will pass (or fail) its test,  and
henc e the probabilit y that it is in good (or bad) shap e give n each possibl e test outcome .
c. Calculat e the optima l decision s give n eithe r a pass or a fail, and their expecte d utilities .
d. Calculat e the valu e of informatio n of the test,  and deriv e an optima l conditiona l plan for
the buyer .
16.11 Prov e that the valu e of informatio n is nonnegative , as state d in Sectio n 16.6 .
16.12 Ho w muc h is a micromor t wort h to you? Devis e a protocol  to determin e this.
The answer s for Exercis e 16.1 (wher e M stand s for million) : Firs t set: 3M, 1.6M , 1541 , 41M ,
1611 , 221, 649M , 295M , 132, 25546 . Secon d set: 1917 , 155M , 4500M , 11M , 120000 , 1.1M ,
1636, 19340,1595,41710 .
17MAKIN G COMPLE X
DECISION S
In which we examine  methods  for deciding  what  to do  today,  given  that we will have
a chance  to act again  tomorrow.
SEQUENTIA L
DECISIO N
PROBLEM SIn this chapter , we addres s the computationa l issue s involve d in makin g decisions . Wherea s
Chapte r 16 was concerne d with singl e decisio n problems , in whic h the utilit y of each action' s
outcom e was well­known , in this chapte r we will be concerne d with sequentia l decisio n prob ­
lems , wher e the agent' s utilit y depend s on a sequenc e of decisions . Sequentia l decisio n problems ,
whic h includ e utilities , uncertainty , and sensing , generaliz e the searc h and plannin g problem s
describe d in Parts II and IV.
The chapte r divide s roughl y into two parts . Section s 17.1 throug h 17.3 deal with classica l
technique s from contro l theory , operation s research , and decisio n analysi s that were develope d to
solve sequentia l decisio n problem s unde r uncertainty . The y operat e in muc h the same way that
the searc h algorithm s of Part II solve d sequentia l decisio n problem s in deterministi c domains :
by lookin g for a sequenc e of action s that lead s to a good state . The differenc e is that wha t they
return is not the fixed sequenc e of actions , but rathe r a policy —tha t is, a set of situation­actio n
rules for each state—arrive d at by calculatin g utilitie s for each state .
The secon d part , Section s 17.4 throug h 17.6 , develop s a complet e sketc h of a decision ­
theoreti c agen t usin g a riche r representatio n of state s in term s of rando m variable s in a belie f
network . We also show how to efficientl y updat e the networ k over time , and how to be able to
safely forge t thing s abou t the past .
17.1 SEQUENTIA L DECISIO N PROBLEM S
Suppos e that an agen t is situate d in the environmen t show n in Figur e 17.1 . Beginnin g in the start |
state, it mus t execut e a sequenc e of actions . The environmen t terminate s whe n the agen t reache s
one of the state s marke d +1 or ­1. In each location , the availabl e action s are calle d North,  South,
East,  and West.  We will assum e for now that the agen t know s whic h state it is in initially , and |
that it know s the effect s of all of its action s on the state of the world .
498
Sectio n 17.1 . Sequentia l Decisio n Problem s 499
STAR T
Figur e 17.1 A  simpl e environmen t that present s the agen t with a sequentia l decisio n problem .
In the deterministi c versio n of the problem , each actio n reliabl y move s one squar e in
the intende d direction , excep t that movin g into a wall result s in no chang e in position . In the
stochasti c version , the action s are unreliable . Eac h actio n achieve s the intende d effec t with
probabilit y 0.8, but the rest of the time , the actio n move s the agen t at righ t angle s to the intende d
direction . For example , from the start squar e (1,1) , the actio n North  move s the agen t to (1,2 )
with probabilit y 0.8, but with probabilit y 0.1, it move s East  to (2,1) , and with probabilit y 0.1, it
TRANSITIO N MODE L move s West,  bump s into the wall , and stay s in (1,1) . We will use the term transitio n mode l (or
just "model, " wher e no confusio n can arise ) to refer to the set of probabilitie s associate d with the
possibl e transition s betwee n state s after any give n action . The notatio n M" f mean s the probabilit y
of reachin g state j if actio n a is done in state i.
The trick y part is the utilit y function . Othe r than the termina l state s (the ones marke d +1
and ­1), ther e is no indicatio n of a state' s utility . So we have to base the utilit y functio n on
a sequenc e of states—a n environmen t history —rathe r than on a singl e state . Let us suppos e
that the utilit y for a sequenc e will be the termina l state' s valu e minu s l/25t h the lengt h of the
sequence , so a sequenc e of lengt h 6 that lead s to the +1 box has utilit y 0.76 .
In the deterministi c case , with knowledg e of the initia l state and the effect s of actions , the
proble m can be solve d directl y by the searc h algorithm s describe d in Chapte r 3. Thi s is true
regardless  of whethe r the environmen t is accessibl e or inaccessible . The agen t know s exactl y
whic h state it will be in after any give n action , so there is no need for sensing .
In the mor e general , stochasti c case , the agen t will not know exactl y whic h state it will
reach afte r any give n sequenc e of actions . For example , if the agen t is in locatio n (3,2) , then the
actio n sequenc e [North,  East]  migh t end up in any of five state s (see Exercis e 17.1) , and reache s
the +1 state at (4,3) with probabilit y only 0.64 .
One temptin g way to deal with actio n sequence s woul d be to conside r sequence s as long
actions . The n one coul d simpl y appl y the basic Maximu m Expecte d Utilit y principl e to sequences .
The rationa l actio n woul d then be the first actio n of the optima l sequence . Now , althoug h this
approac h is closel y relate d to the way that searc h algorithm s work , it has a fundamenta l flaw . It
500 Chapte r 17 . Makin g Comple x Decision s
POLIC Y
MARKO V DECISIO N
PROBLE M
MDP
MARKO V PROPERT Y
POMD Passume s that the agen t is require d to commit  to an entir e sequenc e of action s befor e executin g
it. If the agen t has no sensors , then this is the best it can do. Bu t if the agen t can acquir e
new sensor y informatio n after each action , then committin g to an entir e sequenc e is irrational .
For example , conside r the sequenc e [North,  East},  startin g at (3,2) . Wit h probabilit y 0.1, North
bump s the agen t into the wall , leavin g it still in (3,2) . In this case , carryin g on with the sequenc e
and executin g East  woul d be a bad choice .
In reality , the agen t will have the opportunit y to choos e a new actio n after each step, given
whatever  additional  information  its sensors  provide.  We therefor e need an approac h muc h more
like the conditiona l plannin g algorithm s of Chapte r 13, rathe r than the searc h algorithm s of
Chapte r 3. Of course , these will have to be extende d to handl e probabilitie s and utilities . We will
also have to deal with the fact that the "conditiona l plan " for a stochasti c environmen t may have
to be of infinit e size, becaus e it is possible , althoug h unlikely , for the agen t to get stuc k in one
place (or in a loop ) no matte r how hard it tries not to.
We begi n our analysi s with the case of accessibl e environments . In an accessibl e envi ­
ronment , the agent' s percep t at each step will identif y the state it is in. If it can calculat e the
optima l actio n for each state , then that will completel y determin e its behavior . No matte r wha t
the outcom e of any action , the agen t will alway s know wha t to do next .
A complet e mappin g from state s to action s is calle d a policy . Give n a policy , it is possibl e
to calculat e the expecte d utilit y of the possibl e environmen t historie s generate d by that policy .
The problem , then , is not to calculat e the optima l actio n sequence , but to calculat e the optima l
policy—tha t is, the polic y that result s in the highes t expecte d utility . An optima l polic y for the
world in Figur e 17.1 is show n in Figur e 17.2(a) . Notic e that becaus e the cost of takin g a step is
fairly smal l compare d to the penalt y for endin g up in (4,2 ) by accident , the optima l polic y for
the state (3,1 ) is conservative . The polic y recommend s takin g the long way round , rathe r than
takin g the shor t cut and thereb y riskin g enterin g (4,2) . As the cost of takin g a step is increased ,
the optima l polic y will , at som e point , switc h over to the more direc t rout e (see Exercis e 17.4) .
As the cost of a step is decreased , the polic y will becom e extremely  conservative . For example ,
if the cost is 0.01 , the polic y for the state (3,2) is to head West  directl y into the wall , thereb y
avoidin g any chanc e of fallin g into (4,2) .
Once a polic y has been calculate d from the transitio n mode l and the utilit y function , it
is a trivia l matte r to decid e wha t to do. A polic y represent s the agen t functio n explicitly , and
is therefor e a descriptio n of a simpl e refle x agent , compute d from the informatio n used for a
utility­base d agent . Figur e 17.3 show s the correspondin g agen t design .
The proble m of calculatin g an optima l polic y in an accessible , stochasti c environmen t
with a know n transitio n mode l is calle d a Marko v decisio n proble m (MDP) , after the Russia n
statisticia n Andre i A. Markov . Markov' s work is so closel y associate d with the assumptio n of
accessibility , that decisio n problem s are ofte n divide d into "Markov " and "non­Markov. " Mor e
strictly , we say the Marko v propert y hold s if the transitio n probabilitie s from any give n state
depen d only on the state and not on previou s history . The next two section s give algorithm s for
calculatin g optima l policie s in Marko v decisio n problems .
In an inaccessibl e environment , the percep t does not provid e enoug h informatio n to de­
termin e the state or the associate d transitio n probabilities . In the operation s researc h literature ,
such problem s are calle d partiall y observabl e Marko v decisio n problems , or POMDP . Meth ­
ods used for MDP s are not directl y applicabl e to POMDPs . For example , suppos e our agen t
Sectio n 17.1 . Sequentia l Decisio n Problem s 501
Figur e 17.2 (a ) An optima l polic y for the stochasti c environment , (b) The utilitie s of the states .
functio n SIMPLE­POLICY­AGENT(perce/? 0 return s an action
static : M, a transitio n mode l
U, a utilit y functio n on environmen t historie s
P, a policy , initiall y unknow n
if P is unknow n then P *
retur n P\percepi\­ the optima l polic y give n U, M
Figur e 17.3 A n agen t that calculate s and uses an optima l policy .
is equippe d with a sona r ring that give s it the distanc e to the neares t wall in each of the four
directions . For such an agent , the location s (2,1 ) and (2,3 ) are indistinguishable , yet differen t
action s are neede d in each . Furthermore , the Marko v propert y does not hold for percept s (as
oppose d to states) , becaus e the next percep t does not depen d just on the current  percept  and the
actio n taken .
The correc t approac h for POMDP s is to calculat e a probabilit y distributio n over the possibl e
states give n all previou s percepts , and to base decision s on this distribution . Althoug h the optima l
decisio n is not uniquel y determine d by the curren t percept , it is determine d uniquel y (up to ties)
by the agent' s probabilit y distributio n over the possibl e state s that it coul d be in. For example ,
the sonar­equippe d agen t migh t believ e that it is in state (2,1 ) with probabilit y 0.8 and in state
(2,3)  with probabilit y 0.2. The utilit y of actio n A is then
0.8 x utilit y of doin g A in (2,1) +
0.2 x utilit y of doin g A in (2,3)
This seem s simpl e enough . Unfortunately , in POMDPs , calculatin g the utilit y of an actio n in a
state is mad e mor e difficul t by the fact that action s will caus e the agen t to obtai n new percepts ,
whic h will caus e the agent' s belief s to chang e in comple x ways . Essentially , the agen t mus t take
502 Chapte r 17 . Makin g Comple x Decision s
into accoun t the information  that it migh t obtain , as wel l as the state it will reach . POMDP s
therefor e includ e the valu e of informatio n (Sectio n 16.6 ) as one componen t of the decisio n
problem .
The standar d metho d for solvin g a POMD P is to construct a  new MD P in whic h this
probabilit y distributio n play s the,rol e of the state variable . Unfortunately , the new MD P is not
easy to solve . The new state spac e is characterize d by real­value d probabilities , and is therefor e
infinite . Exac t solutio n method s for POMDP s requir e some fairl y advance d tools , and are beyon d
the scop e of this book . The bibliographica l note s at the end of this chapte r provid e pointer s to
suitabl e additiona l reading .
Instea d of tryin g to find exac t solutions , one can ofte n obtai n a good approximatio n usin g
a limite d lookahead . (See , for example , the algorithm s in Chapte r 5.) Sectio n 17.4 show s how
this approac h can be realize d for POMDP s usin g the technolog y of decisio n networks . Befor e
tacklin g POMDPs , however , we firs t presen t the mos t commo n solutio n method s for makin g
decision s in accessibl e worlds .
17.2 VALU E ITERATIO N
VALU E ITERATIO N
SEPARABILIT Y
ADDITIV EIn this section , we presen t an algorith m for calculatin g an optima l polic y calle d valu e iteration .
The basic idea is to calculat e the utilit y of each state , U(state),  and then use the state utilitie s to
selec t an optima l actio n in each state .
The difficul t part abou t calculatin g U(state)  is that we do not kno w wher e an actio n will
lead. We can thin k of a sequenc e of action s as generatin g a tree of possibl e histories , with the
curren t state as the root of the tree, and each path from the root to a leaf representin g a possibl e
histor y of states . We use the notatio n H(state,policy)  to denot e the histor y tree startin g from
state and takin g actio n accordin g to policy.  This can be though t of as a rando m variabl e that is
dependen t on the transitio n mode l M. The n the utilit y of a state ;' is give n by the expecte d utilit y
of the histor y beginnin g at that state and followin g an optima l policy :
U(i) = EU(H(i,  policy*  )\M) = ^ P(H(i,policy*)\M)U h(H(i,policy'r)) (17.1 )
wher e policy*  is an optima l polic y define d by the transitio n mode l M and the utilit y functio n on
historie s U/,. We will explai n shortl y how to deriv e an optimal  policy .
Havin g a utilit y functio n on state s is only usefu l to the exten t that it can be used to mak e
rationa l decisions , usin g the Maximu m Expecte d Utilit y principl e (Equatio n (16.1) , page 472) .
In the case of sequentia l decisions , we have to be quit e carefu l abou t this. For a utilit y functio n on
states (U) to mak e sense , we requir e that the utilit y functio n on historie s (£//, ) have the propert y
of separability . A utilit y functio n Uh is separabl e if and only if we can find a function / such
that
Uh([s0,sl ,...,*„] ) =f(s 0, U h([si,. . .,*„]) )
(Exercis e 17.2 asks you to construc t a utilit y functio n violatin g this property. ) The simples t form
of separabl e utilit y functio n is additive :
io, si,... , .?„]) = tf(so ) + Uh([s t,..., «„]))
Sectio n 17.2 . Valu e Iteratio n 503
REWAR D FUNCTIO N wher e R is calle d a rewar d function. ' Conside r agai n the utilit y functio n define d for Figur e 17.1 :
an environmen t histor y of lengt h n terminatin g in a state of valu e v has a utilit y of v ­ (l/25)« .
This utilit y functio n is separabl e and additive , and the rewar d functio n R is ­1/2 5 for nontermina l
states , +1 for state (4,3 ) and ­1 for state (4,2) . As we discus s in wha t follows , utilit y function s
over historie s are almos t alway s additiv e in practice . Notic e that additivit y was implici t in our
use of path cost function s in heuristi c searc h algorithm s (Chapte r 4).
Give n an additiv e utilit y functio n [//,, we can recove r the standar d Maximu m Expecte d
Utilit y principl e that an optima l actio n is one with maxima l expecte d utilit y of outcom e states :
policy*  (i) = arg max Yj MyU(j) (17.2 )
DYNAMI C
PROGRAMMIN Gwher e My is the probabilit y of reachin g state j if actio n a is take n in state i, and arg max a/(a)
return s the valu e of a with the highes t valu e for/(a) . Similarly , the utilit y of a state can be
expresse d in term s of the utilit y of its successors :
U(f) = R(i)  + max VM"U(j )
a z—' J(17.3 )
Equatio n (17.3 ) is the basi s for dynami c programming , an approac h to solvin g sequentia l
decisio n problem s develope d in the late 1950 s by Richar d Bellma n (1957) .
The simples t dynami c programmin g contex t involve s an n­ste p decisio n problem , wher e
the state s reache d afte r « step s are considere d termina l state s and have know n utilities . If there
are \A\ possibl e action s at each step , then the total complexit y of a naiv e approach—exhaustiv e
enumeration—woul d be 0(|/4|") . The dynami c programmin g approac h start s by calculatin g the
utilitie s of all state s at step n — 1 in term s of the utilitie s of the termina l states . One then calculat e
the utilitie s of state s at step n ­ 2, and so on. Becaus e calculatin g the utilit y of one state ,
using Equatio n (17.3) , cost s O(|A|) , the tota l cost of solvin g the decisio n proble m is no mor e
than O(n|/4||5|) , wher e |,S| is the numbe r of possibl e states . In smal l state spaces , this can be a
huge saving.2 Dynami c programmin g has sinc e becom e a field of its own , with a huge array of
application s and a large librar y of technique s for differen t type s of separabl e utilit y functions .
In mos t of the decisio n problem s that AI is intereste d in (includin g the worl d of Figur e 17.1) ,
the environmen t historie s are potentiall y of unbounde d lengt h becaus e of loops . This mean s that
there is no n for whic h to start the n­ste p dynami c programmin g algorithm . Fortunately , there
is a simpl e algorith m for approximatin g the utilitie s of state s to any degre e of accurac y usin g an
iterativ e procedure . We appl y Equatio n (17.3 ) repeatedly , on each step updatin g the utilit y of
each state base d on the old utilit y estimate s of the neighborin g states :
Ut+i (i) <­ R(f) + max (17.4 )
wher e U,(i)  is the utilit y estimat e for state i after t iterations . As t ­^ oo, the utilit y value s will
converg e to stabl e value s give n certai n condition s on the environment . The algorithm , calle d
VALUE­ITERATION , is show n in Figur e 17.4.
1 Thus , the utilit y functio n is additive , in the sens e define d in Chapte r 16, give n attribute s correspondin g to the reward s
receive d in each state in the sequence . Ther e are som e problem s in applyin g this equatio n to infinit e histories , whic h will
be discusse d later .
2 Th e savin g is of the sam e sort as that achieve d by checkin g repeate d state s durin g searc h (Sectio n 3.6) .
504 Chapte r 17 . Makin g Comple x Decision s
functio n VALUE­!TERATION(M , K) return s a utilit y functio n
inputs : M, a transitio n mode l
R, a rewar d functio n on state s
local variables : U, utilit y function , initiall y identica l to R
U' , utilit y function , initiall y identica l to R
repea t
U*­U'
for each state i do
f/'fi] ^R[i]  + max, ,
end
until CLOSE­ENOUGH([/ , U')
retur n Ul U\j]
Figur e 17.4 Th e valu e iteratio n algorith m for calculatin g utilitie s of states .
Given a utilit y functio n on states , it is trivia l to calculat e a correspondin g polic y usin g
Equatio n (17.2) . Furthermore , the polic y will actuall y be optimal , as prove d by Bellma n and
Dreyfu s (1962) . We can appl y valu e iteratio n to the environmen t show n in Figur e 17.1 , whic h
yield s the utilit y value s show n in Figur e 17.2(b) . In Figur e 17.5 , we show the utilit y value s
of some of the state s at each iteratio n step of the algorithm . Notic e how the state s at differen t
distance s from (4,3) accumulat e negativ e rewar d until , at som e point , a path is foun d to (4,3)
whereupo n the utilitie s start to increase .
0.5­ (4,3)
­ (3.3)
' (2,3)
,4.2,
10 1 5 2 0
Numbe r of iteration s
Figur e 17.5 Th e utilit y value s for selecte d state s at each iteratio n step in the applicatio n of
VALUE­ITERATIO N to the 4x3 worl d in Figur e 17.1 .
Sectio n 17.3 . Polic y Iteratio n 505
RMS ERRO R
POLIC Y LOSSHow long shoul d valu e iteratio n be allowe d to run? Do we requir e the value s to converge ?
These are nontrivia l questions . Ther e are two obviou s way s to measur e the progres s of valu e
iteration . The first uses the RM S erro r (RM S stand s for "roo t mean square" ) of the utilit y value s
compare d to the correc t values . The secon d assume s that the estimate d utilit y value s are not in
themselve s important—wha t count s is the polic y that they imply . The polic y correspondin g to
an estimate d utilit y functio n U, is derive d usin g Equatio n (17.2) . We can measur e the qualit y of
a polic y usin g the expecte d polic y loss—the differenc e betwee n the expecte d utilit y obtaine d by
an agen t usin g the policy , compare d with an agen t usin g the optima l policy . Figur e 17.6 show s
how both measure s approac h zero as the valu e iteratio n proces s proceeds . Notic e that the polic y
(whic h is chose n from a discrete , finit e set of possibl e policies ) become s exactl y optima l long
befor e the utilit y estimate s have converge d to their correc t values .
1 1  ——————————————————— — '  ——————————————————— —
0.8
| 0.6
I 0.4
0.2/Xx
\
" ­ ­._0.8
1 0.6
I 0.4
0.2' X—  ,
0 —— —— — —
0 5  1 0 1 5 2 0 0  5  1 0 1 5 2 0
Numbe r of iteration s Numbe r of iteration s
(a) (b )
Figur e 17.6 (a ) The RMS (roo t mea n square ) erro r of the utilit y estimate s compare d to the
correc t values , as a functio n of iteratio n numbe r durin g valu e iteration , (b) The expecte d polic y
loss compare d to the optima l policy .
17.3 POLIC Y ITERATIO N
POLIC Y ITERATIO N
VALU E
DETERMINATIO NIn the previou s section , we observe d that the optima l polic y is ofte n not very sensitiv e to the
exact utilit y values . This insigh t suggest s an alternativ e way to find optima l policies . The polic y
iteratio n algorith m work s by pickin g a policy , then calculatin g the utilit y of each state given
that policy.  It then update s the polic y at each state usin g the utilitie s of the successo r state s
(Equatio n (17.2)) , and repeat s unti l the polic y stabilizes . The step in whic h utilit y value s are
determine d from a give n polic y is calle d valu e determination . The basi c idea behin d polic y
iteration , as compare d to valu e iteration , is that valu e determinatio n shoul d be simple r than valu e
iteratio n becaus e the actio n in each state is fixe d by the policy . The polic y iteratio n algorith m is
show n in Figur e 17.7 .
506 Chapte r 17 . Makin g Comple x Decision s
functio n POLICY­!TERATION(M , R) return s a polic y
inputs : M, a transitio n mode l
R, a rewar d functio n on state s
local variables : U, a utilit y function , initiall y identica l to R
P, a policy , initiall y optima l with respec t to U
repea t
U — VALUE­DETERMINATION(P , U, M, K)
unchanged?^­  true
for each state ; do
if max fl £  Mfj UW >
Pl/1­aigmax, , £
i
unchanged?  <— false
end
until unchanged?
retur n PW the n
Figur e 17.7 Th e polic y iteratio n algorith m for calculatin g an optima l policy .
The VALUE­DETERMINATIO N algorith m can be implemente d in one of two ways . The first
is a simplificatio n of the VALUE­ITERATIO N algorithm , replacin g Equatio n (17.4 ) with
and usin g the curren t utilit y estimate s from polic y iteratio n as the initia l values . (Her e Policy(i)  is
the actio n suggeste d by the polic y in state i.) Whil e this can work well in som e environments , it
will often take a very long time to converg e in the early stage s of polic y iteration . This is becaus e
the polic y will be mor e or less random , so that man y step s can be require d to reac h termina l
states .
The secon d approac h is to solve for the utilitie s directly . Give n a fixed polic y P, the utilitie s
of state s obey a set of equation s of the form
For example , suppos e P is the polic y show n in Figur e 17.2(a) . The n usin g the transitio n mode l
M, we can construc t the followin g set of equations :
«(l,2 ) = 0.8H(i, 3)+0.2M(|,2 )
and so on. This give s a set of 1 1 linea r equation s in 1 1 unknowns , whic h can be solve d by linea r
algebr a method s such as Gaussia n elimination . For smal l state spaces , valu e determinatio n usin g
exact solutio n method s is ofte n the mos t efficien t approach .
Sectio n 17.3 . Polic y Iteratio n 50 7
HOW IMMORTA L AGENT S DECID E WHA T TO DO
Makin g decision s is not easy whe n one live s forever . The tota l rewar d obtaine d by
a polic y can easil y be unbounded . Becaus e one canno t easil y compar e infinities ,
it is difficul t to say whic h polic y is rational ; moreover , both valu e iteratio n and
polic y iteratio n will fail to terminate . If the agent' s lifetim e is finit e but contain s
million s of steps , then thes e algorithm s are intractable . Thes e difficultie s arise from
fundamenta l problem s associate d with specifyin g utilitie s over historie s so that the
resultin g "optimal " behavio r make s intuitiv e sense . The same issue s arise for humans .
Shoul d one live fast and die young , or live an unexcitin g life to a ripe old age?
One of the mos t commo n approache s is to use discounting . A discountin g
functio n consider s reward s receive d in futur e tim e step s to be less valuabl e than
reward s receive d in the curren t time step . Suppos e that an environmen t histor y H
contain s a strea m of rewards , such that the agen t receive s rewar d Rf at the ith futur e time
step. The standar d metho d of discountin g uses the utilit y functio n U(H)  = ]TV 7'/?,­ ,
wher e 7 is the discoun t factor . Provide d 0 < 7 < 1, this sum will converg e to a finit e
amount . Discountin g can be interprete d in at least three differen t ways :
• As a trick to get rid of the infinities . Essentially , it is a smoothed­ou t versio n of
the limited­horizo n algorithm s used in game­playing—th e smalle r the valu e of
7, the shorte r the effectiv e horizon .
• As an accurat e mode l of both anima l and huma n preferenc e behavior . In
economics , it is widel y used in assessin g the valu e of investments .
• As a natura l preference­independenc e assumptio n associate d with reward s over
time. Discountin g follows  from an assumptio n of stationarity . Stationarit y
mean s the following : if two rewar d sequence s R\,R2,Rj,...  and Si, 52, £3, • • •
begin with the sam e rewar d (i.e., R\ = S\) then the two sequence s shoul d be
preference­ordere d the same way as the sequence s /?2, /?3, • • • and £2, £3,. . •• If
stationarit y seem s like a reasonabl e assumption , then discountin g is a reasonabl e
way to mak e decisions .
If the agent' s lifetim e is long (in numbe r of steps ) compare d to the numbe r of
states , then the optima l polic y will be repetitive.  For example , a taxi drive r aimin g
to maximiz e his incom e wil l usuall y adop t a standar d set of waitin g pattern s for
times whe n he does not have a passenger . The syste m gain is define d as the averag e
rewar d obtaine d per unit time . It can be show n that after an initia l "transient"  period ,
any optima l polic y has a constan t syste m gain . Thi s fact can be used to comput e
optima l policies—fo r example , tellin g the taxi drive r wher e to wait at differen t time s
of day—usin g a versio n of polic y iteration .
508 Chapte r 17 . Makin g Comple x Decision s
17.4 DECISION­THEORETI C AGEN T DESIG N
In this section , we outlin e a comprehensiv e approac h to agen t desig n for environment s with
uncertainty . It ties togethe r belie f and decisio n network s with the technique s for sequentia l
decisio n problem s discusse d earlier . It addresse s the proble m of large state space s by decomposin g
the state descriptio n into a set of rando m variables , muc h as the plannin g algorithm s in Part IV
used logica l representation s to decompos e the state spac e used by searc h algorithms . We begi n
by describin g the basi c approach , whic h hark s back to the sketc h of the utility­base d agen t
provide d in Chapte r 2. We then show how sensin g work s in an uncertain , partiall y accessibl e
environment . Sectio n 17.5 extend s the idea of belie f network s to cove r environment s that chang e
over time , and then Sectio n 17.6 include s decisions , providin g a complet e agen t design .
DECISIO N CYCL EThe decisio n cycl e of a rationa l agen t
Figur e 17.8 repeat s the schemati c agen t desig n for rationa l agent s first show n in Figur e 14.1 . At
each step , the processin g done by the agen t is calle d the decisio n cycle . In this section , we will
make component s of the cycl e more precise . We begi n with the first step, that of determinin g the
curren t state of the world .
functio n DECISION­THEORETlC­AGENT(/;><?rfe/7f ) return s action
calculat e update d probabilitie s for curren t state base d on
availabl e evidenc e includin g curren t percep t and previou s actio n
calculat e outcom e probabilitie s for action s
given actio n description s and probabilitie s of curren t state s
selec t action  with highes t expecte d utilit y
given probabilitie s of outcome s and utilit y informatio n
retur n action
Figur e 17.8 A  decision­theoreti c agen t (repea t of earlie r figure) .
STAT E VARIABLE SIn general , we will assum e that we have a set of rando m variable s X, that refe r to the
curren t state of the world . We will call thes e the state variables . For example , if the agen t is a
robot movin g in the X­Y plane , then we migh t use X, and Y, to refe r to the robot' s positio n at
time t, and X, and Y, to refer to the velocity . Notic e the similarit y to the propositiona l versio n
of situatio n calculu s used in Chapte r 6 for the first logical­agen t design . Thi s similarit y is not
a coincidence : probabilit y theor y essentiall y combine s propositiona l logic with uncertainty . As
in situatio n calculus , it is importan t to distinguis h betwee n belief s abou t a changin g worl d and
changin g belief s abou t a give n world . The forme r is achieve d by havin g differen t proposition s
referrin g to differen t times , and the latte r by conditionin g the probabilit y of a give n propositio n
on additiona l evidence . Thus , if the percept  histor y up to and includin g time t is E|, ..., E<
Sectio n 17.4 . Decision­Theoreti c Agen t Desig n 509
PREDICTIO N PHAS E(wher e each E, may also consis t of observation s on severa l rando m variables) , and the previou s
action s have been A i . . . A,_ ] , then wha t we are intereste d in is
that is, the probabilit y distributio n over the curren t state give n all availabl e evidence . We refe r to
this quantit y as Bel(X t) — the belie f abou t the state at time t, give n all evidenc e up to time t.
This is a rathe r complicate d expression , and direc t evaluatio n is out of the questio n becaus e
it require s conditionin g on a large numbe r of variables . As in Chapte r 1 4, we can use conditiona l
independenc e statement s to simplif y this expression . The mai n assumptio n is that the proble m
is Markovian  — the probabilit y distributio n for the curren t state of the worl d depend s only on the
previou s state and the actio n take n in it. If X, is the state of the worl d at time t and  A, is the actio n
taken at time t, then we have
P(X,|X|...X f_,,A , ...A,_, ) = (17.5 )
Whethe r the Marko v propert y hold s depend s on whic h state variable s the agen t is tracking , and on
the detail s of the environment . For example , in the case of a robo t movin g in the X­Y plane , the
previou s positio n and velocit y migh t well be enoug h to predic t the curren t positio n and velocity ,
given the previou s moto r comman d — one can simpl y use Newton' s laws to calculat e the new
positio n and velocity . On the othe r hand , if the robo t is battery­powered , then the effec t of an
actio n will depen d on whethe r the batter y is exhausted . Becaus e this in turn depend s on how
much powe r was used by all previou s commands , the Marko v propert y is violated . We can restor e
the Marko v propert y by includin g Battery­Level,  as one of the state variable s that compris e X,.
We also assum e that each percep t depend s only on the state at the time . Thi s amount s to
the assertio n that percept s (E,) are causall y determine d by the state of the world :
P(E,|X, . ..X,,A| . ..A,­i , E,...E,_, ) = P(E f|X,) (17.6 )
Finally , we assum e that a simila r equatio n hold s for actions . The actio n take n depend s only on
the percept s the agen t has receive d to date :
i. ..£,_, ) (17.7 )
This fina l assertio n is vali d becaus e of the structur e of the agen t itself : its only inpu t from the
outsid e is the percep t at each time step.
Take n together , Equation s (17.5) , (17.6) , and (17.7 ) allow us to simplif y the calculatio n of
the curren t state estimat e Bel(X,).  The calculatio n take s plac e in two phases :
<C> Predictio n phase : first , we predic t the probabilit y distributio n over state s we would  have
expected,  give n our knowledg e of the previou s state and how action s affec t states . We call
this Bel, and calculat e it by addin g up the probabilitie s of arrivin g in a give n state at time t
for each of the state s we coul d have been in at time t — I:
Bel(X t) = T  P(X , X r_, =x f_,,A,_ 1)fie/(X,_ , =x,_ , (17.8 )
wher e x,_ i range s over all possibl e value s of the state variable s X,_ t.
ESTIMATIO N PHAS E 0  Estimatio n phase : now we hav e a distributio n over the curren t stat e variables , give n
everythin g but the mos t recen t observation . The estimatio n phas e update s this usin g the
510 Chapte r 17 . Makin g Comple x Decision s
percep t E,. Becaus e both the state variable s and the percep t refer to the sam e time , this is
a simpl e matte r of Bayesia n updating , usin g Bel(X,)  as the prior :
Bel(X t) = aP(E , Xt)Bel(X t) (17.9 )
wher e a is a normalizatio n constant .
Exercis e 17.5 asks you to deriv e thes e equation s from the assumption s liste d earlier .
It is wort h notin g that the equation s for Bel and Bel are a generalizatio n of the techniqu e
KALMA N FILTERIN G know n in classica l contro l theor y as Kalma n filterin g (Kalman , 1960) . Kalma n filterin g assume s
that each state variabl e is real­value d and distribute d accordin g to a Gaussia n distribution ; that
each senso r suffer s from unbiase d Gaussia n noise ; that each actio n can be describe d as a vecto r of
real values , one for each state variable ; and that the new state is a linea r functio n of the previou s
state and the action . Thes e assumptions , take n together , allow predictio n and estimatio n to be
implemente d by som e simpl e matri x calculations , even with a large numbe r of state variables .
Kalma n filterin g is universall y applie d in monitorin g and controllin g all sort s of dynamica l
systems , from chemica l plant s to guide d missiles . It has good succes s even in domain s wher e
not all the assumption s are satisfied .
Give n a probabilit y distributio n over the curren t state , it is a simpl e matte r to carr y out
the remainin g step s of the decisio n cycle , whic h involv e projectin g forwar d the possibl e result s
of the availabl e action s and choosin g the one with maxima l expecte d utility . The belie f updat e
equation s also allow us to desig n an agen t that keep s aroun d just the curren t belie f vecto r for
the state variables . Th e complet e desig n is show n in Figur e 17.9 . Althoug h the formula s
look quit e complicated , bea r in min d that they simpl y instantiat e the basi c desig n give n in
Figur e 17.8 . Furthermore , the conditiona l probabilitie s appearin g in the variou s expression s are
SENSO R MODE L exactl y wha t we woul d expec t to see. We have P(E, Xr), the senso r model , whic h describe s
ACTIO N MODE L ho w the environmen t generate s the senso r data; and we have P(X, X,_ i, A,_ i), the actio n model ,
whic h describe s the effect s of action s (and similarl y for /,/ + I).3 Thes e are the sam e kind s
of informatio n that we have used throughou t the book to mak e decisions . The actio n mode l
generalize s the transitio n mode l used earlie r for sequentia l decisio n problems . The senso r mode l
was not used there , of course , becaus e we assume d an accessibl e environmen t in whic h the
percep t and the state can be equated .
The followin g section s describ e senso r and actio n model s in mor e detail , and show how
the complet e agen t desig n can be implemente d directl y usin g the belie f and decisio n networ k
technolog y describe d in the previou s chapters .
STATIONAR Y
SENSO R MODE LSensin g in uncertai n world s
We begi n with the senso r model , whic h we denne d previousl y as P(E,|X,) , the probabilit y of a
percep t give n a state of the world . Actually , this is unnecessaril y complicated , becaus e it allow s
the senso r mode l to vary with time . We will instea d assum e a stationar y senso r model :
Vr P(E f|X,) =
3 Th e notatio n used to describ e the actio n mode l migh t mak e it look as if it only allow s for action s by the agent . In fact,
becaus e the action s of othe r agent s are presumabl y themselve s determine d by the state variable s X,_ i , the mode l can
certainl y handl e multipl e agent s as is.
Sectio n 17.4 . Decision­Theoreti c Agen t Desig n 511
functio n DECISION­THEORETIC­AGENT(£, ) return s an actio n
inputs : Et, the percep t at time t
static : BN,  a belie f networ k with node s X
Bel(X),  a vecto r of probabilities , update d over time
­ J]Xi_| P(X , | X,_i=x,_i,A,_i ) Be/(X,_,=x,_i )
Bel(X,)  — a  P(E, | P X,) Bel(\,)
action  <— arg max..i , J^ X| \ Bel(X,=x,)  5^x, +, ­P(X,+ i = x,+i X,=x,,/4, ) t/(x,+i )
retur n action
Figur e 17.9 Detaile d desig n for a decision­theoreti c agent .
wher e E and X are rando m variable s rangin g over percept s and states , respectively . Wha t this
mean s is that give n a state of the world , the chance s of the senso r givin g a certai n readin g will be
the sam e toda y as it was yesterday . Thi s does not mean , for example , that the sensor  can neve r
break ; it just mean s that we have to includ e in X all the variable s that are importan t to the sensor' s
performance . The advantag e of the stationar y senso r mode l is that the fixe d mode l P(E|X ) can
then be used at each time step.
The basi c idea of a senso r mode l can be implemente d very simpl y in a belie f network ,
becaus e the assumptio n embodie d in Equatio n (17.6 ) effectivel y isolate s the senso r variable s
for a give n tim e step from the rest of the network . Figur e 17.10(a ) show s an abstrac t belie f
networ k fragmen t with generalize d state and senso r variables . The senso r mode l itsel f is the
conditiona l probabilit y table associate d with the percep t node . The directio n of the arrow is the
crucia l elemen t here : the state of the worl d causes  the senso r to take on a particula r value.4
The sensor  mode l is therefor e an exampl e of the genera l principle , state d in Chapte r 7 and
again in Chapte r 15, that causa l model s are to be preferre d whe n possible . If the senso r give s
a perfec t repor t of the actua l state , then the senso r model—th e conditiona l probabilit y table —
will be purel y deterministic . Nois e and error s in the senso r are reflecte d in the probabilitie s of
"incorrect " readings . In fact , we have alread y seen example s of senso r model s of this kind : in
the burglar­alar m networ k (Figur e 15.1) , both JohnCalls  and MaryCalls  can be viewe d as senso r
node s for the Alarm  state variable . Thei r conditiona l probabilit y tables , show n in Figur e 15.2 ,
show how reliabl e they are as sensors .
The next step is to brea k apar t the generalize d state and senso r variable s into their compo ­
nents . Typically , each senso r only measure s som e smal l aspec t of the total state . An exampl e is
show n in Figur e 17.10(b) , wher e temperatur e and pressur e gauge s measur e the actua l temperatur e
and pressur e of som e system . Decomposin g the overal l senso r mode l in Figur e 17.10(a ) into
its separat e component s greatl y reduce s the size of the CPTs , unles s the sensor s are very badl y
designed . As an exampl e of how not to coupl e sensor  node s to state nodes : conside r two sensor s
4 Of course , the proces s of inference  will go the othe r way : evidenc e arrive s at the senso r node , and is propagate d to the
state variable . The inferenc e proces s essentiall y inverts  the senso r model . Thi s is basicall y wha t make s a Kalma n filte r
simple . If P(E\X)  is Gaussian , then P(X\E)  is also Gaussia n with an identica l distribution , so that inversio n is trivial .
512 Chapte r 17 . Makin g Comple x Decision s
SENSO R MODE L
(a) (b)
Figur e 17.1 0 (a ) Belie f networ k fragmen t showin g the genera l relationshi p betwee n state
variable s and senso r variables , (b) An exampl e with pressur e and temperatur e gauges .
that (somehow ) measur e Pressure/Temperature  and Pressure  x Temperature.  Eac h of the two
state node s woul d have to be connecte d to both senso r nodes , resultin g in large CPT s for the
senso r models .
Often , we will have severa l sensor s that are measurin g the same state variable . In Fig­
ure 17.11 , we show an exampl e wher e two gauge s are bein g used to measur e the actua l temperatur e
..,:ffc ­, o f som e object , perhap s a superconductor . The  crucial  thing  to notice  here  is that  the sensor
• S values  are conditionally  independent  of each  other,  given  the actual  value.  The reasonin g here is
simila r to the reasonin g for the conditiona l independenc e of symptom s give n a disease . Althoug h
the sensor s are not unconditionall y independent—i n fact, they will usuall y displa y approximatel y
the same reading—the y are correlate d only inasmuc h as they depen d on the actua l temperature .
When we have multipl e sensor s for the same state variables , the resultin g inferenc e proces s is
SENSO R FUSIO N calle d senso r fusio n or data fusion . To see how importan t this can be, conside r the situatio n
DATA FUSIO N whe n Gaug e 1 read s 13.6°K , whil e Gaug e 2 read s 14.4 ° K. If each gaug e is accurat e to withi n
0.5 °K, as represente d in the senso r models , then the networ k will infer that the actua l temperatur e
is betwee n 13.9° K and 14.1 °K. Integratin g the result s of multipl e sensor s can provid e greate r
accurac y than any one senso r on its own . Thi s is true whethe r the sensor s are similar , as in the
case of temperatur e gauges , or very different , as in the case of sona r and infrare d distanc e sensor s
used in robotics . Detaile d senso r model s have been buil t for both sona r and infrared , whic h have
somewha t complementar y performance . Sona r has a long range , but is subjec t to "ghost " image s
cause d by multipl e reflection s and specularities . Infrare d is only accurat e over shor t distances .
By usin g sensor  fusion , it is ofte n possibl e to creat e an accurat e mode l of the robot' s environmen t
in case s wher e the sensor s used separatel y woul d be lost.
Anyon e with hands­o n experienc e of robotics , computerize d proces s control , or othe r form s
of automati c sensin g will readil y testif y to the fact that sensor s fail. Whe n a senso r fails , it does
not necessaril y send a signa l saying , "Oh, by the way , the data I'm abou t to send you is a load of .j|
nonsense. " Instead , it simpl y send s the nonsense . Thi s can be dangerou s if take n literally . For
example , a robot' s sona r distanc e senso r migh t start sendin g "infinity, " meanin g that no objec t 1
Sectio n 17.4 . Decision­Theoreti c Agen t Desig n 513
Figur e 17.1 1 Measurin g temperatur e usin g two separat e gauges .
was detecte d withi n the sonar' s range . Thi s coul d be becaus e the robo t has wandere d outsid e
or it coul d be becaus e the sonar' s detecto r is broken . In the latte r case , the robo t coul d start
crashin g into walls . In order  for the system  to handle  sensor  failure,  the sensor  model  must
include  the possibility of failure.  For example , in the case of sonar , a senso r mode l that says
that the senso r is accurat e to withi n 10 cm explicitl y disallows  the possibilit y of failure , and
therefor e force s the robo t to take the sona r readin g literally . For any give n actual  distance , the
sonar mode l shoul d allow the possibilit y that the observed  distanc e will be "infinity. " The n the
robot can handl e senso r failur e mor e appropriately . For example , if the robo t is in a corridor ,
its prediction  will be that the closes t objec t remain s abou t 60 cm away . If the sona r suddenl y
report s "infinity, " then the most likel y conclusio n is that the senso r has failed , not that the corrido r
has disappeared . Furthermore , if the robo t has mor e than one distanc e sensor , the senso r fusio n
proces s will automaticall y discoun t the reading s of the faile d sensor .
It is also possibl e to use more detaile d model s of senso r failur e by incorporatin g additiona l
state variable s representin g the conditio n of the sensor . Figur e 17.1 2 show s a mode l of a vision ­
based lane­positio n sensor . Suc h sensor s are used in autonomou s vehicle s to keep them in the
cente r of their lane . The y also coul d be used to soun d a warnin g in a human­drive n car whe n
it start s to stray off the road . The sensor' s accurac y is directl y affecte d by rain and an uneve n
road surface . Furthermore , rain migh t also caus e the senso r to fail by damagin g the electronics ,
as migh t a bump y road . Senso r failur e in turn affect s the sensor' s accuracy . Thi s kind of mode l
is capabl e of som e quit e subtl e reasoning . For example , if the syste m believe s (perhap s from
anothe r sensor' s input ) that it is raining , then that will alter the senso r accurac y variable , raisin g
the likelihoo d of large r error in the lane­positio n sensor . Whe n an unexpecte d readin g occurs , the
syste m will be less likel y to assum e that the car is out of position . Conversely , a large discrepanc y
betwee n expecte d and observe d positio n can increas e the system' s belie f that it is raining ! A
really seriou s discrepanc y woul d raise the posterio r probabilit y of senso r failure ; henc e this kind
of networ k can perfor m "diagnosis " of the sensors . In the next section , we will see how this
capabilit y can be extende d by reasonin g over time .
514 Chapte r 17 . Makin g Comple x Decision s
t Positio n \
Senso r '
Figur e 17.1 2 A  mode l of a lane­positio n senso r for an automate d vehicle .
17.5 DYNAMI C BELIE F NETWORK S
STAT E EVOLUTIO N
MODE L
MARKO V CHAI N
DYNAMI C BELIE F
NETWOR K
PROBABILISTI C
PROJECTIO NWe now conside r the evolutio n of the state of the environmen t over time , and how this can be
represente d in a dynami c belie f network . As we said earlier , the evolutio n of the environmen t
is modelle d by the conditiona l probabilit y distributio n P(X , X,_|,A,_|) , whic h describe s how
the state depend s on the previou s state and the actio n of the agent . As with the senso r model ,
we mak e a stationarit y assumption : the conditiona l probabilitie s are the sam e for all t. In this
section , we will cove r the case wher e the agen t is passivel y monitorin g and predictin g a changin g
environment , rathe r than actin g on it. The agen t is thus concerne d with a sequenc e of X, values ,
wher e each one is determine d solel y by the previou s one: P(X , X,_i) . Thi s sequenc e is calle d
a state evolution  mode l or Marko v chain . Monitorin g and predictio n is importan t in its own
right , and it also make s the explanatio n simpler . In the next section , we will show how an agen t
can use the X, to mak e decision s and take action .
In principle , we wan t to buil d a belie f networ k with one nod e for each state and senso r
variable , for each time step . A networ k of this kind is calle d a dynami c belie f networ k (DBN) .
The generi c structur e of a DBN is show n in Figur e 17.13 . (In a real networ k for a specifi c
problem , the state and percep t node s woul d be replace d by severa l node s each , with appropriat e
connections . Notic e also the resemblanc e to Figur e 7.3.) If t is the curren t time step , then we
have have evidenc e for the percep t node s up to and includin g time t. The task of the networ k is
then to calculat e the probabilit y distributio n for the state at time t. One may also wan t to know
how the state will evolv e into the future—th e probabilit y distribution s for State,+i  and so on.
This task is calle d probabilisti c projection . Bot h task s can be carrie d out usin g the standar d
algorithm s from Chapte r 15.
A littl e though t reveal s that althoug h the previou s sentenc e is true , it may not be very
useful . A dynami c belie f networ k of the kind show n in Figur e 17.1 3 coul d be extremel y large ,
so the belie f net algorithm s coul d be extremel y inefficient . Now we will see the benefi t of all
the wor k that wen t into Equation s (17.8 ) and (17.9)  (for Bel and Bel).  We can implemen t the
predictio n and estimatio n phase s as operation s on the belie f network . Furthermore , we need only
Sectio n 17.5 . Dynami c Belie f Network s 515
SLICE S
ROLLU PSTAT E EVOLUTIO N MODE L
j­"­"""——~~——"""­­• ^
state­1 ")4­r ( State.t+ 1 >­j­k ( State.t+ 2
Percept.t+1 ) (Percept.t+ 2
SENSO R MODE L
Figur e 17.1 3 Th e generi c structur e of a dynami c belie f network . The shade d node s show the
evidenc e that has been accumulate d so far.
keep enoug h networ k structur e to represen t two time steps (otherwis e know n as two slice s of the
network) . Figur e 17.1 4 show s the prediction­estimatio n proces s in operation , a trul y beautifu l
thing . Each cycl e of the proces s work s as follows :
<) Prediction : we begi n with a two­slic e network ; let us call the slice s t — I and /. We assum e
that we have alread y calculate d Bel(X,_  \), incorporatin g all evidenc e up to and includin g
Er_j. Notic e that slice ?—1 has no connection s to previou s slices . The state variable s in
t—\ hav e prio r probabilitie s associate d with them (see the nex t step) . We then calculat e
the belie f vecto r Bel(X t), accordin g to Equatio n (17.8) . This is actuall y the standar d belie f
networ k updatin g proces s applie d to evidenc e E,_ i.
<) Rollup : now we remove  slice t — 1. This require s addin g a prior probabilit y table for the
state variable s at time /. This prio r is just Bel(X r).
<0> Estimation : now we add the new percep t Er, applyin g standar d belie f networ k updatin g to
calculat e Bel(X,),  the probabilit y distributio n over the curren t state . We then add the slice
for t + 1. The networ k is now read y for the next cycle .
This proces s implement s the forma l algorith m specifie d in Figur e 17.9 , usin g the belie f networ k
inferenc e machiner y for all the calculations . Notic e that, as in the forma l algorithm , the percep t
histor y is summarize d in the belie f vecto r for the curren t state— a summarizatio n justifie d by
Equatio n (17.5) .
Probabilisti c projectio n is also straightforward . We can take the networ k after step (c), add
slices for futur e times , and appl y a belie f networ k inferenc e algorith m to calculat e the posterio r
probabilit y distribution s for the futur e states , give n the curren t percept . Unlik e the updat e cycle ,
this migh t be expensiv e becaus e it involve s inferenc e in a temporall y extende d network . However ,
this networ k has a specia l property : non e of the futur e node s has any evidenc e associate d with it.
This mean s that a simpl e stochasti c simulatio n techniqu e such as logi c samplin g (see page 455)
will wor k well , becaus e ever y run can be consisten t with the evidence . Give n a desire d accurac y
level for the samplin g process , the time complexit y will usuall y be O(n).
As an exampl e of the applicatio n of dynami c belie f networks , conside r agai n the sensor ­
failur e mode l show n in Figur e 17.12 . We can exten d this into a DBN (Figur e 17.15 ) by addin g
state evolutio n model s for state variable s Weather, Terrain  and SensorFailure,  as wel l as for the
516 Chapte r 17 . Makin g Comple x Decision s
(a) Predictio nState.t­ 1 State. !
(b) Rollu p
(c) Estimatio nPercept. ! j
State. t
SP
( Percep!. ! J
C Slale. l /­rr ( State.t+ 1
Figur e 17.1 4 Th e steps in updatin g a dynami c belie f networ k over time . Eac h step is describe d
in detai l in the text.
principa l state variabl e LanePosition.  The mode l of interes t here is that for SensorFailure.  The
mode l is quite simple : basically , once a senso r has broken , it usuall y stays broken . Wha t happen s
over time is that as the senso r continue s to send nonsens e signals , it become s mor e and more
likely that they are incorrect . Thi s is especiall y true if there are othe r sensor s throug h whic h
the networ k can infe r LanePosition  indirectly . It will even work , however , just usin g the state
evolutio n mode l for LanePosition,  whic h will usuall y put limit s on how muc h latera l motio n we
can expec t for a vehicle .
17.6 DYNAMI C DECISIO N NETWORK S
NE™ORKSECISIO N A1 1 we nee d in orde r to conver t dynami c belie f network s into dynami c decisio n network s
(DDNs ) is to add utilit y node s and decisio n node s for actions . Figur e 17.1 6 show s the generi c
structur e of a DDN for a sequentia l decisio n proble m wher e the termina l state s are three steps
ahead . The decisio n proble m involve s calculatin g the valu e of D, that maximize s the agent' s
expecte d utilit y over the remainin g state sequence.5 In additio n to the decisio n node s for the.
5 Usually , the fina l utilit y will be calculate d as a sum of expecte d reward s R, + R,+\ .... We omi t the rewar d node s IB |
order to simplif y the diagram .
Sectio n 17.6 . Dynami c Decisio n Network s 517
Figur e 17.15 A  two­slic e fragmen t of a dynami c belie f networ k for continuou s monitorin g of
the lane positionin g of an automate d vehicle . Evidenc e variable s are shaded .
curren t and futur e time steps , notic e that the networ k also contain s the previou s decision , Dt­ \,
as an evidenc e node . It is treate d as evidenc e becaus e it has alread y happened .
The evaluatio n algorith m for DDN s is essentiall y the sam e as that for ordinar y decisio n
networks . In the wors t case , the DDN calculate s the expecte d utilit y of each decisio n sequenc e
by fixin g the decisio n node s and applyin g probabilisti c inferenc e to calculat e the fina l state . As
in our discussio n of sequentia l decisio n problem s earlie r in the chapter , we mus t also be carefu l
to take into accoun t the fact that, for each futur e decision , the agen t does not currentl y know wha t
informatio n will be availabl e at the time the futur e decisio n is made . Tha t is, for decisio n D, +(,
the agen t will have availabl e percept s E,+i , ..., E, +i; but currently , it does not know wha t thos e
percept s will be. For example , an autonomou s vehicl e migh t be contemplatin g a lane chang e at
time t + i, but it will not know unti l then if there is anothe r car blockin g its path .
In our earlie r discussion , we handle d this by iterativel y computin g a polic y that associate s
a decisio n with each state.  Wit h DDNs , we do not have this optio n becaus e the state s are
represente d implicitl y by the set of state variables . Furthermore , in inaccessibl e environments ,
the agen t will not know wha t state it is in anyway . Wha t we mus t do instea d is conside r each
possibl e instantiatio n of the futur e senso r variable s as well as each possibl e instantiatio n of the
futur e decisio n variables . The expecte d utilit y of each decisio n sequenc e is then the weighte d
sum of the utilitie s compute d usin g each possibl e percep t sequence , wher e the weigh t is the
probabilit y of the percep t sequenc e give n the decisio n sequence . Thus , the DD N provide s
approximat e solution s for partiall y observabl e Marko v decisio n problems , wher e the degre e of
approximatio n depend s on the amoun t of lookahead .
The precedin g paragrap h boil s dow n to this: in evaluatin g an action , one mus t conside r
not only its effec t on the environment , but also its effec t on the interna l state of the agen t via the
percept s it generate s (see also Sectio n 16.6) . In still plaine r terms : such consideration s allow the
agen t to see the valu e of (actively ) lookin g befor e leaping , to hun t for lost keys , and so on.
518 Chapte r 17 . Makin g Comple x Decision s
Figur e 17.1 6 Th e generi c structur e of a dynami c decisio n network , showin g a sequentia l
decisio n proble m with three steps . Evidenc e variable s are shaded .
Just as we used a limite d horizo n in gam e playin g and with valu e iteratio n and polic y
iteration , we can limi t the exten t of forwar d projectio n in the DON in orde r to reduc e complexity .
This, combine d with a heuristi c estimat e for the utilit y of the remainin g steps , can provid e
a reasonabl e approximatio n to rationa l action . Ther e are man y othe r possibl e approximatio n '.
techniques , such as usin g less detaile d state variable s for state s in the distan t future ; usin g a
greed y heuristi c searc h throug h the spac e of decisio n sequences ; assumin g "mos t likely " value s
for futur e percept  sequence s rathe r than considerin g all possibl e values ; and so on. Ther e remai n
many possibl e technique s for DON evaluatio n that are as yet unexplored .
Discussio n
All in all, the DON promise s potentia l solution s to man y of the problem s that arise as AI system s •;
are move d from static , accessible , and abov e all simple  environment s to dynamic , inaccessible ,
comple x environment s that are close r to the real world .
• The y can handl e uncertaint y correctly , and sometime s efficiently .
• The y deal with continuou s stream s of senso r input .
• The y can handl e unexpecte d event s becaus e they have no fixed "plan. "
• The y can handl e nois y sensor s and senso r failure .
• The y can act in orde r to obtai n relevan t information .
• The y can handl e relativel y large state space s becaus e they decompos e the state into a set,
of state variable s with spars e connections .
• The y exhibi t "gracefu l degradation " unde r time pressur e and in comple x environments ,
using variou s approximatio n techniques .
Wha t is missing ? The first , and probabl y the mos t important , defec t of DDN s is that they retain !
the propert y of forwar d searc h throug h concret e state s that is typica l of the searc h algorithms !
studie d in Part II. In Part IV, we explaine d how the abilit y to conside r partiall y ordered, |
abstrac t plan s usin g goal­directe d searc h provide d a massiv e increas e in problem­solvin g power» |
Sectio n 17.7 . Summar y 51 9
particularl y whe n combine d with plan libraries . At present , we do not reall y kno w how to
exten d thes e method s into the probabilisti c domain . A second , relate d proble m is the basicall y
prepositiona l natur e of our probabilisti c language . It is impossible , within  the languag e of
probabilit y theory , to state properl y belief s such as "If any car hits a lamp post goin g over 30
mph, the occupant s of the car will be injure d with probabilit y 0.6," becaus e probabilit y theor y
has no quantifier s ("an y car" ) and no function s ("occupant s of the car") . Wha t this mean s
in practic e is that som e of wha t goes on in DBN s and DDN s is that program s (rathe r than
pure probabilisti c inferences ) are responsibl e for choosin g whic h rando m variable s to instantiat e
and for fillin g in thei r conditiona l probabilit y tables . If we had an appropriat e combinatio n of
first­orde r logic with probability , man y of these difficultie s coul d be addresse d withi n a well ­
understoo d reasonin g system.  Wor k on such a languag e is one of the mos t importan t topic s in
knowledg e representatio n research , and som e progres s has been mad e recentl y (Bacchus , 1990 ;
Bacchu s etal,  1992) .
Overall , the potentia l payof f of combinin g DDN­lik e technique s with plannin g method s is
enormous . The technica l and mathematica l problem s involve d in gettin g it righ t are difficult , but
it is an importan t area of curren t research .
17.7 SUMMAR Y
This chapte r show s how to use knowledg e abou t the worl d to mak e decision s even whe n the
outcome s of an actio n are uncertai n and the payoff s will not be reape d unti l severa l (or many )
action s have passed . The main point s are as follows :
• Sequentia l decisio n problem s in uncertai n environment s can be solve d by calculatin g a
polic y that associate s an optima l decisio n with ever y state that the agen t migh t reach .
• Valu e iteratio n and polic y iteratio n are two method s for calculatin g optima l policies .
Both are closel y relate d to the genera l computationa l techniqu e of dynami c programming .
• Slightl y more comple x method s are neede d to handl e the case wher e the lengt h of the actio n
sequenc e is unbounded . We briefl y discusse d the use of syste m gain and discounting .
• State­base d method s for sequentia l decisio n problem s do not scal e well to larg e state
spaces . Heuristi c technique s usin g best­firs t searc h and a limite d horizo n seem to mitigat e
this to som e extent , but suffe r from loca l minima .
• Decomposin g the state into a set of state variable s provide s a significan t advantage . It also
simplifie s the handlin g of inaccessibl e environments .
• We derive d a simpl e updatin g cycl e for a decision­theoreti c agent , usin g a set of Marko v
assumptions .
• We showe d how dynami c belie f network s can handl e sensin g and updatin g over time , and
provid e a direc t implementatio n of the updat e cycle .
• We showe d how dynami c decisio n network s can solv e sequentia l decisio n problems ,
handlin g man y (but not all) of the issue s arisin g for agent s in complex , uncertai n domains .
520 Chapte r 17 . Makin g Comple x Decision s
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
Richar d Bellma n (1957 ) initiate d the moder n approac h to sequentia l decisio n problems , and
propose d the dynami c programmin g approac h in genera l and the valu e iteratio n algorith m in
particular . A remarkabl e Ph.D . thesi s by Ron Howar d (1960 ) introduce d polic y iteratio n and
the idea of syste m gain for solvin g infinite­horizo n problems . Severa l additiona l result s were
introduce d by Bellma n and Dreyfu s (1962) . The analysi s of discountin g in term s of stationar y
preference s is due to Koopman s (1972) . Bertseka s (1987 ) provide s an authoritativ e moder n text
on dynami c programming , whic h has becom e one of the mos t widel y used tools for searc h and
optimizatio n problems .
The observatio n that partiall y observabl e Marko v decisio n problem s can be transforme d
into a regula r Marko v decisio n proble m usin g the belie f state s is due to Astro m (1965) . The
first complet e algorith m for exac t solutio n of partially­observabl e Marko v decisio n problem s
(POMDPs ) was propose d by Edwar d Sondi k (1971 ) in his Ph.D . thesis . (A later journa l pape r
by Smallwoo d and Sondi k (1973 ) contain s som e error s but is mor e accessible. ) Lovejo y (1991 )
survey s the state of the art in POMDPs . In AI, Cassandr a el al. (1994 ) have investigate d the
applicatio n of POMD P algorithm s to plannin g problems .
Severa l recen t paper s have attempte d to combin e dynami c programmin g algorithm s such
as polic y iteratio n with plannin g and searc h model s from AI (Dea n et al., 1993;Tas h and Russell ,
1994) . Thi s line of wor k involve s approximatin g a Marko v decisio n proble m usin g a limite d
horizo n and abstrac t states , in an effor t to overcom e the combinatoric s of large state spaces .
Heuristic s base d on the valu e of informatio n can be used to selec t area s of the state spac e wher e
a loca l expansio n of the horizo n will yield a significan t improvemen t in decisio n quality . Agent s
using this approac h can tailo r thei r effor t to handl e time pressure , and generat e som e interestin g
behavior s such as usin g familia r "beate n paths " to find their way aroun d the state spac e quickl y
withou t havin g to recomput e optima l decision s at each point .
Many of the basi c idea s for estimatin g the state of dynamica l system s cam e from the
mathematicia n C. F. Gaus s (1809) . The prediction­estimatio n cycle for monitorin g environment s
unde r uncertaint y was propose d by Kalma n (Kalman , 1960) , buildin g on classifie d wartim e
researc h by Wiene r (1942 ) and Kolmogoro v (1941) . Kalma n filtering , whic h Kalma n derive d
for linea r system s with Gaussia n noise , has sinc e becom e an industr y in itsel f (Gelb , 1974 ;
Bar­Shalo m and Fortmann , 1988) . Leonar d and Durrant­Whyt e (1992 ) describ e probabilisti c
senso r model s in detail , with particula r attentio n to the modellin g of sona r sensors .
Dynami c belie f network s (DBNs ) can be viewe d as a spars e encodin g of a Marko v process ,
and were first used in AI by Dean and Kanazaw a (1989) , Nicholso n (1992) , and Kjaerulf f (1992) .
The last work include s a generi c extensio n to the HUGI N belie f net syste m to provid e the necessar y
facilitie s for dynami c belie f networ k generatio n and compilation . The developmen t give n in this
chapte r owes  a good deal to the book by Dea n and Wellma n (1991) , whic h provide s extensiv e
discussio n of the use of DBN s and DDN s (dynami c decisio n networks ) in mobil e robots.  Huan g
et al. (1994 ) describ e an applicatio n of DBN s to the analysi s of freewa y traffi c usin g compute r
vision . A notatio n and an evaluatio n algorith m for additiv e DDN s are provide d by Tatma n and
Shachter(1990) .
Sectio n 17.7. Summar y 521
EXERCISE S
L17.1 Fo r the stochasti c versio n of the worl d show n in Figur e 17.1 , calculat e whic h square s can
be reache d by the actio n sequenc e [Up,Right],  and with wha t probabilities .
17.2 Fo r a specifi c environmen t (whic h you can mak e up), construc t a utilit y functio n on
historie s that is not separable . Explai n how the concep t of utilit y on state s fails in this case .
17.3 Conside r the stochasti c versio n of the environmen t show n in Figur e 17.1 .
a. Implemen t an environmen t simulato r for this environment , such that the specifi c geograph y
of the environmen t is easil y altered .
b. Creat e a SlMPLE­PoLiCY­AGEN T that uses polic y iteration , and measur e its performanc e in
the environmen t simulato r from variou s startin g states .
c. Experimen t with increasin g the size of the environment . How does the executio n time per
actio n vary with the size of the environment ?
d. Analyz e the polic y iteratio n algorith m to find its worst­cas e complexity , assumin g that
value determinatio n is done usin g a standar d equation­solvin g method .
e. Doe s valu e iteratio n terminat e if the utilit y value s are require d to converg e exactly?
17.4 Fo r the environmen t show n in Figur e 17.1 , find all the threshol d value s for the cost of a
step, such that the optima l polic y change s whe n the threshol d is crossed .
17.5 Prov e that the calculation s in the predictio n and estimatio n phase s of the basi c decisio n
cycle (Equation s (17.8 ) and (17.9) ) do in fact yiel d the correc t valu e for5e/(X,) , give n assump ­
tions (17.5) , (17.6) , and (17.7) .
17.6 In this exercise , we will conside r part of the proble m of buildin g a robo t that play s
Ping­Pong .
One of the thing s it will have to do is find out wher e the ball is and estimat e its trajectory . Let
us suppos e for a momen t that we have a visio n syste m that can retur n an estimate d instantaneou s
(x,y,z)  positio n for the ball. This positio n estimat e for time f, will be used as an evidenc e node
Oi in a belie f networ k that infer s the desire d information . The idea is that node s Xj and V\ in
the networ k represen t the ball' s instantaneou s positio n and velocity . The trajector y of the ball is
followe d by havin g multipl e copie s of these nodes , one for each time step . In additio n to these
nodes , we also have node s for the instantaneou s frictio n forc e F­, actin g on the ball due to air
resistance , whic h depend s only on the curren t velocity . The time s t\, t2, t^,...  can be assume d to
be separate d by som e smal l interva l fit.
a. Whic h of the belie f network s in Figur e 17.1 7 correctl y (but not necessaril y efficiently ) repre ­
sent the precedin g information ? (You may assum e that second­orde r effects—proportiona l
to 6t2—are ignored. )
b. Whic h is the best network ?
c. Whic h network s can be solve d by loca l propagatio n algorithms ?
522 Chapte r 17 . Makin g Comple x Decision s
©
(a) (b) (c)
Figur e 17.1 7 Som e propose d network s for Ping­Pon g tracking .
d. The node s in the network s show n in the figur e refer to past and presen t times . Explai n how
the network s can be extende d and used to predic t the positio n at any futur e time .
e. Now conside r the visio n subsystem , whos e job it is to provid e observatio n evidence . Its
inpu t is an arra y of intensit y value s from the camer a image . Assumin g that the ball is
white , wherea s mos t of the room is darker , explai n briefl y how to calculat e the positio n of
the ball give n the array .
f. Doe s it mak e more sens e to calculat e the positio n in coordinate s relativ e to the robo t or
relativ e to the room ? Why ?
Part VI
LEARNIN G
So far we have assume d that all the "intelligence " in an agen t has been built in by
the agent' s designer . The agen t is then let loos e in an environment , and does the
best it can give n the way it was programme d to act. But this is not necessaril y
the best approach—fo r the agen t or the designer . Wheneve r the designe r has
incomplet e knowledg e of the environmen t that the agen t will live in, learnin g is
the only way that the agen t can acquir e wha t it need s to know.  Learnin g thus
provide s autonom y in the sens e define d in Chapte r 1. It also provide s a good
way to build high­performanc e systems—b y givin g a learnin g syste m experienc e
in the applicatio n domain .
The four chapter s in this part cove r the field of machin e learning —the
subfiel d of AI concerne d with program s that learn from experience . Chapte r 18
introduce s the basic desig n for learnin g agents , and addresse s the genera l proble m
of learnin g from examples . Chapte r 19 discusse s the proces s of learnin g in neura l
networks—collection s of simpl e nonlinea r processin g elements—an d in belie f
networks . In Chapte r 20, we tackl e the genera l proble m of improvin g the behavio r
of an agen t given some feedbac k as to its performance . Finally , Chapte r 21 show s
how learnin g can be improve d by usin g prio r knowledge .
18LEARNIN G FRO M
OBSERVATION S
In which we  describe  agents  that can improve  their  behavior  through  diligent  study
of their  own  experiences.
The idea behin d learnin g is that percept s shoul d be used not only for acting , but also for improvin g
the agent' s abilit y to act in the future . Learnin g takes place as a resul t of the interactio n betwee n
the agen t and the world , and from observatio n by the agen t of its own decision­makin g processes .
Learnin g can rang e from trivia l memorizatio n of experience , as exhibite d by the wumpu s agen t
in Chapte r 7, to the creatio n of entir e scientifi c theories , as exhibite d by Alber t Einstein . Thi s
chapte r start s with the desig n of genera l learnin g agents , and describe s inductiv e learning —
constructin g a descriptio n of a functio n from a set of input/outpu t examples . We then give severa l
algorithm s for inductiv e learnin g in logica l agent s and a theoretica l analysi s that explain s why
learnin g works .
18.1 A GENERAL MODEL OF LEARNING AGENTS
LEARNIN G ELEMEN TA learnin g agen t can be divide d into four conceptua l components , as show n in Figur e 18.1 . The
most importan t distinctio n is betwee n the learnin g element , whic h is responsibl e for makin g
improvements , and the performanc e element , whic h is responsibl e for selectin g externa l actions .
The performanc e elemen t is wha t we have previousl y considere d to be the entir e agent : it takes in
percept s and decide s on actions . The learnin g elemen t take s som e knowledg e abou t the learnin g
elemen t and som e feedbac k on how the agen t is doing , and determine s how the performanc e
elemen t shoul d be modifie d to (hopefully ) do bette r in the future . The desig n of the learnin g
elemen t depend s very muc h on the desig n of the performanc e element . Whe n tryin g to desig n an
agent that learn s a certai n capability , the first questio n is not "How am I goin g to get it to learn
this?" but "Wha t kind of performanc e elemen t will my agen t need to do this once it has learne d
how? " For example , the learnin g algorithm s for producin g rule s for logica l system s are quit e
differen t from the learnin g algorithm s for producin g belie f networks . We will see, however , that
the principles  behin d the learnin g algorithm s are muc h the same .
525
526 Chapte r 18 . Learnin g from Observation s
Performanc e standar d
Figur e 18.1 A  genera l mode l of learnin g agents .
CRITI C
PROBLE M
GENERATO RThe criti c is designe d to tell the learnin g elemen t how well the agen t is doing . The critic
employ s a fixe d standar d of performance . Thi s is necessar y becaus e the percept s themselve s
provid e no indicatio n of the agent' s success . For example , a ches s progra m may receiv e a percep t
indicatin g that it has checkmate d its opponent , but it need s a performanc e standar d to know that
this is a good thing ; the percep t itsel f does not say so. It is importan t that the performanc e
standar d is a fixe d measur e that is conceptuall y outsid e the agent ; otherwis e the agen t coul d
adjus t its performanc e standard s to mee t its behavior . In humans , this form of irrationalit y is
called "sou r grapes " and is characterize d by comment s such as "Oh well , neve r mind , I didn' t
want that stupi d Nobe l prize anyway. "
The last componen t of the learnin g agen t is the proble m generator . It is responsibl e for
suggestin g action s that will lead to new and informativ e experiences . The poin t is that if the
performanc e elemen t had its way , it woul d keep doin g the action s that are best , give n wha t it
knows . But if the agen t is willin g to explor e a little , and do some perhap s suboptima l action s in the
short run, it migh t discove r muc h bette r action s for the long run. The proble m generator' s job is
to sugges t these explorator y actions . This is wha t scientist s do whe n they carry out experiments .
Galile o did not thin k that droppin g rock s from the top of a towe r in Pisa was valuabl e in itself ;
he was not tryin g to brea k the rocks , nor to rende r pedestrian s unconscious . His aim was to
demonstrat e a bette r theor y of the motio n of objects .
To mak e the overal l desig n more concrete , let us retur n to the automate d taxi example . The
performanc e elemen t consist s of whateve r collectio n of knowledg e and procedure s the taxi has
for selectin g its drivin g action s (turning , accelerating , braking , honking , and so on). The taxi goes
out on the road and drives , usin g this performanc e element . The learnin g elemen t formulate s
goals , for example , to lear n bette r rule s describin g the effect s of brakin g and accelerating , to
learn the geograph y of the area , to lear n how the taxi behave s on wet roads , and to learn what
Sectio n 18.1 . A  Genera l Mode l of Learnin g Agent s 527
cause s annoyanc e to othe r drivers . The criti c observe s the worl d and passe s informatio n alon g
to the learnin g element . For example , after the taxi make s a quic k left turn acros s three lane s of
traffic , the criti c observe s the shockin g languag e used by othe r drivers , the learnin g elemen t is
able to formulat e a rule sayin g this was a bad action , and the performanc e elemen t is modifie d
by installin g the new rule . Occasionally , the proble m generato r kick s in with a suggestion : try
takin g 7th Avenu e uptow n this time , and see if it is faste r than the norma l route .
The learnin g elemen t is also responsibl e for improvin g the efficiency  of the performanc e
element . For example , whe n aske d to mak e a trip to a new destination , the taxi migh t take a
whil e to consul t its map and plan the best route . But the next time a simila r trip is requested , the
SPEEDU P LEARNIN G plannin g proces s shoul d be muc h faster . Thi s is calle d speedu p learning , and is deal t with in
Chapte r 21. In this chapter , we concentrat e on the acquisitio n of knowledge .
Machin e learnin g researcher s have com e up with a large variet y of learnin g elements . To
understan d them , it will help to see how their desig n is affecte d by the contex t in whic h they will
operate . The desig n of the learnin g elemen t is affecte d by four majo r issues :
• Whic h components  of the performanc e elemen t are to be improved .
• Wha t representation  is used for thos e components .
• What  feedback  is available .
• Wha t prior information  is available .
Component s of the performanc e elemen t
We have seen that ther e are man y way s to buil d the performanc e elemen t of an agent . Th e
component s can includ e the following :
1. A direc t mappin g from condition s on the curren t state to actions .
2. A mean s to infe r relevan t propertie s of the worl d from the percep t sequence .
3. Informatio n abou t the way the worl d evolves .
4. Informatio n abou t the result s of possibl e action s the agen t can take.
5. Utilit y informatio n indicatin g the desirabilit y of worl d states .
6. Action­valu e informatio n indicatin g the desirabilit y of particula r action s in particula r states .
7. Goal s that describ e classe s of state s whos e achievemen t maximize s the agent' s utility .
Each of the component s can be learned , give n the appropriat e feedback . For example , if the
agent does an actio n and then perceive s the resultin g state of the environment , this informatio n
can be used to learn a descriptio n of the result s of action s (4). If the taxi exert s a certai n brakin g
pressur e whe n drivin g on a wet road , then it will soon find out how muc h actua l deceleratio n is
achieved . Similarly , if the criti c can use the performanc e standar d to deduc e utilit y value s from
the percepts , then the agen t can lear n a usefu l representatio n of its utilit y functio n (5). If the
taxi receive s no tips from passenger s who have been thoroughl y shake n up durin g the trip, it can
learn a usefu l componen t of its overal l utilit y function . In a sense , the performanc e standar d can
be seen as definin g a set of distinguished  percepts  that will be interprete d as providin g direc t
feedbac k on the qualit y of the agent' s behavior . Hardwire d performanc e standard s such as pain
and hunge r in animal s can be understoo d in this way .
528 Chapte r 18 . Learnin g from Observation s
Representatio n of the component s
Any of these component s can be represente d usin g any of the representatio n scheme s in this book .
We have seen severa l examples : deterministi c description s such as linea r weighte d polynomial s
for utilit y function s in game­playin g program s and propositiona l and first­orde r logica l sentence s
for all of the component s in a logica l agent ; and probabilisti c description s such as belie f network s
for the inferentia l component s of a decision­theoreti c agent . Effectiv e learnin g algorithm s have
been devise d for all of these . The detail s of the learnin g algorith m will be differen t for each
representation , but the main idea remain s the same .
SUPERVISED
LEARNING
REINFORCEMEN T
LEARNING
REINFORCEMEN T
UNSUPERVISE D
LEARNINGAvailabl e feedbac k
For som e components , such as the componen t for predictin g the outcom e of an action , the
availabl e feedbac k generall y tells the agen t wha t the correc t outcom e is. Tha t is, the agen t
predict s that a certai n actio n (braking ) will have a certai n outcom e (stoppin g in 10 feet) , and the
environmen t immediatel y provide s a percep t that describe s the actua l correc t outcom e (stoppin g
in 15 feet) . Any situatio n in whic h both the input s and output s of a componen t can be perceive d
is calle d supervise d learning . (Often , the output s are provide d by a friendl y teacher. )
On the othe r hand , in learnin g the condition­actio n component , the agen t receive s som e
evaluatio n of its actio n (suc h as a hefty bill for rear­endin g the car in front ) but is not told the
correc t actio n (to brak e mor e gentl y and muc h earlier) . This is calle d reinforcemen t learning ;
the hefty bill is calle d a reinforcement.1 The subjec t is covere d in Chapte r 20.2
Learnin g whe n ther e is no hint at all abou t the correc t output s is calle d unsupervise d
learning . An unsupervise d learne r can alway s lear n relationship s amon g its percept s usin g
supervise d learnin g methods—tha t is, it can learn to predic t its futur e percept s give n its previou s
percepts . It canno t learn what  to do unles s it alread y has a utilit y function .
Prior knowledg e
The majorit y of learnin g researc h in AI, compute r science , and psycholog y has studie d the case
in whic h the agen t begin s with no knowledg e at all abou t wha t it is tryin g to learn . It only has
acces s to the example s presente d by its experience . Althoug h this is an importan t specia l case , it
is by no mean s the genera l case. Mos t huma n learnin g takes place in the contex t of a good deal
of backgroun d knowledge . Som e psychologist s and linguist s claim that even newbor n babie s
exhibi t knowledg e of the world . Whateve r the truth of this claim , ther e is no doub t that prior
knowledg e can help enormousl y in learning . A physicis t examinin g a stack of bubble­chambe r
photograph s may be able to induc e a theor y positin g the existenc e of a new particl e of a certai n
mass and charge ; but an art critic examinin g the sam e stac k migh t learn nothin g more than that
the "artist " mus t be som e sort of abstrac t expressionist . In Chapte r 21, we see severa l way s in
whic h learnin g is helpe d by the use of existin g knowledge .
1 Th e term s rewar d and punishmen t are also used as synonym s for reinforcement .
2 Drawin g the line betwee n supervise d and reinforcemen t learnin g is somewha t arbitrary ; reinforcemen t learnin g can
also be though t of as supervise d learnin g with a less informativ e feedbac k signal .
Sectio n 18.2 . Inductiv e Learnin g 529
Bringin g it all togethe r
Each of the seve n component s of the performanc e elemen t can be describe d mathematicall y as
a function : for example , informatio n abou t the way the worl d evolve s can be describe d as a
functio n from a worl d state (the curren t state ) to a worl d state (the next state or states) ; a goal can
be describe d as a functio n from a state to a Boolea n valu e (0 or 1) indicatin g whethe r the state
satisfie s the goal . The key poin t is that all learning can be  seen  as learning the representation  of
a function.  We can choos e whic h componen t of the performanc e elemen t to improv e and how it
is to be represented . The availabl e feedbac k may be more or less useful , and we may or may not
have any prior knowledge . The underlyin g proble m remain s the same .
18.2 INDUCTIV E LEARNIN G
EXAMPL E
PURE INDUCTIV E
INFERENC E
HYPOTHESI S
BIAS
INCREMENTA L
LEARNIN GIn supervise d learning , the learnin g elemen t is give n the correc t (or approximatel y correct ) valu e
of the functio n for particula r inputs , and changes  its representatio n of the functio n to try to matc h
the informatio n provide d by the feedback . Mor e formally , we say an exampl e is a pair (x,f(x)),
wher e x is the inpu t and/(jt ) is the outpu t of the functio n applie d to x. The task of pure inductiv e
inferenc e (or induction ) is this : give n a collectio n of example s of/, retur n a functio n h that
approximates/ . The functio n h is calle d a hypothesis .
Figur e 18.2 show s an exampl e of this from plan e geometry . The example s in Figur e 18.2(a )
are (x,y)  point s in the plane , wher e y = f(x), and the task is to find a functio n h(x) that fits the
point s well . In Figur e 18.2(b ) we have a piecewise­linea r h function , whil e in Figur e 18.2(c ) we
have a mor e complicate d h function . Bot h function s agre e with the exampl e points , but diffe r
on the >> value s they assig n to othe r x inputs . In (d) we have a functio n that apparentl y ignore s
one of the exampl e points , but fits the other s with a simpl e function . The true / is unknown ,
so ther e are man y choice s for h, but withou t furthe r knowledge , we have no way to prefe r (b),
(c), or (d). Any preferenc e for one hypothesi s over another , beyon d mere consistenc y with the
examples , is calle d a bias. Becaus e there are almos t alway s a large numbe r of possibl e consisten t
hypotheses , all learnin g algorithm s exhibi t som e sort of bias. We will see man y example s in this
and subsequen t chapters .
To get back to agents , suppos e we have a refle x agent3 that is bein g taugh t by a teacher .
Figur e 18.3 show s that the REFLEX­LEARNING­ELEMEN T update s a globa l variable , examples,
that hold s a list of (percept,  action)  pairs . Th e percep t coul d be a ches s boar d position ,
and the actio n coul d be the best mov e as determine d by a helpfu l grandmaster . Whe n the
REFLEX­PERFORMANCE­ELEMEN T is faced with a percep t it has been told about , it choose s the
correspondin g action . Otherwise , it calls a learnin g algorith m INDUC E on the example s it has
seen so far. INDUC E return s a hypothesi s h whic h the agen t uses to choos e an action .
There are man y variant s on this simpl e scheme . For example , the agen t coul d perfor m
incrementa l learning : rathe r than applyin g the learnin g algorith m to the entir e set of example s
each time a new predictio n is needed , the agen t coul d just try to updat e its old hypothesi s wheneve r
Recal l that refle x agent s map directl y from percept s to actions .
530 Chapte r 18 . Learnin g from Observation s
(a) (b) (c) (d)
Figur e 18.2 In (a) we have som e exampl e (input,output)  pairs . In (b), (c), and (d) we have
three hypothese s for function s from whic h these example s coul d be drawn .
globa l examples  — {}
functio n REFLEX­PERFORMANCE­ELEMENT(percgpf ) return s an actio n
if (percept,  a) in examples  then retur n a
else
h <­ lNDVCE(examples)
retur n h( percept)
procedur e REFEEX­LEARNiNG­ELEMENT(perce/?r , action)
inputs : percept,  feedbac k percep t
action,  feedbac k actio n
examples^­  examples  U {(percept,action)}
Figur e 18.3 Skeleto n for a simpl e refle x learnin g agent . The learnin g elemen t just store s each
exampl e percept/actio n pair. The performanc e elemen t eithe r does whateve r was done last time
for a give n percept , or it induce s an actio n from simila r percepts . The set of examples  is a globa l
variabl e that is share d by the learnin g and performanc e elements .
a new exampl e arrives . Also , the agen t migh t receiv e som e feedbac k concernin g the qualit y of
the action s it chooses . Thes e variants , and man y others , are examine d in this part.
REFLEX­PERFORMANCE­ELEMEN T make s no commitmen t to the way in which the hypoth ­
esis is represented . Becaus e of its expressivenes s and well­understoo d semantics , logic has been
intensivel y studie d as the targe t languag e for learnin g algorithms . In this chapter , we discus s two
approache s to learnin g logica l sentences : decisio n tree methods , whic h use a restricte d represen ­
tation of logica l sentence s specificall y designe d for learning , and the version­spac e approach ,
whic h is mor e genera l but often rathe r inefficient . In Chapte r 19, we discus s neura l networks ,
whic h are a genera l representatio n for nonlinear , numerica l functions . The linea r weighte d poly ­
nomial s used for game­playin g evaluatio n function s are a specia l case of neura l networks . The
Sectio n 18.3 . Learnin g Decisio n Tree s 531
desig n of learnin g algorithm s for belie f network s is also a very activ e area of research , and a
brief sketc h is provide d in Sectio n 19.6 .
The choic e of representation  for the desire d functio n is probabl y the mos t importan t issue
facin g the designe r of a learnin g agent . As well as affectin g the natur e of the learnin g algorithm ,
it can affec t whethe r the proble m is feasibl e at all. As with reasoning , in learnin g there is
a fundamenta l trade­of f betwee n expressiveness —is the desire d functio n representabl e in the
representatio n language—an d efficiency— is the learnin g proble m goin g to be tractabl e for a
given choic e of representatio n language . If one choose s to learn sentence s in a nice, expressiv e
languag e such as first­orde r logic , then one will probabl y have to pay a heav y penalt y in term s of
both computatio n time and the numbe r of example s require d to learn a good set of sentences .
By "a good set of sentences, " we mean a set that not only correctl y reflect s the experience s
the agen t has alread y had, but also one that correctl y predict s its futur e experiences . Therei n
lies one of the mos t vexin g philosophica l problem s of all time . How can one possibl y know that
one's learnin g algorith m has produce d a theor y that will correctl y predic t the future ? And if one
does not, then how can one say that the algorith m is any good ? Certainly , if one canno t say for
sure that an algorith m is any good , then one canno t hope to desig n good learnin g algorithms !
In Sectio n 18.6 , we discus s a mathematica l approac h to the stud y of inductio n algorithm s that
provide s tentativ e answer s to these questions , and also shed s considerabl e light on the complexit y
of learnin g differen t kind s of functio n representations .
18.3 LEARNIN G DECISIO N TREE S
Decisio n tree inductio n is one of the simples t and yet most successfu l form s of learnin g algorithm .
It serve s as a good introductio n to the area of inductiv e learning , and is easy to implement . We
first describ e the performanc e element , and then show how to learn it. Alon g the way , we will
introduc e man y of the idea s and term s that appea r in all area s of inductiv e learning .
Decisio n trees as performanc e element s
DECISIO N TREE A  decisio n tree takes as inpu t an objec t or situatio n describe d by a set of properties , and output s a
yes/n o "decision. " Decisio n trees therefor e represen t Boolea n functions . Function s with a large r
range of output s can also be represented , but for simplicit y we will usuall y stick to the Boolea n
case. Eac h interna l node in the tree correspond s to a test of the valu e of one of the properties ,
and the branche s from the node are labelle d with the possibl e value s of the test. Eac h leaf node
in the tree specifie s the Boolea n valu e to be returne d if that leaf is reached .
As an example , conside r the proble m of whethe r to wait for a table at a restaurant . The aim
GOAL PREDICAT E her e is to learn a definitio n for the goal predicate4 WillWait,  wher e the definitio n is expresse d as a
.4 Th e term goal concep t is ofte n used . Unfortunately , the wor d "concept " has been used in so man y differen t way s in
machin e learnin g that we think it best to avoi d it for a few years .
532 Chapte r 18 . Learnin g from Observation s
decisio n tree.5 In settin g this up as a learnin g problem , we first have to decid e wha t propertie s or
attributes  are availabl e to describ e example s in the domain.6 Suppos e we decid e on the followin g
list of attributes :
1. Alternate:  whethe r there is a suitabl e alternativ e restauran t nearby .
2. Bar.  whethe r the restauran t has a comfortabl e bar area to wait in.
3. Fri/Sat:  true on Friday s and Saturdays .
4. Hungry:  whethe r we are hungry .
5. Patrons:  how man y peopl e are in the restauran t (value s are None,  Some,  and Full).
6. Price:  the restaurant' s price rang e ($, $$, $$$) .
7. Raining:  whethe r it is rainin g outside .
8. Reservation:  whethe r we mad e a reservation .
9. Type:  the kind of restauran t (French , Italian , Thai , or Burger) .
10. WaitEstimate:  the wait estimate d by the host (0­10 minutes , 10­30 , 30­60 , >60) .
The decisio n tree usuall y used by the first autho r for this domai n is show n in Figur e 18.4 . Notic e
that the tree does not use the Price  and Type  attributes , considerin g these to be irrelevan t give n the
data it has seen . Logically , the tree can be expresse d as a conjunctio n of individua l implication s
correspondin g to the path s throug h the tree endin g in Yes nodes . For example , the path for a
restauran t full of patrons , with an estimate d wait of 10­3 0 minute s whe n the agen t is not hungr y
is expresse d by the logica l sentenc e
Vr Patrons(r,Full)f\  WaitEstimate(r,0­\0)  A Hungry(r,N)  => WillWait(r)
Expressivenes s of decisio n tree s
If decisio n trees correspon d to sets of implicatio n sentences , a natura l questio n is whethe r they
can represen t any set. The answe r is no, becaus e decisio n trees are implicitl y limite d to talkin g
abou t a singl e object . Tha t is, the decisio n tree languag e is essentiall y prepositional , with each
attribut e test bein g a proposition . We canno t use decisio n trees to represen t tests that refer to two
or more differen t objects , for example ,
3 ^2 Nearby(r2,  r) A Price(r,p)  A Price(r2,p2)  A Cheaper(p2,p)
(is ther e a cheape r restauran t nearby) . Obviously , we coul d add anothe r Boolea n attribut e with
the nam e CheaperRestaurantNearby,  but it is intractabl e to add all such attributes .
Decisio n trees are full y expressiv e withi n the class of prepositiona l languages , that is, any
Boolea n functio n can be writte n as a decisio n tree. This can be done triviall y by havin g each row
in the truth table for the functio n correspon d to a path in the tree. This woul d not necessaril y be
a good way to represen t the function , becaus e the truth table is exponentiall y large in the numbe r
of attributes . Clearly , decisio n trees can represen t man y function s with muc h smalle r trees .
5 Meanwhile , the automate d taxi is learnin g whethe r to wait for the passenger s in case they give up waitin g for a table
and wan t to go on to anothe r restaurant .
6 On e migh t ask why this isn't the job of the learnin g program . In fact , it is, but we will not be able to explai n how it is
done unti l Chapte r 21.
Sectio n 18.3 . Learnin g Decisio n Tree s 533
For som e kind s of functions , however , this is a real problem . For example , if the functio n
PARIT Y FUNCTIO N i s the parit y function , whic h return s 1 if and only if an even numbe r of input s are 1, then an
exponentiall y large decisio n tree will be needed . It is also difficul t to use a decisio n tree to
MAJORIT Y FUNCTIO N represen t a majorit y function , whic h return s 1 if more than half of its input s are 1.
In othe r words , decisio n trees are good for som e kind s of functions , and bad for others .
Is ther e any kind of representatio n that is efficien t for all kind s of functions ? Unfortunately ,
the answe r is no. We can show this in a very genera l way . Conside r the set of all Boolea n
function s on n attributes . How man y differen t function s are in this set? This is just the numbe r
of differen t truth table s that we can writ e down , becaus e the functio n is define d by its truth table .
The truth table has 2" rows , becaus e each inpu t case is describe d by n attributes . We can conside r
the "answer " colum n of the table as a 2" bit numbe r that define s the function . No matte r wha t
representatio n we use for functions , som e of the function s (almos t all of them , in fact) are goin g
to requir e at least  this man y bits to represent .
If it take s 2" bits to defin e the function , this mean s that there are 22" differen t function s
on n attributes . This is a scary number . For example , with just six Boolea n attributes , there are
abou t 2 x 1019 differen t function s to choos e from . We will need som e ingeniou s algorithm s to
find consisten t hypothese s in such a large space .
Reservation ? Fri/Sat ?llil Alternate ?
mm iii ls^;;;ilj:te,g;» i tK^J  Ksr;?;,  «<<• ­­^L&JIS^S
Figur e 18.4 A  decisio n tree for decidin g whethe r to wait for a table .
534 Chapte r 18 . Learnin g from Observation s
Inducin g decisio n tree s from example s
An exampl e is describe d by the value s of the attribute s and the valu e of the goal predicate . We
CLASSIFICATIO N cal l the valu e of the goal predicat e the classificatio n of the example . If the goal predicat e is
true for som e example , we call it a positiv e example ; otherwis e we call it a negativ e example .
A set of example s X\,...  ,X\2  for the restauran t domai n is show n in Figur e 18.5 . The positiv e
example s are ones wher e the goal WillWait  is true (X\,Xi,,.. .) and negativ e example s are ones
TRAININ G SET wher e it is false (X 2,X5,...). The complet e set of example s is calle d the trainin g set.
X,
X2
Xj
X,
xsxh
X!
X8
X,
X\o
XH
Xl2Attribute s
Alt
Yes
Yes
No
Yes
Yes
No
No
No
No
Yes
No
YesBat­
No
No
Yes
No
No
Yes
Yes
No
Yes
Yes
No
YesFri
No
No
No
Yes
Yes
No
No
No
Yes
Yes
No
YesHun
Yes
Yes
No
Yes
No
Yes
No
Yes
No
Yes
No
YesPat
Some
Full
Some
Full
Full
Some
None
Some
Full
Full
None
FullPrice
$$$
$
$
s
$$$
$$
$
$$
$
$$$
$
sRain
No
No
No
No
No
Yes
Yes
Yes
Yes
No
No
NoRes
Yes
No
No
No
Yes
Yes
No
Yes
No
Yes
No
NoType
French
Thai
Burger
Thai
French
Italian
Burger
Thai
Burger
Italian
Thai
BurgerEst
0­10
30­60
0­10
10­30
>60
0­10
0­10
0­10
>60
10­30
0­10
30­60Goal
WillWait
Yes
No
Yes
Yes
No
Yes
No
Yes
No
No
No
Yes
Figur e 18.5 Example s for the restauran t domain .
The proble m of findin g a decisio n tree that agree s with the trainin g set migh t seem difficult ,
but in fact there is a trivia l solution . We coul d simpl y construc t a decisio n tree that has one path
to a leaf for each example , wher e the path tests each attribut e in turn and follow s the valu e for
the example , and the leaf has the classificatio n of the example . Whe n give n the sam e exampl e
again,7 the decisio n tree will com e up with the righ t classification . Unfortunately , it will not have
much to say abou t any othe r cases !
The proble m with this trivia l tree is that it just memorize s the observations . It does not
extrac t any patter n from the example s and so we canno t expec t it to be able to extrapolat e to
example s it has not seen .
Extractin g a patter n mean s bein g able to describ e a larg e numbe r of case s in a concis e
way. Rathe r than just tryin g to find a decisio n tree that agree s with the examples , we shoul d try
to find a concis e one, too. Thi s is an exampl e of a genera l principl e of inductiv e learnin g ofte n
called Ockham' s razor:8 The  most  likely  hypothesis  is the simplest one that  is consistent  with
all observations.  Som e peopl e interpre t this as meanin g "the worl d is inherentl y simple. " Eve n
if the worl d is complex , however , Ockham' s razo r still make s sense . Ther e are far fewe r simpl e
7 Th e sam e exampl e or an example with  the same  description —this  distinctio n is very importan t and we will retur n to
it in Chapte r 21.
8Sometime s spelle d "Occam, " althoug h the origi n of this corruptio n is obscure .
Sectio n 18.3 . Learnin g Decisio n Tree s 53 5
hypothese s than comple x ones , so that ther e is only a smal l chanc e that any simpl e hypothesi s
that is wildl y incorrec t will be consisten t with all observations . Hence , othe r thing s bein g equal ,
a simpl e hypothesi s that is consisten t with the observation s is mor e likel y to be correc t than a
comple x one. We discus s hypothesi s qualit y furthe r in Sectio n 18.6 .
Unfortunately , findin g the smallest  decisio n tree is an intractabl e problem , but with some
simpl e heuristics , we can do a good job of findin g a smallis h one. The basi c idea behin d the
DECISION­TREE­LEARNIN G algorith m is to test the mos t importan t attribut e first . By "mos t
important, " we mea n the one that make s the mos t differenc e to the classificatio n of an example .
This way , we hope to get to the correc t classificatio n with a smal l numbe r of tests , meanin g that
all path s in the tree will be shor t and the tree as a whol e will be small .
Figur e 18.6 show s how the algorith m gets started . We are give n 12 trainin g examples ,
whic h we classif y into positiv e and negativ e sets. We then decid e whic h attribut e to use as the
first test in the tree. Figur e 18.6(a ) show s that Patrons  is a fairl y importan t attribute , becaus e if the
value is None  or Some,  then we are left with exampl e sets for whic h we can answe r definitivel y
(No and Yes,  respectively) . (If the valu e is Full,  we will need additiona l tests. ) In Figur e 18.6(b )
we see that Type  is a poor attribute , becaus e it leave s us with four possibl e outcomes , each of
whic h has the same numbe r of positiv e and negativ e answers . We conside r all possibl e attribute s
in this way , and choos e the mos t importan t one as the root test. We leav e the detail s of how
importanc e is measure d for Sectio n 18.4 , becaus e it does not affec t the basic algorithm . For now ,
assum e the mos t importan t attribut e is Patrons.
After the first attribut e test split s up the examples , each outcom e is a new decisio n tree
learnin g proble m in itself , with fewe r example s and one fewe r attribute . Ther e are four case s to
conside r for these recursiv e problems :
1. If there are som e positiv e and som e negativ e examples , then choos e the best attribut e to
split them . Figur e 18.6(c ) show s Hungry  bein g used to split the remainin g examples .
2. If all the remainin g example s are positiv e (or all negative) , then we are done : we can
answe r Yes or No. Figur e 18.6(c ) show s example s of this in the None  and Some  cases .
3. If there are no example s left, it mean s that no such exampl e has been observed , and we
retur n a defaul t valu e calculate d from the majorit y classificatio n at the node' s parent .
4. If there are no attribute s left, but both positiv e and negativ e examples , we have a problem . It
mean s that these example s have exactl y the same description , but differen t classifications .
NOISE Thi s happen s whe n som e of the data are incorrect ; we say there is nois e in the data. It also
happen s whe n the attribute s do not give enoug h informatio n to full y describ e the situation ,
or whe n the domai n is truly nondeterministic . One simpl e way out of the proble m is to use
a majorit y vote .
We continu e to apply the DECISION­TREE­LEARNIN G algorith m (Figur e 18.7) unti l we get the tree
show n in Figur e 18.8 . The tree is distinctl y differen t from the origina l tree show n in Figur e 18.4 ,
despit e the fact that the data were actuall y generate d from an agen t usin g the origina l tree.
One migh t conclud e that the learnin g algorith m is not doin g a very good job of learnin g
the correc t function . This woul d be the wron g conclusio n to draw . The learnin g algorith m look s
at the examples,  not at the correc t function , and in fact, its hypothesi s (see Figur e 18.8 ) not only
agree s with all the examples , but is considerabl y simple r than the origina l tree. The learnin g
algorith m has no reaso n to includ e tests for Raining  and Reservation,  becaus e it can classif y all
536 Chapte r 18 . Learnin g from Observation s
(a)
(b)
Frenc h Burge r
(c)
None
Figur e 18.6 Splittin g the example s by testin g on attributes . In (a), we see that Patrons  is a
good attribut e to test first ; in (b), we see that Type  is a poor one; and in (c), we see that Hungry  is
a fairl y goo d secon d test, give n that Patrons  is the first test.
the example s withou t them . It has also detecte d an interestin g regularit y in the data (namely , that
the first autho r will wai t for Thai food on weekends ) that was not even suspected . Man y hour s
have been waste d by machin e learnin g researcher s tryin g to debu g their learnin g algorithm s when
in fact the algorith m was behavin g properl y all along .
Of course,  if we were to gathe r more examples , we migh t induc e a tree mor e simila r to the
original . The tree in Figur e 18.8 is boun d to mak e a mistake ; for example , it has neve r seen a
case wher e the wait is 0­10 minute s but the restauran t is full. For a case wher e Hungry  is false ,
the tree says not to wait , but the autho r woul d certainl y wait . This raise s an obviou s question : if
the algorith m induce s a consisten t but incorrec t tree from the examples , how incorrec t will the
tree be? The next sectio n show s how to analyz e this experimentally .
Sectio n 18.3 . Learnin g Decisio n Tree s 537
functio n DECISION­TREE­LEARNING(CTamp/e.? , attributes, default)  return s a decisio n tree
inputs : examples,  set of example s
attributes,  set of attribute s
default,  defaul t valu e for the goal predicat e
if examples  is empt y then retur n default
else if all examples  have the same classificatio n then retur n the classificatio n
else if attributes  is empt y then retur n MAJORITY ­ VALVE(examples)
else
best  <— CHOOSE­  ATTR\B\]TB(attributes,  examples)
tree  «— a new decisio n tree with root test best
for each valu e v, of best do
example.fi  — {element s of examples  with best  = v,}
subtre e — DECISION­TREE­LEARNING(e«™/?fe.s, , attributes  — best,
MAJORITY ­ VALUE(examples})
add a branc h to tree with labe l v, and subtre e subtree
end
retur n tree
Figur e 18.7 Th e decisio n tree learnin g algorithm .
Figur e 18.8 Th e decisio n tree induce d from the 12­exampl e trainin g set.
538 Chapte r 18 . Learnin g from Observation s
Assessin g the performanc e of the learnin g algorith m
A learnin g algorith m is goo d if it produce s hypothese s that do a good job of predictin g the
classification s of unsee n examples . In Sectio n 18.6 , we will see how predictio n qualit y can be
estimate d in advance . For now , we will look at a methodolog y for assessin g predictio n qualit y
after the fact.
Obviously , a predictio n is good if it turn s out to be true , so we can asses s the qualit y of a
hypothesi s by checkin g its prediction s agains t the correc t classificatio n once we know it. We do
TEST SET thi s on a set of example s know n as the test set. If we train on all our availabl e examples , then
we will have to go out and get som e mor e to test on, so ofte n it is mor e convenien t to adop t the
followin g methodology :
1. Collec t a large set of examples .
2. Divid e it into two disjoin t sets: the trainin g set and the test set.
3. Use the learnin g algorith m with the trainin g set as example s to generat e a hypothesi s H.
4. Measur e the percentag e of example s in the test set that are correctl y classifie d by H.
5. Repea t step s 1 to 4 for differen t size s of trainin g sets and differen t randoml y selecte d
trainin g sets of each size .
The resul t of this is a set of data that can be processe d to give the averag e predictio n qualit y
as a functio n of the size of the trainin g set. Thi s can be plotte d on a graph , givin g wha t is
LEARNIN G CURV E calle d the learnin g curv e for the algorith m on the particula r domain . The learnin g curv e for
DECISION­TREE­LEARNIN G with the restauran t example s is show n in Figur e 18.9. Notic e that as
the trainin g set grows , the predictio n qualit y increases . (Fo r this reason , such curve s are also
calle d happ y graphs.)  This is a good sign that there is indee d som e patter n in the data and the
learnin g algorith m is pickin g it up.
The key idea of the methodolog y is to keep the trainin g and test data separate , for the same
reaso n that the result s of an exam woul d not be a good measur e of qualit y if the student s saw the
test beforehand . The methodolog y of randoml y dividin g up the example s into trainin g and test
sets is fair whe n each run is independen t of the others—i n that case , no run can "cheat " and tell
the othe r runs wha t the righ t answer s are. But there is the proble m that you, as the designe r of
the learnin g algorithm , can cheat . If you run som e examples , notic e a pattern , and chang e eithe r
the learnin g or the performanc e element , then the runs are no longe r independent , and you have
effectivel y passe d on informatio n abou t the test set. In theory , ever y time you mak e a chang e to
the algorithm , you shoul d get a new set of example s to work from . In practice , this is too difficult ,
so peopl e continu e to run experiment s on tainte d sets of examples .
Practica l uses of decisio n tree learnin g
Decisio n trees provid e a simpl e representatio n for prepositiona l knowledg e that can be used for
decisio n makin g and classificatio n of objects . Althoug h decisio n tree learnin g canno t generat e
interestin g scientifi c theorie s becaus e of its representationa l restrictions , it has been used in a
wide variet y of applications . Her e we describ e just two.
Sectio n 18.3 . Learnin g Decisio n Tree s 539
1
0.9
tj
a 0.8a)
c° 0.7o
g
8 0.6
tf=
0.5
0.4
20 40 6 0
Trainin g set size80 100
Figur e 18.9 A  learnin g curv e for the decisio n tree algorith m on 100 randoml y generate d
example s in the restauran t domain . The grap h summarize s 20 trials .
Designin g oil platfor m equipmen t
In 1986 , BP deploye d an exper t syste m calle d GASOI L for designin g gas­oi l separatio n system s
for offshor e oil platforms . Gas­oi l separatio n is done at the wellhea d by a very large , complex ,
and expensiv e separatio n system , whos e desig n depend s on a numbe r of attribute s includin g
the relativ e proportion s of gas, oil, and water , and the flow rate , pressure , density , viscosity ,
temperature , and susceptibilit y to waxing . At the time , GASOI L was the larges t commercia l
exper t syste m in the world , containin g approximatel y 2500 rules . Buildin g such a syste m by
hand woul d have take n roughl y 10 person­years . Usin g decision­tre e learnin g method s applie d
to a databas e of existin g designs , the syste m was develope d in 100 person­day s (Michie , 1986) .
It is said to outperfor m huma n expert s and to have save d BP man y million s of dollars .
Learnin g to fly
There are two way s to desig n an automati c controlle r for a comple x system . One can construc t
a precis e mode l of the dynamic s of the system , and use one of a variet y of forma l method s
(includin g AI plannin g methods ) to desig n a controlle r that has certai n guarantee d properties .
Alternatively , one can simpl y lear n the correc t mappin g from the state of the syste m to the
correc t action . For very comple x systems , such as aircraf t and electorates , developin g a detaile d
mode l may be infeasible , whic h leave s the secon d alternative . Sammu t et al. (1992 ) adopte d this
alternativ e for the task of learnin g to fly a Cessn a on a fligh t simulator . The data was generate d
by watchin g three skille d huma n pilot s performin g an assigne d fligh t plan 30 time s each . Eac h
time the pilo t took an actio n by settin g one of the contro l variable s such as thrus t or flaps , a
trainin g exampl e was created . In all, 90,00 0 example s were obtained , each describe d by 20 state
variable s and labelle d by the actio n taken . Fro m thes e examples , a decisio n tree was extracte d
540 Chapte r 18 . Learnin g from Observation s
using the C4.5 syste m (Quinlan , 1993) . The decisio n tree was then converte d into C code and
inserte d into the fligh t simulator' s contro l loop so that it coul d fly the plan e itself .
The result s are surprising : not only does the progra m learn to fly, it learn s to fly somewha t
better  than its teachers . Thi s is becaus e the generalizatio n proces s clean s up the occasiona l
mistake s mad e by humans . Suc h result s sugges t that machin e learnin g technique s may yiel d
controller s that are more robus t than conventional , manuall y programme d autopilots . For difficul t
tasks such as flyin g helicopter s carryin g heav y load s in high winds , no autopilot s are availabl e
and very few human s are competent . Suc h task s are potentiall y suitabl e for system s base d on
automate d learning .
18.4 USIN G INFORMATIO N THEOR Y
This sectio n look s at a mathematica l mode l for choosin g the best attribut e and at method s for
dealin g with nois e in the data . It can be skippe d by thos e who are not intereste d in thes e details .
The schem e used in decisio n tree learnin g for selectin g attribute s is designe d to minimiz e
the dept h of the fina l tree. The idea is to pick the attribut e that goes as far as possibl e towar d
providin g an exac t classificatio n of the examples . A perfec t attribut e divide s the example s into
sets that are all positiv e or all negative . The Patrons  attribut e is not perfect , but it is fairl y good .
A reall y useles s attribut e such as Type  leave s the exampl e sets with roughl y the same  proportio n
of positiv e and negativ e example s as the origina l set.
All we need , then , is a forma l measur e of "fairl y good " and "reall y useless " and we
can implemen t the CHOOSE ­ ATTRIBUT E functio n of Figur e 18.7 . The measur e shoul d have its
maximu m valu e whe n the attribut e is perfec t and its minimu m valu e whe n the attribut e is of no
INFORMATIO N us e at all . One suitabl e measur e is the expecte d amoun t of informatio n provide d by the attribute ,
wher e we use the term in the mathematica l sens e first define d in (Shanno n and Weaver , 1949) .
To understan d the notio n of information , thin k abou t it as providin g the answe r to a question , for
example , whethe r a coin will com e up heads . If one alread y has a good gues s abou t the answer ,
then the actua l answe r is less informative . Suppos e you are goin g to bet $1 on the flip of a coin ,
and you believ e that the coin is rigge d so that it will com e up head s with probabilit y 0.99. You
will bet head s (obviously) , and have expecte d valu e $0.9 8 for the bet. Tha t mean s you woul d
only be willin g to pay less than $0.0 2 for advanc e informatio n abou t the actua l outcom e of the
flip. If the coin were fair, your expecte d valu e woul d be zero , and you woul d be willin g to pay
up to $ 1 .00 for advanc e informatio n — the less you know , the more valuabl e the information .
Informatio n theor y uses this sam e intuition , but instea d of measurin g the valu e of infor ­
matio n in dollars , it measure s informatio n conten t in bits. One bit of informatio n is enoug h to
answe r a yes/n o questio n abou t whic h one has no idea , such as the flip of a fair coin . In general ,
if the possibl e answer s v, hav e probabilitie s P(v,) , then the informatio n conten t / of the actua l
answe r is give n by
, P( Vn)) = ~P(Vi)  10g 2 P(Vi)
Sectio n 18.4 . Usin g Informatio n Theor y 54 1
This is just the averag e informatio n conten t of the variou s event s (the — Iog 2 P terms ) weighte d
by the probabilitie s of the events . To chec k this equation , for the tossin g of a fair coin we get
Vll\ =_l ]o 1 1, 1
If the coin is loade d to give 99% head s we get / (1/100,99/100 ) = 0.08 bits, and as the probabilit y
of head s goes to 1, the informatio n of the actua l answe r goes to 0.
For decisio n tree learning , the questio n that need s answerin g is: for a give n example , wha t
is the correc t classification ? A correc t decisio n tree will answe r this question . An estimat e of
the probabilitie s of the possibl e answer s befor e any of the attribute s have been teste d is give n
by the proportion s of positiv e and negativ e example s in the trainin g set. Suppos e the trainin g
set contain s p positiv e example s and n negativ e examples . The n an estimat e of the informatio n
containe d in a correc t answe r is
, ( P n \ P  , P  « , «/ —— , —— = ­— — Iog 2 —— ­ —— Iog 2 ——\p + n p + n) p  + n p  + n p + n p  + n
For the restauran t trainin g set show n in Figur e 18.5 , we have p = n = 6, so we need 1 bit of
information .
Now a test on a singl e attribut e A will not usuall y tell us this muc h information , but it will
give us som e of it. We can measur e exactl y how muc h by lookin g at how muc h informatio n we
still need after  the attribut e test. Any attribut e A divide s the trainin g set E into subset s E\,...,  Ev
accordin g to their value s for A, wher e A can have v distinc t values . Eac h subse t £, has />/ positiv e
example s and n, negativ e examples , so if we go alon g that branc h we will need an additiona l
/ (pjl(pi  + «,), HiKpi  + «,­)) bits of informatio n to answe r the question . A rando m exampl e has the
z'th valu e for the attribut e with probabilit y (/?,• + «,)/(/ ? + n), so on average , after testin g attribut e A,
we will need
V
Remainder(A)  ­ V^ ———­I^ p + n
INFORMATIO N GAIN bit s of informatio n to classif y the example . Th e informatio n gain from the attribut e test is
denne d as the differenc e betwee n the origina l informatio n requiremen t and the new requirement :
LGain(A)  = 1 ( ­?—,  —H— 1 ­ Remainder^)\p + n p + nj
and the heuristi c used in the CHOOSE­ATTRIBUT E functio n is just to choos e the attribut e with the
larges t gain .
Lookin g at the attribute s Patrons  and Type  and thei r classifyin g power , as show n in Fig­
ure 18.6 , we have
[2 4  6/ 2 4\1Gain(Patrons)  = 1 ­ — /(0,1 ) + ­r^/O.O ) + — / ­, ­ %  0.54 1 bits[12 1 2 1 2 \6 6/J
f2 /I 1\ 2  /I 1\ 4  fl 2\ 4  /2 2c°*'"»»' ­ •­ y G­ o+n' G­ o+12' (4­ 4­) <­12' (4­«
In fact, Patrons  has the highes t gain of any of the attribute s and woul d be chose n by the decision ­
tree learnin g algorith m as the root.
542 Chapte r 18 . Learnin g from Observation s
OVERFITTIN G
DECISIO N TRE E
PRUNIN G
NULL HYPOTHESI SNoise and overfittin g
We saw earlie r that if there are two or more example s with the sam e description s (in term s of
the attributes ) but differen t classifications , then the DEClSlON­TREE­LEARNiN G algorith m mus t
fail to find a decisio n tree consisten t with all the examples . The solutio n we mentione d befor e is
to have each leaf node repor t eithe r the majorit y classificatio n for its set of example s or repor t
the estimate d probabilitie s of each classificatio n usin g the relativ e frequencies . The forme r is
appropriat e for an agen t that require s the decisio n tree to represen t a stric t logica l function ,
wherea s the latte r can be used by a decision­theoreti c agent .
Unfortunately , this is far from the whol e story . It is quit e possible , and in fact likely , that
even whe n vita l informatio n is missing , the decisio n tree learnin g algorith m will find a decisio n
tree that is consisten t with all the examples . This is becaus e the algorith m can use the irrelevant
attributes , if any, to mak e spuriou s distinction s amon g the examples .
Conside r the proble m of tryin g to predic t the roll of a die. Suppos e that experiment s are
carrie d out durin g an extende d perio d of time with variou s dice, and that the attribute s describin g
each trainin g exampl e are as follows :
1. Day:  the day on whic h the die was rolle d (Mon , Tue, Wed , Thu) .
2. Month:  the mont h in whic h the die was rolle d (Jan or Feb) .
3. Color:  the colo r of the die (Red or Blue) .
As long as there are no two example s with identica l descriptions , DECISION­TREE­LEARNIN G will
find an exac t hypothesis . Thi s will , however , be totall y spurious . Wha t we woul d like is that
DECISION­TREE­LEARNIN G retur n a singl e leaf node with probabilitie s close to 1/6 for each roll,
once it has seen enoug h examples .
Wheneve r there is a large set of possibl e hypotheses , one has to be carefu l not to use the
resultin g freedo m to find meaningles s "regularity " in the data. This proble m is calle d overfitting .
It is a very genera l phenomenon , and occur s even whe n the targe t functio n is not at all random .
It afflict s ever y kind of learnin g algorithm , not just decisio n trees .
A complet e mathematica l treatmen t of overfittin g is beyond  the scop e of this book . Here we
presen t a simpl e techniqu e calle d decisio n tree pruning . Prunin g work s by preventin g recursiv e
splittin g on attribute s that are not clearl y relevant , even whe n the data at that node in the tree is
not uniforml y classified . The questio n is, how do we detec t an irrelevan t attribute ?
Suppos e we split a set of example s usin g an irrelevan t attribute . Generall y speaking , we
woul d expec t the resultin g subset s to have roughl y the sam e proportion s of each class as the
origina l set. In this case , the informatio n gain will be close to zero.9 Thus , the informatio n gain
is a good clue to irrelevance . Now the questio n is, how large a gain shoul d we requir e in orde r to,
split on a particula r attribute ?
This is exactl y the sort of questio n addresse d by classica l tests for statistica l significance .
A significanc e test begin s by assumin g that ther e is no underlyin g patter n (the so­calle d null ,
hypothesis) . The n the actua l data are analyze d to calculat e the exten t to whic h it deviate s from |
a perfec t absenc e of pattern . If the degre e of deviatio n is statisticall y unlikel y (usuall y take n to f
mean a 5% probabilit y or less) , then that is considere d to be good evidenc e for the presenc e of a  >
9 In fact , the gain be will be greate r than zero unles s the proportion s are all exactl y the sam e (see Exercis e 18.9) .
Sectio n 18.4 . Usin g Informatio n Theor y 543
significan t patter n in the data . The probabilitie s are calculate d from standar d distribution s of the
amoun t of deviatio n one woul d expec t to see due to rando m sampling .
In this case , the null hypothesi s is that the attribut e is irrelevant , and henc e the informatio n
gain for an infinitel y large sampl e woul d be zero . We need to calculat e the probabilit y that, unde r
the null hypothesis , a sampl e of size v woul d exhibi t the observe d deviatio n from the expecte d
distributio n of positiv e and negativ e examples . We can measur e the deviatio n by comparin g
the actua l number s of positiv e and negativ e example s in each subset , p\ and n,, to the expecte d
number s /7, and h\ assumin g true irrelevance :
Pi=PPi + nt
p + n p  + n
A convenien t measur e of the total deviatio n is give n by
(Pi ~ Pi) ("i  ~ »i
Pi n,
Unde r the null hypothesis , the valu e of D is distribute d accordin g to the \2 (chi­squared ) distri ­
butio n with v — 1 degree s of freedom . The probabilit y that the attribut e is reall y irrelevan t can be
calculate d with the help of standar d \2 tables , or with statistica l software . Exercis e 18.1 0 asks
you to mak e the appropriat e change s to DECISION­TREE­LEARNIN G to implemen t this form of
\2 PRUNIN G pruning , whic h is know n as \2 pruning .
With pruning , noise can be tolerated—classificatio n error s give a linea r increas e in predic ­
tion error , wherea s error s in the description s of example s (i.e, the wron g outpu t for a give n input )
have an asymptoti c effec t that gets wors e as the tree shrink s dow n to smalle r sets. Tree s con­
structe d with prunin g perfor m significantl y bette r than trees constructe d withou t prunin g whe n
the data contai n a large amoun t of noise . The prune d trees are often muc h more compact , and
are therefor e easie r to understand .
CROSS­VALIDATIO N Cross­validatio n is anothe r techniqu e that eliminate s the danger s of overfitting . The basi c
idea of cross­validatio n is to try to estimat e how well the curren t hypothesi s will predic t unsee n
data. Thi s is done by settin g aside som e fractio n of the know n data , and usin g it to test the
predictio n performanc e of a hypothesi s induce d from the rest of the know n data . Thi s can be
done repeatedl y with differen t subset s of the data , with the result s averaged . Cross­validatio n can
be used in conjunctio n with any tree­constructio n metho d (includin g pruning ) in orde r to selec t
a tree with good predictio n performance .
Broadenin g the applicabilit y of decisio n trees
In orde r to exten d decisio n tree inductio n to a wide r variet y of problems , a numbe r of issue s mus t
be addressed . We will briefl y mentio n each , suggestin g that a full understandin g is best obtaine d
by doin g the associate d exercises :
0 Missin g data : In man y domains , not all the attribut e value s will be know n for ever y
example . The value s may not have been recorded , or they may be too expensiv e to obtain .
This give s rise to two problems . First , give n a complete  decisio n tree, how shoul d one
classif y an objec t that is missin g one of the test attributes ? Second , how shoul d one modif y
544 Chapte r 18 . Learnin g from Observation s
the informatio n gain formul a whe n som e example s have unknow n value s for the attribute ?
These question s are addresse d in Exercis e 18.11 .
0 Multivalue d attributes:Whe n an attribut e has a larg e numbe r of possibl e values , the
informatio n gain measur e give s an inappropriat e indicatio n of the attribute' s usefulness .
Conside r the extrem e case wher e ever y exampl e has a differen t valu e for the attribute—fo r
instance , if we were to use an attribut e RestaurantName  in the restauran t domain . In such
a case , each subse t of example s is a singleto n and therefor e has a uniqu e classification , so
the informatio n gain measur e woul d have its highes t valu e for this attribute . However , the
GAIN RATI O attribut e may be irrelevan t or useless . On e possibl e solutio n is to use the gain ratio , as
describe d in Exercis e 18.12 .
0 Continuous­value d attributes : Attribute s such as Height  and Weight  have a large or
infinit e set of possibl e values . The y are therefor e not well­suite d for decision­tre e learnin g
in raw form . An obviou s way to deal with this proble m is to discretiz e the attribute .
For example , the Price  attribut e for restaurant s was discretize d into $, $$, and $$$ values .
Normally , such discret e range s woul d be define d by hand . A bette r approac h is to preproces s
the raw attribut e value s durin g the tree­growin g proces s in orde r to find out whic h range s
give the mos t usefu l informatio n for classificatio n purposes .
A decision­tre e learnin g syste m for real­worl d application s mus t be able to handl e all of these
problems . Handlin g continuous­value d variable s is especiall y important , becaus e both physica l
and financia l processe s provid e numerica l data . Severa l commercia l package s been buil t that
meet these criteria , and they have been used to develo p severa l hundre d fielde d systems .
18.5 LEARNIN G GENERA L LOGICA L DESCRIPTION S
In this section , we examin e way s in whic h mor e genera l kind s of logica l representation s can be
learned . In the process , we will construc t a genera l framewor k for understandin g learnin g algo ­
rithms , centere d aroun d the idea that inductiv e learnin g can be viewe d as a proces s of searchin g
HYPOTHESI S SPAC E fo r a good hypothesi s in a larg e space—th e hypothesi s space —define d by the representatio n
languag e chose n for the task. We will also explai n wha t is goin g on in logica l terms : the logica l
connection s amon g examples , hypotheses , and the goal . Althoug h this may seem like a lot of
extra work at first , it turn s out to clarif y man y of the issue s in learning . It enable s us to go well
beyon d the simpl e capabilitie s of the decisio n tree learnin g algorithm , and allow s us to use the
full powe r of logica l inferenc e in the servic e of learning .
CANDIDAT E
DEFINITIO NHypothese s
The situatio n is usuall y this: we start out with a goal predicate , whic h we will genericall y call Q­
(For example , in the restauran t domain , Q will be WillWait.)  Q will be a unar y predicate , and we
are tryin g to find an equivalen t logica l expressio n that we can use to classif y example s correctly .
Each hypothesi s propose s such an expression , whic h we call a candidat e definitio n of the goal
Sectio n 18.5 . Learnin g Genera l Logica l Description s 545
predicate . Usin g C, to denot e the candidat e definition , each hypothesi s //, is a sentenc e of the
form MX Q(x) o  C/(jt) . For example , the decisio n tree show n in Figur e 18.8 expresse s the
followin g logica l definitio n (whic h we will call Hr for futur e reference) :
Mr WillWait(r)  O Patrons(r,Some)
V Patrpns(r,  Full)  A ^Hungry(r)  A Type(r,  French)
V Patrons(r,  Full)  A ­iHungry(r)  A Type(r,  Thai)  A Fri/Sat(r)
V Patrons(r,  Full)  A ­^Hungry(r)  A Type(r,  Burger)
The hypothesi s space is then the set of all hypothese s that the learnin g algorith m is designe d
to entertain . For example , the DECISION­TREE­LEARNIN G algorith m can entertai n any decisio n
tree hypothesi s define d in term s of the attribute s provided ; its hypothesi s spac e therefor e consist s
of all thes e decisio n trees . We will genericall y use the lette r H to denot e the hypothesi s spac e
[Hi ,...,//„} . Presumably , the learnin g algorith m believe s that one of the hypothese s is correct ;
that is, it believe s the sentenc e
//, V H2 V #3 V ... V Hn (18.1 )
Each hypothesi s predict s that a certai n set of examples—namely , thos e that satisf y its candidat e
EXTENSIO N definition—wil l be example s of the goal predicate . Thi s set is calle d the extensio n of the
predicate . Tw o hypothese s with differen t extension s are therefor e logicall y inconsisten t with
each other , becaus e they disagre e on their prediction s for at least one example . If they have the
same extension , they are logicall y equivalent .
FALS E NEGATIV EExample s
Logicall y speaking , an exampl e is an objec t to whic h the goal concep t may or may not apply ,
and that has som e logica l description . Let us genericall y call the z'th exampl e Xt. Its descriptio n
will be the sentenc e A(X,) , wher e D, can be any logica l expressio n takin g a singl e argument .
The classificatio n will be give n by a sentenc e Q(Xj)  if the exampl e is positive , and ­*Q(Xi)  if
the exampl e is negative . For instance , the first exampl e from Figur e 18.5 is describe d by the
sentence s
Alternate^)  A
and the classificatio nA ^Fri/Sat(X }) A Hungry(X }) A .
WillWait(Xi)
The complet e trainin g set is then just the conjunctio n of all these sentences . A hypothesi s agree s
with all the example s if and only if it is logicall y consisten t with the trainin g set; ideally , we
woul d like to find such a hypothesis .
Let us examin e this notio n of consistenc y mor e carefully . Obviously , if hypothesi s //, is
consisten t with the entir e trainin g set, it has to be consisten t with each example . Wha t woul d it
mean for it to be inconsisten t with an example ? This can happe n in one of two ways :
• An exampl e can be a false negativ e for the hypothesis , if the hypothesi s says it shoul d be
negativ e but in fact it is positive . For instance , the new exampl e X\^ describe d by
Patmns(Xi 3,Full)  A Wait(X }3,0­10)  A ­<Hungry(Xi 3) A ... A WillWait(X n)
546 Chapte r 18 . Learnin g from Observation s
woul d be a false negativ e for the hypothesi s Hr give n earlier . Fro m Hr and the exampl e
description , we can deduc e both WillWait(Xii),  whic h is what  the exampl e says , and
­^WillWait(X\3),  whic h is wha t the hypothesi s predicts . The hypothesi s and the exampl e
are therefor e logicall y inconsistent .
FALSE POSITIV E •  An exampl e can be a false positiv e for the hypothesis , if the hypothesi s says it shoul d be
positiv e but in fact it is negative.10
If an exampl e is a fals e positiv e or fals e negativ e for a hypothesis , then the exampl e and the
hypothesi s are logicall y inconsisten t with each other . Assumin g that the exampl e is a correc t
observatio n of fact, then the hypothesi s can be ruled out. Logically , this is exactl y analogou s to
the resolutio n rule of inferenc e (see Chapte r 9), wher e the disjunctio n of hypothese s correspond s
to a claus e and the exampl e correspond s to a litera l that resolve s agains t one of the literal s in the
clause . An ordinar y logica l inferenc e syste m therefor e could , in principle , learn from the exampl e
by eliminatin g one or mor e hypotheses . Suppose , for example , that the exampl e is denote d by
the sentenc e 7], and the hypothesi s spac e is H\ V HI V HT, V #4. The n if I\ is inconsisten t with
//2 and //3, the logica l inferenc e syste m can deduc e the new hypothesi s spac e H\ V ftj.
We therefor e can characteriz e inductiv e learnin g in a logica l settin g as a proces s of graduall y
eliminatin g hypothese s that are inconsisten t with the examples , narrowin g dow n the possibilities .
Becaus e the hypothesi s spac e is usuall y vast (or even infinit e in the case of first­orde r logic) , we
do not recommen d tryin g to buil d a learnin g syste m usin g resolution­base d theore m provin g and
a complet e enumeratio n of the hypothesi s space . Instead , we will describ e two approache s that
find logicall y consisten t hypothese s with muc h less effort .
CURRENT­BEST ­
HYPOTHESI S
GENERALIZATIO N
SPECIALIZATIO NCurrent­best­hypothesi s searc h
The idea behin d current­best­hypothesi s searc h is to maintai n a singl e hypothesis , and to adjus t
it as new example s arriv e in orde r to maintai n consistency . The basi c algorith m was describe d
by John Stuar t Mill (1843) , and may well have appeare d even earlier .
Suppos e we have som e hypothesi s such as Hr, of whic h we have grow n quit e fond . As
long as each new exampl e is consistent , we need do nothing . Then alon g come s a false negativ e
example , X\T,.  Wha t do we do?
Figur e 18.10(a ) show s Hr schematicall y as a region : everythin g insid e the rectangl e is part
of the extensio n of H,. The example s that have actuall y been seen so far are show n as "+" or
"­", and we see that Hr correctl y categorize s all the example s as positiv e or negativ e example s of
WillWait.  In Figur e 18.10(b) , a new exampl e (circled ) is a false negative : the hypothesi s says it
shoul d be negativ e but it is actuall y positive . The extensio n of the hypothesi s must be increase d to
includ e it. This is calle d generalization ; one possibl e generalizatio n is show n in Figur e 18.10(c) .
Then in Figur e 18.10(d) , we see a false positive : the hypothesi s says the new exampl e (circled )
shoul d be positive , but it actuall y is negative . The extensio n of the hypothesi s mus t be decrease d
to exclud e the example . Thi s is calle d specialization ; in Figur e 18.10(e ) we see one possibl e
specializatio n of the hypothesis .
10 The term s "fals e positive " and "fals e negative " were first used in medicin e to describ e erroneou s result s from laborator y
tests. A resul t is a fals e positiv e if it indicate s that the patien t has the diseas e whe n in fact no diseas e is present .
Sectio n 18.5 . Learnin g Genera l Logica l Description s 547
­_
(a)­
­­
(b)i.­­
(c)­­
i|a
(d)­
­ illliiLiia|g| i
(e)3­
Figur e 18.10 (a ) A consisten t hypothesis , (b) A false negative , (c) The hypothesi s is general ­
ized, (d) A false positive , (e) The hypothesi s is specialized .
We can now specif y the CURRENT­BEST­LEARNIN G algorithm , show n in Figur e 18.11 . No­
tice that each time we conside r generalizin g or specializin g the hypothesis , we mus t chec k for con­
sistenc y with the othe r examples , becaus e there is no guarante e that an arbitrar y increase/decreas e
in the extensio n will avoi d including/excludin g any othe r negative/positiv e examples .
functio n CURRENT­EEST­LEARNWG(examples)  return s a hypothesi s
H — any hypothesi s consisten t with the first exampl e in examples
for each remainin g exampl e in examples  do
if e is false positiv e for H then
H — choos e a specializatio n of H consisten t with examples
else if e is false negativ e for H then
H ^­ choos e a generalizatio n of H consisten t with examples
if no consisten t specialization/generalizatio n can be foun d then fail
end
retur n H
Figur e 18.1 1 Th e current­best­hypothesi s learnin g algorithm . It searche s for a consisten t
hypothesi s and backtrack s whe n no consisten t specialization/generalizatio n can be found .
We have define d generalizatio n and specializatio n as operation s that chang e the extension
of a hypothesis . Now we need to determin e exactl y how they can be implemente d as syntacti c
operation s that chang e the candidat e definitio n associate d with the hypothesis , so that a progra m
can carr y them out. Thi s is done by first notin g that generalizatio n and specializatio n are also
logical  relationship s betwee n hypotheses . If hypothesi s H\, with definitio n C,, is a generalizatio n
of hypothesi s //2 with definitio n €2, then we mus t have
V* C 2(x) => Ci(x)
Therefor e in orde r to construc t a generalizatio n of //2, we simpl y need to find a definitio n C\
that is logicall y implie d by €2­ Thi s is easil y done . For example , if C2(x)  is Alternate(x)  A
548 Chapte r 18 . Learnin g from Observation s
Patrons(x,  Some),  then one possibl e generalizatio n is give n by Cj (x) = Patrons(x,  Some).  This is
CONDIT!OGNS calle d droppin g conditions . Intuitively , it generate s a weake r definitio n and therefor e allow s a
large r set of positiv e examples . Ther e are a numbe r of other generalizatio n operations , dependin g
on the languag e bein g operate d on. Similarly , we can specializ e a hypothesi s by addin g extra
condition s to its candidat e definitio n or by removin g disjunct s from a disjunctiv e definition . Let
us see how this work s on the restauran t example , usin g the data in Figur e 18.5 .
• The first exampl e X\ is positive . Alternate(X\ ) is true, so let us assum e an initia l hypothesi s
HI: VJ E WillWait(x)  & Alternate^)
• The secon d exampl e X2 is negative . H\ predict s it to be positive , so it is a false positive .
Therefore , we need to specializ e H\. This can be done by addin g an extra conditio n that
will rule out Xi. One possibilit y is
//2 : Vz WillWait(x)  <£> Alternate(x)  A Patrons(x,  Some)
• The third exampl e X^ is positive . HI predict s it to be negative , so it is a false negative .
Therefore , we need to generaliz e HI. This can be done by droppin g the Alternate  condition ,
yieldin g
H3: MX  WillWait(x)  <£> Patrons(x,Some)
• The fourt h exampl e X/\ is positive . HT, predict s it to be negative , so it is a false negative .
We therefor e need to generaliz e HT,. We canno t drop the Patrons  condition , becaus e that
woul d yield an all­inclusiv e hypothesi s that woul d be inconsisten t with X2. One possibilit y
is to add a disjunct :
H4 : Vx  WillWait(x)  <=> Patmns(x,  Some)  V (Patrons(x,  Full)  A Fri/Sat(x))
Already , the hypothesi s is startin g to look reasonable . Obviously , ther e are othe r possibilitie s
consisten t with the first four examples ; here are two of them :
H'4 : \/x  WillWait(x)  O ­^WaitEstimate(x,  30­60)
H% : Vx WillWait(x)  <£> Patmns(x,Some)
V(Patmns(x,  Full)  A WaitEstimate(x,  10­30))
The CURRENT­BEST­LEARNIN G algorith m is describe d nondeterministically , becaus e at any point ,
there may be severa l possibl e specialization s or generalization s that can be applied . The choice s
that are mad e will not necessaril y lead to the simples t hypothesis , and may lead to an unrecoverabl e
situatio n wher e no simpl e modificatio n of the hypothesi s is consisten t with all of the data . In
such cases , the progra m mus t backtrac k to a previou s choic e point .
The CURRENT­BEST­LEARNIN G algorith m and its variant s have been used in a many machin e
learnin g systems , startin g with Patric k Winston' s (1970 ) "arch­learning " program . Wit h a large
numbe r of instance s and a large space , however , som e difficultie s arise :
1. Checkin g all the previou s instance s over agai n for each modificatio n is very expensive .
2. It is difficul t to find good searc h heuristics , and backtrackin g all over the place can take
forever . As we saw earlier , hypothesi s spac e can be a doubl y exponentiall y large place .
Sectio n 18.5 . Learnin g Genera l Logica l Description s 549
VERSIO N SPAC E
CANDIDAT E
ELIMINATIO NLeast­commitmen t searc h
Backtrackin g arise s becaus e the current­best­hypothesi s approac h has to choose  a particula r
hypothesi s as its best gues s even thoug h it does not have enoug h data yet to be sure of the choice .
Wha t we can do instea d is to keep aroun d all and only thos e hypothese s that are consisten t with
all the data so far. Eac h new instanc e will eithe r have no effec t or will get rid of som e of the
hypotheses . Recal l that the origina l hypothesi s spac e can be viewe d as a disjunctiv e sentenc e
//i V H2 V //3 ... V //„
As variou s hypothese s are foun d to be inconsisten t with the examples , this disjunctio n shrinks ,
retainin g only thos e hypothese s not ruled out. Assumin g that the origina l hypothesi s spac e does in
fact contai n the righ t answer , the reduce d disjunctio n mus t still contai n the righ t answe r becaus e
only incorrec t hypothese s have been removed . Th e set of hypothese s remainin g is calle d the
versio n space , and the learnin g algorith m (sketche d in Figur e 18.12 ) is calle d the versio n spac e
learnin g algorith m (also the candidat e eliminatio n algorithm) .
BOUNDAR Y SETfunctio n VERSiON­SPACE­LEARNiNG(?.raHf/7/<>.r) return s a versio n spac e
local variables : V, the versio n space : the set of all hypothese s
V — the set of all hypothese s
for each exampl e e in examples  do
if V is not empt y then V—  VERS1ON­SPACE­LJPDATE(V , e)
end
retur n V
functio n VERSiON­SpACE­UPDATE(V.e ) return s an update d versio n spac e
V— {h G V  : h is consisten t with e}
Figur e 18.1 2 Th e versio n spac e learnin g algorithm . It find s a subse t of Vtha t is consisten t
with the examples.
One importan t propert y of this approac h is that it is incremental:  one neve r has to go back
and reexamin e the old examples . All remainin g hypothese s are guarantee d to be consisten t with
them anyway . It is also a least­commitmen t algorith m becaus e it make s no arbitrar y choice s
(cf. the partial­orde r plannin g algorith m in Chapte r 11) . But ther e is an obviou s problem . We
alread y said that the hypothesi s spac e is enormous , so how can we possibl y writ e dow n this
enormou s disjunction ?
The followin g simpl e analog y is very helpful . How do you represen t all the real number s
betwee n 1 and 2? Afte r all, ther e is an infinit e numbe r of them ! The answe r is to use an interva l
representatio n that just specifie s the boundarie s of the set: [1,2] . It work s becaus e we have an
ordering  on the real numbers .
We also have an orderin g on the hypothesi s space , namely , generalization/specialization .
This is a partia l ordering , whic h mean s that each boundar y will not be a poin t but rathe r a set
of hypothese s calle d a boundar y set. The grea t thin g is that we can represen t the entir e versio n
550 Chapte r 18 . Learnin g from Observation s
G­SE T
S­SE Tspace usin g just two boundar y sets: a mos t genera l boundar y (the G­set ) and a mos t specifi c
boundar y (the S­set) . Everything  in between  is guaranteed to  be consistent  with  the examples.
Befor e we prov e this, let us recap :
• The curren t versio n spac e is the set of hypothese s consisten t with all the example s so far.
It is represente d by the S­set and G­set , each of whic h is a set of hypotheses .
• Ever y membe r of the S­se t is consisten t with all observation s so far, and ther e are no
consisten t hypothese s that are more specific .
• Ever y membe r of the G­se t is consisten t with all observation s so far, and ther e are no
consisten t hypothese s that are mor e general .
We wan t the initia l versio n spac e (befor e any example s have been seen ) to represen t all possibl e
hypotheses . We do this by settin g the G­se t to contai n just True  (the hypothesi s that contain s
everything) , and the S­se t to contai n just False  (the hypothesi s whos e extensio n is empty) .
Figur e 18.1 3 show s the genera l structur e of the boundar y set representatio n of the versio n
space . In order to show that the representatio n is sufficient , we need the followin g two properties :
1. Ever y consisten t hypothesi s (othe r than those in the boundar y sets) is more specifi c than
some membe r of the G­set , and mor e genera l than som e membe r of the S­set . (Tha t is,
there are no "stragglers " left outside. ) This follow s directl y from the definition s of S and
G. If there were a straggle r h, then it woul d have to be no more specifi c than any membe r
of G, in whic h case it belong s in G; or no mor e genera l than any membe r of S, in whic h
case it belong s in S.
2. Ever y hypothesi s more specifi c than som e membe r of the G­se t and mor e genera l than
some membe r of the S­set is a consisten t hypothesis . (Tha t is, there are no "holes " betwee n
the boundaries. ) Any h betwee n S and G mus t rejec t all the negativ e example s rejecte d by
each membe r of G (becaus e it is more specific) , and mus t accep t all the positiv e example s
accepte d by any membe r of S (becaus e it is more general) . Thus , h mus t agre e with all the
examples , and therefor e canno t be inconsistent . Figur e 18.1 4 show s the situation : there are
no know n example s outsid e S but insid e G, so any hypothesi s in the gap must be consistent .
We have therefor e show n that ifS and G are maintaine d accordin g to their definitions , then they
provid e a satisfactor y representatio n of the versio n space . The only remainin g proble m is how to
update  S and G for a new exampl e (the job of the VERSION­SPACE­UPDAT E function) . This may
appea r rathe r complicate d at first, but from the definition s and with the help of Figur e 18.13 , it is
not too hard to reconstruc t the algorithm .
We need to worr y abou t the member s S, and G, of the S­ and G­sets . For each one, the new
instanc e may be a false positiv e or a false negative .
1. Fals e positiv e for S,•: This mean s S, is too general , but there are no consisten t specialization s
of 5,­ (by definition) , so we throw it out of the S­set .
2. Fals e negativ e for S,: Thi s mean s S,­ is too specific , so we replac e it by all its immediat e
generalizations .
3. Fals e positiv e for G,: This mean s G, is too general , so we replac e it by all its immediat e
specializations .
4. Fals e negativ e for G,: This mean s G, is too specific , but there are no consisten t generaliza ­
tions of G, (by definition ) so we thro w it out of the G­set .
Sectio n 18.5 . Learnin g Genera l Logica l Description s 551
this regio n all inconsisten tMore genera l
More specifi c
Figur e 18.1 3 Th e versio n spac e contain s all hypothese s consisten t with the examples .
SI
Figur e 18.1 4 Th e extension s of the member s of G and 5. No know n example s lie in between .
We continu e these operation s for each new instanc e unti l one of three thing s happens :
1. We have exactl y one concep t left in the versio n space , in whic h case we retur n it as the
uniqu e hypothesis .
2. The versio n spac e collapses —either  S or G become s empty , indicatin g that there are no
consisten t hypothese s for the trainin g set. This is the sam e case as the failur e of the simpl e
versio n of the decisio n tree algorithm .
552 Chapte r 18 . Learnin g from Observation s
3. We run out of example s with severa l hypothese s remainin g in the versio n space . Thi s
mean s the versio n spac e represent s a disjunctio n of hypotheses . For any new example ,
if all the disjunct s agree , then we can retur n their classificatio n of the example . If they
disagree , one possibilit y is to take the majorit y vote .
We leav e as an exercis e the applicatio n of the VERSION­SPACE­LEARNIN G algorith m to the
restauran t data .
GENERALIZATIO N
HIERARCH YDiscussio n
There are two principa l drawback s to the version­spac e approach :
• If the domai n contain s nois e or insufficien t attribute s for exac t classification , the versio n
space will alway s collapse .
• If we allow unlimite d disjunctio n in the hypothesi s space , the S­se t will alway s contai n a
singl e most­specifi c hypothesis , namely , the disjunctio n of the description s of the positiv e
example s seen to date. Similarly , the G­se t will contai n just the negatio n of the disjunctio n
of the description s of the negativ e examples .
To date , no completel y successfu l solutio n has been foun d for the proble m of noise . The proble m
of disjunctio n can be addresse d by allowin g limite d form s of disjunctio n or by includin g a gen­
eralizatio n hierarch y of more genera l predicates . For example , instea d of usin g the disjunctio n
WaitEstimate(x,  30­60)  V WaitEstimate(x,  >60) , we migh t use the singl e litera l LongWait(x).  The
set of generalizatio n and specializatio n operation s can be easil y extende d to handl e this.
The pure versio n spac e algorith m was first applie d in the Meta­DENDRA L system , whic h
was designe d to lear n rule s for predictin g how molecule s woul d brea k into piece s in a mas s
spectromete r (Buchana n and Mitchell , 1978) . Meta­DENDRA L was able to generat e rule s that
were sufficientl y nove l to warran t publicatio n in a journa l of analytica l chemistry—th e first real
scientifi c knowledg e generate d by a compute r program . It was also used in the elegan t LEX
syste m (Mitchel l et ai, 1983) , whic h was able to learn to solv e symboli c integratio n problem s
by studyin g its own successe s and failures . Althoug h versio n spac e method s are probabl y not
practica l in mos t real­worl d learnin g problems , mainl y becaus e of noise , they provid e a good
deal of insigh t into the logica l structur e of hypothesi s space .
18.6 WH Y LEARNIN G WORK S : COMPUTATIONA L LEARNIN G THEOR Y
Learnin g mean s behavin g bette r as a resul t of experience . We have show n severa l algorithm s for
inductiv e learning , and explaine d how they fit into an agent . The main unanswere d questio n was
posed in Sectio n 18.2 : how can one possibl y know that one' s learnin g algorith m has produce d a
theor y that will correctl y predic t the future ? In term s of the definitio n of inductiv e learning , how
do we know that the hypothesi s h is close to the targe t function / if we don' t know what / is?
Thes e question s have been pondere d for severa l centuries , but unles s we find some answers ,
machin e learnin g will , at best , be puzzle d by its own success . Fortunately , withi n the last decade ,
Sectio n 18.6. Wh y Learnin g Works : Computationa l Learnin g Theor y 553
COMPUTATIONA L
LEARNIN G THEOR Y
PROBABL Y
APPROXIMATEL Y
CORREC T
PAC­LEARNIN G
STATIONAR Yanswer s hav e begu n to emerge . We will focu s on the answer s provide d by computationa l
learnin g theory , a field at the intersectio n of AI and theoretica l compute r science .
The underlyin g principl e is the following : any hypothesi s that is seriousl y wron g will
almos t certainl y be "foun d out" with high probabilit y after a smal l numbe r of examples , becaus e
it will mak e an incorrec t prediction . Thus , any hypothesi s that is consisten t with a sufficientl y
large set of trainin g example s is unlikel y to be seriousl y wrong—tha t is, it mus t be Probabl y
Approximatel y Correct . PAC­learnin g is the subfiel d of computationa l learnin g theor y that is
devote d to this idea.
There are som e subtletie s in the precedin g argument . The main questio n is the connectio n
betwee n the trainin g and the test examples—afte r all, we want the hypothesi s to be approximatel y
correc t on the test set, not just on the trainin g set. The key assumption , introduce d by Valiant , is
that the trainin g and test sets are draw n randoml y from the same populatio n of example s usin g
the same  probability distribution.  Thi s is calle d the stationarit y assumption . It is muc h mor e
precis e than the usua l proposal s for justifyin g induction , whic h mutte r somethin g abou t "the
futur e bein g like the past. " Withou t the stationarit y assumption , the theor y can mak e no claim s
at all abou t the futur e becaus e there woul d be no necessar y connectio n betwee n futur e and past.
The stationarit y assumptio n amount s to supposin g that the proces s that select s example s is not
malevolent . Obviously , if the trainin g set consiste d only of weir d examples—two­heade d dogs ,
for instance—the n the learnin g algorith m canno t help but mak e unsuccessfu l generalization s
abou t how to recogniz e dogs .
ERRO R
C­BAL L
,How man y example s are needed ?
In orde r to put these insight s into practice , we will need some notation :
• Let X be the set of all possibl e examples .
• Let D be the distributio n from whic h example s are drawn .
• Let H be the set of possibl e hypotheses .
• Let m be the numbe r of example s in the trainin g set.
Initially , we will assum e that the true function / is a membe r of H. Now we can defin e the erro r
of a hypothesi s h with respec t to the true function / give n a distributio n D over the example s as
the probabilit y that h is differen t from / on an example :
error(/ 0 = P(h(x)  if(x)\x  draw n from D)
This is the same quantit y bein g measure d experimentall y by the learnin g curve s show n earlier .
A hypothesi s h is calle d approximatel y correc t if error(/z ) < e, wher e e is a smal l constant .
The plan of attac k is to show that after seein g m examples , with high probability , all consisten t
hypothese s will be approximatel y correct . One can thin k of an approximatel y correc t hypothesi s
as bein g "close " to the true functio n in hypothesi s space—i t lies insid e wha t is calle d the f­bal l
aroun d the true function/ . Figur e 18.1 5 show s the set of all hypothese s H, divide d into the e­bal l
around / and the remainder , whic h we call Hbad ­
We can calculat e the probabilit y that a "seriousl y wrong " hypothesi s hi, e Hb ad is consisten t
with the first m example s as follows . We kno w that error(/z/, ) > e. Thus , the probabilit y that it
554 Chapte r 18 . Learnin g from Observation s
H
Hbad /  \
w
Figur e 18.15 Schemati c diagra m of hypothesi s space , showin g the "c­ball " aroun d the true
function/ .
agree s with any give n exampl e is < (1 — t). The boun d for m example s is
P(hb  agree s with m examples ) < (1 — f)'"
For Hba d to contai n a consisten t hypothesis , at leas t one of the hypothese s in Hb ad mus t be
consistent . The probabilit y of this occurrin g is bounde d by the sum of the individua l probabilities :
P(H bad contain s a consisten t hypothesis ) < |H bad|0 ­ <0'"
SAMPL E
COMPLEXIT YWe woul d like to reduc e the probabilit y of this even t belo w som e smal l numbe r 6:
We can achiev e this if we allow the algorith m to see
», > 1 (in ­ + In i
f 6(18­2 )
examples . Thus , if a learnin g algorith m return s a hypothesi s that is consisten t with this man y
examples , then with probabilit y at least 1 ­ 6, it has error at mos t ( . In othe r words , it is probabl y
approximatel y correct . The numbe r of require d examples , as a functio n of e and 6, is calle d the
sampl e complexit y of the hypothesi s space .
It appears , then , that the key questio n is the size of the hypothesi s space . As we saw
earlier , if H is the set of all Boolea n function s on n attributes , then |H| = 22". Thus , the sampl e
complexit y of the spac e grow s as 2". Becaus e the numbe r of possibl e example s is also 2", this
says that any learnin g algorith m for the spac e of all Boolea n function s will do no bette r than a
looku p table , if it merel y return s a hypothesi s that is consisten t with all know n examples . Anothe r
way to see this is to observ e that for any unsee n example , the hypothesi s spac e will contai n as
many consisten t hypothese s predictin g a positiv e outcom e as predic t a negativ e outcome .
Sectio n 18.6 . Wh y Learnin g Works : Computationa l Learnin g Theor y 555
The dilemm a we face , then , is that unles s we restric t the spac e of function s the algorith m
can consider , it will not be able to learn ; but if we do restric t the space , we may eliminat e the
true functio n altogether . Ther e are two way s to "escape " this dilemma . The first way is to insis t
that the algorith m return s not just any consisten t hypothesis , but preferabl y the simples t one. The
theoretica l analysi s of such algorithm s is beyon d the scop e of this book , but in mos t cases , findin g
the simples t hypothesi s is intractable . The secon d escape , whic h we pursu e here , is to focu s on
learnabl e subset s of the entir e set of Boolea n functions . The idea is that in mos t case s we do
not need the full expressiv e powe r of Boolea n functions , and can get by with mor e restricte d
languages . We now examin e one such restricte d languag e in more detail .
Learnin g decisio n lists
DECISIO N LIST A  decisio n list is a logica l expressio n of a restricte d form . It consist s of a serie s of tests , each
of whic h is a conjunctio n of literals . If a test succeed s whe n applie d to an exampl e description ,
the decisio n list specifie s the valu e to be returned . If the test fails , processin g continue s with the
next test in the list." Decisio n lists resembl e decisio n trees , but their overal l structur e is simpler ,
wherea s the individua l tests are more complex . Figur e 18.1 6 show s a decisio n list that represent s
the hypothesi s H4 obtaine d by the earlie r CURRENT­BEST­LEARNIN G algorithm :
V* WillWait(x)  & Patrons(x,  Some)  V (Patmns(x,  Full)  A FrilSat(x))
&­DL
k­DTPatronsfx,  Some) Patrons(x,Full)  A Fri/Sat(x) No
Yes Yes
Figur e 18.1 6 A  decisio n list for the restauran t problem .
If we allow tests of arbitrar y size, then decisio n lists can represen t any Boolea n functio n
(Exercis e 18.13) . On the othe r hand , if we restric t the size of each test to at mos t k literals ,
then it is possibl e for the learnin g algorith m to generaliz e successfull y from a smal l numbe r of
examples . We call this languag e &­DL . The exampl e in Figur e 18.1 6 is in 2­DL . It is easy to
show (Exercis e 18.13 ) that k­DL  include s as a subse t the languag e k­m,  the set of all decisio n
trees of dept h at mos t k. It is importan t to remembe r that the particula r languag e referre d to by
k­DL depend s on the attribute s used to describ e the examples . We will use the notatio n &­DL(« ) to
denote  a &­DL languag e usin g n Boolea n attributes .
The first task is to show that k­DL  is learnabl e — that is, any functio n in &­DL can be accuratel y
approximate d afte r seein g a reasonabl e numbe r of examples . To do this, we need to calculat e
the numbe r of hypothese s in the language . Let the languag e of tests — conjunction s of at mos t k
literal s usin g n attribute s — be Conj(n,  k). Becaus e a decisio n list is constructe d of tests , and each
test can be attache d to eithe r a Yes or a No outcom e or can be absen t from the decisio n list, there
1 ' A decisio n list is therefor e identica l in structur e to a CON D statemen t in Lisp .
556 Chapte r 18 . Learnin g from Observation s
are at mos t 3lc""./<"­*) l distinc t sets of componen t tests . Eac h of these sets of tests can be in any
order , so
The numbe r of conjunction s of If literal s from n attribute s is give n by
;=o
Hence , afte r som e work , we obtai n
We can plug this into Equatio n (18.2 ) to show that the numbe r of example s neede d for PAC ­
learnin g a /C­D L functio n is polynomia l in n:
m > ­ ( In ­ + O(nk Iog 2(nk))
Therefore , any algorith m that return s a consisten t decisio n list will PAC­lear n a k­DL  functio n in
a reasonabl e numbe r of examples , for smal l k. The next task is to find an efficien t algorith m that
return s a consisten t decisio n list. We will use a greed y algorith m calle d DECISION­LIST­LEARNIN G
that repeatedl y find s a test that agree s exactl y with som e subse t of the trainin g set. Onc e it find s
such a test, it add s it to the decisio n list unde r constructio n and remove s the correspondin g
examples , It then construct s the remainde r of the decisio n list usin g just the remainin g examples .
This is repeate d unti l there are no example s left. The algorith m is show n in Figur e 18.17 .
This algorith m does not specif y the metho d for selectin g the next test to add to the decisio n
list. Althoug h the forma l result s give n earlie r do not depen d on the selectio n method , it woul d
seem reasonabl e to prefe r smal l tests that matc h larg e sets of uniforml y classifie d examples , so
that the overal l decisio n list will be as compac t as possible . Th e simples t strateg y is to find
the smalles t test t that matche s any uniforml y classifie d subset , regardles s of the size of the
subset . Eve n this approac h work s quit e well . The result s for the restauran t data are show n in
Figur e 18.18 , and sugges t that learnin g decisio n lists is an effectiv e way to mak e predictions .
functio n DECisiON­LiST­LEARNiNG(e.ra;wp/<?.v ) return s a decisio n list, No or failur e
if examples  is empt y then retur n the valu e No
t — a test that matche s a nonempt y subse t examples,  of examples
such that the member s of examples,  are all positiv e or all negativ e
if there is no such t then retur n failur e
if the example s in examples,  are positiv e then o <— Yes
else o — No
retur n a decisio n list with initia l test / and outcom e o
and remainin g element s give n by DEClsiON­LlST­LEARNING(<?.«zmp/e s ­ examples,)
Figur e 18.1 7 A n algorith m for learnin g decisio n lists .
Sectio n 18.6 . Wh y Learnin g Works : Computationa l Learnin g Theor y 557
l
0.9
3 0.8
1 0.7u
8 0.6
0.5
0.4
(/'"* ^^'^  DL L ——
­ V ' J  DT L •
:'; /
^
'L
) 2 0 4 0 6 0 8 0 10 0
Trainin g set size
Figur e 18.1 8 Grap h showin g the predictiv e performanc e of the DECISION­LlST­LEARNIN G
algorith m on the restauran t data , as a functio n of the numbe r of example s seen . The curv e for
DECISION­TREE­LEARNIN G is show n for comparison .
IDENTIFICATIO N IN
THE LIMITDiscussio n
Computationa l learnin g theor y has generate d a new way of lookin g at the proble m of learning .
In the early 1960s , the theor y of learnin g focusse d on the proble m of identificatio n in the limit .
An identificatio n algorith m mus t retur n a hypothesi s that exactl y matche s the true function .
The standar d approac h combine s the current­best­hypothesi s and versio n spac e methods : the
curren t best hypothesi s is the first consisten t hypothesi s in som e fixe d simplicit y orderin g of the
hypothesi s space . As example s arrive , the learne r abandon s simple r hypothese s as they becom e
inconsistent . Onc e it reache s the true function , it will neve r abando n it. Unfortunately , in man y
hypothesi s spaces , the numbe r of example s and the computatio n time require d to reac h the true
functio n is enormous . Computationa l learnin g theor y doe s not insis t that the learnin g agen t
find the "one true law " governin g its environment , but instea d that it find a hypothesi s with a
certai n degre e of predictiv e accuracy . It also bring s sharpl y into focu s the trade­of f betwee n the
expressivenes s of the hypothesi s languag e and the complexit y of learning .
The result s we hav e show n are worst­cas e complexit y results , and do not necessaril y
reflec t the average­cas e sampl e complexit y as measure d by the learnin g curve s we have shown .
An average­cas e analysi s mus t also mak e assumption s as to the distributio n of example s and
the distributio n of true function s that the algorith m will have to learn . As thes e issue s becom e
bette r understood , computationa l learnin g theor y is providin g valuabl e guidanc e to machin e
learnin g researcher s who are intereste d in predictin g or modifyin g the learnin g abilit y of their
algorithms . Beside s decisio n lists , result s have been obtaine d for almos t all know n subclasse s
of Boolea n functions , for neura l network s (see Chapte r 19) and for sets of first­orde r logica l
sentence s (see Chapte r 21). The result s show that the pure inductiv e learnin g problem , wher e the
agen t begin s with no prio r knowledg e abou t the targe t function , is generall y very hard . As we
558 Chapte r 18 . Learnin g from Observation s
show in Chapte r 21, the use of prior knowledg e to guid e inductiv e learnin g make s it possibl e to
learn quit e large sets of sentence s from reasonabl e number s of examples , even in a languag e as
expressiv e as first­orde r logic .
18.7 SUMMAR Y
We hav e seen that all learnin g can be seen as learnin g a function , and in this chapte r we
concentrat e on induction : learnin g a functio n from exampl e input/outpu t pairs . The main point s
were as follows :
• Learnin g in intelligen t agent s is essentia l for dealin g with unknow n environment s (i.e.,
compensatin g for the designer' s lack of omniscienc e abou t the agent' s environment) .
• Learnin g is also essentia l for buildin g agent s with a reasonabl e amoun t of effor t (i.e.,
compensatin g for the designer' s laziness , or lack of time) .
• Learnin g agent s can be divide d conceptuall y into a performanc e element , whic h is re­
sponsibl e for selectin g actions , and a learnin g element , whic h is responsibl e for modifyin g
the performanc e element .
• Learnin g take s man y forms , dependin g on the natur e of the performanc e element , the
availabl e feedback , and the availabl e knowledge .
• Learnin g any particula r componen t of the performanc e elemen t can be cast as a proble m
of learnin g an accurat e representatio n of a function .
• Learnin g a functio n from example s of its input s and output s is calle d inductiv e learning .
• The difficult y of learnin g depend s on the chose n representation . Function s can be repre ­
sente d by logica l sentences , polynomials , belie f networks , neura l networks , and others .
• Decisio n trees are an efficien t metho d for learnin g deterministi c Boolea n functions .
• Ockham' s razo r suggest s choosin g the simples t hypothesi s that niatche s the observe d
examples . The informatio n gain heuristi c allow s us to find a simpl e decisio n tree.
• The performanc e of inductiv e learnin g algorithm s is measure d by thei r learnin g curve ,
whic h show s the predictio n accurac y as a functio n of the numbe r of observe d examples .
• We presente d two genera l approache s for learnin g logica l theories . Th e current­best ­
hypothesi s approac h maintain s and adjust s a singl e hypothesis , wherea s the versio n spac e
approac h maintain s a representatio n of all consisten t hypotheses . Bot h are vulnerabl e to
noise in the trainin g set.
• Computationa l learnin g theor y analyse s the sampl e complexit y and computationa l com ­
plexit y of inductiv e learning . Ther e is a trade­of f betwee n the expressivenes s of the
hypothesi s languag e and the ease of learning .
Sectio n 18.7 . Summar y 55 9
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
The genera l architectur e for learnin g system s portraye d in Figur e 18.1 is classi c in the machin e
learnin g literatur e (Buchana n et^al.,  1978) . The architectur e itself , as embodie d in programs ,
goes back at least as far as Arthu r Samuel' s (1959 ; 1967 ) progra m for playin g checkers , whic h
improve d its performanc e throug h learning . It is describe d furthe r in Chapte r 20.
Claud e Shanno n was the invento r of informatio n theor y (Shanno n and Weaver , 1949) ,
whic h revolutionize d the stud y of communicatio n as wel l as contributin g to the succes s of
decision­tre e learning . He also contribute d one of the earlies t example s of machin e learning , a
mechanica l mous e name d Theseu s that learne d how to trave l efficientl y throug h a maz e by trial
and error . EPAM , the "Elementar y Perceive r And Memorizer " (Feigenbaum , 1961) , was one of
the earlies t system s to use decisio n trees (or discriminatio n nets) . EPA M was intende d as a
cognitive­simulatio n mode l of huma n concep t learning . CLS (Hun t et al., 1966 ) used a heuristi c
lookahea d metho d to construc t decisio n trees . ID3 (Quinlan , 1979 ) adde d the crucia l idea of using
informatio n conten t to provid e the heuristi c function . Quinla n (1986 ) experimentall y investigate s
the effec t of nois e on learnin g and describe s a modificatio n of ID3 designe d to handl e noise . A
copy of C4.5 , an industrial­strengt h versio n of ID3, can be foun d in Quinla n (1993) .
Willia m of Ockha m (c. 1285­1349) , the mos t influentia l philosophe r of his centur y and a
majo r contribute r to medieva l epistemology , logic , and metaphysics , is credite d with a statemen t
calle d "Ockham' s Razor"—i n Latin , Entia  non sunt  multiplicanda  praeter  necessitatem,  and in
English , "Entitie s are not to be multiplie d withou t necessity. " Unfortunately , this laudabl e piece
of advic e is nowher e to be foun d in his writing s in precisel y these words .
The current­best­hypothesi s approac h has been a mainsta y of the philosoph y of science ,
as a mode l of scientifi c theor y formation , for centuries . In AI, the approac h is mos t closel y
associate d with the work of Patric k Winston , whos e Ph.D . thesi s (Winston , 1970 ) addresse d the
proble m of learnin g description s of comple x objects . Tom Mitchel l (1977 ; 1982 ) invente d versio n
space s and the learnin g algorith m that uses them . They were initiall y used in the Meta­DENDRA L
exper t syste m for chemistr y (Buchana n and Mitchell , 1978) , and later in Mitchell' s (1983 ) LEX
system , whic h learn s to solv e calculu s problems . The importanc e of bias in inductiv e learnin g
was emphasize d by Mitchel l (1980) , who showe d that an unbiase d algorith m is incapabl e of
learnin g becaus e its hypothesi s spac e alway s contain s equa l number s of consisten t hypothese s
that predic t Q and ­<Q for any exampl e and for any goal predicat e Q.
Historically , the earlies t researc h on the theoretica l analysi s of learnin g focuse d on the
proble m of "identificatio n in the limit " (Gold , 1967) , whic h involve s showin g that a learnin g
algorith m will eventuall y converg e on the correc t hypothesi s once it has seen enoug h examples .
This approac h was motivate d in part by model s of scientifi c discover y from the philosoph y
of scienc e (Popper , 1962) , but has been applie d mainl y to the proble m of learnin g grammar s
from exampl e sentences . Osherson , Stob , and Weinstei n (1986 ) provid e a moder n and rigorou s
treatmen t of the field .
Wherea s the identification­in­the­limi t approac h concentrate s on eventua l convergence ,
COMPLEX! ^ m e stud y of Kolmogoro v complexit y or algorithmi c complexity , develope d independentl y by
Solomonof f (1964 ) and Kolmogoro v (1965) , attempt s to provid e a forma l definitio n for the notio n
of simplicit y used in Ockham' s razor . To escap e the proble m that simplicit y depend s on the way
560 Chapte r 18 . Learnin g from Observation s
MINIMU M
DESCRIPTIO N
LENGT H
UNIFOR M
CONVERGENC E
THEOR Y
OCKHA M
ALGORITH Min whic h informatio n is represented , it is propose d that simplicit y be measure d by the lengt h of
the shortes t progra m for a universa l Turin g machin e that correctl y reproduce s the observe d data.
Althoug h there are man y possibl e universa l Turin g machines , and henc e man y possibl e "shortest "
programs , thes e program s diffe r in lengt h by at mos t a constan t that is independen t of the amoun t
of data. Thi s beautifu l insight , whic h essentiall y show s that any initia l representatio n bias will
eventuall y be overcom e by the data itself , is marre d only by the undecidabilit y of computin g the
lengt h of the shortes t program . Approximat e measure s such as the minimu m descriptio n lengt h
or MDL (Rissanen , 1984 ) can be used instead , and have produce d excellen t result s in practice .
The recen t text by Li and Vitany i (1993 ) is the best sourc e for Kolmogoro v complexity .
Computationa l learnin g theor y in the moder n sense , that is, the theor y of PAC­learning , was
inaugurate d by Lesli e Valian t (1984) , but also has roots in the subfiel d of statistic s calle d unifor m
convergenc e theor y (Vapni k and Chervonenkis , 1971) . Valiant' s work brough t computationa l
complexit y issue s into the picture , and emphasize d the idea that the learne r need find only
approximately  correc t hypotheses . Wit h Michae l Kearn s (1990) , Valian t showe d that severa l
concep t classe s canno t be PAC­learne d tractabl y even thoug h sufficien t informatio n is availabl e
in the examples . Som e positiv e result s have been obtaine d for classe s such as decisio n lists (Rivest ,
1987) , althoug h mos t PAC­learnin g result s have been negative .
PAC­learnin g theor y and unifor m convergenc e theor y were unifie d by the "fou r Germans "
(none of who m are actuall y German) : Blumer , Ehrenfeucht , Haussler , and Warmut h (1989) .
PAC learnin g is also relate d to othe r theoretica l approache s throug h the notio n of an Ockha m
algorithm . Suc h algorithm s are capabl e of findin g a consisten t hypothesi s that achieve s a
"significant " compressio n of the data it represents . The four German s showe d that if an Ockha m
algorith m exist s for a give n clas s of concepts , then the clas s is PAC­learnable  (Blume r et al.,
1990) . Boar d and Pitt (1992 ) showed  that the implicatio n also goes the othe r way : if a concep t
class is PAC­learnable , then there mus t exist an Ockha m algorith m for it.
A large numbe r of importan t paper s on machin e learnin g have been collecte d in Readings
in Machine  Learning  (Shavli k and Dietterich , 1990) . The two volume s Machine Learning  1
(Michalsk i et al., 1983 ) and Machine  Learning  2 (Michalsk i et al., 1986 ) also contai n man y
importan t paper s as well as huge bibliographies . Weis s and Kulikowsk i (1991 ) provid e a broa d
introductio n to function­learnin g method s from machin e learning , statistics , and neura l networks .
The STATLO G projec t (Michi e et al., 1994 ) is by far the mos t exhaustiv e investigatio n into the
comparativ e performanc e of learnin g algorithms . Goo d curren t researc h in machin e learnin g is
publishe d in the annua l proceeding s of the Internationa l Conferenc e on Machin e Learning , in
the journa l Machine  Learning,  and in mainstrea m Al journals . Wor k in computationa l learnin g
theor y appear s in the annua l ACM Worksho p on Computationa l Learnin g Theor y (COLT) , in
Machine  Learning,  and in severa l theoretica l journals .
EXERCISE S
18.1 Conside r the proble m face d by an infan t learnin g to spea k and understan d a language .
Explai n how this proces s fits into the genera l learnin g model , identifyin g each of the component s
of the mode l as appropriate .
Sectio n 18.7 . Summar y 56 1
18.2 Repea t Exercis e 18.1 for the case of learnin g to play tenni s (or som e othe r competitiv e
sport with whic h you are familiar) . Is this supervise d learnin g or reinforcemen t learning ?
18.3 Dra w a decisio n tree for the proble m of decidin g whethe r or not to mov e forwar d at a road
intersectio n give n that the ligh t has just turne d green .
18.4 W e neve r test the same attribut e twic e alon g one path in a decisio n tree. Why not?
18.5 A good "stra w man " learnin g algorith m is as follows : creat e a table out of all the trainin g
examples . Determin e whic h outpu t occur s mos t ofte n amon g the trainin g examples ; call it d.
Then whe n give n an inpu t that is not in the table , just retur n d. For input s that are in the table ,
return the outpu t associate d with it (or the mos t frequen t output , if there is more than one) . If
the inpu t does not appea r in the table , then retur n d. Implemen t this algorith m and see how well
it does in a sampl e domain . Thi s shoul d give you an idea of the baselin e for the domain—th e
minima l performanc e that any algorith m shoul d be able to obtai n (althoug h man y publishe d
algorithm s have manage d to do worse) .
18.6 Loo k back at Exercis e 3.16 , whic h aske d you to predic t from a sequenc e of number s
(such as [1,4,9,16] ) the functio n underlyin g the sequence . Wha t technique s from this chapte r are
applicabl e to this problem ? How woul d they allow you to do bette r than the problem­solvin g
approac h of Exercis e 3.16 ?
18.7 In the recursiv e constructio n of decisio n trees , it sometime s occur s that a mixe d set of
positiv e and negativ e example s remain s at a leaf node , even after all the attribute s have been used .
Suppos e that we have/ ) positiv e example s and n negativ e examples .
a. Sho w that the solutio n used by DECISION­TREE­LEARNING , whic h pick s the majorit y
classification , minimize s the absolut e error over the set of example s at the leaf.
CLASS PROBABILIT Y b . Sho w that returnin g the class probabilit y pl(p + n) minimize s the sum of square d errors .
18.8 Suppos e that a learnin g algorith m is tryin g to find a consisten t hypothesi s whe n the
classification s of example s are actuall y bein g generate d randomly . Ther e are n Boolea n attributes ,
and example s are draw n uniforml y from the set of 2" possibl e examples . Calculat e the numbe r of
example s require d befor e the probabilit y of findin g a contradictio n in the data reache s 0.5.
18.9 Suppos e that an attribut e split s the set of example s E into subset s £,, and that each subse t
has pi positiv e example s and «, negativ e examples . Sho w that unles s the ratio />//(/?, • + «,) is the
same for all /, the attribut e has strictl y positiv e informatio n gain .
18.10 Modif y DECISION­TREE­LEARNIN G to includ e x2­pruning . You may wish to consul t
Quinla n (1986 ) for details .
18.11 The standar d DECISION­TREE­LEARNIN G algorith m describe d in the chapte r does not
handl e case s in whic h som e example s have missin g attribut e values .
a. First , we need to find a way to classif y such examples , give n a decisio n tree that include s
tests on the attribute s for whic h value s may be missing . Suppos e an exampl e X has a
missin g valu e for attribut e A, and that the decisio n tree tests for A at a node that X reaches .
One way to handl e this case is to preten d that the exampl e has all possibl e value s for the
562 Chapte r 18 . Learnin g from Observation s
attribute , but to weigh t each valu e accordin g to its frequenc y amon g all of the example s that
reach that node in the decisio n tree. The classificatio n algorith m shoul d follo w all branche s
at any node for whic h a valu e is missing , and shoul d multipl y the weight s alon g each path .
Write a modifie d classificatio n algorith m for decisio n trees that has this behavior .
b. Now modif y the informatio n gain calculatio n so that in any give n collectio n of example s C
at a give n node in the tree durin g the constructio n process , the example s with missin g value s
for any of the remainin g attribute s are give n "as­if " value s accordin g to the frequencie s of
those value s in the set C.
18.12 In the chapter , we note d that attribute s with man y differen t possibl e value s can caus e
problem s with the gain measure . Suc h attribute s tend to split the example s into man y smal l classe s
or even singleto n classes , thereb y appearin g to be highl y relevan t accordin g to the gain measure .
The gain rati o criterio n select s attribute s accordin g to the ratio betwee n thei r gain and thei r
intrinsi c informatio n content , that is, the amoun t of informatio n containe d in the answe r to the
question , "Wha t is the valu e of this attribute? " The gain ratio criterio n therefor e tries to measur e
how efficientl y an attribut e provide s informatio n on the correc t classificatio n of an example .
Write a mathematica l expressio n for the informatio n conten t of an attribute , and implemen t the
gain ratio criterio n in DECISION­TREE­LEARNING .
18.13 In this ^exercise , we wil l conside r the expressivenes s of decisio n lists , as define d in
Sectio n 18.6 .
a. Sho w that if the tests can be of any size, decisio n lists can represen t any Boolea n function .
b. Sho w that if the tests can contai n at mos t k literal s each , then decisio n lists can represen t
any functio n that can be represente d by a decisio n tree of dept h k.
18.14 W e have show n how a learnin g elemen t can improv e the performanc e element . Wha t if
we wante d to improv e the learnin g elemen t (or the criti c or the proble m generator) ? Giv e som e
example s of this kind of improvemen t in the taxi domain . Is it possibl e to represen t this kind of
learnin g with our genera l mode l of learnin g agents ? How ?
19LEARNIN G IN NEURA L
AND BELIE F NETWORK S
In which we  see how to  train complex  networks  of simple  computing  elements,  thereby
perhaps  shedding  some  light  on the workings  of the brain.
This chapte r can be viewe d in two ways . From a computationa l viewpoint , it is abou t a metho d
of representin g function s usin g network s of simpl e arithmeti c computin g elements , and abou t
method s for learnin g such representation s from examples . Thes e network s represen t function s
in muc h the same way that circuit s consistin g of simpl e logic gates represen t Boolea n functions .
Such representation s are particularl y usefu l for comple x function s with continuous­value d output s
and large number s of nois y inputs , wher e the logic­base d technique s in Chapte r 18 sometime s
have difficulty .
From a biologica l viewpoint , this chapte r is abou t a mathematica l mode l for the operatio n
of the brain . The simpl e arithmeti c computin g element s correspon d to neurons —the cells that
perfor m informatio n processin g in the brain—an d the networ k as a whol e correspond s to a
NEURA L NETWORK S collectio n of interconnecte d neurons . For this reason , the network s are calle d neura l networks.1
Beside s thei r usefu l computationa l properties , neura l network s may offe r the best chanc e of
understandin g many psychologica l phenomen a that arise from the specifi c structur e and operatio n
of the brain . We will therefor e begi n the chapte r with a brief look at wha t is know n abou t brains ,
becaus e this provide s muc h of the motivatio n for the stud y of neura l networks . In a sense , we
thereb y depar t from our intention , state d in Chapte r 1, to concentrat e on rationa l actio n rathe r than
on imitatin g humans . Thes e conflictin g goal s have characterize d the stud y of neura l network s
ever sinc e the very first pape r on the topi c by McCulloc h and Pitts (1943) . Methodologicall y
speaking , the goal s can be reconcile d by acknowledgin g the fact that human s (and othe r animals )
do think , and use thei r power s of though t to act quit e successfull y in comple x domain s wher e
curren t computer­base d agent s woul d be lost. It is instructiv e to try to see how they do it.
Sectio n 19.2 then present s the idealize d model s that are the main subjec t of study . Simple ,
single­laye r network s calle d perceptron s are covere d in Sectio n 19.3 , and genera l multilaye r
network s in Sectio n 19.4 . Sectio n 19.5 illustrate s the variou s uses of neura l networks .
1 Othe r name s that hav e been used for the fiel d includ e connectionism , paralle l distribute d processing , neura l
computation , adaptiv e networks , and collectiv e computation . It shoul d be emphasize d that thes e are artificia l neura l
networks ; there is no attemp t to build computin g element s out of anima l tissue .
563
564 Chapte r 19 . Learnin g in Neura l and Belie f Network s
The networ k them e is continue d in Sectio n 19.6 , wher e we discus s method s for learnin g
belief network s from examples . The connectio n is deepe r than the superficia l similarit y implie d
by the wor d "network"—no t only do the two field s shar e som e learnin g methods , but in som e
cases , it can be show n that neura l network s are belie f networks .
19.1 Ho w THE BRAI N WORK S
NEURO N
" SDM A
DENDRITE S
AXON
SYNAPS E
ACTIO N POTENTIA L
EXCITATOR Y
INHIBITOR Y
PLASTICIT YThe exac t way in whic h the brai n enable s though t is one of the grea t mysterie s of science . It has
been appreciate d for thousand s of year s that stron g blow s to the head can lead to unconsciousness ,
temporar y loss of memory , or even  permanen t loss of menta l capability . Thi s suggest s that the
brain is someho w involve d in thought . It has also long been know n that the huma n brai n is
someho w different ; in abou t 335 B.C . Aristotl e wrote , "Of all the animals , man has the larges t
brain in proportio n to his size."2 Still , it was not unti l the middl e of the eighteent h centur y
that the brai n was widel y recognize d as the seat of consciousness , and it was not unti l the late
nineteent h centur y that the functiona l region s of anima l brain s bega n to be mappe d out. Befor e
the nineteent h century , candidat e location s for the seat of consciousnes s include d the heart , the
spleen , and the pinea l body , a smal l appendag e of the brai n presen t in all vertebrates .
We do know that the neuron , or nerv e cell, is the fundamenta l functiona l unit of all nervou s
syste m tissue , includin g the brain . Eac h neuro n consist s of a cell body , or soma , that contain s
a cell nucleus . Branchin g out from the cell bod y are a numbe r of fiber s calle d dendrite s and a
singl e long fiber calle d the axon . Dendrite s branc h into a bush y networ k aroun d the cell, wherea s
the axon stretche s out for a long distance—usuall y abou t a centimete r (100 time s the diamete r
of the cell body) , and as far as a mete r in extrem e cases . Eventually , the axon also branche s
into strand s and substrand s that connec t to the dendrite s and cell bodie s of othe r neurons . The
connectin g junctio n is calle d a synapse . Eac h neuro n form s synapse s wit h anywher e from a
dozen to a hundre d thousan d othe r neurons . Figur e 19.1 show s the parts of a neuron .
Signal s are propagate d from neuro n to neuro n by a complicate d electrochemica l reaction .
Chemica l transmitte r substance s are release d from the synapse s and ente r the dendrite , raisin g
or lowerin g the electrica l potentia l of the cell body . Whe n the potentia l reache s a threshold ,
an electrica l puls e or actio n potentia l is sent dow n the axon . The puls e spread s out alon g the
branche s of the axon , eventuall y reachin g synapse s and releasin g transmitter s into the bodie s of
other cells . Synapse s that increas e the potentia l are calle d excitatory , and thos e that decreas e it
are calle d inhibitory . Perhap s the mos t significan t findin g is that synapti c connection s exhibi t
plasticity —long­ter m change s in the strengt h of connection s in respons e to the patter n of stimu ­
lation . Neuron s also form new connection s with othe r neurons , and sometime s entir e collection s
of neuron s can migrat e from one plac e to another . Thes e mechanism s are though t to form the
basis for learnin g in the brain .
Most informatio n processin g goes on in the cerebra l cortex , the oute r laye r of the brain .
The basi c organizationa l uni t appear s to be a barrel­shape d modul e of tissu e abou t 0.5 mm in
2 Sinc e then , it has been discovere d that som e specie s of dolphin s and whale s have relativel y large r brains . The larg e
size of huma n brain s is now though t to be enable d in part by recen t improvement s in its coolin g system .
Sectio n 19.1 . Ho w the Brai n Work s 565
Axona l arborizatio n
Axon from anothe r cell
Synapse s
Cell body or Som a
Figur e 19.1 Th e part s of a nerv e cell or neuron . In reality , the lengt h of the axon shoul d be
abou t 100 time s the diamete r of the cell body .
diameter , extendin g the full dept h of the cortex , whic h is abou t 4 mm in humans . A modul e
l contain s abou t 2000 neurons . It is know n that certai n area s of the brai n have specifi c functions .
In 1861 , Pierr e Paul Broc a was able to demonstrat e that the third left fronta l convolutio n of the
APHASI A cerebra l corte x is importan t for speec h and languag e by his studie s of patient s with aphasia —an
inabilit y to speak , ofte n brough t on by brai n damage . Thi s soon led to surgica l experiment s on
animal s that mappe d out the connectio n betwee n area s of the corte x and specifi c moto r controls .
We now have som e data on the mappin g betwee n area s of the brai n and the part s of the body
that they control , or from whic h they receiv e sensor y input . Suc h mapping s seem to be able to
chang e radicall y over the cours e of a few weeks , and som e animal s seem to have multipl e maps .
Moreover , we do not fully understan d how othe r area s can take over function s whe n one area is
damaged . Ther e is almos t no theor y abou t how an individua l memor y is stored .
• :, Th e trul y amazin g thin g is that a collection  of simple  cells  can lead  to thought,  action,
* " and  consciousness.  Neurobiolog y is a long way from a complet e theor y of consciousness , but
even if there are som e importan t electrica l or chemica l processe s that have been overlooked , the
amazin g conclusio n is the same : brains  cause  minds  (Searle , 1992) . The only real alternativ e
theor y is mysticism : that there is som e mystica l realm in whic h mind s operat e that is beyon d
physica l science .
Comparin g brain s with digita l computer s
Brain s and digita l computer s perfor m quit e differen t tasks , and have differen t properties . Fig ­
ure 19.2 show s that there are more neuron s in the typica l huma n brain than there are bits in a typica l
high­en d compute r workstation . We can predic t that this will not hold true for long , becaus e the
huma n brai n is evolvin g very slowly , wherea s compute r memorie s are growin g rapidly . In any
566 Chapte r 19 . Learnin g in Neura l and Belie f Network s
Computationa l unit s
Storag e unit s
Cycl e time
Bandwidt h
Neuro n updates/se cCompute r
1 CPU , 105 gate s
109 bits RAM , 10'° bits disk
10~8 sec
109 bits/sec
105Huma n Brai n
1011 neuron s
1011 neurons , 10
10­3 sec
1014 bits/se c
1014
Figur e 19.2 A  crud e compariso n of the raw computationa l resource s availabl e to
(circa  1994 ) and brains .14 synapse s
computer s
GRACEFU L
DEGRADATIO Ncase, the differenc e in storag e capacit y is mino r compare d to the differenc e in switchin g spee d
and in parallelism . Compute r chip s can execut e an instructio n in tens of nanoseconds , wherea s
neuron s requir e millisecond s to fire. Brain s mor e than mak e up for this, however , becaus e all
the neuron s and synapse s are activ e simultaneously , wherea s mos t curren t computer s have only
one or at mos t a few CPUs . A neura l networ k runnin g ori a seria l compute r require s hundred s
of cycle s to decid e if a singl e neuron­lik e unit will fire, wherea s in a real brain , all the neuron s
do this in a singl e step. Thus , even  though  a computer is a  million  times  faster  in raw switching
speed,  the brain ends up  being a  billion  times  faster  at what it  does.  One of the attraction s of the
neura l networ k approac h is the hope that a devic e coul d be buil t that combine s the parallelis m of
the brain with the switchin g spee d of the computer . Full­scal e hardwar e developmen t will depen d
on findin g a famil y of neura l networ k algorithm s that provide s a basis for long­ter m investment .
A brain can perfor m a comple x task—recogniz e a face, for example—i n less than a second ,
whic h is only enoug h time for a few hundre d cycles . A seria l compute r require s billion s of cycle s
to perfor m the same task less well . Clearly , there is an opportunit y for massiv e parallelis m here.
Neura l network s may provid e a mode l for massivel y paralle l computatio n that is more successfu l
than the approac h of "parallelizing " traditiona l seria l algorithms .
Brain s are mor e fault­toleran t than computers . A hardwar e erro r that flip s a singl e bit
can doom an entir e computation , but brain cells die all the time with no ill effec t to the overal l
functionin g of the brain . It is true that there are a variet y of disease s and trauma s that can affec t
a brain , but for the mos t part, brain s manag e to muddl e throug h for 70 or 80 year s with no need
to replac e a memor y card , call the manufacturer' s servic e line , or reboot . In addition , brain s
are constantl y face d with nove l input , yet manag e to do somethin g with it. Compute r program s
rarely work as well with nove l input , unles s the programme r has been exceptionall y careful . The
third attractio n of neura l network s is gracefu l degradation : they tend to have a gradua l rathe r
than shar p drop­of f in performanc e as condition s worsen .
The fina l attractio n of neura l network s is that they are designe d to be traine d usin g an
inductiv e learnin g algorithm . (Contrar y to the impressio n give n by the popula r media , of course ,
neura l network s are far from bein g the only AI system s capabl e of learning. ) Afte r the networ k
is initialized , it can be modifie d to improv e its performanc e on input/outpu t pairs . To the exten t
that the learnin g algorithm s can be mad e genera l and efficient , this increase s the valu e of neura l
network s as psychologica l models , and make s them usefu l tool s for creatin g a wide variet y of
high­performanc e applications .
Sectio n 19.2 . Neura l Network s 567
19.2 NEURA L NETWORK S
UNITS
LINKS
WEIGH T
ACTIVATIO N LEVE LA neura l networ k is compose d of a numbe r of nodes , or units , connecte d by links . Eac h link
has a numeri c weigh t associate d with it. Weight s are the primar y mean s of long­ter m storag e
in neura l networks , and learnin g usuall y take s plac e by updatin g the weights . Som e of the unit s
are connecte d to the externa l environment , and can be designate d as inpu t or outpu t units . The
weight s are modifie d so as to try to brin g the network' s input/outpu t behavio r more into line with
that of the environmen t providin g the inputs .
Each unit has a set of inpu t link s from othe r units , a set of outpu t link s to othe r units , a
curren t activatio n level , and a mean s of computin g the activatio n leve l at the next step in time ,
given its input s and weights . The idea is that each unit does a local computatio n base d on input s
from its neighbors , but withou t the need for any globa l contro l over the set of unit s as a whole .
In practice , mos t neura l networ k implementation s are in softwar e and use synchronou s contro l
to updat e all the unit s in a fixed sequence .
To buil d a neura l networ k to perfor m som e task , one mus t first decid e how man y unit s are
to be used , wha t kind of unit s are appropriate , and how the unit s are to be connecte d to form a
network . One then initialize s the weight s of the network , and train s the weight s usin g a learnin g
algorith m applie d to a set of trainin g example s for the task.3 The use of example s also implie s
that one mus t decid e how to encod e the example s in term s of input s and output s of the network.
Notatio n
Neura l network s have lots of pieces , and to refe r to them we will need to introduc e a variet y of
mathematica l notations . For convenience , thes e are summarize d in Figur e 19.3 .
INPU T FUNCTIO N
ACTIVATIO N
FUNCTIO NSimpl e computin g element s
Figur e 19.4 show s a typica l unit . Eac h unit perform s a simpl e computation : it receive s signal s
from its inpu t link s and compute s a new activatio n leve l that it send s alon g each of its outpu t
links . The computatio n of the activatio n level is base d on the value s of each inpu t signa l receive d
from a neighborin g node , and the weight s on each inpu t link . Th e computatio n is spli t into
two components . Firs t is a linear  component , calle d the inpu t function , int, that compute s the
weighte d sum of the unit' s inpu t values . Secon d is a nonlinear  componen t calle d the activatio n
function , g, that transform s the weighte d sum into the fina l valu e that serve s as the unit' s activatio n
value , a,. Usually , all unit s in a networ k use the sam e activatio n function . Exercis e 19.3 explain s
why it is importan t to have a nonlinea r component .
The total weighte d inpu t is the sum of the inpu t activation s time s thei r respectiv e weights :
3 In this chapter , we will assum e that all example s are labelle d with the correc t outputs . In Chapte r 20, we will see how
to relax this assumption .
568 Chapte r 19 . Learnin g in Neura l and Belie f Network s
Notatio n
at
a,
g
s'
Err­,
Erf
I,
Ir
in­,
N
O
Oi
O
t
T
T
T
W/,i
Wi
w,
wMeanin g
Activatio n valu e of unit i (also the outpu t of the unit )
Vecto r of activatio n value s for the input s to unit i
Activatio n function1
Derivativ e of the activatio n functio n
Error (differenc e betwee n outpu t and target ) for unit /
Error for exampl e e
Activatio n of a unit / in the inpu t laye r
Vecto r of activation s of all inpu t unit s
Vecto r of input s for exampl e e
Weighte d sum of input s to unit ;'
Total numbe r of unit s in the networ k
Activatio n of the singl e outpu t unit of a perceptro n
Activatio n of a unit ;' in the outpu t laye r
Vecto r of activation s of all unit s in the outpu t laye r
Threshol d for a step functio n
Targe t (desired ) outpu t for a perceptro n
Targe t vecto r whe n there are severa l outpu t unit s
Targe t vecto r for exampl e e
Weigh t on the link from unit y to unit i
Weigh t from unit r to the outpu t in a perceptro n
Vecto r of weight s leadin g into unit i
Vecto r of all weight s in the networ k
Figur e 19.3 Neura l networ k notation . Subscript s denot e units ; superscript s denot e examples .
Input  Activation  „ .r, ' . ^  .  Output/• unction  r  unction  'Output
Links
Figur e 19.4 A  unit .
Sectio n 19.2 . Neura l Network s 569
wher e the fina l expressio n illustrate s the use of vecto r notation . In this notation , the weight s on
links into node i are denote d by W, the set of inpu t value s is calle d a,, and the dot produc t denote s
the sum of the pairwis e products .
The elementar y computatio n step in each unit compute s the new activatio n valu e for the
unit by applyin g the activatio n function , g, to the resul t of the inpu t function :
g(irii)  = g
Differen t model s are obtaine d by usin g differen t mathematica l function s for g. Thre e commo n
choice s are the step, sign , and sigmoi d functions , illustrate d in Figur e 19.5 . The step functio n
has a threshol d t such that it output s a 1 whe n the inpu t is greate r than its threshold , and output s
a 0 otherwise . The biologica l motivatio n is that a 1 represent s the firin g of a puls e dow n the
axon , and a 0 represent s no firing . The threshol d represent s the minimu m total weighte d inpu t
necessar y to caus e the neuro n to fire. Threshol d version s of the sign and sigmoi d function s can
also be defined .
In mos t cases , we will find it mathematicall y convenien t to replac e the threshol d with an
extra inpu t weight . This allow s for a simple r learnin g elemen t becaus e it need only worr y abou t
adjustin g weights , rathe r than adjustin g both weight s and thresholds . Thus , instea d of havin g a
threshol d t for each unit, we add an extra inpu t whos e activatio n GO is fixed at ­1 . The extra weigh t
WQ,, associate d with OQ serve s the functio n of a threshol d at t, provide d that WQJCIQ  = —t. The n
all unit s can have a fixed threshol d at 0. Mathematically , the two representation s for threshold s
are entirel y equivalent :
a, = step, = step 0 wher e Wo,,­ = t and OQ = ­ 1
+1 +1
in,
(a) Step functio n (b) Sign functio n (c) Sigmoi d functio n
Figur e 19.5 Thre e differen t activatio n function s for units .
. , ,l,ifx>t  .  . . (+l,ifx>0step,(jc ) = ' ­  ­.­­'.*­  i —. ..,  ^sigmoid W =1 +e~*
570 Chapte r 19 . Learnin g in Neura l and Belie f Network s
We can get a feel for the operatio n of individua l unit s by comparin g them with logic gates .
One of the origina l motivation s for the desig n of individua l unit s (McCulloc h and Pitts , 1943 ) was
their abilit y to represen t basic Boolea n functions . Figur e 19.6 show s how the Boolea n function s
AND,  OR,  and NOT  can be represente d by unit s with suitabl e weight s and thresholds . Thi s is
importan t becaus e it mean s we can use thes e unit s to buil d a networ k to comput e any Boolea n
functio n of the inputs .
W= 1 W= 1
W= 1 W= 1
AND OR NOT
Figur e 19.6 Unit s with a step functio n for the activatio n functio n can act as logic gates , give n
appropriat e threshold s and weights .
Networ k structure s
There are a variet y of kind s of networ k structure , each of whic h result s in very differen t com ­
FEED­FORWAR D putationa l properties . The main distinctio n to be mad e is betwee n feed­forwar d and recurren t
RECURREN T networks . In a feed­forwar d network , link s are unidirectional , and ther e are no cycles . In a
recurren t network , the link s can form arbitrar y topologies . Technicall y speaking , a feed­forwar d
networ k is a directe d acycli c grap h (DAG) . We will usuall y be dealin g with network s that are
arrange d in layers . In a layere d feed­forwar d network , each unit is linke d only to units in the next
layer ; there are no links betwee n unit s in the same layer , no links backwar d to a previou s layer ,
and no links that skip a layer . Figur e 19.7 show s a very simpl e exampl e of a layere d feed­forwar d
network . Thi s networ k has two layers ; becaus e the inpu t unit s (squar e nodes ) simpl y serv e to
pass activatio n to the next layer , they are not counte d (althoug h some author s woul d describ e this
as a three­laye r network) .
The significanc e of the lack of cycle s is that computatio n can procee d uniforml y from inpu t
units to outpu t units . The activatio n from the previou s time step play s no part in the computation ,
becaus e it is not fed back to an earlie r unit . Hence , a feed­forwar d networ k simpl y compute s a
functio n of the inpu t value s that depend s on the weigh t settings—i t has no internal  state  othe r
than the weight s themselves . Suc h network s can implemen t adaptiv e version s of simpl e refle x
agent s or they can functio n as component s of more comple x agents . In this chapter , we will focu s
on feed­forwar d network s becaus e they are relativel y well­understood .
Obviously , the brain canno t be a feed­forwar d network , else we woul d have no short­ter m
memory . Som e region s of the brai n are largel y feed­forwar d and somewha t layered , but there
are rampan t back­connections . In our terminology , the brain is a recurren t network . Becaus e
activatio n is fed back to the unit s that cause d it, recurren t network s have interna l state store d in
the activatio n level s of the units . Thi s also mean s that computatio n can be muc h less orderly
Sectio n 19.2 . Neura l Network s 571
HOPFIEL D
NETWORK S
ASSOCIATIV E
MEMOR Y
BOLTZMAN N
MACHINE S
INPU T UNIT S
OUTPU T UNIT S
HIDDE N UNIT S
PERCEPTRON S
MULTILAYE R
NETWORK Sthan in feed­forwar d networks . Recurren t network s can becom e unstable , or oscillate , or exhibi t
chaoti c behavior . Give n som e inpu t values , it can take a long time to comput e a stabl e output ,
and learnin g is mad e mor e difficult . On the othe r hand , recurren t network s can implemen t more
comple x agen t design s and can mode l system s with state . Becaus e recurren t network s requir e
some quit e advance d mathematica l methods , we can only provid e a few pointer s here .
Hopfiel d network s are probabl y the best­understoo d class of recurren t networks . The y
use bidirectional  connection s with symmetric  weight s (i.e. , W/./ = W/,,­) ; all of the unit s are both
input and outpu t units ; the activatio n functio n g is the sign function ; and the activatio n level s can
only be ± 1. A Hopfiel d networ k function s as an associativ e memory —afte r trainin g on a set of
examples , a new stimulu s will caus e the networ k to settl e into an activatio n patter n correspondin g
to the exampl e in the trainin g set that most  closely  resembles  the new stimulus . For example , if
the trainin g set consist s of a set of photographs , and the new stimulu s is a smal l piec e of one of
the photographs , then the networ k activatio n level s will reproduc e the photograp h from whic h the
piece was taken . Notic e that the origina l photograph s are not store d separatel y in the network ;
each weigh t is a partia l encodin g of all the photographs . One of the mos t interestin g theoretica l
result s is that Hopfiel d network s can reliabl y store up to 0.1387 V trainin g examples , wher e N is
the numbe r of unit s in the network .
Boltzman n machine s also use symmetri c weights , but includ e unit s that are neithe r inpu t
nor outpu t unit s (cf. the unit s labelle d HT, and H$ in Figur e 19.7) . The y also use a stochastic
activatio n function , such that the probabilit y of the outpu t bein g 1 is som e functio n of the total
weighte d input . Boltzman n machine s therefor e underg o state transition s that resembl e a simulate d
annealin g searc h for the configuratio n that best approximate s the trainin g set (see Chapte r 4).
It turn s out that Boltzman n machine s are formall y identica l to a specia l case of belie f network s
evaluate d with a stochasti c simulatio n algorith m (see Sectio n 15.4) .
Returnin g to feed­forwar d networks , ther e is one mor e importan t distinctio n to be made .
Examin e Figur e 19.7 , whic h show s the topolog y of a very simpl e neura l network . On the left are
the inpu t units . The activatio n valu e of each of these unit s is determine d by the environment . At
the right­han d end of the networ k are four outpu t units . In between , the node s labelle d H^ and
//4 have no direc t connectio n to the outsid e world . Thes e are calle d hidde n units , becaus e they
canno t be directl y observe d by notin g the input/outpu t behavio r of the network . Som e networks ,
called perceptrons , have no hidde n units . Thi s make s the learnin g proble m muc h simpler , but
it mean s that perceptron s are very limite d in wha t they can represent . Network s with one or
more layer s of hidde n unit s are calle d multilaye r networks . Wit h one (sufficientl y large ) laye r
of hidde n units , it is possibl e to represen t any continuou s functio n of the inputs ; with two layers ,
even discontinuou s function s can be represented .
With a fixe d structur e and fixe d activatio n function s g, the function s representabl e by a
feed­forwar d networ k are restricte d to hav e a specifi c parameterize d structure . Th e weight s
chose n for the networ k determin e whic h of these function s is actuall y represented . For example ,
the networ k in Figur e 19.7 calculate s the followin g function :
(19.1 )
wher e g is the activatio n function , and a, is the outpu t of node i. Notic e that becaus e the activatio n
function s g are nonlinear , the whol e networ k represent s a comple x nonlinea r function . If you
572 Chapte r 19 . Learnin g in Neura l and Belie f Network s
NONLINEA R
REGRESSIO NFigur e 19.7 A  very simple , two­layer , feed­forwar d networ k with two inputs , two hidde n
nodes , and one outpu t node .
think of the weight s as parameter s or coefficient s of this function , then learning  just becomes  a
process  of tuning  the parameters  to fit the data in  the training  set—a proces s that statistician s
call nonlinea r regression . From the statistica l viewpoint , this is wha t neura l network s do.
NERF SOptima l networ k structur e
So far we have considere d network s with a fixed structure , determine d by som e outsid e authority .
This is a potentia l weak  point , becaus e the wron g choic e of networ k structur e can lead to poor
performance . If we choos e a networ k that is too small , then the mode l will be incapabl e of
representin g the desire d function . If we choos e a networ k that is too big, it will be able to
memoriz e all the example s by formin g a large looku p table , but will not generaliz e well to input s
that have not been seen before . In othe r words , like all statistica l models , neura l network s are
subjec t to overfittin g whe n there are too man y parameter s (i.e. , weights ) in the model . We saw
this in Figur e 18.2 (pag e 530) , wher e the high­paramete r model s (b) and (c) fit all the data , but
may not generaliz e as well as the low­paramete r mode l (d).
It is know n that a feed­forwar d networ k with one hidde n laye r can approximat e any
continuou s functio n of the inputs , and a networ k with two hidde n layer s can approximat e any
functio n at all. However , the numbe r of unit s in each laye r may grow exponentiall y with the
numbe r of inputs . As yet, we have no good theor y to characteriz e NERFs , or Networ k Efficientl y
Representabl e Functions—function s that can be approximate d with a smal l numbe r of units .
We can thin k of the proble m of findin g a good networ k structur e as a searc h problem . One
approac h that has been used is to use a geneti c algorith m (Chapte r 20) to searc h the spac e of
networ k structures . However , this is a very large space , and evaluatin g a state in the spac e mean s
runnin g the whol e neura l networ k trainin g protocol , so this approac h is very CPU­intensive .
Therefore , it is mor e commo n to see hill­climbin g searche s that selectivel y modif y an existin g
networ k structure . Ther e are two way s to do this: start with a big networ k and mak e it smaller ,
or start with a smal l one and mak e it bigger .
The zip code readin g networ k describe d on page 586 uses an approac h calle d optima l
brain damag e to remov e weight s from the initia l fully­connecte d model . Afte r the networ k is
Sectio n 19.3 . Perceptron s 57 3
initiall y trained , an informatio n theoreti c approac h identifie s an optima l selectio n of connection s
that can be droppe d (i.e., the weight s are set to zero) . The networ k is then retrained , and if it is
performin g as well or better , the proces s is repeated . Thi s proces s was able to eliminat e 3/4 of
the weights , and improv e overal l performanc e on test data . In additio n to removin g connections ,
it is also possibl e to remov e units , that are not contributin g muc h to the result .
Severa l algorithm s have been propose d for growin g a large r networ k from a smalle r one.
The tilin g algorith m (Mezar d and Nadal , 1989 ) is interestin g becaus e it is simila r to the decisio n
tree learnin g algorithm . The idea is to start with a singl e unit that does its best to produc e the
correc t outpu t on as man y of the trainin g example s as possible . Subsequen t unit s are adde d to
take care of the example s that the first unit got wrong . The algorith m adds only as man y unit s as
are neede d to cove r all the examples .
The cross­validatio n technique s of Chapte r 18 are usefu l for decidin g whe n we have foun d
a networ k of the righ t size.
19.3 PERCEPTRON S
Layere d feed­forwar d network s were first studie d in the late 1950 s unde r the nam e perceptrons .
Althoug h network s of all sizes and topologie s were considered , the only effectiv e learnin g elemen t
at the time was for single­layere d networks , so that is wher e mos t of the effor t was spent . Today ,
the nam e perceptro n is used as a synony m for a single­layer , feed­forwar d network . The left­han d
side of Figur e 19.8 show s such a perceptro n network . Notic e that each outpu t unit is independen t
of the other s — each weigh t only affect s one of the outputs . That mean s that we can limi t our
study to perceptron s with a singl e outpu t unit , as in the right­han d side of Figur e 19.8 , and use
severa l of them to buil d up a multi­outpu t perceptron . For convenience , we can drop subscripts ,
denotin g the outpu t unit as O and the weigh t from inpu t unit y to O as Wj. The activatio n of inpu t
unity is give n by //. The activatio n of the outpu t unit is therefor e
O = Step 0 W^ = Ste PQ(Vi ­I) (19.2 )
where , as discusse d earlier , we have assume d an additiona l weigh t Wo to provid e a threshol d for
the step function , with IQ = — 1 .
Wha t perceptron s can represen t
We saw in Figur e 19.6 that unit s can represen t the simpl e Boolea n function s AND , OR, and
NOT , and that therefor e a feed­forwar d networ k of unit s can represen t any Boolea n function , if
we allo w for enoug h layer s and units . Bu t wha t Boolea n function s can be represente d with a
single­laye r perceptron ?
Some comple x Boolea n function s can be represented . For example , the majorit y function ,
whic h output s a 1 only if more than half of its n input s are 1, can be represente d by a perceptro n
with each W, = 1 and threshol d / = «/2. Thi s woul d requir e a decisio n tree with O(2" ) nodes .
574 Chapte r 19 . Learnin g in Neura l and Belie f Network s
Wj
Input
UnitsOutpu t
UnitsInput
UnitsO
Outpu t
Unit
Perceptro n Networ k Singl e Perceptro n
Figur e 19.8 Perceptrons .
LINEARL Y
SEPARABL EThe perceptron , with 1 uni t and n weights , give s a muc h mor e compac t representatio n of this
function . In accordanc e with Ockham' s razor , we woul d expec t the perceptro n to do a muc h
bette r job of learnin g a majorit y function , as we will soon see.
Unfortunately , it turn s out that perceptron s are severel y limite d in the Boolea n function s
they can represent . The proble m is that any inpu t lj can only influenc e the fina l outpu t in one
direction , no matte r what  the othe r inpu t value s are. Conside r some inpu t vecto r a. Suppos e that
this vecto r has a/ = 0 and that the vecto r produce s a 0 as output . Furthermore , suppos e that whe n
Oj is replace d with 1, the outpu t change s to 1. Thi s implie s that Wj mus t be positive . It also
implie s that there can be no inpu t vecto r b for whic h the outpu t is 1 whe n bj = 0, but the outpu t is
0 whe n bj is replace d with 1. Becaus e this limitatio n applie s to each input , the resul t is a sever e
limitatio n in the total numbe r of function s that can be represented . For example , the perceptro n
is unabl e to represen t the functio n for decidin g whethe r or not to wait for a table at a restauran t
(show n as a decisio n tree in Figur e 18.4) .
A little geometr y help s mak e clea r wha t is goin g on. Figur e 19.9 show s thre e differen t
Boolea n function s of two inputs , the AND , OR, and XOR functions . Each functio n is represente d
as a two­dimensiona l plot, base d on the value s of the two inputs . Blac k dots indicat e a poin t
in the inpu t spac e wher e the valu e of the functio n is 1, and whit e dots indicat e a poin t wher e
the valu e is 0. As we will explai n shortly , a perceptro n can represen t a functio n only if there is
some line that separate s all the whit e dots from the blac k dots . Suc h function s are calle d linearl y
separable . Thus , a perceptro n can represen t AND and OR, but not XOR .
Sectio n 19.3 . Perceptron s 575
""h1C) \
A f\UCJ
0
(a) /,
Figur e 19.97> IJ
• .  Hi '  I
> •  H
k ^  _­ . r \ ^> O
N ^  ­
1 / 2 '' 0  ''•­. . 1/ 2 0  1/ 2
and 7 2 (b ) 7, or 72 (c ) I\ xor 72
Linea r separabilit y in perceptrons .
The fact that a perceptro n can only represen t linearl y separabl e function s follow s directl y
from Equatio n (19.2) , whic h define s the functio n compute d by a perceptron . A perceptro n output s
a 1 only if W • I > 0. This mean s that the entir e inpu t spac e is divide d in two alon g a boundar y
define d by W • I  = 0, that is, a plan e in the inpu t spac e with coefficient s give n by the weights .
With n inputs , the inpu t spac e is n­dimensional , and linea r separabilit y can be rathe r hard to
visualiz e if n is too large . It is easies t to understan d for the case wher e n = 2. In Figur e 19.9(a) ,
one possibl e separatin g "plane " is the dotte d line define d by the equatio n
=­/ 2 or
The regio n abov e the line, wher e the outpu t is 1 , is therefor e give n by
­1.5 +/, +/ 2>0
or, in vecto r notation ,
>0
With three inputs , the separatin g plan e can still be visualized . Figur e 19.10(a ) show s an exampl e
in thre e dimensions . The functio n we are tryin g to represen t is true if and only if a minorit y of
its three input s are true . The shade d separatin g plan e is define d by the equatio n
/I +/2 + /3 = 1.5
This time the positiv e output s lie belo w the plane , in the regio n
(­/,) + (­/2) + (­/ 3)>­1. 5
Figur e 19.10(b ) show s a unit to implemen t the function .
Learnin g linearl y separabl e function s
As with any performanc e element , the questio n of wha t perceptron s can represen t is prio r to
the questio n of wha t they can learn . We have just seen that a functio n can be represente d by a
perceptro n if and only if it is linearl y separable . Tha t is relativel y bad news , becaus e ther e are
not man y linearl y separabl e functions . The (relatively ) goo d new s is that there  is a perceptron
algorithm  that will learn  any linearly  separable  function,  given  enough  training examples.
576 Chapte r 19 . Learnin g in Neura l and Belie f Network s
sk­­.o
(a) Separatin g plan e (b) Weight s and threshol d
Figur e 19.1 0 Linea r separabilit y in three dimensions—representin g the "minority " function .
EPOCH S
LEARNIN G RAT E
PERCEPTRO N
LEARNIN G RUL EMost neura l networ k learnin g algorithms , includin g the perceptro n learnin g method , follo w
the current­best­hypothesi s (CBH ) schem e describe d in Chapte r 18. In this case , the hypothesi s
is a network , define d by the curren t value s of the weights . The initia l networ k has randoml y
assigne d weights , usuall y from the rang e [­0.5,0.5] . The networ k is then update d to try to mak e
it consisten t with the examples . Thi s is don e by makin g smal l adjustment s in the weight s to
reduc e the differenc e betwee n the observe d and predicte d values . Th e mai n differenc e from
the logica l algorithm s is the need to repea t the updat e phas e severa l time s for each exampl e in
order to achiev e convergence . Typically , the updatin g proces s is divide d into epochs . Eac h
epoch involve s updatin g all the weight s for all the examples . The genera l schem e is show n as
NEURAL­NETWORK­LEARNIN G in Figur e 19.11 .
For perceptrons , the weigh t updat e rule is particularl y simple . If the predicte d outpu t for
the singl e outpu t unit is O, and the correc t outpu t shoul d be T, then the error is give n by
Err = T-O
If the error is positive , then we need to increas e O; if it is negative , we need to decreas e 0. Now
each inpu t unit contribute s Wjlj to the total input , so if// is positive , an increas e in Wj will tend to
increas e O, and if// is negative , an increas e in Wj will tend to decreas e O. Thus , we can achiev e
the effec t we wan t with the followin g rule:
Wj <­ Wj + a x lj x Err
wher e the term a is a constan t calle d the learnin g rate . Thi s rule is a sligh t varian t of the
perceptro n learnin g rule propose d by Fran k Rosenblat t in 1960 . Rosenblat t prove d that a
learnin g syste m usin g the perceptro n learnin g rule will converg e to a set of weight s that correctl y
represent s the examples , as long as the example s represen t a linearl y separabl e function .
The perceptro n convergenc e theore m create d a goo d deal of excitemen t whe n it was
announced . Peopl e were amaze d that such a simpl e procedur e coul d correctl y lear n any rep­
resentabl e function , and ther e wer e grea t hope s that intelligen t machine s coul d be buil t from
Sectio n 19.3 . Perceptron s 577
functio n NEURAL­NETWORK­LEARNiNG(e.ram/7fe,s ) return s network
network  <— a networ k with randoml y assigne d weight s
repea t
for each e in examples  do ,
O <— NEURAL­NETWORK­OUTPUT(«em>fc>rA: , e)
T *— the observe d outpu t value s from e
updat e the weight s in network  base d on e, O, and T
end
until all example s correctl y predicte d or stoppin g criterio n is reache d
retur n network
Figur e 19.1 1 Th e generi c neura l networ k learnin g method : adjus t the weight s unti l predicte d
outpu t value s O and true value s T agree .
LOCA L ENCODIN G
DISTRIBUTE D
ENCODIN Gperceptrons . It was not unti l 1969 that Minsk y and Paper t undertoo k wha t shoul d have been the
first step : analyzin g the clas s of representabl e functions . Thei r book Perceptrons  (Minsk y and
Papert , 1969 ) clearl y demonstrate d the limit s of linearl y separabl e functions .
In retrospect , the perceptro n convergenc e theore m shoul d not have been surprising . The
perceptro n is doin g a gradien t descen t searc h throug h weigh t spac e (see Chapte r 4). It is fairl y
easy to show that the weigh t spac e has no loca l minima . Provide d the learnin g rate paramete r is
not so large as to caus e "overshooting, " the searc h will converg e on the correc t weights . In short ,
perceptro n learnin g is easy becaus e the spac e of representabl e function s is simple .
We can examin e the learnin g behavio r of perceptron s usin g the metho d of constructin g
learnin g curves , as describe d in Chapte r 18. Ther e is a sligh t differenc e betwee n the exampl e
description s used for neura l network s and thos e used for othe r attribute­base d method s such as
decisio n trees . In a neura l network , all input s are real number s in som e fixe d range , wherea s
decisio n trees allo w for multivalue d attribute s with a discret e set of values . For example , the
attribut e for the numbe r of patron s in the restauran t has value s None,  Some,  and Full.  Ther e are
two way s to handl e this. In a local encoding , we use a singl e inpu t unit and pick an appropriat e
numbe r of distinc t value s to correspon d to the discret e attribut e values . For example , we can use
None  = 0.0, Some =  0.5, and Full  = 1.0. In a distribute d encoding , we use one inpu t unit for
each valu e of the attribute , turnin g on the unit that correspond s to the correc t value .
Figur e 19.1 2 show s the learnin g curv e for a perceptro n on two differen t problems . On
the left, we show the curv e for learnin g the majorit y functio n with 11 Boolea n input s (i.e. , the
functio n output s a 1 if 6 or mor e input s are 1). As we woul d expect , the perceptro n learn s the
functio n quit e quickl y becaus e the majorit y functio n is linearl y separable . On the othe r hand ,
the decisio n tree learne r make s no progress , becaus e the majorit y functio n is very hard (althoug h
not impossible ) to represen t as a decisio n tree. On the righ t of the figure , we have the opposit e
situation . Th e WillWait  proble m is easil y represente d as a decisio n tree , but is not linearl y
separable . The perceptro n algorith m draw s the best plan e it can throug h the data , but can manag e
no more than 65% accuracy .
578 Chapte r 19 . Learnin g in Neura l and Belie f Network s
1
0.9
'f.
83 °'8
C
£ 0.7
s
8 0.6
0.5
0.4
(«.~*^°""°'~v
j
­ _ , Perceptro n •>
f Decisio n tree ­­­­ ­
10 20 30 40 50 60 70 80 90 1 C1
0.9
CJ
? 0.86
Z 0.7
a
8 0.6
0.5
0.4
)0 C., " .'.­,',• — V.»AV"'*'*i""*~'Vi *
Perceptro n »
L Decisio n tree ——
": '° \
­ ^ ^^­­"­ ­ "  '  V
10 20 30 40 50 60 70 80 90 10 0
Trainin g set size Trainin g set size
(a) (b )
Figur e 19.12 Comparin g the performanc e of perceptron s and decisio n trees , (a) Perceptron s
are bette r at learnin g the majorit y functio n of 1 1 inputs , (b) Decisio n trees are bette r at learnin g
the WiHWait  predicat e for the restauran t example .
19.4 MULTILAYE R FEED­FORWAR D NETWORK S
BACK­PROPAGATIO NRosenblat t and other s describe d multilaye r feed­forwar d network s in the late 1950s , but concen ­
trated thei r researc h on single­laye r perceptrons . Thi s was mainl y becaus e of the difficult y of
findin g a sensibl e way to updat e the weight s betwee n the input s and the hidde n units ; wherea s
an erro r signa l can be calculate d for the outpu t units , it is harde r to see wha t the erro r signa l
shoul d be for the hidde n units . Whe n the book Perceptrons  was published , Minsk y and Paper t
(1969 ) state d that it was an "importan t researc h problem " to investigat e multilaye r network s more
thoroughly , althoug h they speculate d that "ther e is no reaso n to suppos e that any of the virtue s
[of perceptrons ] carry over to the many­layere d version. " In a sense , they were right . Learnin g
algorithm s for multilaye r network s are neithe r efficien t nor guarantee d to converg e to a globa l
optimum . On the othe r hand , the result s of computationa l learnin g theor y tell us that learnin g
genera l function s from example s is an intractabl e proble m in the wors t case , regardles s of the
method , so we shoul d not be too dismayed .
The mos t popula r metho d for learnin g in multilaye r network s is calle d back­propagation .
It was first invente d in 196 9 by Bryso n and Ho, but was mor e or less ignore d unti l the mid ­
1980s . The reason s for this may be sociological , but may also have to do with the computationa l
requirement s of the algorith m on nontrivia l problems .
Back­Propagatio n Learnin g
Suppos e we wan t to construc t a networ k for the restauran t problem . We hav e alread y seen
that a perceptro n is inadequate , so we will try a two­laye r network . We have ten attribute s
Sectio n 19.4 . Multilaye r Feed­Forwar d Network s 579
Outpu t units
Hidde n units
Input units /,
Figur e 19.1 3 A  two­laye r feed­forwar d networ k for the restauran t problem .
describin g each example , so we will need ten inpu t units . How man y hidde n unit s are needed ?
In Figur e 19.13 , we show a networ k with four hidde n units . This turn s out to be abou t right for
this problem . The proble m of choosin g the righ t numbe r of hidde n unit s in advanc e is still not
well­understood . We cove r wha t is know n on page 572.
Learnin g in such a networ k proceed s the same way as forperceptrons : exampl e input s are
presente d to the network , and if the networ k compute s an outpu t vecto r that matche s the target ,
nothin g is done . If there is an error (a differenc e betwee n the outpu t and target) , then the weight s
are adjuste d to reduc e this error . The  trick is to assess  the blame  for an error  and divide  it among
the contributing  weights.  In perceptrons , this is easy , becaus e ther e is only one weigh t betwee n
each inpu t and the output . But in multilaye r networks , ther e are man y weight s connectin g each
input to an output , and each of these weight s contribute s to more than one output .
The back­propagatio n algorith m is a sensibl e approac h to dividin g the contributio n of each
weight . As in the perceptro n learnin g algorithm , we try to minimiz e the error betwee n each targe t
outpu t and the outpu t actuall y compute d by the network.4 At the outpu t layer , the weigh t updat e
rule is very simila r to the rule for the perceptron . Ther e are two differences : the activatio n of the
hidde n unit a/ is used instea d of the inpu t value ; and the rule contain s a term for the gradien t of
the activatio n function . If £rr, is the error (Tt ­ O{) at the outpu t node , then the weigh t updat e
rule for the link from unit y to unit i is
Wjj *— W,;, + a x dj x Erri  x g'(ini)
wher e g' is the derivativ e of the activatio n functio n g. We will find it convenien t to defin e a new
error term A,, whic h for outpu t node s is define d as A, = Err/g'O'/z,­) . The updat e rule then become s
Wj,t <­ Wjj + a xqx  Ai (19.3 )
For updatin g the connection s betwee n the inpu t unit s and the hidde n units , we need to defin e
a quantit y analogou s to the erro r term for outpu t nodes . Her e is wher e we do the erro r back ­
propagation . The idea is that hidde n node j is "responsible " for som e fractio n of the error A, in
Actually , we minimiz e the squar e of the error ; Sectio n 19.4 explain s why , but the resul t is almos t the same .
580 Chapte r 19 . Learnin g in Neura l and Belie f Network s
TRAININ G CURV Eeach of the outpu t node s to whic h it connects . Thus , the A, value s are divide d accordin g to the
strengt h of the connectio n betwee n the hidde n node and the outpu t node , and propagate d back to
provid e the A, value s for the hidde n layer . The propagatio n rule for the A value s is the following :
A/ = e' (/'«,) > Wjj&j  (19.4 )j <>^.i'  / j .'• ' •  '
i
Now the weigh t updat e rule for the weight s betwee n the input s and the hidde n laye r is almos t
identica l to the updat e rule for the outpu t layer :
WkJ — W kj + Q x 7A­ x A/
The detaile d algorith m is show n in Figur e 19.14 . It can be summarize d as follows :
• Comput e the A value s for the outpu t units usin g the observe d error .
• Startin g with outpu t layer , repea t the followin g for each laye r in the network , unti l the
earlies t hidde n laye r is reached :
­ Propagat e the A value s back to the previou s layer .
­ Updat e the weight s betwee n the two layers .
Recal l that in computin g the observe d error for a give n example , NEURAL­NETWORK­LEARNIN G
first feed s the exampl e to the networ k input s in orde r to calculat e the predicte d outpu t values .
Durin g this computation , it is a good idea to save som e of the intermediat e value s compute d
in each unit . In particular , cachin g the activatio n gradien t #'(w/ ) in each unit speed s up the
subsequen t back­propagatio n phas e enormously .
Now that we have a learnin g metho d for multilaye r networks , we can test our claim that
addin g a hidde n laye r make s the networ k mor e expressive . In Figur e 19.15 , we show two curves .
The first is a trainin g curve , whic h show s the mea n square d erro r on a give n trainin g set of 100
restauran t example s durin g the weight­updatingprocess . This demonstrate s that the networ k does
indee d converg e to a perfec t fit to the trainin g data . The secon d curv e is the standar d learnin g
curve for the restauran t data , with one mino r exception : the y­axi s is no longe r the proportio n
of correc t answer s on the test set, becaus e sigmoi d unit s do not give 0/1 outputs . Instead , we
use the mea n square d erro r on the test set, whic h happen s to coincid e with the proportio n of
correc t answer s in the 0/1 case.  The curv e clearl y show s that the networ k is capabl e of learnin g
in the restauran t domain ; indeed , the curv e is very simila r to that for decision­tre e learning , albei t
somewha t shallower .
Back­propagatio n as gradien t descen t searc h
We have give n som e suggestiv e reason s why the back­propagatio n equation s are reasonable .
It turn s out that the equation s also can be give n a very simpl e interpretatio n as a metho d for
ERRO R SURFAC E performin g gradien t descen t in weigh t space . In this case , the gradien t is on the erro r surface :
the surfac e that describe s the erro r on each exampl e as a functio n of the all the weight s in the
network . An exampl e erro r surfac e is show n in Figur e 19.16 . The curren t set of weight s define s
a poin t on this surface . At that point , we look at the slop e of the surfac e alon g the axis forme d
by each weight . Thi s is know n as the partial  derivative  of the surfac e with respec t to each
weight—ho w muc h the erro r woul d chang e if we mad e a smal l chang e in weight . We then alter
Sectio n 19.4 . Multilaye r Feed­Forwar d Network s 581
functio n BACK­PROP­UPDATE(«efH>6rfc , examples,  a) return s a networ k with modifie d weight s
inputs : network,  a multilaye r networ k
examples,  a set of input/outpu t pairs
a, the learnin g rate
repea t
for each e in examples  do
/ * Compute  the output  for this example  * I
O <— RuN­NETWORK(«efworA: , F)
/ * Compute  the error  and A for units  in the  output  layer  * /
Err"­I ' ­ O
/ * Update  the weights  leading to  the output  layer  * /
Wjj — W/.i + o x cij x Err'­  x g'(in,)
for each subsequen t laye r in net\vork  do
/ * Compute  the error  at each  node  * /
A;^g'(^)£ , Wj.,b
/ * Update  the weights  leading  into the layer  * I
Wt.j <­ WLj + a  x /<­ x Ay
end
end
until network  has converge d
retur n network
Figur e 19.1 4 Th e back­propagatio n algorith m for updatin g weight s in a multilaye r network .Total erro r on trainin g set
oro­p^oocof^­f *
50 10 0 15 0 20 0 25 0 30 0 35 0 4C
Numbe r of epoch s
(a)
0
% correc t on test set
p p p p o p
4^ Lf t C ^ '­~t  0 0 s O —
*+ r­*7­'" ^ ' ^»y* ^ ; '­•", *^*^^i ^^­WS*''  .
f*
if*V»< # /**/f a *
f>­
<ri
­'! Multilaye r networ k ­ ^ —
"' ; Decisio n tree
) 1 0 20 30 40 50 60 70 80 90 1 00
Trainin g set size
(b)
Figur e 19.15 (a ) Trainin g curv e showin g the gradua l reductio n in erro r as weight s are modifie d
over severa l epochs , for a given set of example s in the restauran t domain , (b) Comparativ e learnin g
curve s for a back­propagatio n and decision­tre e learning .
582 Chapte r 19 . Learnin g in Neura l and Belie f Network s
Figur e 19.1 6 A n error surfac e for gradien t descen t searc h in weigh t space . Whe n w\ = a and
H>2 = b, the error on the trainin g set is minimized .
the weight s in an amoun t proportiona l to the slop e in each direction . This move s the networ k as
a whol e in the directio n of steepes t descen t on the error surface .
Basically , this is the key: back­propagation  provides  a way  of dividing  the calculation
of the gradient  among  the units,  so the change  in each  weight  can be calculated  by the unit  to
which  the weight  is attached,  using  only  local  information.  Lik e any gradien t descen t search ,
back­propagatio n has problem s with efficienc y and convergence , as we will discus s shortly .
Nonetheless , the decompositio n of the learnin g algorith m is a majo r step toward s parallelizabl e
and biologicall y plausibl e learnin g mechanisms .
For the mathematicall y inclined , we will now deriv e the back­propagatio n equation s from
first principles . We begi n with the error functio n itself . Becaus e of its convenien t form , we use
the sum of square d error s over the outpu t values :
The key insight , again , is that the outpu t value s 0, are a functio n of the weight s (see Equa ­
tion (19.1) , for example) . For a genera l two­laye r network , we can writ e
(19.5 )
Sectio n 19.4 . Multilaye r Feed­Forwar d Network s 58 3
Notic e that althoug h the a, term in the first line represent s a comple x expression , it does not
depen d on W, i(. Also , only one of the term s in the summatio n over / andy depend s on a particula r
Wjj, so all the othe r term s are treate d as constant s with respec t to W/,, and will disappea r whe n
differentiated . Hence , whe n we differentiat e the first line with respec t to Wjj, we obtai n
with A, define d as before . The derivatio n of the gradien t with respec t to W^j is slightl y mor e
complex , but has a simila r result :
dE
To obtai n the updat e rule s for the weights , we have to remembe r that the objec t is to minimize
the error , so we need to take a smal l step in the directio n opposite  to the gradient .
There is one mino r technica l observatio n to mak e abou t these updat e rules . The y requir e
the derivativ e of the activatio n functio n g, so we canno t use the sign or step functions . Back ­
propagatio n network s usuall y use the sigmoi d function , or som e varian t thereof . The sigmoi d
also has the convenien t propert y that the derivativ e g' = g(l — g), so that little extra calculatio n
is neede d to find g'(inj).
Discussio n
Let us stan d back for a momen t from the delightfu l mathematic s and the fascinatin g biology ,
and ask whethe r back­propagatio n learnin g in multilaye r network s is a good metho d for machin e
learning . We can examin e the same set of issue s that aros e in Chapte r 18:
<> Expressiveness : Neura l network s are clearl y an attribute­base d representation , and do not
have the expressiv e powe r of genera l logica l representations . The y are well­suite d for
continuou s input s and outputs , unlik e most decisio n tree systems . The class of multilaye r
network s as a whole  can represen t any desire d functio n of a set of attributes , but any
particular  networ k may have too few hidde n units . It turn s out that 2n/n hidde n unit s are
neede d to represen t all Boolea n function s of n inputs . Thi s shoul d not be too surprising .
Such a networ k has O(2")  weights , and we need at least  2" bits to specif y a Boolea n
function . In practice , mos t problem s can be solve d with man y fewe r weights . Designin g a
good topolog y is, however , a blac k art.
<> Computationa l efficiency : Computationa l efficienc y depend s on the amoun t of compu ­
tation time require d to train the networ k to fit a give n set of examples . If ther e are m
examples , and |W| weights , each epoc h take s O(m|W| ) time . However , wor k in computa ­
tiona l learnin g theor y has show n that the worst­cas e numbe r of epoch s can be exponentia l
in n, the numbe r of inputs . In practice , time to convergenc e is highl y variable , and a vast
array of technique s have been develope d to try to spee d up the proces s usin g an assortmen t
of tunabl e parameters . Loca l minim a in the erro r surfac e are also a problem . Network s
quite ofte n converg e to give a constan t "yes " or "no" output , whicheve r is mos t commo n
584 Chapte r 19 . Learnin g in Neura l and Belie f Network s
in the trainin g set. At the cost of som e additiona l computation , the simulate d annealin g
metho d (Chapte r 4) can be used to assur e convergenc e to a globa l optimum .
0 Generalization : As we have seen in our experiment s on the restauran t data, neura l network s
can do a good job of generalization . On e can say, somewha t circularly,  that they will
generaliz e well on function s for whic h they are well­suited . Thes e seem to be function s in
whic h the interaction s betwee n input s are not too intricate , and for whic h the outpu t varie s
smoothl y with the input . Ther e is no theore m to be prove d here , but it does seem that
neura l network s have had reasonabl e succes s in a numbe r of real­worl d problems .
<) Sensitivit y to noise : Becaus e neura l network s are essentiall y doin g nonlinea r regression ,
they are very toleran t of nois e in the inpu t data . The y simpl y find the best fit give n the
constraint s of the networ k topology . On the othe r hand , it is ofte n usefu l to have som e idea
of the degre e of certaint y of the outpu t values . Neura l network s do not provid e probabilit y
distribution s on the outpu t values . For this purpose , belie f network s seem more appropriate .
<) Transparency : Neura l network s are essentiall y blac k boxes . Eve n if the networ k does a
good job of predictin g new cases , man y user s will still be dissatisfie d becaus e they will
have no idea why  a give n outpu t valu e is reasonable . If the outpu t valu e represents , for
example , a decisio n to perfor m open hear t surgery , then an explanatio n is clearl y in order .
With decisio n trees and othe r logica l representations , the outpu t can be explaine d as a
logica l derivatio n and by appea l to a specifi c set of cases that support s the decision . This
is not currentl y possibl e with neura l networks .
<C> Prio r knowledge : As we mentione d in Chapte r 18, learnin g system s can ofte n benefi t
from prior knowledg e that is availabl e to the user or expert . Prio r knowledg e can mean the
differenc e betwee n learnin g from a few well­chose n example s and failin g to learn anythin g
at all. Unfortunately , becaus e of the lack of transparency , it is quit e hard to use one' s
knowledg e to "prime " a networ k to learn better . Som e tailorin g of the networ k topolog y
can be done—fo r example , whe n trainin g on visua l image s it is commo n to connec t only
small sets of nearb y pixel s to any give n unit in the first hidde n layer . On the othe r hand ,
such "rule s of thumb " do not constitut e a mechanism  by whic h previousl y accumulate d
knowledg e can be used to learn from subsequen t experience . It is possibl e that learnin g
method s for belie f network s can overcom e this proble m (see Sectio n 19.6) .
All thes e consideration s sugges t that simpl e feed­forwar d networks , althoug h very promisin g
as constructio n tool s for learnin g comple x input/outpu t mappings , do not fulfil l our need s for a
comprehensiv e theor y of learnin g in their presen t form . Researcher s in AI, psychology , theoretica l
compute r science , statistics , physics , and biolog y are workin g hard to overcom e the difficulties .
19.5 APPLICATION S OF NEURA L NETWORK S
In this section , we give just a few example s of the man y significan t application s of neura l
networks . In each case , the networ k desig n was the resul t of severa l month s of trial­and­erro r
experimentatio n by researchers . Fro m thes e examples , it can be seen that neura l network s have
Sectio n 19.5 . Application s of Neura l Network s 58 5
wide applicability , but that they canno t magicall y solv e problem s withou t any though t on the
part of the networ k designer . Joh n Denker' s remar k that "neura l network s are the secon d best
way of doin g just abou t anything " may be an exaggeration , but it is true that neura l network s
provid e passabl e performanc e on man y tasks that woul d be difficul t to solve explicitl y with other
programmin g techniques . We encourag e the reade r to experimen t with neura l networ k algorithm s
to get a feel for wha t happen s whe n data arriv e at an unprepare d network .
Pronunciatio n
Pronunciatio n of writte n Englis h text by a compute r is a fascinatin g proble m in linguistics , as
well as a task with high commercia l payoff . It is typicall y carrie d by first mappin g the text
stream to phonemes —basi c soun d elements—an d then passin g the phoneme s to an electroni c
speec h generator . The proble m we are concerne d with here is learnin g the mappin g from text
to phonemes . Thi s is a good task for neura l network s becaus e mos t of the "rules " are only
approximatel y correct . For example , althoug h the lette r "k" usuall y correspond s to the soun d [k],
the lette r "c" is pronounce d [k] in cat and [s] in cent.
The NETtal k progra m (Sejnowsk i and Rosenberg , 1987 ) is a neura l networ k that learn s to
pronounc e writte n text. The inpu t is a sequenc e of character s presente d in a windo w that slide s
throug h the text. At any time , the inpu t include s the characte r to be pronounce d alon g with the
precedin g and followin g three characters . Eac h characte r is actuall y 29 inpu t units—on e for each
of the 26 letters , and one each for blanks , periods , and othe r punctuation . Ther e were 80 hidde n
units in the versio n for whic h result s are reported . The outpu t laye r consist s of feature s of the
soun d to be produced : whethe r it is high or low, voice d or unvoiced , and so on. Sometimes , it
takes two or more letter s to produc e a singl e sound ; in this case , the correc t outpu t for the secon d
letter is nothing .
Trainin g consiste d of a 1024­wor d text that had been hand­transcribe d into the prope r
phonemi c features . NETtal k learn s to perfor m at 95% accurac y on the training  set after 50 passe s
throug h the trainin g data . One migh t thin k that NETtal k shoul d perfor m at 100 % on the text it
has traine d on. But any progra m that learn s individua l word s rathe r than the entir e text as a whol e
will inevitabl y score less than 100% . The difficult y arise s with word s like lead,  whic h in som e
cases shoul d be pronounce d to rhym e with bead  and sometime s like bed.  A progra m that look s
at only a limite d windo w will occasionall y get such word s wrong .
So muc h for the abilit y of the networ k to reproduc e the trainin g data . Wha t abou t the gen­
eralizatio n performance ? This is somewha t disappointing . On the test data , NETtalk' s accurac y
goes dow n to 78% , a level that is intelligible , but muc h wors e than commerciall y availabl e pro­
grams . Of course , the commercia l system s require d year s of development , wherea s NETtal k only
require d a few doze n hour s of trainin g time plus a few month s of experimentatio n with variou s
networ k designs . However , ther e are othe r technique s that requir e even less developmen t and
perfor m just as well . For example , if we use the inpu t to determin e the probabilit y of producin g
a particula r phonem e give n the curren t and previou s characte r and then use a Marko v mode l to
find the sequenc e of phoneme s with maxima l probability , we do just as well as NETtalk .
NETtal k was perhap s the "flagship " demonstratio n that converte d man y scientists , partic ­
ularl y in cognitiv e psychology , to the caus e of neura l networ k research . A post hoc analysi s
586 Chapte r 19 . Learnin g in Neura l and Belie f Network s
suggest s that this was not becaus e it was a particularl y successfu l program , but rathe r becaus e it
provide d a good showpiec e for the philosoph y of neura l networks . Its author s also had a flair for
the dramatic : they recorde d a tape of NETtal k startin g out with poor , babblin g speech , and then
graduall y improvin g to the poin t wher e the outpu t is understandable . Unlik e conventiona l speec h
generators , whic h use a midrang e teno r voic e to generat e the phonemes , they used a high­pitche d
generator . The tape give s the unmistakabl e impressio n of a child learnin g to speak .
Handwritte n characte r recognitio n
In one of the larges t application s of neura l network s to date , Le Cun et al. (1989 ) have imple ­
mente d a networ k designe d to read zip code s on hand­addresse d envelopes . The syste m uses a
preprocesso r that locate s and segment s the individua l digit s in the zipcode ; the networ k has to
identif y the digit s themselves . It uses a 16 x 16 array of pixel s as input , three  hidde n layers , and a
distribute d outpu t encodin g with 10 outpu t unit s for digit s 0­9. The hidde n layer s containe d 768,
192, and 30 units , respectively . A full y connecte d networ k of this size woul d contai n 200,00 0
weights , and woul d be impossibl e to train . Instead , the networ k was designe d with connection s
D!TECTOR S intende d to act as featur e detectors . For example , each unit in the first hidde n laye r was con­
necte d by 25 link s to a 5 x 5 regio n in the input . Furthermore , the hidde n laye r was divide d into
12 group s of 64 units ; withi n each grou p of 64 units , each unit used the same  set of 25 weights .
Henc e the hidde n laye r can detec t up to 12 distinc t features , each of whic h can occu r anywher e
in the inpu t image . Overall , the complet e networ k used only 9760 weights .
The networ k was traine d on 7300 examples , and teste d on 2000 . One interestin g propert y
of a networ k with distribute d outpu t encodin g is that it can displa y confusio n over the correc t
answe r by settin g two or mor e outpu t unit s to a high value . Afte r rejectin g abou t 12% of the test
set as marginal , usin g a confusio n threshold , the performanc e on the remainin g case s reache d
99%, whic h was deeme d adequat e for an automate d mail­sortin g system . The fina l networ k has
been implemente d in custo m VLSI , enablin g letter s to be sorte d at high speed .
Drivin g
ALVIN N (Autonomou s Land Vehicl e In a Neura l Network ) (Pomerleau , 1993 ) is a neura l networ k
that has performe d quite well in a domai n wher e som e othe r approache s have failed . It learn s to
steer a vehicl e alon g a singl e lane on a highwa y by observin g the performanc e of a huma n driver .
We describe d the syste m briefl y on page 26, but here we take a look unde r the hood .
ALVIN N is used to contro l the NavLa b vehicle s at Carnegi e Mello n University . NavLa b 1
is a Chev y van, and NavLa b 2 is a U.S. Arm y HMMW V personne l carrier . Bot h vehicle s are
speciall y outfitte d with computer­controlle d steering , acceleration , and braking . Sensor s includ e
color stere o video , scannin g laser rang e finders , radar , and inertia l navigation . Researcher s ride
along in the vehicl e and monito r the progres s of the compute r and the vehicl e itself . (Bein g insid e
the vehicl e is a big incentiv e to makin g sure  the progra m does not "crash." )
The signa l from the vehicle' s vide o camer a is preprocesse d to yield an array of pixel value s
that are connecte d to a 30 x 32 grid of inpu t unit s in a neura l network . The output  is a laye r of
30 units , each correspondin g to a steerin g direction . The outpu t unit with the highes t activatio n
Sectio n 19.5 . Application s of Neura l Network s 58 7
is the directio n that the vehicl e will steer . The networ k also has a laye r of five hidde n units that
are full y connecte d to the inpu t and outpu t layers .
ALVINN' S job is to comput e a functio n that map s from a singl e vide o imag e of the road
in fron t of it to a steerin g direction . To learn this function , we need som e trainin g data—som e
image/directio n pairs with the correc t direction . Fortunately , it is easy to collec t this data just
by havin g a human  drive the vehicl e and recordin g the image/directio n pairs . Afte r collectin g
abou t five minute s of trainin g data (and applyin g the back­propagatio n algorith m for abou t ten
minutes) , ALVIN N is read y to drive on its own .
One fine poin t is wort h mentioning . Ther e is a potentia l proble m with the methodolog y of
trainin g base d on a huma n driver : the huma n is too good . If the huma n neve r stray s from the
prope r cours e then there will be no trainin g example s that show how to recove r whe n you are off
course . ALVIN N correct s this proble m by rotatin g each vide o imag e to creat e additiona l view s of
what the road woul d look like from a positio n a little to the right or left.
The result s of the trainin g are impressive . ALVIN N has drive n at speed s up to 70 mph for
distance s up to 90 mile s on publi c highway s near Pittsburgh . It has also drive n at norma l speed s
on singl e lane dirt roads , pave d bike paths , and two lane suburba n streets .
ALVIN N is unabl e to drive on a road type for whic h it has not been trained , and is also not
very robus t with respec t to change s in lightin g condition s or the presenc e of othe r vehicles . A
more gerlera l capabilit y is exhibite d by the MANIA C syste m (Joche m et al., 1993) . MANIA C is a
neura l networ k that has as subnet s two or more ALVIN N model s that have each been traine d for
a particula r type of road . MANIA C take s the outpu t from each subne t and combine s them in a
secon d hidde n layer . Wit h suitabl e training , MANIA C can perfor m well on any of the road type s
for whic h the componen t subnet s have been trained .
Some previou s autonomou s vehicle s employe d traditiona l visio n algorithm s that used
variou s image­processin g technique s on the entir e scen e in orde r to find the road and then follo w
it. Such system s achieve d top speed s of 3 or 4 mph.5 Why has ALVIN N prove n to be successful ?
There are two reasons . Firs t and foremost , a neura l networ k of this size make s an efficien t
performanc e element . Onc e it has been trained , ALVIN N is able to comput e a new steerin g
directio n from a vide o imag e 10 time s a second . Thi s is importan t becaus e it allows  for som e
slack in the system . Individua l steerin g direction s can be off by 10% from the idea l as long
as the syste m is able to mak e a correctio n in a few tenth s of a second . Second , the use of a
learnin g algorith m is mor e appropriat e for this domai n than knowledg e engineerin g or straigh t
programming . Ther e is no good existin g theor y of driving , but it is easy to collec t sampl e
input/outpu t pairs of the desire d functiona l mapping . This argue s for a learnin g algorithm , but
not necessaril y for neura l nets. But drivin g is a continuous , nois y domai n in whic h almos t all
of the inpu t feature s contribut e some usefu l information ; this mean s that neura l nets are a bette r
choic e than , say, decisio n trees . Of course , ALVIN N and MANIA C are pure refle x agents , and
canno t execut e maneuver s that are muc h mor e comple x than lane­following , especiall y in the
presenc e of othe r traffic . Curren t researc h by Pomerlea u and othe r member s of the grou p is
aime d at combinin g ALVINN' S low­leve l expertis e with higher­leve l symboli c knowledge . Hybri d
system s of this kind are becomin g more commo n as AI move s into the real (physical ) world .
5 A notabl e exceptio n is the wor k by Dickmann s and Zapp (1987) , whos e autonomou s vehicl e drov e severa l hundre d
miles at 75 mph usin g traditiona l imag e processin g and Kalma n filterin g to track the lane boundaries .
588 Chapte r 19 . Learnin g in Neura l and Belie f Network s
19.6 BAYESIA N METHOD S FOR LEARNIN G BELIE F NETWORK S
Part V mad e the case for the importanc e of probabilisti c representation s of uncertai n knowledge ,
and presente d belie f network s as a genera l and usefu l performanc e elemen t base d on probabilit y
theory . In this section , we discus s the genera l proble m of learnin g probabilisti c knowledge , and
the specifi c proble m of learnin g belie f networks . We will see that a Bayesia n view of learnin g is
extremel y powerful , providin g genera l solution s to the problem s of noise , overfilling , and optima l
prediction . We will also find strikin g parallel s betwee n belie f network s and neura l network s in
their amenabilit y to local , gradient­descen t learnin g methods . Mos t of this sectio n is fairl y
mathematical , althoug h the genera l lesson s can be understoo d withou t plungin g into the details .
It may be helpfu l at this poin t to revie w the materia l in Chapter s 14 and 15.
BAYESIA N LEARNIN G
MAXIMU M A
POSTERIOR IBayesia n learnin g
Bayesia n learnin g view s the proble m of constructin g hypothese s from data as a subproble m of the
more fundamenta l proble m of makin g predictions . The idea is to use hypothese s as intermediarie s
betwee n data and predictions . First , the probabilit y of each hypothesi s is estimated , give n the
data. Prediction s are then mad e from the hypotheses , usin g the posterio r probabilitie s of the
hypothese s to weigh t the predictions . As a simpl e example , conside r the proble m of predictin g
tomorrow' s weather . Suppos e the availabl e expert s are divide d into two camps : som e propos e
mode l A, and som e propos e mode l B. The Bayesia n method , rathe r than choosin g betwee n A
and B, give s som e weigh t to each base d on their likelihood . The likelihoo d will depen d on how
much the know n data suppor t each of the two models .
Suppos e that we have data D and hypothese s H\,H 2, ••• , and that we are intereste d in
makin g a predictio n concernin g an unknow n quantit y X. Furthermore , suppos e that each //,•
specifie s a complet e distributio n for X. The n we have
P(X|D ) = V P(X\D,Hi)P(Hi\D)  = P(X\Hi)P(Hi\D)
This equatio n describe s full Bayesia n learning , and may requir e a calculatio n of P(//,­|D ) for all
Hi. In mos t cases , this is intractable ; it can be shown , however , that there is no bette r way to
make predictions .
The mos t commo n approximatio n is to use a most  probable  hypothesis , that is, an Hi that
maximize s P(//,|D) . This often calle d a maximu m a posterior i or MAP hypothesi s //MAP '
P(X|D ) w P(X|// MAp)P(//MAp!£> )
The proble m is now to find //MAP ­ By applyin g Bayes ' rule , we can rewrit e P(H f\D) as follows :
P(D\H i)P(H i)P(H,\D)  =P(D)
Notic e that in comparin g hypotheses , P(D)  remain s fixed . Hence , to find //MAP , we need only
maximiz e the numerato r of the fraction .
The first term , P(D|//,) , represent s the probabilit y that this particula r data set woul d have
been observed , give n //, as the underlyin g mode l of the world . The secon d term represent s the
Sectio n 19.6 . Bayesia n Method s for Learnin g Belie f Network s 589
MAXIMUM ­
LIKELIHOO Dprior probabilit y assigne d to the give n model . Argument s over the natur e and significanc e of this
prior probabilit y distribution , and its relatio n to preferenc e for simple r hypothese s (Ockham' s
razor) , have rage d unchecke d in the statistic s and learnin g communitie s for decades . The only
reasonabl e polic y seem s to be to assig n prior probabilitie s base d on som e simplicit y measur e on
hypotheses , such that the prio r of the entir e hypothesi s spac e adds up to 1. The mor e we bias
the prior s toward s simple r hypotheses , the more we will be immun e to noise and overrating . Of
course , if the prior s are too biased , then we get underfilling , wher e the data is largel y ignored .
There is a carefu l trade­of f lo make .
In som e cases , a unifor m prio r over belie f network s seem s to be appropriate , as we shal l
see. Wit h a unifor m prior , we need only choos e an //, that maximize s P(D|//,) . This is calle d a
maximum­likelihoo d (ML ) hypothesis , //ML ­
Belie f networ k learnin g problem s
The learnin g proble m for belie f network s come s in severa l varieties . The structur e of the networ k
can be known  or unknown,  and the variable s in the networ k can be observable  or hidden.
0 Know n structure , full y observable : In this case , the only learnabl e part is the set of
conditiona l probabilit y tables . Thes e can be estimate d directl y usin g the statistic s of the set
of examples . Som e belie f networ k system s incorporat e automati c updatin g of conditiona l
probabilit y table entrie s to reflec t the case s seen .
<> Unknow n structure , full y observable : In this case , the proble m is to reconstruc t the
topolog y of the network . Thi s proble m can be cast as a searc h throug h the spac e of
structures , guide d by the abilit y of each structur e to mode l the data correctly . Fittin g
the data to a give n structur e reduce s to the fixed­structur e problem , and the MA P or ML
probabilit y valu e can be used as heuristi c for hill­climbin g or simulate d annealin g search .
0 Know n structure , hidde n variables : This case is analogou s to neura l networ k learning .
We discus s method s for this proble m in the next section .
<C> Unknow n structure , hidde n variables : Whe n som e variable s are sometime s or alway s
unobservable , the prio r technique s for recoverin g structur e becom e difficul t to apply ,
becaus e they essentiall y requir e averagin g over all possibl e combination s of value s of the
unknow n variables . At present , no good , genera l algorithm s are know n for this problem .
Learnin g network s with fixed structur e
Experienc e in constructin g belie f network s for application s has show n that findin g the topolog y
of the networ k is ofte n the easy part . Human s find it easy to say wha t cause s what , but hard to
put exac t number s on the links . Thi s is particularl y true whe n som e of the variable s canno t be
observe d directl y in actua l cases . The "know n structure , hidde n variable " learnin g proble m is
therefor e of grea t importance .
One migh t ask why the proble m canno t be reduce d to the fully observabl e case by elimi ­
natin g the hidde n variable s usin g marginalizatio n ("averagin g out") . Ther e are two reason s for
this. First , it is not necessaril y the case that any particula r variabl e is hidde n in all the observe d
590 Chapte r 19 . Learnin g in Neura l and Belie f Network s
ADAPTIV E
PROBABILISTI C
NETWORK Scases (althoug h we do not rule this out) . Second , network s with hidde n variable s can be more
compact  than the correspondin g fully observabl e network . Figur e 19.1 7 show s an example . If
the underlyin g domai n has significan t local structure , then with hidde n variable s it is possibl e to
take advantag e of that structur e to find a more concis e representatio n for the joint distributio n on
the observabl e variables . This , in turn , make s it possibl e to learn from fewe r examples .
Figur e 19.1 7 A  networ k with a hidde n variabl e (labelle d H), and the correspondin g fully
observabl e network . If the variable s are Boolean , then the hidden­variabl e networ k require s 17
independen t conditiona l probabilit y values , wherea s the fully observabl e networ k require s 27.
If we are to approac h this proble m in Bayesia n terms , then the "hypotheses " Hj are the
differen t possibl e complet e assignment s to all the conditiona l probabilit y table (CPT ) entries .
We will assum e that all possibl e assignment s are equall y likel y a priori , whic h mean s that we are
lookin g for a maximu m likelihoo d hypothesis . Tha t is, we wish to find the set of CPT entrie s
that maximize s the probabilit y of the data , P(D\Hi).
The metho d we will use to do this is quite simila r to the gradien t descen t metho d for neura l
networks . We will writ e the probabilit y of the data as a functio n of the CPT entries , and then
calculat e a gradient . As with neura l networks , we will find that the gradien t can be calculate d
locall y by each node usin g informatio n that is availabl e in the norma l cours e of belie f networ k
calculations . Thus , the CPT entrie s are analogou s to the weights , and P(D\H t) is (inversely )
analogou s to the error E. Belie f networ k system s equippe d with this kind of learnin g schem e are
called adaptiv e probabilisti c network s (APNs) .
Suppos e we have a trainin g set D = {D\,...  ,D m}, wher e each case D; consist s of an
assignmen t of value s to some subse t of the variable s in the network . We assum e that each case is
draw n independentl y from some underlyin g distributio n that we are tryin g to model . The proble m
is to adjus t the conditiona l probabilitie s in the networ k in orde r to maximiz e the likelihoo d of
the data . We will writ e this likelihoo d as P(D) . The reade r shoul d bear in mind that here P(­)
is reall y Pw(­), that is, the probabilit y accordin g to a join t distributio n specifie d by w, the set
of all of the conditiona l probabilit y value s in the network . In orde r to construc t a hill­climbin g
algorith m to maximiz e the likelihood , we need to calculat e the derivativ e of the likelihoo d with
respec t to each of the conditiona l probabilit y value s w, in w.
It turn s out to be easies t to comput e the derivativ e of the logarith m of the likelihood .
Becaus e the log­likelihoo d is monotonicall y relate d to the likelihoo d itself , a maximu m on the
Sectio n 19.6 . Bayesia n Method s for Learnin g Belie f Network s 59 1
log­likelihoo d surfac e is also a maximu m on the likelihoo d surface . We calculat e the gradien t
using partia l derivatives , varyin g a singl e valu e w, whil e keepin g the other s constant:6
dlnP(D ) ^ dlnYljP(Dj)  ^^  d\nP(Dj)  ^ ^ dP(Dj)/dw,
dwi ~  dwi  ~ ^ dwi  ~  2­^ P(Dj)
Hence , we can calculat e separatel y the gradien t contributio n of each case and sum the results .
Now the aim is to find an expressio n for the gradien t contributio n from a singl e case , such
that the contributio n can be calculate d usin g only informatio n loca l to the node with whic h Wj is
associated . Let w, be a specifi c entr y in the conditiona l probabilit y table for a node X give n its
paren t variable s U. We'l l assum e that it is the entry for the case X =  xf give n U = u,:
Wi = P(X = Xi | U = u/) = P(xi u,­)
In orde r to get an expressio n in term s of loca l information , we introduc e X and U by averagin g
over their possibl e values :
P(Dj)  P(Dj)
x,u)P(x  u)P(u) )
For our purposes , the importan t propert y of this expressio n is that w, appear s only in linea r form .
In fact , Wj appear s only in one term in the summation , namel y the term for z, and u,. For this
term, P(x \ u) is just H>,, henc e
dP(Dj)ldwi  P(Dj  Jc/,u,­)P(u,­ )
P(Dj)  P(Dj)
Furthe r manipulatio n reveal s that the gradien t calculatio n can "piggyback " on the calculation s of
posterio r probabilitie s done in the norma l cours e of belie f networ k operatio n — thatis , calculation s
of the probabilitie s of networ k variable s give n the observe d data . To do this, we appl y Bayes '
theore m to the abov e equation , yieldin g
P(Xi,Ui | Dj)P(Dj)P(Ui)  = P(Xj,Uj  \ Dj)  = P(X,,UJ  \ Df)
~~ ~ P(Dj)  ~ P(Xi,
In mos t implementation s of belie f networ k inference , the term P(x,,  u, j £>,) is eithe r
compute d directl y or is easil y obtaine d by summin g a smal l numbe r of tabl e entries . Th e
complet e gradien t vecto r is obtaine d by summin g the abov e expressio n over the data cases to give
the gradien t componen t with respec t to each w­, for the likelihoo d of the entir e trainin g set. Thus ,
the necessar y informatio n for calculatin g the gradien t can be derive d directl y from the norma l
computation s done by belie f network s as new evidenc e is obtained .
Once we have a locall y computabl e expressio n for the gradient , we can appl y the same
kinds of hill­climbin g or simulate d annealin g method s as are used for neura l networks . Learnin g
with belie f network s has the advantag e that a huma n exper t can easil y provid e a structur e for the
6 We also need to includ e the constrain t that the conditiona l probabilit y value s for any give n conditionin g case mus t
remai n normalized . A forma l analysi s show s that the derivativ e of the constraine d syste m (wher e the column s sum to
one) is equa l to the orthogonall y projectio n of the unconstraine d derivativ e onto the constrain t surface .
592 Chapte r 19 . Learnin g in Neura l and Belie f Network s
networ k that reflect s the causa l structur e of the domain . Thi s prio r knowledg e shoul d help the
networ k to learn muc h faste r from a give n set of examples . Moreover , the result s of learnin g
are mor e easil y understood , and, becaus e probabilitie s are produced , the result s can be used in
makin g rationa l decisions .
One can also use the gradient­descen t metho d in associatio n with an algorith m designe d
to generat e the structur e of the network . Becaus e such algorithm s usuall y wor k by evaluatin g
candidat e structure s for their abilit y to mode l the data , one can simpl y use the gradien t descen t
to find the best fit betwee n any candidat e structur e and the data .
A compariso n of belie f network s and neura l network s
Give n the close similarit y betwee n belie f network s (particularl y the adaptiv e variety ) and neura l
networks , a detaile d compariso n is in order . The two formalism s can be compare d as represen ­
tation systems , inferenc e systems , and learnin g systems .
Both neura l network s and belie f network s are attribute­base d representations . Both handl e
discret e and continuou s inputs , althoug h algorithm s for handlin g continuou s variable s in belie f
network s are less developed . The principa l differenc e is that belie f network s are localize d rep­
resentations , wherea s neura l network s are distribute d representations . Node s in belie f network s
represen t proposition s with well­define d semantic s and well­define d probabilisti c relationship s to
other propositions . Unit s in neura l networks , on the othe r hand , typicall y do not represen t specifi c
propositions . Eve n if they did, the calculation s carrie d by the networ k do not treat  proposition s
in any semanticall y meaningfu l way . In practica l terms , this mean s that human s can neithe r
construc t nor understan d neura l networ k representations . The well­define d semantic s of belie f
network s also mean s that they can be constructe d automaticall y by program s that manipulat e
first­orde r representations .
Anothe r representationa l differenc e is that belie f networ k variable s have two dimension s
of "activation"—th e rang e of value s for the proposition , and the probabilit y assigne d to each
of thos e values . The output s of a neura l networ k can be viewe d as either  the probabilit y of a
Boolea n variable , or an exac t valu e for a continuou s variable , but neura l network s canno t handl e
both probabilitie s and multivalue d or continuou s variable s simultaneously .
As inferenc e mechanisms—onc e they have been trained—feedforwar d neura l network s
can execut e in linea r time , wherea s genera l belie f networ k inferenc e is NP­hard . On close r
inspection , this is not as clea r an advantag e as it migh t seem , becaus e a neura l networ k woul d in
some cases have to be exponentiall y large r in orde r to represen t the same input/outpu t mappin g as
a belie f networ k (else we woul d be able to solv e hard problem s in polynomia l time) . Practicall y
speaking , any neura l networ k that can be traine d is smal l enoug h so that inferenc e is fast, wherea s
it is not hard to construc t belie f network s that take a long time to run. One othe r importan t aspec t
of belie f network s is their flexibility , in the sens e that at any time any subse t of the variable s can
be treate d as inputs , and any othe r subse t as outputs , wherea s feedforwar d neura l network s have
fixed input s and outputs .
With respec t to learning , a compariso n is difficul t becaus e adaptiv e probabilisti c network s
(APNs ) are a very recen t development . One can expec t the time per iteratio n of an APN to be
slower , becaus e it involve s an inferenc e process . On the othe r hand , a huma n (or anothe r part of
Sectio n 19.7 . Summar y 59 3
the agent ) can provid e prior knowledg e to the APN learnin g proces s in the form of the networ k
structur e and/o r conditiona l probabilit y values . Sinc e this reduce s the hypothesi s space , it shoul d
allow the APN to learn from fewe r examples . Also , the abilit y of belie f network s to represen t
proposition s locall y may mea n that they converg e faste r to a correc t representatio n of a domai n
that has loca l structure—tha t is,, in whic h each propositio n is directl y affecte d by only a smal l
numbe r of othe r propositions .
19.7 SUMMAR Y
Learnin g in comple x networ k representation s is currentl y one of the hottes t topic s in science . It
promise s to have broa d application s in compute r science , neurobiology , psychology , and physics .
This chapte r has presente d som e of the basi c idea s and techniques , and give n a flavo r of the
mathematica l underpinnings . The basic point s are as follows :
• A neura l networ k is a computationa l mode l that share s som e of the propertie s of brains : it
consist s of man y simpl e unit s workin g in paralle l with no central  control . The connection s
betwee n unit s have numeri c weight s that can be modifie d by the learnin g element .
• The behavio r of a neura l networ k is determine d by the connectio n topolog y and the natur e
of the individua l units . Feed­forwar d networks , in whic h the connection s form an acycli c
graph , are the simples t to analyze . Feed­forwar d network s implemen t state­fre e functions .
• A perceptro n is a feed­forwar d networ k with a singl e laye r of units , and can only represen t
linearl y separabl e functions . If the data are linearl y separable , the perceptro n learnin g
rule can be used to modif y the network' s weight s to fit the data exactly .
• Multilaye r feed­forwar d network s can represen t any function , give n enoug h units .
• The back­propagatio n learnin g algorith m work s on multilaye r feed­forwar d networks ,
using gradien t descen t in weigh t spac e to minimiz e the outpu t error . It converge s to a
locall y optima l solution , and has been used with some succes s in a variet y of applications .
As with all hill­climbin g techniques , however , there is no guarante e that it will find a globa l
solution . Furthermore , its convergenc e is ofte n very slow .
• Bayesia n learnin g method s can be used to learn representation s of probabilisti c functions ,
particularl y belie f networks . Bayesia n learnin g method s mus t trad e off the prio r belie f in
a hypothesi s agains t its degre e of agreemen t with the observe d data .
• Ther e are a variet y of learnin g problem s associate d with belie f networks , dependin g on
whethe r the structur e is fixe d or unknown , and whethe r variable s are hidde n or observable .
• Wit h a fixe d structur e and hidde n variables , belie f networ k learnin g has a remarkabl e
similarit y to neura l networ k learning . Gradien t descen t method s can be used , but belie f
network s also have the advantag e of a well­understoo d semantic s for individua l nodes .
This allow s the provisio n of prior knowledg e in orde r to spee d up the learnin g process .
594 Chapte r 19 . Learnin g in Neura l and Belie f Network s
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
McCulloc h and Pitts (1943 ) introduce d the fundamenta l idea of analyzin g neura l activit y via
threshold s and weighte d sums . Earl y cybernetic s and contro l theor y (Wiener , 1948) , base d on the
notio n of negativ e feedbac k loops , playe d a role as a mode l for learnin g in neura l networks . The
Organization  of Behavior  (Hebb , 1949 ) was influentia l in promotin g the hypothesi s that huma n
and anima l long­ter m memor y is mediate d by permanen t alteration s in the synapses . Design
for a Brain  (Ashby , 1952 ) put fort h the idea that intelligenc e coul d be create d by the use of
"homeostatic " device s whic h learn throug h a kind of exhaustiv e search .
Minsk y and Paper t (1988 , pp. ix­x) mentio n a machin e built by Marvi n Minsk y in 1951 that
may well be the first actua l neura l networ k learnin g syste m ever built . Minsky' s (1954 ) doctora l
dissertatio n continue d the exploratio n of neura l networks . The aptly­name d "Pandemonium "
syste m (Selfridge , 1959 ; SelfridgeandNeisser , 1960 ) involve d a relativel y fine­graine d distribute d
contro l regim e reminiscen t of neura l networks . Crag g and Temperle y (1954 ; 1955 ) drew parallel s
betwee n McCulloch­Pitt s neura l network s and "spi n systems " in physics . Caianell o (1961 )
designe d a statistica l theor y of learnin g in neura l networks , drawin g on classica l statistica l
mechanics . Von Neuman n (1958 ) provide s a compariso n betwee n the functionin g of the brain
and the operatio n of digita l computers . Fran k Rosenblat t (1957 ) invente d the moder n "perceptron "
style of neura l network , compose d of trainabl e threshol d units .
Simila r device s calle d "adalines " (for "Adaptiv e Linear" ) were invente d abou t the same
time (Widro w and Hoff , 1960 ; Widrow , 1962) . Hawkin s (1961 ) give s a detaile d histor y of early
work in "self­organizin g systems " or "neura l cybernetics, " as these approache s were then called .
Frank Rosenblat t (1960 ) foun d the firs t proo f of the perceptro n convergenc e theorem ,
althoug h it had been foreshadowe d by purel y mathematica l wor k outsid e the contex t of neura l
network s (Agmon , 1954 ; Motzki n and Schoenberg , 1954) . Tw o good book s on this perio d
of researc h are Neurodynamics  (Rosenblatt , 1962 ) and Learning Machines  (Nilsson , 1965) .
Nilsson' s book is especiall y comprehensiv e and detailed . It has recentl y been republishe d as The
Mathematical  Foundations  of Learning  Machines  (Nilsson , 1990 ) with a new introductio n by
Terrenc e Sejnowsk i and Halber t White .
Most wor k in neura l network s befor e 197 0 focuse d on the one­laye r perceptro n type
of machine , but ther e wer e som e exceptions . Widro w designe d multilaye r machine s calle d
"madalines"(Widrow , 1962) . Othe r early multilaye r machine s are describe d in (Palmier i and
Sanna , 1960 ; Gamb a et al., 1961) .
The publicatio n of Perceptrons  (Minsk y and Papert , 1969 ) marke d the end of an era. The
author s were severel y critica l of the unguide d experimentatio n and lack of mathematica l rigo r
that characterize d muc h of the early work on perceptrons . The y establishe d the linea r separabilit y
criterio n for tasks that coul d be accomplishe d by one­laye r perceptrons , thus explainin g the failur e
of the early effort s at solvin g problem s that violate d this criterion . Minsk y and Paper t also gave
some result s on early multilaye r systems . In the Epilogu e to the expande d editio n of Perceptrons
(Minsk y and Papert , 1988) , they forcefull y rebu t the charg e that the publicatio n of the first editio n
was responsibl e for the long perceptro n winte r of the 1970s , arguin g that perceptro n researc h had
alread y lost its momentu m and that the first editio n merel y explaine d this phenomenon . The y
reaffir m the long­ter m promis e of mathematicall y soun d neura l networ k research , whil e at the
Sectio n 19.7 . Summar y 59 5
same time criticizin g contemporar y connectionis m circa 1988 for the same lack of rigor that had
plague d the early perceptro n work .
The paper s in (Hinto n and Anderson , 1981) , base d on a conferenc e in San Dieg o in 1979 ,
can be regarde d as markin g the renaissanc e of connectionism . The two­volum e "PDF " (Paralle l
Distribute d Processing ) antholog y (Rumelhar t et al., 1986 ) reall y put neura l network s on the
map for man y AI researchers , as well as popularizin g the back­propagatio n algorithm . Severa l
advance s mad e this possible . Hopfiel d (1982 ) analyze d symmetri c network s usin g statistica l
mechanic s and analogie s from physics . The Boltzman n Machin e (Hinto n and Sejnowski , 1983 ;
Hinto n and Sejnowski , 1986 ) and the analysi s of neura l network s usin g the physica l theor y of
magneti c spin glasse s (Ami t et al., 1985 ) tightene d the link s betwee n statistica l mechanic s and
neura l networ k theory—providin g not only usefu l mathematica l insight s but also respectability.
The back­propagatio n techniqu e had been invente d quite early (Bryso n and Ho, 1969 ) but it was
rediscovere d severa l time s (Werbos , 1974 ; Parker , 1985) . Minsk y and Paper t (1988 ) criticiz e the
generalize d delta rule as a straightforwar d varian t of simpl e hill­climbing , just as the perceptro n
learnin g algorith m had been .
The expressivenes s of multilaye r network s was investigate d by Cybenk o (1988 ; 1989) ,
who showe d that two hidde n layer s are enoug h to represen t any functio n and a singl e laye r is
enoug h to represen t any continuous  function . Thes e results , althoug h reassuring , are not very
excitin g whe n one realize s that they are achieve d by allocatin g a separat e collectio n of units to
represen t the outpu t valu e for each smal l regio n of the (exponentiall y large ) inpu t space .
The proble m of findin g a good structur e for a multilaye r networ k was addresse d usin g
geneti c algorithm s by Harp et al. (1990 ) and by Mille r et al. (1989) . The "optima l brain damage "
metho d for removin g useles s connection s is by LeCu n et al. (1989) , and Sietsm a and Dow (1988 )
show how to remov e useles s units . The tilin g algorith m for growin g large r structure s is by Mezar d
and Nada l (1989) . Simila r algorithm s that grow slightl y differen t topologie s were propose d by
Marchan d et al. (1990 ) and by Frea n (1990) .
The complexit y of neura l networ k learnin g has been investigate d by researcher s in compu ­
tationa l learnin g theory . Som e of the earlies t result s were obtaine d by Judd (1990) , who showe d
that the genera l proble m of findin g a set of weight s consisten t with a set of example s is NP­
complete , even unde r very restrictiv e assumptions . Avri m Blum and Ron Rives t (1992 ) prove d
that trainin g even a three­node network  is NP­complete ! Thes e result s sugges t that weigh t spac e
can contai n an exponentia l numbe r of local minima , for otherwis e a random­restar t hill­climbin g
algorith m woul d be able to find a globa l optimu m in polynomia l time .
One topi c of grea t curren t interes t in neura l networ k researc h is the use of specialize d
paralle l hardware , includin g analo g computation . System s may use analo g VLS I (Alspecto r et
al., 1987 ; Mead , 1989) , optoelectronic s (Farhate f al, 1985;Petersonera/. , 1990) , or exotic,  fully
optica l computin g technologie s such as spatia l light modulatio n (Abu­Mostaf a and Psaltis , 1987 ;
UsuetaL,  1988) .
Neura l network s constitut e a large field of stud y with an abundanc e of resource s availabl e
for the inquirer . Probabl y the best availabl e textboo k is Introduction to the  Theory  of Neural
Computation  (Hert z et al., 1991) , whic h emphasize s the connection s with statistica l mechanic s
(the author s are physicists) . Self­Organization  and Associative  Memory  (Kohonen , 1989 ) pro­
vides considerabl e mathematica l background . For biologica l nervou s systems , a very thoroug h
introductio n is (Kande l et al., 1991) . A good introductio n to the detaile d functionin g of individua l
596 Chapte r 19 . Learnin g in Neura l and Belie f Network s
neuron s is (Miles , 1969) . Article s by Cowa n and Shar p (1988b ; 1988a ) presen t inclusiv e survey s
of the histor y of neura l networ k research . A very comprehensiv e bibliograph y is availabl e in
NeuralSource  (Wasserma n and Oetzel , 1990) .
The mos t importan t conferenc e in the field is the annua l NIPS (Neura l Informatio n Process ­
ing Conference ) conference , whos e proceeding s are publishe d as the serie s Advances  in Neural
Information  Processing  Systems,  startin g with (Touretzky , 1989) . Curren t researc h also appear s
in the Internationa l Join t Conferenc e on Neura l Network s (IJCNN) . Majo r journal s for the field
includ e Neural  Computation;  Neural  Networks;  IEEE  Transactions  on Neural  Networks;  the
International  Journal  of 'Neural  Systems;  and Concepts  in Neuroscience.
The topi c of learnin g belie f network s has receive d attentio n only very recently . For the
fixed­structure , fully observabl e case , Spiegelhalter , Dawid , Lauritzen , and Cowel l (Spiegelhalte r
et al., 1993 ) provid e a thoroug h analysi s of the statistica l basis of belie f networ k modificatio n
using Dirichle t priors . The y also give a heuristi c approximatio n for the hidden­variabl e case .
Pearl (1988 , Chapte r 8) describe s an algorith m for learnin g polytree s with unknow n structur e
and full y observabl e variables . Heckerman , Geiger , and Chickerin g (1994 ) describ e an elegan t
and effectiv e heuristi c algorith m for recoverin g the structur e of genera l network s in the full y
observabl e case , buildin g on the wor k of Coope r and Herskovit s (1992) . For the case of hidde n
variable s and unknow n structure , see (Spirte s et al., 1993) .
The genera l proble m of recoverin g distribution s from data with missin g value s and hidde n
variable s is addresse d by the EM algorith m (Dempste r et al., 1977) . Th e algorith m in the
chapte r (Russel l et al., 1994 ) can be seen as a varian t of EM in whic h the "maximize " phas e is
carrie d out by a gradient­followin g method . Lauritze n (1991 ) also consider s the applicatio n of
EM to belie f networks . A gradient­followin g algorith m for learnin g sigmoi d network s (belie f
network s in whic h each CPT represent s the same functio n as a standar d neural­networ k unit ) was
propose d by Radfor d Neal (1991) , who wen t on to show that Boltzman n Machine s are a specia l
case of belie f networks . Nea l was amon g the first to poin t out the extremel y clos e connectio n
betwee n neura l and belie f networks .
EXERCISE S
19.1 Construc t by hand a neura l networ k that compute s the XOR functio n of two inputs . Mak e
sure to specif y wha t sort of unit s you are using .
19.2 W e know that a simpl e perceptro n canno t represen t XOR (or, generally , the parit y functio n
of its inputs) . Describ e wha t happen s to the weight s of a four­input , step­functio n perceptron ,
beginnin g with all weight s set to 0.1, as example s of the parit y functio n arrive .
19.3 Suppos e you had a neura l networ k with linea r activatio n functions . Tha t is, for each unit
the outpu t is som e constan t c time s the weighte d sum of the inputs .
a. Assum e that the networ k has one hidde n layer . For a give n assignmen t to the weight s W,
write dow n equation s for the valu e of the unit s in the outpu t laye r as a functio n of W and
Sectio n 19.7 . Summar y 597
the inpu t laye r I, withou t any explici t mentio n to the outpu t of the hidde n layer . Sho w that
there is a networ k with no hidde n unit s that compute s the same function .
b. Repea t the calculatio n in part (a), this time for a networ k with any numbe r of hidde n layers .
What can you conclud e abou t linea r activatio n functions ?
19.4 Considerth e followin g set of examples . Each exampl e has six input s and one targe t output :
/I
h
h
h
Is
/6
T1 1
0 0
1 1
0 1
0 0
0 0
1 11
0
1
0
1
0
11
1
0
0
1
1
11
1
1
1
0
0
11 1
0 0
0 0
0 0
1 1
1 0
1 00
1
1
1
0
1
10
1
1
0
1
1
00
0
0
1
1
0
00
1
0
1
0
1
00
0
0
1
0
1
00 0
1 1
1 1
0 1
1 0
1 0
0 0
a. Run the perceptro n learnin g rule on this exampl e set, and show the resultin g set of weights .
b. Run the decisio n tree learnin g rule , and show the resultin g decisio n tree.
c. Commen t on your results .
19.5 Implemen t a data structur e for layered , feed­forwar d neura l networks , rememberin g to
provid e the informatio n neede d for both forwar d evaluatio n and backwar d propagation . Usin g
this data structure , writ e a functio n NEURAL­NETWORK­OUTPU T that takes an exampl e and a
networ k and compute s the appropriat e outpu t values .
19.6 Suppos e that a trainin g set contain s only a singl e example , repeate d 100 times . In 80 of
the 100 cases , the singl e outpu t valu e is 1; in the othe r 20, it is 0. Wha t will a back­propagatio n
networ k predic t for this example , assumin g that it has been traine d and reache s a globa l optimum ?
(Hint:  to find the globa l optimum , differentiat e the error functio n and set to zero. )
19.7 Th e networ k in Figur e 19.1 3 has four hidde n nodes . Thi s numbe r was chose n somewha t
arbitrarily . Run systemati c experiment s to measur e the learnin g curve s for network s with differen t
number s of hidde n nodes . Wha t is the optima l number ? Woul d it be possibl e to use a cross ­
validatio n metho d to find the best networ k befor e the fact?
20REINFORCEMEN T
LEARNIN G
In which  we examine  how  an agent  can learn  from  success  and failure,  reward  and
punishment.
20.1 INTRODUCTIO N
In the previou s two chapters , we have studie d learnin g method s that learn from examples . Tha t
is, the environmen t provide s input/outpu t pairs , and the task is to learn a functio n that coul d
have generate d thos e pairs . Thes e supervise d learnin g method s are appropriat e whe n a teache r
is providin g correc t value s or whe n the function' s outpu t represent s a predictio n abou t the futur e
that can be checke d by lookin g at the percept s in the next time step. In this chapter , we will stud y
how agent s can learn in muc h less generou s environments , wher e the agen t receive s no examples ,
and start s with no mode l of the environmen t and no utilit y function .
For example , we know an agen t can learn to play ches s by supervise d learning—b y being
given example s of gam e situation s alon g with the best mov e for that situation . But if there is
no friendl y teache r providin g examples , wha t can the agen t do? By tryin g rando m moves , the
agent can eventuall y build a predictiv e mode l of its environment : wha t the boar d will be like after
it make s a give n move , and even how the opponen t is likel y to repl y in a give n situation . But
withou t som e feedbac k as to wha t is good and wha t is bad, the agen t will have no ground s for
decidin g whic h mov e to make . Fortunately , the chess­playin g agen t does receiv e som e feedback ,
even withou t a friendl y teacher—a t the end of the game , the agen t perceive s whethe r it has won
REWAR D o r lost. Thi s kind of feedbac k is calle d a reward , or reinforcement . In game s like chess , the
TERMINA L STAT E reinforcemen t is receive d only at the end of the game . We call this a termina l state in the state
histor y sequence . In othe r environments , the reward s com e more frequently—i n ping­pong , each
poin t score d can be considere d a reward . Sometime s reward s are give n by a teache r who says
"nice move " or "uh­oh " (but does not say wha t the best mov e is).
The task of reinforcemen t learnin g is to use reward s to learn a successfu l agen t function .
This is difficul t becaus e the agen t is neve r told what  the righ t action s are, nor whic h reward s are
598
Sectio n 20.1.  Introductio n 599
PASSIV E LEARNE R
ACTIV E LEARNE R
ACTION­VALU E
Q­LEARNIN Gdue to whic h actions . A game­playin g agen t may play flawlessl y excep t for one blunder , and at
the end of the gam e get a singl e reinforcemen t that says "you lose. " The agen t mus t someho w
determin e whic h mov e was the blunder .
Withi n our framewor k of agents  as function s from percept s to actions , a rewar d can be
provide d by a percept , but the agen t mus t be "hardwired " to recogniz e that percep t as a rewar d
rathe r than as just anothe r sensor y input . Thus , animal s seem to be hardwire d to recogniz e pain
and hunge r as negativ e rewards , and pleasur e and food as positiv e rewards . Trainin g a dog is
made easie r by the fact that human s and dogs happe n to agre e that a low­pitche d soun d (eithe r
a grow l or a "bad dog!" ) is a negativ e reinforcement . Reinforcemen t has been carefull y studie d
by anima l psychologist s for over 60 years .
In man y comple x domains , reinforcemen t learnin g is the onl y feasibl e way to train a
progra m to perfor m at high levels . For example , in gam e playing , it is very hard for a huma n
to provid e accurat e and consisten t evaluation s of large number s of positions , whic h woul d be
neede d to train an evaluatio n functio n directl y from examples . Instead , the progra m can be told
when it has won or lost, and can use this informatio n to learn an evaluatio n functio n that give s
reasonabl y accurat e estimate s of the probabilit y of winnin g from any give n position . Similarly ,
it is extremel y difficul t to progra m a robo t to juggle ; yet give n appropriat e reward s ever y time a
ball is droppe d or caught , the robo t can learn to juggl e by itself .
In a way , reinforcemen t learnin g is a restatemen t of the entir e AI problem . An agen t in
an environmen t gets percepts , map s som e of them to positiv e or negativ e utilities , and then has
to decid e wha t actio n to take . To avoi d reconsiderin g all of AI and to get at the principle s of
reinforcemen t learning , we need to conside r how the learnin g task can vary :
• The environmen t can be accessibl e or inaccessible . In an accessibl e environment , state s
can be identifie d with percepts , wherea s in an inaccessibl e environment , the agen t mus t
maintai n som e interna l state to try to keep track of the environment .
• The agen t can begi n with knowledg e of the environmen t and the effect s of its actions ; or it
will have to learn this mode l as well as utilit y information .
• Reward s can be receive d only in termina l states , or in any state .
• Reward s can be component s of the actua l utilit y (point s for a ping­pon g agen t or dollar s
for a bettin g agent ) that the agen t is tryin g to maximize , or they can be hint s as to the actua l
utilit y ("nic e move " or "bad dog") .
• The agen t can be a passiv e learne r or an activ e learner . A passiv e learne r simpl y watche s
the worl d goin g by, and tries to learn the utilit y of bein g in variou s states ; an activ e learne r
must also act usin g the learne d information , and can use its proble m generato r to sugges t
exploration s of unknow n portion s of the environment .
Furthermore , as we saw in Chapte r 2, ther e are severa l differen t basic design s for agents . Becaus e
the agen t will be receivin g reward s that relat e to utilities , there are two basi c design s to consider :
• The agen t learn s a utilit y functio n on state s (or state histories ) and uses it to selec t action s
that maximiz e the expecte d utilit y of their outcomes .
• The agen t learn s an action­valu e functio n givin g the expecte d utilit y of takin g a give n
actio n in a give n state . This is calle d Q­learning .
600 Chapte r 20 . Reinforcemen t Learnin g
An agen t that learn s utilit y function s mus t also have a mode l of the environmen t in orde r to mak e
decisions , becaus e it mus t know the state s to whic h its action s will lead . For example , in orde r
to mak e use of a backgammo n evaluatio n function , a backgammo n progra m mus t kno w wha t
its lega l move s are and how they  affect  the board  position.  Onl y in this way can it appl y the
utilit y functio n to the outcom e states . An agen t that learn s an action­valu e function , on the othe r
hand , need not have such a model . As long as it know s its lega l moves , it can compar e their
value s directl y withou t havin g to conside r their outcomes . Action­valu e learner s therefor e can
be slightl y simple r in desig n than utilit y learners . On the othe r hand , becaus e they do not know
wher e their action s lead , they canno t look ahead ; this can seriousl y restric t their abilit y to learn ,
as we shal l see.
We first addres s the proble m of learnin g utilit y functions , whic h has been studie d in AI
since the earlies t days of the field . (See the discussio n of Samuel' s checke r playe r in Chapte r 5.)
We examin e increasingl y comple x version s of the problem , whil e keepin g initiall y to simpl e
state­base d representations . Sectio n 20.6 discusse s the learnin g of action­valu e functions , and
Sectio n 20.7 discusse s how the learne r can generaliz e acros s states . Throughou t this chapter ,
we will assum e that the environmen t is nondeterministic . At this poin t the reade r may wish to
revie w the basic s of decisio n makin g in complex , nondeterministi c environments , as covere d in
Chapte r 17.
20.2 PASSIV E LEARNIN G IN A KNOW N ENVIRONMEN T
TRAININ G
SEQUENC ETo keep thing s simple , we star t with the case of a passiv e learnin g agen t usin g a state­base d
representatio n in a known , accessibl e environment . In passiv e learning , the environmen t generate s
state transition s and the agen t perceive s them.1 Conside r an agen t tryin g to learn the utilitie s of
the state s show n in Figur e 20.1 (a). We assume , for now , that it is provide d with a mode l My
givin g the probabilit y of a transitio n from state i to state j, as in Figur e 20.1(b) . In each trainin g
sequence , the agen t start s in state (1,1 ) and experience s a sequenc e of state transition s unti l it
reache s one of the termina l state s (3,2 ) or (3,3) , wher e it receive s a reward.2 A typica l set of
trainin g sequence s migh t look like this:
(1,1 )­>(! , 2)—(1,3)­>(2,3)­(2,2)­>(2,3)­>(3,3 ) ±1
(1,1)—(1,2)—(1 , !)—(! , 2)­<l , 1)^(2,1)^(2,2)­<2,3)­+(3,3 ) ±1
(1? !)­>. ( 1,2)^(2,2)­Kl , 2)^(1,3)—(2,3)­*(l , 3)­<2,3)­K3,3 ) ±i
(1,1)—K2,1)^(2,2)^(2,1)—Kl , D—(1,2)—»(1.3)—»(2,3)—*­(2,2)—»(3,2 ) ­1
(1,1)­K2,1)­K1,1)­<1,2)­K2,2)^(3,2)^ 1
The objec t is to use the informatio n abou t reward s to learn the expecte d utilit y U(i)  associate d
with each nontermina l state i. We will mak e one big simplifyin g assumption : the utilit y of a
sequenc e is the sum of the reward s accumulate d in the state s of the sequence . Tha t is, the utilit y
1 Anothe r way to thin k of a passiv e learne r is as an agen t with a fixe d polic y tryin g to determin e its benefits .
2 Th e perio d from initia l state to termina l state is ofte n calle d an epoch .
Sectio n 20.2 . Passiv e Learnin g in a Know n Environmen t 601
=•!
STAR T (XX) C
(b)
Figur e 20.1 (a ) A simpl e stochasti c environment . Stat e (1,1 ) is the start state , (b) Each state
transition s to a neighborin g state with equa l probabilit y amon g all neighborin g states . Stat e (4,2 )
is termina l with rewar d ­1, and state (4,3) is termina l with rewar d +1. (c) The exac t utilit y values .
REWARD­TO­G O functio n is additiv e in the sens e define d on page 502. We defin e the reward­to­g o of a state as
the sum of the reward s from that state unti l a termina l state is reached . Give n this definition , it is
. ^.. ­ .­ eas y to see that the expected  utility  of a state  is the expected  reward­to­go  of that state.
' S: Th e generi c agen t desig n for passiv e reinforcemen t learnin g of utilitie s is show n in Fig­
ure 20.2 . The agen t keep s an estimat e U of the utilitie s of the states , a tabl e N of count s of
how man y time s each state was seen , and a table M of transitio n probabilitie s from state to state .
We assum e that each percep t e is enoug h to determin e the STAT E (i.e. , the state is accessible) ,
the agen t can determin e the REWAR D componen t of a percept , and the agen t can tell if a percep t
indicate s a TERMINAL ? state . In general , an agen t can updat e its curren t estimate d utilitie s after
each observe d transition . The key to reinforcemen t learnin g lies in the algorith m for updatin g
the utilit y value s give n the trainin g sequences . The followin g subsection s discus s thre e possibl e
approache s to UPDATE .
ADAPTIV E CONTRO L
THEOR YNaiv e updatin g
A simpl e metho d for updatin g utilit y estimate s was invente d in the late 1950 s in the area of
adaptiv e contro l theor y by Widro w and Hof f (1960) . We will call it the LMS (leas t mea n
squares ) approach . In essence , it assume s that for each state in a trainin g sequence , the observed
reward­to­g o on that sequenc e provide s direc t evidenc e of the actua l expecte d reward­to­go . Thus ,
at the end of each sequence , the algorith m calculate s the observe d reward­to­g o for each state and
update s the estimate d utilit y for that state accordingly . It can easil y be show n (Exercis e 20.1 ) that
the LMS approac h generate s utilit y estimate s that minimiz e the mea n squar e erro r with respec t
to the observed  data . Whe n the utilit y functio n is represente d by a table of value s for each state ,
the updat e is simpl y done by maintainin g a runnin g average , as show n in Figur e 20.3 .
If we thin k of the utilit y functio n as a function , rathe r than just a table , then it is clear that
the LMS approac h is simpl y learnin g the utilit y functio n directl y from examples . Eac h exampl e
has the state as inpu t and the observe d reward­to­g o as output . This mean s that we have reduce d
reinforcemen t learnin g to a standar d inductiv e learnin g problem , as discusse d in Chapte r 18. As
602 Chapte r 20 . Reinforcemen t Learnin g
functio n PASSiVE­RL­AGENT(e ) return s an actio n
static : U, a table of utilit y estimate s
N, a table of frequencie s for state s
M, a table of transitio n probabilitie s from state to state
percepts,  a percep t sequenc e (initiall y empty )
add e to percepts
incremen t W[STATE[e] ]
U<— UPDATE([/ , e, percepts, M,  N)
if TERMINAL?[e ] then percepts^­  the empt y sequenc e
retur n the actio n Observe
Figur e 20.2 Skeleto n for a passiv e reinforcemen t learnin g agen t that just observe s the worl d
and tries to learn the utilities , U, of each state . The agen t also keep s track of transitio n frequencie s
and probabilities . The rest of this sectio n is largel y devote d to definin g the UPDAT E function .
functio n LMS­UPDATE((7 , e, percepts,  M, N)  return s an update d U
if TERMINAL?[e ] then reward­to­go  — 0
for each e, in percepts  (startin g at end) do
reward­to­go  <— reward­to­go +  REWARD[e, ]
C/[STATE[e,] ] — RUNNING­AVERAGE([/[STATE[e,]] , reward­to­go,  ATSTATE[e,]] )
end
Figur e 20.3 Th e updat e functio n for leas t mea n squar e (LMS ) updatin g of utilities .
we will show , it is an easy matte r to use mor e powerfu l kind s of representation s for the utilit y
function , such as neura l networks . Learnin g technique s for thos e representation s can be applie d
directl y to the observe d data .
One migh t thin k that the LMS approac h mor e or less solve s the reinforcemen t learnin g
problem—o r at least , reduce s it to one we alread y know a lot about . In fact, the LMS approac h
misse s a very importan t aspec t of the reinforcemen t learnin g problem , namely , the fact that the
utilitie s of state s are not independent ! The structur e of the transition s amon g the state s in fact
impose s very stron g additiona l constraints : The  actual  utility  of a state is  constrained to be  the
probability­weighted  average  of its successors'  utilities,  plus  its own  reward.  By ignorin g these
constraints , LMS­UPDAT E usuall y ends up convergin g very slowl y on the correc t utilit y value s for
the problem . Figur e 20.4 show s a typica l run on the 4 x 3 environmen t in Figur e 20.1 , illustratin g
both the convergenc e of the utilit y estimate s and the gradua l reductio n in the root­mean­squar e
error with respec t to the correct  utilit y values . It take s the agen t well over a thousan d trainin g
sequence s to get close to the correc t values .
Sectio n 20.2 . Passiv e Learnin g in a Know n Environmen t 603
1estimate s
oo u .
;j3
3 ­0.5
­1
(. —— — ———— — _ ——— — —— — —— ­ ­(4,3 ) ­
'; ,,.. , .  .....,­­­...­ ­ (3,3 )
;. 7\.­"7­ ­ .7.7. . ­777 . ­  (2,3 ) ­
­I ­­••­­ ­ •­­­•­­ ­ ­.­­­­ ­ ­ ­­­ ­^1) ­
... ...... . ... . .. .. ... (4>l )
:..:. ............. . ........ . ........ . ....... . ^(4,2) ­
) 20 0 40 0 60 0 80 0 100 0
Numbe r of epoch s0.6
0.5
i 0.4
OJ| °'2
0.1
0
(i ;
!
.
­ V' X^,/­­­^""­­­^"^­­­, .
) 20 0 40 0 60 0 80 0 100 0
Numbe r of epoch s
(a) (b )
Figur e 20.4 Th e LMS learnin g curve s for the 4x3 worl d show n in Figur e 20. 1 . (a) The utilit y
estimate s of the state s over time , (b) The RMS error compare d to the correc t values .
ADAPTIV E DYNAMI C
PROGRAMMIN G
ADPAdaptiv e dynami c programmin g
Program s that use knowledg e of the structur e of the environmen t usuall y learn muc h faster . In the
exampl e in Figur e 20.5 (from (Sutton , 1988)) , the agen t alread y has a fair amoun t of experienc e
with the three state s on the right , and has learne d the value s indicated . However , the path followe d
from the new state reache s an unusuall y good termina l state . The LMS algorith m will assig n a
utilit y estimat e of+1 to the new state , wherea s it is clear that the new state is muc h less promising ,
becaus e it has a transitio n to a state know n to have utilit y & —0.8 , and no other know n transitions .
Fortunately , this drawbac k can be fixed . Conside r again the poin t concernin g the constraint s
on neighborin g utilities . Because,  for now , we are assumin g that the transitio n probabilitie s
are liste d in the know n table.Af/,­ , the reinforcemen t learnin g proble m become s a well­define d
sequentia l decisio n proble m (see Chapte r 17) as soon as the agen t has observe d the reward s for all
the states . In our 4x3 environment , this usuall y happen s after a handfu l of trainin g sequences ,
at whic h poin t the agen t can comput e the exac t utilit y value s for all states . The utilitie s are
compute d by solvin g the set of equation s
(20.1 )
wher e R(i) is the rewar d associate d with bein g in state i, and M// is the probabilit y that a transitio n
will occu r from state i to state j. This set of equation s simpl y formalize s the basic poin t mad e in
the previou s subsection . Notic e that becaus e the agen t is passive , no maximizatio n over action s
is involve d (unlik e Equatio n (17.3)) . The proces s of solvin g the equation s is therefor e identica l
to a singl e valu e determinatio n phas e in the polic y iteratio n algorithm . The exac t utilitie s for
the state s in the 4x 3 worl d are show n in Figur e 20.1(c) . Notic e how the "safest " square s are
those alon g the top row, awa y from the negativ e rewar d state .
We will use the term adaptiv e dynami c programmin g (or ADP ) to denot e any reinforce ­
ment learnin g metho d that work s by solvin g the utilit y equation s with a dynami c programmin g
algorithm . In term s of its abilit y to mak e goo d use of experience , AD P provide s a standar d
604 Chapte r 20 . Reinforcemen t Learnin g
Figur e 20.5 A n exampl e wher e LMS does poorly . A new state is reache d for the first time ,
and then follow s the path marke d by the dashe d lines , reachin g a termina l state with rewar d +1.
agains t whic h to measur e othe r reinforcemen t learnin g algorithms . It is, however , somewha t
intractabl e for large state spaces . In backgammon , for example , it woul d involv e solvin g roughl y
1050 equation s in 105() unknowns .
TEMPORAL ­
DIFFERENC ETempora l differenc e learnin g
It is possibl e to have (almost ) the best of both worlds—tha t is, one can approximat e the constrain t
equation s show n earlie r withou t solvin g them for all possibl e states . The  key is to use the
observed  transitions  to adjust the  values  of the observed  states  so that  they  agree  with  the
constraint  equations.  Suppos e that we observ e a transitio n from state / to state j, wher e currentl y
U(i) = ­0.5 and [/(/) = +0.5 . Thi s suggest s that we shoul d conside r increasin g U(i)  to mak e it
agree bette r with its successor . Thi s can be achieve d usin g the followin g updatin g rule :
U(i) — U(i)  + o (R(i) + U(j) ­ U(i))  (20.2 )
wher e a is the learnin g rate parameter . Becaus e this updat e rule uses the differenc e in utilitie s
betwee n successiv e states , it is often calle d the temporal­difference , or TD, equation .
The basi c idea of all temporal­differenc e method s is to firs t defin e the condition s that
hold locall y whe n the utilit y estimate s are correct ; and then to writ e an updat e equatio n that
move s the estimate s towar d this idea l "equilibrium " equation . In the case of passiv e learning ,
the equilibriu m is give n by Equatio n (20.1) . Now Equatio n (20.2 ) does in fact caus e the agen t to
reach  the equilibriu m give n by Equatio n (20.1) , but there is som e subtlet y involved . First , notic e
that the updat e only involve s the actua l successor , wherea s the actua l equilibriu m condition s
involv e all possibl e next states . One migh t thin k that this cause s an improperl y large chang e in
[7(0 whe n a very rare transitio n occurs ; but, in fact, becaus e rare transition s occu r only rarely ,
the average  value  of U(i) will converg e to the correc t value . Furthermore , if we chang e a from
a fixe d paramete r to a functio n that decrease s as the numbe r of time s a state has been visite d
increases , then U(i)  itsel f will converg e to the correc t valu e (Dayan , 1992) . Thi s give s us the
algorith m TD­UPDATE , show n in Figur e 20.6 . Figur e 20.7 show s atypica l run of the TD learnin g
algorith m on the worl d in Figur e 20.1 . Althoug h TD generate s noisie r values , the RM S erro r is
actuall y significantl y less than that for LMS afte r 1000 iterations .
Sectio n 20.3 . Passiv e Learnin g in an Unknow n Environmen t 605
functio n TD­UPDATE(C/ , e, percepts,  M, iV) return s the utilit y tabl e U
if TERMINAL?[<? ] then
C/[STATE[e] ] <­ RUNNING­AVERAGE([/[STATE[e]] , REWARD[<?] , W[STATE[>]] )
else if percepts  contain s mor e than one elemen t then
e' <— the penultimat e elemen t of percepts
/. j — STATE|e'] , STATE H
f/[i] — U[i]  + o'(#[i'l)(REWARD[e' ] + U\j] ­ U[i])
Figur e 20.6 A n algorith m for updatin g utilit y estimate s usin g tempora l differences .
1
* 0.5
V35
i£ 0
3 ­0.5
_,——————————————— — ­ ­  —— (4,3) ­
:M; ' • :. '" '•'' . ;• . ' . • . ' (3,3 )
; .  • • : ;•• • : . • •••/ • .' ­. ' . (2,3 )
'":,­ >  . • 'x  '•';.'.:•  •'•.;•' . ' '
••';• >: . (1, 0
•;''. ::'' ;  ­'.­ • . . ­. .­..­ . ­. ­•­ (3,o ­
•'• . . .. . ­  (4.1 )05
i1
~B 0. 43
C
o 0.3
| 0.2
0.1
ni
!i' \
­* : i ­
0 20 0 40 0 60 0 80 0 100 0 0  20 0 40 0 60 0 80 0 100 0
Numbe r of epoch s Numbe r of epoch s
(a) (b )
Figur e 20.7 Th e TD learnin g curve s for the 4 x 3 world , (a) The utilit y estimate s of the state s
over time , (b) The RMS erro r compare d to the correc t values .
20.3 PASSIV E LEARNIN G IN AN UNKNOW N ENVIRONMEN T
The previou s sectio n deal t with the case in whic h the environmen t mode l M is alread y known .
Notic e that of the three approaches , only the dynami c programmin g metho d used the mode l in full.
TD uses informatio n abou t connectednes s of states , but only from the curren t trainin g sequence .
(As we mentione d before , all utility­learnin g method s will use the mode l durin g subsequen t actio n
selection. ) Henc e LMS and TD will operat e unchange d in an initiall y unknow n environment .
The adaptiv e dynami c programmin g approac h simpl y adds a step to PASSIVE­RL­AGEN T
that update s an estimate d mode l of the environment . The n the estimate d mode l is used as the
basis for a dynami c programmin g phas e to calculat e the correspondin g utilit y estimate s after each
observation . As the environmen t mode l approache s the correc t model , the utilit y estimate s will ,
606 Chapte r 20 . Reinforcemen t Learnin g
of course , converg e to the correc t utilities . Becaus e the environmen t mode l usuall y change s only
slightl y with each observation , the dynami c programmin g phas e can use valu e iteratio n with the
previou s utilit y estimate s as initia l value s and usuall y converge s quite quickly .
The environmen t mode l is learne d by direc t observatio n of transitions . In an accessi ­
ble environment , each percep t identifie s the state , and henc e each transitio n provide s a direc t
input/outpu t exampl e for the transitio n functio n represente d by M. The transitio n functio n is
usuall y stochastic—tha t is, it specifie s a probabilit y for each possibl e successo r rathe r than a
single state . A reinforcemen t learnin g agen t can use any of the technique s for learnin g stochasti c
function s from example s discusse d in Chapter s 18 and 19. We discus s their applicatio n furthe r
in Sectio n 20.7 .
Continuin g with our tabula r representatio n for the environment , we can updat e the envi ­
ronmen t mode l M simpl y by keepin g track of the percentag e of time s each state transition s to
each of its neighbors . Usin g this simpl e techniqu e in the 4 x 3 worl d from Figur e 20.1 , we obtai n
the learnin g performanc e show n in Figur e 20.8 . Notic e that the ADP metho d converge s far faste r
than eithe r LMS or TD learning .
1ty estimate s
o
O in
S ­0.5
­1
(­ —— — ——— — —— — —— — (4,3) ­
/" """ ­" ".'"""  '"".."  "" ­­­­­ ­­­­ ­ (3,3 )
, •' ' '" ' '  ­ ­ ' ­ ­­­­ ­ ­ (2,3 )
(1,1)
­ ..­..­.. ­ — ­­­­•­­•­­­­­•­•­­.­._­­­­ . (3>1) ­
•' .­••• ­ ­­ ­•­ •  ' ­­­­­­­­­­­­­­­­­­­ ­ (4,1 )
­­­­­ ­­­­ ­ , ,  ,  ,  (4.2 ) 1
) 10 0 20 0 30 0 40 0 50 0
Numbe r of epoch s0.6
0.5
>>
I 0.4
1 °­3
g 0. 2
Oi
0.1
0
C"i \
V ~X/~X­­­­/ \­_ — .. ^
) 10 0 20 0 30 0 40 0 50 0
Numbe r of epoch s
(a) (b )
Figur e 20.8 Th e ADP learnin g curve s for the 4x3 world .
The ADP approac h and the TD approac h are actuall y closel y related . Both try to make local
adjustment s to the utilit y estimate s in orde r to mak e each state "agree " with its successors . One
mino r differenc e is that TD adjust s a state to agre e with its observed  successo r (Equatio n (20.2)) ,
wherea s ADP adj usts the state to agre e with all of the successor s that migh t occu r give n an optima l
actio n choice , weighte d by their probabilitie s (Equatio n (20.1)) . Thi s differenc e disappear s
when the effect s of TD adjustment s are average d over a large numbe r of transitions , becaus e
the frequenc y of each successo r in the set of transition s is approximatel y proportiona l to its
probability . A mor e importan t differenc e is that wherea s TD make s a singl e adjustmen t per
observe d transition , ADP make s as man y as it need s to restor e consistenc y betwee n the utilit y
estimate s U and the environmen t mode l M. Althoug h the observe d transitio n only make s a local
chang e in M, its effect s may need to be propagate d throughou t U. Thus , TD can be viewe d as a
crude but efficien t first approximatio n to ADP .
LSectio n 20.4 . Activ e Learnin g in an Unknow n Environmen t 60 7
Each adjustmen t mad e by ADP coul d be viewed , from the TD poin t of view , as a resul t of
a "pseudo­experience " generate d by simulatin g the curren t environmen t model . It is possibl e to
exten d the TD approac h to use an environmen t mode l to generat e severa l pseudo­experiences —
transition s that the TD agen t can imagin e might  happe n give n its curren t model . For each observe d
transition , the TD agen t can generat e a large numbe r of imaginar y transitions . In this way , the
resultin g utilit y estimate s will approximat e mor e and mor e closel y thos e of ADP—o f course , at
the expens e of increase d computatio n time .
In a simila r vein , we can generat e more efficien t version s of ADP by directl y approximatin g
the algorithm s for valu e iteratio n or polic y iteration . Recal l that full valu e iteratio n can be
intractabl e whe n the numbe r of state s is large . Man y of the adjustmen t steps , however , are
extremel y tiny . On e possibl e approac h to generatin g reasonabl y good answer s quickl y is to
boun d the numbe r of adjustment s mad e afte r each observe d transition . On e can also use a
heuristi c to rank the possibl e adjustment s so as to carry out only the mos t significan t ones . The
PRIORITIZED ­ prioritized­sweepin g heuristi c prefer s to mak e adjustment s to state s whos e likely  successor s
have just undergon e a large  adjustmen t in thei r own utilit y estimates . Usin g heuristic s like
this, approximat e ADP algorithm s usuall y can learn roughl y as fast as full ADP , in term s of the
numbe r of trainin g sequences , but can be severa l order s of magnitud e more efficien t in term s of
computatio n (see Exercis e 20.3) . This enable s them to handl e state space s that are far too large
for full ADP . Approximat e ADP algorithm s have an additiona l advantage : in the early stage s of
learnin g a new environment , the environmen t mode l M often will be far from correct , so there is
little poin t in calculatin g an exac t utilit y functio n to matc h it. An approximatio n algorith m can
use a minimu m adjustmen t size that decrease s as the environmen t mode l become s mor e accurate .
This eliminate s the very long valu e iteration s that can occu r early in learnin g due to large change s
in the model .
20.4 ACTIV E LEARNIN G IN AN UNKNOW N ENVIRONMEN T
A passiv e learnin g agen t can be viewe d as havin g a fixe d policy , and need not worr y abou t whic h
action s to take. An activ e agen t mus t conside r wha t action s to take, wha t their outcome s may be,
and how they will affec t the reward s received .
The PASSIVE­RL­AGEN T mode l of page 602 need s only mino r change s to accommodat e
action s by the agent :
• The environmen t mode l mus t now incorporat e the probabilitie s of transition s to othe r state s
given  a particular  action.  We will use Mfj to denot e the probabilit y of reachin g state j if
the actio n a is take n in state /.
• The constraint s on the utilit y of each state mus t now take into accoun t the fact that the agen t
has a choic e of actions . A rationa l agen t will maximiz e its expecte d utility , and instea d of
Equatio n (20.1 ) we use Equatio n (17.3) , whic h we repea t here :
U(i) = R(i) + max ^  MfjU(j)  (20.3 )
608 Chapte r 20 . Reinforcemen t Learnin g
• The agen t mus t now choos e an actio n at each step , and will need a performanc e elemen t
to do so. In the algorithm , this mean s callin g PERFORMANCE­ELEMENT(e ) and returnin g
the resultin g action . We assum e that the mode l M and the utilitie s U are share d by the
performanc e element ; that is the whol e poin t of learnin g them .
We now reexamin e the dynami c programmin g and temporal­differenc e approache s in the ligh t
of the first two changes . The questio n of how the agen t shoul d act is covere d in Sectio n 20.5 .
Becaus e the ADP approac h uses the environmen t model , we will need to chang e the
algorith m for learnin g the model . Instea d of learnin g the probabilit y My of a transition , we will
need to learn the probabilit y Af?• of a transitio n conditione d on takin g an actio n a. In the explici t
tabula r representatio n for M, this simpl y mean s accumulatin g statistic s in a three­dimensiona l
table . Wit h an implici t functiona l representation , as we will soon see, the inpu t to the functio n
will includ e the actio n taken . We will assum e that a procedur e UPDATE­ACTIVE­MODE L take s
care of this. Onc e the mode l has been updated , then the utilit y functio n can be recalculate d usin g
a dynami c programmin g algorith m and then the performanc e elemen t choose s wha t to do next .
We show the overal l desig n for ACTIVE­ADP­AGEN T in Figur e 20.9 .
An activ e temporal­differenc e learnin g agen t that learn s utilit y function s also will need
to learn a mode l in orde r to use its utilit y functio n to mak e decisions . The mode l acquisitio n
proble m for the TD agen t is identica l to that for the ADP agent . Wha t of the TD updat e rule
itself ? Perhap s surprisingly , the updat e rule (20.2 ) remain s unchanged . This migh t seem odd, for
the followin g reason . Suppos e the agen t take s a step that normall y lead s to a good destination ,
but becaus e of nondeterminis m in the environmen t the agen t ends up in a catastrophi c state . The
TD updat e rule will take this as seriousl y as if the outcom e had been the norma l resul t of the
functio n ACTIVE­ADP­AGENT(<? ) return s an actio n
static : U, a table of utilit y estimate s
M, a table of transitio n probabilitie s from state to state for each actio n
R, a table of reward s for state s
percepts,  a percep t sequenc e (initiall y empty )
last­action,  the actio n just execute d
add e to percepts
fl[STATE[e] ] — REWARDfe ]
M — UPDATE ­ ACTIVE­MODEL(M , percepts,  last­action)
U — VALUE­lTERATION(£/ , M, R)
if TERMiNAL?[e ] then
percepts  — the empt y sequenc e
last­action  <— PERFORMANCE­EEEMENT(e )
retur n last­action
Figur e 20.9 Desig n for an activ e ADP agent . Th e agen t learn s an environmen t mode l M
by observin g the result s of its actions , and uses the mode l to calculat e the utilit y functio n
U usin g a dynami c programmin g algorith m (her e POLICY­ITERATIO N coul d be substitute d for
VALUE­ITERATION) .
Sectio n 20.5 . Exploratio n 60 9
action , wherea s one migh t suppos e that becaus e the outcom e was a fluke , the agen t shoul d not
worry abou t it too much . In fact, of course , the unlikel y outcom e will only occu r infrequentl y in
a large set of trainin g sequences ; henc e in the long run its effect s will be weighte d proportionall y
to its probability , as we woul d hope . Onc e again , it can be show n that the TD algorith m will
converg e to the same value s as ADP as the numbe r of trainin g sequence s tend s to infinity .
20.5 EXPLORATION___________________________ _
The only remainin g issu e to addres s for activ e reinforcemen t learnin g is the questio n of wha t
action s the agen t shoul d take—tha t is, what PERFORMANCE­ELEMEN T shoul d return . This turns
out to be harde r than one migh t imagine .
One migh t suppos e that the correc t way for the agen t to behav e is to choos e whicheve r
actio n has the highes t expecte d utilit y give n the curren t utilit y estimates—afte r all, that is all the
agen t has to go on. But this overlook s the contributio n of actio n to learning . In essence , an actio n
has two kind s of outcome:3
• It gain s reward s on the curren t sequence .
• It affect s the percept s received , and henc e the abilit y of the agen t to learn—an d receiv e
reward s in futur e sequences .
An agen t therefor e mus t mak e a trade­of f betwee n its immediat e good—a s reflecte d in its curren t
utilit y estimates—an d its long­ter m well­being . An agen t that simpl y choose s to maximiz e its
reward s on the curren t sequenc e can easil y get stuc k in a rut. At the othe r extreme , continuall y
actin g to improv e one' s knowledg e is of no use if one neve r puts that knowledg e into practice . In
the real world , one constantl y has to decid e betwee n continuin g in a comfortabl e existenc e and
strikin g out into the unknow n in the hope s of discoverin g a new and bette r life.
In orde r to illustrat e the danger s of the two extremes , we will need a suitabl e environment .
We will use the stochasti c versio n of the 4x 3 worl d show n in Figur e 17.1 . In this world , the
agen t can attemp t to mov e North,  South,  East,  or West ; each actio n achieve s the intende d effec t
with probabilit y 0.8, but the rest of the time , the actio n move s the agen t at righ t angle s to the
intende d direction . As before , we assum e a rewar d of —0.0 4 (i.e., a cost of 0.04) for each actio n
that doesn' t reach a termina l state . The optima l polic y and utilit y value s for this worl d are show n
in Figur e 17.2 , and the object  of the learnin g agen t is to converg e toward s these .
Let us conside r two possibl e approache s that the learnin g agen t migh t use for choosin g
what to do. The "wacky " approac h acts randomly , in the hope that it will eventuall y explor e
the entir e environment ; and the "greedy " approac h acts to maximiz e its utilit y usin g curren t
estimates . As we see from Figur e 20.10 , the wack y approac h succeed s in learnin g good utilit y
estimate s for all the state s (top left) . Unfortunately , its wack y polic y mean s that it neve r actuall y
gets bette r at reachin g the positiv e rewar d (top right) . The greed y agent , on the othe r hand , ofte n
finds a path to the+ 1 rewar d along the lowe r route via (2,1) , (3,1) , (3,2) , and (3,3) . Unfortunately ,
it then stick s to that path , neve r learnin g the utilitie s of the othe r state s (botto m left) . This mean s
Notic e the direc t analog y to the theor y of informatio n valu e in Chapte r 16.
610 Chapte r 20 . Reinforcemen t Learnin g
0.6
.?1 0.5
rt 0. 4
•2 0. 3
'E "•­| 0.1
0
_ 0­6
S
g. 0.5
I °'4
1 0.3
•­ 0.2
1 0­1
S
* 0) 50 10 0 150 200 250 300 350 400 450 5
Numbe r of epoch s2.5
rj 2
B.
|T 1.5
3
I 1
1 0.5
0
X) (
2.5
t"
1 '
) 5 0 10 0 150 200 250 300 350 400 450 5
Numbe r of epoch s*:
X) <.
­
50 10 0 150 200 250 300 350 400 450 500
Numbe r of epoch s
50 10 0 150 200 250 300 350 400 450 500
Numbe r of epoch s
Figur e 20.10 Curve s showin g the RMS erro r in utilit y estimate s (left ) and the total loss asso ­
ciate d with the correspondin g polic y (right) , for the wack y (top ) and greed y (bottom ) approache s
to exploration .
that it too fails to achiev e perfectio n (botto m right ) becaus e it does not find the optima l rout e via
(1,2) , (1,3) , and (2,3) .
Obviously , we need an approac h somewher e betwee n wackines s and greediness . The agen t
shoul d be mor e wack y whe n it has little idea of the environment , and mor e greed y whe n it has a
mode l that is close to bein g correct . Can we be a little more precis e than this? Is there an optimal
exploratio n policy ? It turn s out that this questio n has been studie d in dept h in the subfiel d of
BANDI T PROBLEM S statistica l decisio n theor y that deals with so­calle d bandi t problem s (see sidebar) .
Althoug h bandi t problem s are extremel y difficul t to solv e exactl y to obtai n an optimal
exploratio n policy , it is nonetheles s possibl e to com e up with a reasonable  polic y that seem s to
have the desire d properties . In a give n state , the agen t shoul d give som e weigh t to action s that
it has not tried very often , whil e bein g incline d to avoi d action s that are believe d to be of low
utility . Thi s can be implemente d by alterin g the constrain t equatio n (20.3 ) so that it assign s a
highe r utilit y estimat e to relativel y unexplore d action­stat e pairs . Essentially , this amount s to
an optimisti c prio r over the possibl e environments , and cause s the agen t to behav e initiall y as
if there were wonderfu l reward s scattere d all over the place . Let us use U+(i) to denot e the
optimisti c estimat e of the utilit y (i.e. , the expecte d reward­to­go ) of the state i, and let N(a,  f)
be the numbe r of time s actio n a has been tried in state i. Suppos e we are usin g valu e iteratio n
Sectio n 20.5 . Exploratio n 611
EXPLORATIO N AND BANDIT S
In Las Vegas , a one­armed  bandit  is a slot machine . A gamble r can inser t a coin , pull
the lever , and collec t the winning s (if any) . An n­arme d bandi t has n levers . The
gamble r mus t choos e whic h leve r to play on each successiv e coin—th e one that has
paid off best, or mayb e one that has not been tried ?
The tt­arme d bandi t proble m is a forma l mode l for real problem s in man y vi­
tally importan t areas , such as decidin g on the annua l budge t for AI researc h and
development . Eac h arm correspond s to an actio n (suc h as allocatin g $20 millio n for
developmen t of new AI textbooks ) and the payof f from pullin g the arm correspond s
to the benefit s obtaine d from takin g the actio n (immense) . Exploration , whethe r it is
exploratio n of a new researc h field or exploratio n of a new shoppin g mall , is risky ,
expensive , and has uncertai n payoffs ; on the othe r hand , failur e to explor e at all mean s
that one neve r discover s any action s that are worthwhile .
To formulat e a bandi t proble m properly , one mus t defin e exactl y wha t is mean t by
optima ] behavior . Mos t definition s in the literatur e assum e that the aim is to maximiz e
the expecte d total rewar d obtaine d over the agent' s lifetime . Thes e definition s requir e
that the expectatio n be taken over the possibl e world s that the agen t coul d be in, as well
as over the possibl e result s of each actio n sequenc e in any given world . Here , a "world "
is define d by the transitio n mode l M(". Thus , in order to act optimally , the agen t need s
a prio r distributio n over the possibl e models . The resultin g optimizatio n problem s
are usuall y wildl y intractable . In som e cases , however , appropriat e independenc e
assumption s enabl e the proble m to be solve d in close d form . Wit h a row of real slot
machines , for example , the reward s in successiv e time steps and on differen t machine s
can be assume d to be independent . It turns out that the fractio n of one's coins investe d
in a give n machin e shoul d drop off proportionall y to the probabilit y that the machin e
is in fact the best, give n the observe d distribution s of rewards .
The forma l result s that have been obtaine d for optimal  exploratio n policie s appl y
only to the case in whic h the agen t represent s the transitio n mode l as an explici t table
and is not able to generaliz e acros s state s and actions . For more realisti c problems ,
it is possibl e to prov e only convergenc e to a correc t mode l and optima l behavio r in
the limi t of infinit e experience . Thi s is easily  obtaine d by actin g randoml y on some
fractio n of steps , wher e that fractio n decrease s appropriatel y over time .
One can use the theor y of «­arme d bandit s to argu e for the reasonablenes s of the
selectio n strateg y in geneti c algorithm s (see Sectio n 20.8) . If you conside r each arm
in an n­arme d bandi t proble m to be a possibl e strin g of genes , and the investmen t of a
coin in one arm to be the reproductio n of those genes , then geneti c algorithm s allocat e
coins optimally , give n an appropriat e set of independenc e assumptions .
L
612 Chapte r 20 . Reinforcemen t Learnin g
EXPLORATIO N
FUNCTIO Nin an ADP learnin g agent ; then we need to rewrit e the updat e equatio n (i.e.. Equatio n (17.4) ) to
incorporat e the optimisti c estimate . The followin g equatio n does this:
max j\ N(a,  i) (20.4 )
wher e f(u,n)  is calle d the exploratio n function . It determine s how gree d (preferenc e for high
value s of u) is trade d off agains t curiosit y (preferenc e for low value s of n, i.e., action s that have not
been tried often) . The function/(w,« ) shoul d be increasin g in u, and decreasin g in n. Obviously ,
there are man y possibl e function s that fit thes e conditions . One particularl y simpl e definitio n is
the following :
/(«,« ) =R+ ifn<N e
u otherwis e
wher e R+ is an optimisti c estimat e of the best possibl e rewar d obtainabl e in any state , and Ne is a
fixed parameter . Thi s will have the effec t of makin g the agen t try each action­stat e pair at least
Ne times .
The fact that U+ rathe r than U appear s on the right­han d side of Equatio n (20.4 ) is very
important . As exploratio n proceeds , the state s and action s near the start state may well be tried
a large numbe r of times . If we used U, the nonoptimisti c utilit y estimate , then the agen t woul d
soon becom e disincline d to explor e furthe r afield . Th e use of U+ mean s that the benefit s of
exploratio n are propagate d back from the edge s of unexplore d regions , so that action s that lead
toward  unexplore d region s are weighte d more highly , rathe r than just action s that are themselve s
unfamiliar . The effec t of this exploratio n polic y can be seen clearl y in Figur e 20.11 , whic h
show s a rapid convergenc e towar d optima l performance , unlik e that of the wack y or the greed y
approaches . A very nearl y optima l polic y is foun d afte r just 18 trials . Notic e that the utilit y
estimate s themselve s do not converg e as quickly . Thi s is becaus e the agen t stop s explorin g the
unrewardin g part s of the state spac e fairl y soon , visitin g them only "by accident " thereafter .
However , it make s perfect  sens e for the agen t not to care abou t the exac t utilitie s of state s that it
know s are undesirabl e and can be avoided .
20.6 LEARNIN G AN ACTION­VALU E FUNCTIO N
An action­valu e functio n assign s an expecte d utilit y to takin g a give n actio n in a give n state ;
as mentione d earlier , such value s are also calle d Q­values . We will use the notatio n Q(a,i)  to i
denot e the valu e of doin g actio n a in state i. Q­value s are directl y relate d to utilit y value s by the ,
followin g equation :
U(i) = max Q(a,  i) (20.5) ;
a
Q­value s play an importan t role in reinforcemen t learnin g for two reasons : first , like condition ­ j
actio n rules , they suffic e for decisio n makin g withoutth e use of a model ; second , unlik e condition ­ j
actio n rules , they can be learne d directl y from rewar d feedback .
Sectio n 20.6 . Learnin g an Action­Valu e Functio n 613
L2
1.5
S 1rt
1 0.5
_>!
i oD
­0.5
­1
(\
\t
­ (4,3 ) ­ ­ .... . ­­ ­ ­­ ­­ ­ ­­­....... ­
(3.3) ­­­­ ­
­ (2,3 )
(1,1)
(3.1) ­ —
" (4,1 ) ­­­ 1
(4,2)
) 2 0 4 0 6 0 8 0 10 0
Numbe r of iteration s
(a)I 1.4
>,
1.2
' . 1
" 0.8
">, 0.6
"! 0.4
t 0.2
OJ
c/35 0* (x i  RM S erro r —  —
L \ i Polic y loss
;• '  '  ——— —
) 2 0 4 0 6 0 8 0 10 0
Numbe r of epoch s
(b)
Figur e 20.11 Performanc e of the explorator y ADP agent , usin g R+ = 2 and Ne = 5. (a) Utilit y
estimate s for selecte d state s over time . Afte r the initia l explorator y phas e in whic h the state s get
an exploratio n bonus , the high­value d state s quickl y reac h their correc t values . The low­value d
states converg e slowl y becaus e they are seldo m visited , (b) The RMS erro r in utilit y value s and
the associate d polic y loss.
As with utilities , we can writ e a constrain t equatio n that mus t hold at equilibriu m whe n the
Q­value s are correct :
Q(a, i) = R(i) + f max Q(a<  J) (20.6 )
As in the ADP learnin g agent , we can use this equatio n directl y as an updat e equatio n for an
iteratio n proces s that calculate s exac t Q­value s give n an estimate d model . Thi s does , however ,
requir e that a mode l be learne d as wel l becaus e the equatio n uses Mfj. The temporal­differenc e
approach , on the othe r hand , require s no model . The updat e equatio n for TD Q­learnin g is
Q(a, i) *­ Q(a,  i) + a (R(i) + max Q(a',j)  ­ Q(a,  /)) (20.7 )
a1
whic h is calculate d after each transitio n from state / to state j.
The complet e agen t desig n for an explorator y Q­learnin g agen t usin g TD is show n in
Figur e 20.12 . Notic e that it uses exactl y the same exploratio n function / as used by the explorator y
ADP agent , henc e the need to keep statistic s on action s take n (the table N). If a simple r exploratio n
polic y is used—say , actin g randoml y on som e fractio n of steps , wher e the fractio n decrease s over
time—the n we can dispens e with the statistics .
Figur e 20.1 3 show s the performanc e of the Q­learnin g agen t in our 4x3 world . Notic e that
the utilit y estimate s (derive d from the Q­value s usin g Equatio n (20.5) ) take muc h longe r to settle
down than they did with the ADP agent . This is becaus e TD does not enforc e consistenc y amon g
value s via the model . Althoug h a good polic y is foun d after only 26 trials , it is considerabl y
furthe r from optimalit y than that foun d by the ADP agen t (Figur e 20.11) .
Althoug h these experimenta l result s are for just one set of trials on one specifi c environment ,
they do raise a genera l question : is it bette r to learn a mode l and a utilit y functio n or to learn
an action­valu e functio n with no model ? In othe r words , wha t is the best way to represen t the
614 Chapte r 20 . Reinforcemen t Learnin g
functio n Q­LEARNiNG­AGENT(e ) return s an actio n
static : Q, a table of actio n value s
N, a table of state­actio n frequencie s
a, the last actio n take n
(', the previou s state visite d
r, the rewar d receive d in state i
j <— STATE[e ]
if (' is non­nul l then
Q[a,i}­Q\a,i]+a(r
if TERMlNAL?[e ] then
i — null
else
r — REWARD[e ]
a — arg max,, , f(Q[a'  ,j\, N[a',j\)
retur n amax,, , Q[a'  J] ­ Q\a,i])
Figur e 20.1 2 A n explorator y Q­learnin g agent . It is an activ e learne r that learn s the valu e
Q(a, i) of each actio n in each situation . It uses the same exploratio n function / as the explorator y
ADP agent , but avoid s havin g to learn the transitio n mode l M|j becaus e the Q­valu e of s state can
be relate d directl y to thos e of its neighbors .
­­­­­­­• " ­­"­' ­ " ­ •­­­; •­­•­•:­­•­ . ,,­•­­­.:­. . ­., .­•..,  ­­­­­­  .:­..­.
•\* 0,5 ­ .  ­  .  ­
OJ^£
B o
s
3 ­0,5
­1­ ­  . (4,3 ) ­ ­  •• ­
(3,3) ­ ­
(2,3)(1,1 ) ,(3,1 ) •— 1
(4,1) ­­­
(4,2)ou
I 1­ 415
•r 1.2O1
D iH '
1 0.8
.j? 0.6
^04
Q
S 0.2<y:
^ A.
RMS error ­ —
Polic y loss ­­­­
­
­­_'
'
___—___­­_—'"" ' — v .... __.­_^_­ " — ­­• — ­ — "
"" r~" "" "" , "" "
0 2 0 4 0 6 0 8 0 10 0 "  "  0 2 0 4 0 6 0 8 0 10 0
Numbe r of iteration s Numbe r of epoch s
(a) (b )
Figur e 20.1 3 Performanc e of the explorator y TD Q­learnin g agent , usin g R+ = 2 and Ne = 5.
(a) Utilit y estimate s for selecte d state s over time , (b) The RM S erro r in utilit y value s and the
associate d polic y loss .
Sectio n 20.7 . Generalizatio n in Reinforcemen t Learnin g 615
agen t function ? Thi s is an issu e at the foundation s of artificia l intelligence . As we state d in
Chapte r 1, one of the key historica l characteristic s of muc h of AI researc h is its (ofte n unstated )
adherenc e to the knowledge­base d approach . This amount s to an assumptio n that the best way
to represen t the agen t functio n is to construc t an explici t representatio n of at least  som e aspect s
of the environmen t in whic h the^agen t is situated .
Some researchers , both insid e and outsid e AI, have claime d that the availabilit y of model ­
free method s such as Q­learnin g mean s that the knowledge­base d approac h is unnecessary . Ther e
is, however , little to go on but intuition . Our intuition , for what it's worth , is that as the environmen t
become s mor e complex , the advantage s of a knowledge­base d approac h becom e mor e apparent .
This is born e out even in game s such as chess , checker s (draughts) , and backgammo n (see next
section) , wher e effort s to learn an evaluatio n functio n usin g a mode l have met with more succes s
than Q­learnin g methods . Perhap s one day ther e will be a deepe r theoretica l understandin g of
the advantage s of explici t knowledge ; but as yet we do not even hav e a forma l definitio n of
the differenc e betwee n model­base d and model­fre e systems . All we have are som e purporte d
example s of each .
20.7 GENERALIZATIO N IN REINFORCEMEN T LEARNIN G
EXPLICI T
REPRESENTATIO N
IMPLICI T
REPRESENTATIO N
INPUT
GENERALIZATIO NSo far we have assume d that all the function s learne d by the agent s (U, M, R, Q) are represente d in
tabula r form —tha t is, an explici t representatio n of one outpu t valu e for each inpu t tuple . Suc h
an approac h work s reasonabl y well for smal l state spaces , but the time to convergenc e and (for
ADP ) the time per iteratio n increas e rapidl y as the spac e gets larger . Wit h carefull y controlled ,
approximat e ADP methods , it may be possibl e to handl e 10,00 0 state s or more.  Thi s suffice s
for two­dimensional , maze­lik e environments , but more realisti c world s are out of the question .
Ches s and backgammo n are tiny subset s of the real world , yet thei r state space s contai n on the
order  of 1050 to 1012() states . It woul d be absur d to suppos e that one mus t visit all thes e state s in
order to learn how to play the game !
The only way to handl e such problem s is to use an implici t representatio n of the function —
a form that allow s one to calculat e the outpu t for any input , but that is usuall y muc h mor e
compac t than the tabula r form . For example , an estimate d utilit y functio n for gam e playin g can
be represente d as a weighte d linea r functio n of a set of boar d feature s f\ ,...,/„ :
U(i)  ­ Wi/i( 0 + W>2/2( 0 + • • • + W,,f n(i)
Thus , instea d of, say, 10120 values , the utilit y functio n is characterize d by the n weights . A
typica l ches s evaluatio n functio n migh t only hav e abou t 10 weights , so this is an enormous
compression . The  compression  achieved  by an implicit  representation  allows  the learning  agent
to generalize  from  states it  has visited  to states  it has not visited.  Tha t is, the mos t importan t
aspec t of an implici t representatio n is not that it take s up less space , but that it allow s for inductiv e
generalizatio n over inpu t states . For this reason , method s that learn such representation s are said
to perfor m inpu t generalization . To give you som e idea of the powe r of inpu t generalization :
by examinin g only one in 1044 of the possibl e backgammo n states , it is possibl e to learn a utilit y
functio n that allows  a progra m to play as well as any huma n (Tesauro , 1992) .
616 Chapte r 20 . Reinforcemen t Learnin g
On the flip side , of course , there is the proble m that there may be no functio n in the chose n
space of implici t representation s that faithfull y approximate s the true utilit y function . As in all
inductiv e learning , ther e is a trade­of f betwee n the size of the hypothesi s spac e and the time
it take s to learn the function . A large r hypothesi s spac e increase s the likelihoo d that a good
approximatio n can be found , but^als o mean s that convergenc e is likel y to be delayed .
Let us now conside r exactl y how the inductiv e learnin g proble m shoul d be formulated .
We begi n by considerin g how to learn utilit y and action­valu e functions , and then mov e on to
learnin g the transitio n functio n for the environment .
In the LMS (leas t mea n squares ) approach , the formulatio n is straightforward . At the end
of each trainin g sequence , the LMS algorith m associate s a reward­to­g o with each state visite d
along the way. The (state,  reward]  pair can be used directl y as a labelle d exampl e for any desire d
inductiv e learnin g algorithm . This yield s a utilit y functio n [/(;').
It is also possibl e for a TD (temporal­difference ) approac h to appl y inductiv e learnin g
directly , once the U and/o r Q table s have been replace d by implici t representations . The value s
that woul d be inserte d into the table s by the updat e rules (20.2 and 20.7 ) can be used instea d as
labelle d example s for a learnin g algorithm . The agen t has to use the learne d functio n on the next
update , so the learnin g algorith m mus t be incremental .
One can also take advantag e of the fact that the TD updat e rules provid e smal l change s in
the valu e of a give n state . This is especiall y true if the functio n to be learne d is characterize d by
a vecto r of weight s w (as in linea r weighte d function s and neura l networks) . Rathe r than updat e
a singl e tabulate d valu e of f/, as in Equatio n (20.2) , we simpl y adjus t the weight s to try to reduc e
the tempora l differenc e in U betwee n successiv e states . Suppos e that the parameterize d utilit y
functio n is Uw(i). The n after a transitio n / —j, we appl y the followin g updat e rule:
w ­ w + a[r + ­ t/w(01 Vwt/ w(i) (20.8 )
This form of updatin g perform s gradien t descen t in weigh t space , tryin g to minimiz e the observe d
local error in the utilit y estimates . A simila r updat e rule can be used for Q­learnin g (Exercise  20.9) .
Becaus e the utilit y and action­valu e function s have real­value d outputs , neura l network s and
other continuou s functio n representation s are obviou s candidate s for the performanc e element .
Decision­tre e learnin g algorithm s that provid e real­value d outpu t can also be used (see for
MODE L TREE S exampl e Quinlan' s (1993 ) mode l trees) , but canno t use the gradien t descen t method .
The formulatio n of the inductiv e learnin g proble m for constructin g a mode l of the envi ­
ronmen t is also very straightforward . Eac h transitio n provide s the agen t with the next state (at
least in an accessibl e environment) , so that labelle d example s consis t of a state­actio n pair as
inpu t and a state as output . It is not so easy , however , to find a suitabl e implici t representatio n
for the model . In orde r to be usefu l for valu e and polic y iteratio n and for the generatio n of
pseudo­experience s in TD learning , the outpu t state descriptio n mus t be sufficientl y detaile d
to allow predictio n of outcome s severa l step s ahead . Simpl e parametri c form s canno t usuall y
sustai n this kind of reasoning . Instead , it may be necessar y to learn genera l actio n model s in
the logica l form used in Chapter s 7 and 11. In a nondeterministi c environment , one can use the
conditional­probability­tabl e representatio n of state evolutio n typica l of dynami c belie f network s
(Sectio n 17.5) , in whic h generalizatio n is achieve d by describin g the state in term s of a large set of
feature s and usin g only spars e connections . Althoug h model­base d approache s have advantage s
in term s of their abilit y to lear n valu e function s quickly , they are currentl y hampere d by a lack
Sectio n 20.7 . Generalizatio n in Reinforcemen t Learnin g 617
of suitabl e inductiv e generalizatio n method s for learnin g the model . It is also not obviou s how
method s such as valu e and polic y iteratio n can be applie d with a generalize d model .
We now turn to example s of large­scal e application s of reinforcemen t learning . We will
see that in cases wher e a utilit y functio n (and henc e a model ) is used , the mode l is usuall y taken as
given . For example , in learnin g an< evaluatio n functio n for backgammon , it is normall y assume d
that the legal moves , and their effects , are know n in advance .
CART­POL E
INVERTE DPENDULU MApplication s to game­playin g
The first significan t applicatio n of reinforcemen t learnin g was also the first significan t learnin g
progra m of any kind—th e checker­playin g progra m writte n by Arthu r Samue l (1959 ; 1967) .
Samue l first used a weighte d linea r functio n for the evaluatio n of positions , usin g up to 16 term s
at any one time . He applie d a versio n of Equatio n (20.8 ) to updat e the weights . Ther e were som e
significan t differences , however , betwee n his progra m and curren t methods . First , he update d
the weight s usin g the differenc e betwee n the curren t state and the backed­u p valu e generate d by
full lookahea d in the searc h tree. This work s fine , becaus e it amount s to viewin g the state spac e
at a differen t granularity . A secon d differenc e was that the progra m did not use any observe d
rewards ! Tha t is, the value s of termina l state s were ignored . This mean s that it is quit e possibl e
for Samuel' s progra m not to converge , or to converg e on a strateg y designe d to lose rathe r than
win. He manage d to avoi d this fate by insistin g that the weigh t for materia l advantag e shoul d
alway s be positive . Remarkably , this was sufficien t to direc t the progra m into area s of weigh t
space correspondin g to good checke r play (see Chapte r 5).
The TD­gammo n syste m (Tesauro , 1992 ) forcefull y illustrate s the potentia l of reinforce ­
ment learnin g techniques . In earlie r work (Tesaur o and Sejnowski , 1989) , Tesaur o tried learnin g
a neura l networ k representatio n of Q(a,  i) directl y from example s of move s labelle d with relativ e
value s by a huma n expert . This approac h prove d extremel y tediou s for the expert . It resulte d in a
program , calle d Neurogammon , that was stron g by compute r standard s but not competitiv e with
huma n grandmasters . The TD­gammo n projec t was an attemp t to learn from self­pla y alone . The
only rewar d signa l was give n at the end of each game . The evaluatio n functio n was represente d
by a full y connecte d neura l networ k with a singl e hidde n laye r containin g 40 nodes . Simpl y by
repeate d applicatio n of Equatio n (20.8) , TD­gammo n learne d to play considerabl y bette r than
Neurogammon , even thoug h the inpu t representatio n containe d just the raw boar d positio n with
no compute d features . This took abou t 200,00 0 trainin g game s and two week s of compute r time .
Althoug h this may seem like a lot of games , it is only a vanishingl y smal l fractio n of the state
space . Whe n precompute d feature s were adde d to the inpu t representation , a networ k with 80
hidde n units was able, after 300,00 0 trainin g games , to reach a standar d of play comparabl e with
the top three huma n player s worldwide .
Applicatio n to robo t contro l
The setu p for the famou s cart­pol e balancin g problem , also know n as the inverte d pendulum ,
is show n in Figur e 20.14 . The proble m is to contro l the positio n x of the cart so that the pole
stays roughl y uprigh t (6 K it 12), whil e stayin g withi n the limit s of the cart trac k as shown .
This proble m has been used as a test bed for researc h in contro l theor y as well as reinforcemen t
618 Chapte r 20 . Reinforcemen t Learnin g
Figur e 20.1 4 Setu p for the proble m of balancin g a long pole on top of a movin g cart. The cart
can be jerke d left or righ t by a controlle r that observe s x, 6, x, and 6.
BANG­BAN G
CONTRO Llearning , and over 200 paper s have been publishe d on it. The cart­pol e proble m differ s from the
problem s describe d earlie r in that the state variable s x, 9, x, and 0 are continuous . The action s
are usuall y discrete—jer k left or jerk right , the so­calle d bang­ban g contro l regime .
The earlies t wor k on learnin g for this proble m was carrie d out by Michi e and Cham ­
bers (1968) . Thei r BOXE S algorith m was able to balanc e the pole for over an hou r after only
abou t 30 trials . Moreover , unlik e man y subsequen t systems , BOXE S was implemente d usin g a
real cart and pole , not a simulation . The algorith m first discretize d the four­dimensiona l state
space into boxes , henc e the name . It then ran trial s unti l the pole fell over or the cart hit the end of
the track . Negativ e reinforcemen t was associate d with the fina l actio n in the fina l box , and then
propagate d back throug h the sequence . It was foun d that the discretizatio n cause s som e problem s
when the apparatu s was initialize d in a differen t positio n from thos e used in training , suggestin g
that generalizatio n was not perfect . Improve d generalizatio n and faste r learnin g can be obtaine d
using an algorith m that adoptively  partition s the state spac e accordin g to the observe d variatio n
in the reward .
More recently , neura l network s have been used to provid e a continuou s mappin g from
the state spac e to the actions , with slightl y bette r results . The mos t impressiv e performance ,
however , belong s to the contro l algorith m derive d usin g classica l contro l theor y for the triple
inverte d pendulum , in whic h three pole s are balance d one on top of anothe r with torqu e control s
at the joint s (Furut a et al., 1984) . (On e is disappointed , but not surprised , that this algorith m was
implemente d only in simulation. )
Sectio n 20.8 . Geneti c Algorithm s and Evolutionar y Programmin g 619
20.8 GENETI C ALGORITHM S AND EVOLUTIONAR Y PROGRAMMIN G
FITNES S FUNCTIO N
GENENatur e has a robus t way of evolvin g successfu l organisms . The organism s that are ill­suite d for an
environmen t die off, wherea s the ones that are fit live to reproduce . Offsprin g are simila r to their
parents , so each new generatio n has organism s that are simila r to the fit member s of the previou s
generation . If the environmen t change s slowly , the specie s can graduall y evolv e alon g with it,
but a sudde n chang e in the environmen t is likel y to wipe out a species . Occasionally , rando m
mutation s occur , and althoug h mos t of these mea n a quic k deat h for the mutate d individual , som e
mutation s lead to new successfu l species . The publicatio n of Darwin' s The Origin  of Species  on
the Basis of  Natural  Selection  was a majo r turnin g poin t in the histor y of science .
It turn s out that what' s good for natur e is also good for artificia l systems . Figur e 20.1 5
show s the GENETIC­ALGORITHM , whic h start s with a set of one or mor e individual s and applie s
selectio n and reproductio n operator s to "evolve " an individua l that is successful , as measure d by
a fitnes s function . Ther e are severa l choice s for wha t the individual s are. The y can be entir e
agent functions , in whic h case the fitnes s functio n is a performanc e measur e or rewar d function ,
and the analog y to natura l selectio n is greatest . The y can be componen t function s of an agent , in
whic h case the fitnes s functio n is the critic . Or they can be anythin g at all that can be frame d as
an optimizatio n problem .
Since the evolutionar y proces s learn s an agen t functio n base d on occasiona l reward s (off­
spring ) as supplie d by the selectio n function , it can be seen as a form of reinforcemen t learning .
Unlik e the algorithm s describe d in the previou s sections , however , no attemp t is mad e to learn
the relationshi p betwee n the reward s and the action s take n by the agen t or the state s of the
environment . GENETIC­ALGORITH M simpl y searche s directl y in the spac e of individuals , with
the goal of findin g one that maximize s the fitnes s function . The searc h is paralle l becaus e each
individua l in the populatio n can be seen as a separat e search . It is hill climbin g becaus e we are
makin g smal l geneti c change s to the individual s and usin g the best resultin g offspring . The key
questio n is how to allocat e the searchin g resources : clearly , we shoul d spen d mos t of our time on
the mos t promisin g individuals , but if we ignor e the low­scorin g ones , we risk gettin g stuck on a
local maximum . It can be show n that, unde r certai n assumptions , the geneti c algorith m allocate s
resource s in an optima l way (see the discussio n of /i­arme d bandit s in, e.g., Goldber g (1989)) .
Befor e we can appl y GENETIC­ALGORITH M to a problem , we need to answe r the followin g
four questions :
• Wha t is the fitnes s function ?
• How is an individua l represented ?
• How are individual s selected ?
• How do individual s reproduce ?
The fitnes s functio n depend s on the problem , but in any case , it is a functio n that take s an
individua l as inpu t and return s a real numbe r as output .
In the "classic " geneti c algorith m approach , an individua l is represente d as a strin g over a
finite alphabet . Eac h elemen t of the strin g is calle d a gene . In real DNA , the alphabe t is AGT C
(adenine , guanine , thymine , cytosine) , but in geneti c algorithms , we usuall y use the binar y alpha ­
bet (0,1) . Som e author s reserv e the term "geneti c algorithm " for case s wher e the representatio n
620 Chapte r 20 . Reinforcemen t Learnin g
EVOLUTIONAR Y
PROGRAMMIN G
CROSS­OVE R
MUTATIO Nfunctio n GENETIC­ ALGOKITHM ( population,  FITNESS­FN ) return s an individua l
inputs : population,  a set of individual s
FiTNESS­FN , a functio n that measure s the fitnes s of an individua l
repea t
parents  <— SELECTION  population,  FiTNESS­FN )
population  <— REPRODUCTION ( parents)
until som e individua l is fit enoug h
retur n the best individua l in population,  accordin g to FITNESS­F N
Figur e 20.1 5 Th e geneti c algorith m find s a fit individua l usin g simulate d evolution .
is a bit string , and use the term evolutionar y programmin g whe n the representatio n is mor e
complicated . Othe r author s mak e no distinction , or mak e a slightl y differen t one.
The selectio n strateg y is usuall y randomized , with the probabilit y of selectio n proportiona l
to fitness . Tha t is, if individua l X score s twic e as high as Y on the fitnes s function , then X is twic e
as likel y to be selecte d for reproductio n than is Y. Usually , selectio n is done with replacement ,
so that a very fit individua l will get to reproduc e severa l times .
Reproductio n is accomplishe d by cross­ove r and mutation . First , all the individual s that
have been selecte d for reproductio n are randoml y paired . The n for each pair, a cross­ove r poin t
is randoml y chosen . Thin k of the gene s of each paren t as bein g numbere d from 1 to N. The
cross­ove r poin t is a numbe r in that range ; let us say it is 10. Tha t mean s that one offsprin g
will get genes  1 throug h 10 from the first parent , and the rest from the secon d parent . Th e
secon d offsprin g will get gene s 1 throug h 10 from the secon d parent , and the rest from the
first. However , each gene can be altere d by rando m mutatio n to a differen t value , with smal l
independen t probability . Figur e 20.1 6 diagram s the process .
For example , suppos e we are tryin g to learn a decisio n list representatio n for the restauran t
waitin g proble m (see pag e 556) . Th e fitnes s functio n in this case is simpl y the numbe r of
example s that an individua l is consisten t with . The representatio n is the trick y part. Ther e are
ten attribute s in the problem , but not all of them are binary . It turn s out that we need 5 bits to
represen t each distinc t attribute/valu e pair:
0000 0 : Alternate^)
0000 1 : ­iAlternate(x)
1011 1 : WaitEstimate(x,Q­W)
11000 : WaitEstimate(x,  10­30 )
1100 1 : WaitEstimate(x,  30­60)
1101 0 : WaitEstimate(x,>60)
We also need one bit for each test to say if the outcom e is Yes or No. Thus , if we wan t to represen t
a k­DL  with a lengt h of up to t tests , we need a representatio n with t(5k +1) bits. We can use
the standar d selectio n and reproductio n approaches . Mutatio n can flip an outcom e or chang e an
attribute . Cross­ove r combine s the head of one decisio n list with the tail of another .
Sectio n 20.9 . Summar y 621
(a) (b ) (c )
Initia l Populatio n Fitnes s Functio n Selectio n(d)
Cross­Ove r(e)
Mutatio n
Figur e 20.1 6 Th e geneti c algorithm . In (a), we have an initia l populatio n of 4 individuals .
They are score d by the fitnes s functio n in (b); the top individua l score s an 8 and the botto m score s
a 5. It work s out that the top individua l has a 32% chanc e of bein g chose n on each selection . In
(c), selectio n has given  us two pair s of mates , and the cross­ove r point s (dotte d lines ) have been
chosen . Notic e that one individua l mate s twice ; one not at all. In (d), we see the new offspring ,
generate d by cross­ove r of their parents ' genes . Finally , in (e), mutatio n has change d the two bits
surrounde d by boxes . This give s us the populatio n for the next generation .
Like neura l networks , geneti c algorithm s are easy to appl y to a wide rang e of problems . The
result s can be very good on som e problems , and rathe r poor on others . In fact, Denker' s remar k
that "neura l network s are the secon d best way of doin g just abou t anything " has been extende d
with "and geneti c algorithm s are the third. " But don' t be afrai d to try a quic k implementatio n of
a geneti c algorith m on a new problem—jus t to see if it does work—befor e investin g mor e time
thinkin g abou t anothe r approach .
20.9 SUMMAR Y
This chapte r has examine d the reinforcemen t learnin g problem—ho w an agen t can becom e profi ­
cient in an unknow n environmen t give n only its percept s and occasiona l rewards . Reinforcemen t
learnin g can be viewe d as a microcos m for the entir e AI problem , but is studie d in a numbe r of
simplifie d setting s to facilitat e progress . The followin g majo r point s were made :
• The overal l agen t desig n dictate s the kind of informatio n that mus t be learned . The two
main design s studie d are the model­base d design , usin g a mode l M and a utilit y functio n
U, and the model­fre e approach , usin g an action­valu e functio n Q.
• The utilit y of a state is the expecte d sum of reward s receive d betwee n now and terminatio n
of the sequence .
• Utilitie s can be learne d usin g three approaches .
1. The LMS (least­mean­square ) approac h uses the tota l observe d reward­to­g o for a
given state as direc t evidenc e for learnin g its utility . LMS uses the mode l only for the
purpose s of selectin g actions .
622 Chapte r 20 . Reinforcemen t Learnin g
2. The ADP (adaptiv e dynami c programming ) approac h uses the valu e or polic y iteratio n
algorith m to calculat e exac t utilitie s of state s give n an estimate d model . AD P make s
optima l use of the loca l constraint s on utilitie s of state s impose d by the neighborhoo d
structur e of the environment .
3. The TD (temporal­difference ) approac h update s utilit y estimate s to matc h thos e of
successo r states , and can be viewe d as a simpl e approximatio n to the ADP approac h
that require s no mode l for the learnin g process . Usin g the mode l to generat e pseudo ­
experience s can, however , resul t in faste r learning .
• Action­valu e functions , or Q­functions , can be learne d by an AD P approac h or a TD
approach . Wit h TD, Q­learnin g require s no mode l in eithe r the learnin g or action­selectio n
phases . Thi s simplifie s the learnin g proble m but potentiall y restrict s the abilit y to learn in
comple x environments .
• Whe n the learnin g agen t is responsibl e for selectin g action s whil e it learns , it mus t trade
off the estimate d valu e of thos e action s agains t the potentia l for learnin g usefu l new
information . Exac t solutio n of the exploratio n proble m is infeasible , but som e simpl e
heuristic s do a reasonabl e job.
• In larg e state spaces , reinforcemen t learnin g algorithm s mus t use an implici t functiona l
representatio n in orde r to perfor m inpu t generalizatio n over states . The temporal­differenc e
signa l can be used directl y to direc t weigh t change s in parametri c representation s such as
neura l networks .
• Combinin g inpu t generalizatio n with an explici t mode l has resulte d in excellen t perfor ­
manc e in comple x domains .
• Geneti c algorithm s achiev e reinforcemen t learnin g by usin g the reinforcemen t to increas e
the proportio n of successfu l function s in a populatio n of programs . The y achiev e the effec t
of generalizatio n by mutatin g and cross­breedin g program s with each other .
Becaus e of its potentia l for eliminatin g hand codin g of contro l strategies , reinforcemen t learnin g
continue s to be one of the mos t activ e area s of machin e learnin g research . Application s in
robotic s promis e to be particularl y valuable . As yet, however , ther e is little understandin g of how
to exten d these method s to the more powerfu l performanc e element s describe d in earlie r chapters .
Reinforcemen t learnin g in inaccessible  environment s is also a topic of curren t research .
BIBLIOGRAPHICA L AND HISTORICA L NOTL S
Arthu r Samuel' s wor k (1959 ) was probabl y the earlies t successfu l machin e learnin g research .
Althoug h this work was informa l and had a numbe r of flaws , it containe d most of the moder n ideas
in reinforcemen t learning , includin g tempora l differencin g and inpu t generalization . Aroun d the
same time , researcher s in adaptiv e contro l theor y (Widro w and Hoff , 1960) , buildin g on work by
Hebb (1949) , were trainin g simpl e network s usin g the LMS rule. (Thi s early connectio n betwee n
neura l network s and reinforcemen t learnin g may have led to the persisten t misperceptio n that the
latter is a subfiel d of the former. ) The cart­pol e wor k of Michi e and Chamber s (1968 ) can also
be seen as a reinforcemen t learnin g metho d with inpu t generalization .
Sectio n 20.9 . Summar y 623
A mor e recen t traditio n spring s from work at the Universit y of Massachusett s in the early
1980s (Bart o et al., 1981) . The pape r by Sutto n (1988 ) reinvigorate d reinforcemen t learnin g
researc h in AI, and provide s a good historica l overview . The Ph.D . these s by Watkin s (1989 )
and Kaelblin g (1990 ) and the surve y by Bart o et al. (1991 ) also contai n good review s of the
field . Watkin' s thesi s originate d Q­learning , and prove d its convergenc e in the limit . Som e
recen t wor k appear s in a specia l issu e of Machine  Learning  (Vol . 8, Nos . 3/4, 1992) , with
an excellen t introductio n by Sutton . Th e presentatio n in this chapte r is heavil y influence d
by Moor e and Atkeso n (1993) , who mak e a clea r connectio n betwee n tempora l differencin g
and classica l dynami c programmin g techniques . The latte r pape r also introduce d the idea of
prioritize d sweeping . An almos t identica l metho d was develope d independentl y by Peng and
William s (1993) . Bandi t problems , whic h mode l the proble m of exploration , are analyze d in
depth by Berr y and Fristed t (1985) .
Reinforcemen t learnin g in game s has also undergon e a renaissanc e in recen t years . In addi ­
tion to Tesauro' s work , a world­clas s Othell o syste m was develope d by Lee and Mahaja n (1988) .
Reinforcemen t learnin g paper s are publishe d frequentl y in the journa l Machine  Learning,  and in
the Internationa l Conference s on Machin e Learning .
Geneti c algorithm s originate d in the work of Friedber g (1958) , who attempte d to produc e
learnin g by mutatin g smal l FORTRA N programs . Sinc e most mutation s to the program s produce d
inoperativ e code , little progres s was made . John Hollan d (1975 ) reinvigorate d the field by usin g
bit­strin g representation s of agent s such that any possibl e strin g represente d a functionin g agent .
John Koza (1992 ) has champione d more comple x representation s of agent s couple d with mutatio n
and matin g technique s that pay carefu l attentio n to the synta x of the representatio n language .
Curren t researc h appear s in the annua l Conferenc e on Evolutionar y Programming .
EXERCISE S
20.1 Sho w that the estimate s develope d by the LMS­UPDAT E algorith m do indee d minimiz e the
mean squar e error on the trainin g data .
20.2 Implemen t a passiv e learnin g agen t in a simpl e environment , such as that show n in
Figur e 20.1 . For the case of an initiall y unknow n environmen t model , compar e the learnin g
performanc e of the LMS,  TD, and ADP algorithms .
20.3 Startin g with the passiv e ADP agent , modif y it to use an approximat e AD P algorith m as
discusse d in the text. Do this in two steps :
a. Implemen t a priorit y queu e for adjustment s to the utilit y estimates . Wheneve r a state is
adjusted , all of its predecessor s also becom e candidate s for adjustment , and shoul d be
added to the queue . The queu e is initialize d usin g the state from whic h the mos t recen t
transitio n took place . Chang e ADP­UPDAT E to allow only a fixed numbe r of adjustments .
b. Experimen t with variou s heuristic s for orderin g the priorit y queue , examinin g thei r effec t
on learnin g rates and computatio n time .
624 Chapte r 20 . Reinforcemen t Learnin g
20.4 Th e environment s used in the chapte r all assum e that trainin g sequence s are finite . In
environment s with no clea r terminatio n point , the unlimite d accumulatio n of reward s can lead
to problem s with infinit e utilities . To avoi d this, a discoun t facto r 7 is ofte n used , wher e 7 < 1.
A rewar d k step s in the futur e is discounte d by a facto r of ­/. For each constrain t and updat e
equatio n in the chapter , explai n how to incorporat e the discoun t factor .
20.5 Th e descriptio n of reinforcemen t learnin g agent s in Sectio n 20.1 uses distinguishe d ter­
mina l state s to indicat e the end of a trainin g sequence . Explai n how this additiona l complicatio n
could be eliminate d by modellin g the "reset " as a transitio n like any other . How will this affec t
the definitio n of utility ?
20.6 Prov e formall y that Equation s (20.1 ) and (20.3 ) are consisten t with the definitio n of utilit y
as the expecte d reward­to­g o of a state .
20.7 Ho w can the valu e determinatio n algorith m be used to calculat e the expecte d loss experi ­
enced by an agen t usin g a give n set of utilit y estimate s U and an estimate d mode l M, compare d
to an agen t usin g correc t values ?
20.8 Adap t the vacuu m worl d (Chapte r 2) for reinforcemen t learnin g by includin g reward s for
pickin g up each piec e of dirt and for gettin g hom e and switchin g off. Mak e the worl d accessibl e
by providin g suitabl e percepts . Now experimen t with differen t reinforcemen t learnin g agents . Is
inpu t generalizatio n necessar y for success ?
20.9 Writ e dow n the updat e equatio n for Q­learnin g with a parameterize d implici t representa ­
tion. Tha t is, writ e the counterpar t to Equatio n (20.8) .
20.10 Exten d the standar d game­playin g environmen t (Chapte r 5) to incorporat e a rewar d
signal . Put two reinforcemen t learnin g agent s into the environmen t (the y may of cours e shar e
the agen t program ) and have them play agains t each other . Appl y the generalize d TD updat e rule
(Equatio n (20.8) ) to updat e the evaluatio n function . You may wish to start with a simpl e linea r
weighte d evaluatio n function , and a simpl e gam e such as tic­tac­toe .
20.11 (Discussio n topic. ) Is reinforcemen t learnin g an appropriat e abstrac t mode l for huma n
learning ? For evolution ?
21KNOWLEDG E IN
LEARNIN G
In which we  examine  the problem  of learning  when  you already  know  something.
In all of the approache s to learnin g describe d in the previou s three chapters , the idea is to construc t
a progra m that has the input/outpu t behavio r observed  in the data . We have seen how this genera l
proble m can be solve d for simpl e logica l representations , for neura l networks , and for belie f
networks . In each case , the learnin g method s can be understoo d as searchin g a hypothesi s spac e
to find a suitabl e program . The learnin g method s also mad e few assumptions , if any, concernin g
the natur e of the correc t program . In this chapter , we go beyon d thes e approache s to stud y
PRIOR KNOWLEDG E learnin g method s that can take advantag e of prio r knowledg e abou t the environment . We also
examin e learnin g algorithm s that can learn genera l first­orde r logica l theories . Thes e are essentia l
steps towar d a truly autonomou s intelligen t agent .
21.1 KNOWLEDG E IN LEARNIN G
We begi n by examinin g the way s in whic h prio r knowledg e can get into the act. In orde r to
do this , it will help to have a genera l logical  formulatio n of the learnin g problem , as oppose d
to the function­learning  characterizatio n of pure inductiv e inferenc e give n in Sectio n 18.2 . The
reaso n that a logica l characterizatio n is helpfu l is that it provide s a very natura l way to specif y
partial  informatio n abou t the functio n to be learned . This is analogou s to the distinctio n betwee n
proble m solvin g (whic h uses a "black­box " functiona l view of state s and goals ) and plannin g
(whic h open s up the blac k boxe s and uses logica l description s of state s and actions) .
Recal l from Sectio n 18.5 that example s are compose d of description s and classifications .
The objec t of inductiv e learnin g in the logica l settin g is to find a hypothesi s that explain s the
classification s of the examples , give n thei r descriptions . We can mak e this logicall y precis e as
follows . If we use Descriptions  to denot e the conjunctio n of all the exampl e descriptions , and
Classifications  to denot e the conjunctio n of all the exampl e classifications , then the Hypothesis
must satisf y the followin g property :
Hypothesis  A Descriptions  (= Classifications  (21.1 )
625
626 Chapte r 21 . Knowledg e in Learnin g
ENTAILMEN T
CONSTRAIN T We call this kind of relationshi p an entailmen t constraint , in whic h Hypothesis  is the "unknown. "
Pure inductiv e learnin g mean s solvin g this constraint , wher e Hypothesis  is draw n from som e
predefine d hypothesi s space . For example , if we conside r a decisio n tree as a logica l formul a (see
page 532) , then a decisio n tree that is consisten t with all the example s will satisf y Equatio n (21.1) .
If we plac e no restriction s on the logica l form of the hypothesis , of course,  then Hypothesis  =
Classifications  also satisfie s the constraint . Normally , Ockham' s razo r tells us to prefe r small,
consisten t hypotheses , so we try to do bette r than simpl y memorizin g the examples .
This simpl e pictur e of inductiv e learnin g persiste d unti l the earl y 1980s . Th e moder n
approac h is to desig n agent s that already  know  something  and are tryin g to learn som e more .
This may not soun d like a terrificall y deep insight , but it make s quit e a differenc e to the way in
whic h we writ e programs . It migh t also have som e relevanc e to our theorie s abou t how scienc e
itself works . The genera l idea is show n schematicall y in Figur e 21.1 .
ObservationsKnowledge­base d
inductiv e learnin g Predictions
Figur e 21.1 A  cumulativ e learnin g proces s uses , and adds to, its stoc k of backgroun d knowl ­
edge over time .
If we wan t to buil d an autonomou s learnin g agen t that uses backgroun d knowledge , the
agen t mus t have som e metho d for obtainin g the backgroun d knowledg e in the first place , in orde r
for it to be used in the new learnin g episodes . This metho d mus t itsel f be a learnin g process . The
agent' s life histor y will therefor e be characterize d by cumulative,  or incremental,  development .
Presumably , the agen t coul d start out with nothing , performin g induction s in vacua  like a good
little pure inductio n program . But once it has eaten from the Tree of Knowledge , it can no longe r
pursu e such naiv e speculations , and shoul d use its backgroun d knowledg e to learn more and more
effectively . The questio n is then how to actuall y do this.
Some simpl e example s
Let us conside r som e commonsens e example s of learnin g with backgroun d knowledge . Man y
apparentl y rationa l case s of inferentia l behavio r in the face of observation s clearl y do not follo w
the simpl e principle s of pure induction .
• Sometime s one leap s to genera l conclusion s after only one observation . Gar y Larso n once
drew a cartoo n in whic h a bespectacle d caveman , Thag , is roastin g his lizar d on the end of
a pointe d stick . He is watche d by an amaze d crow d of his less intellectua l contemporaries ,
who have been usin g thei r bare hand s to hold their victual s over the fire. This enlightenin g
experienc e is enoug h to convinc e the watcher s of a genera l principl e of painles s cooking .
Sectio n 21.1 . Knowledg e in Learnin g 627
• Or conside r the case of the travelle r to Brazi l meetin g her first Brazilian . On hearin g
him spea k Portuguese , she immediatel y conclude s that Brazilian s spea k Portuguese , yet
on discoverin g that his nam e is Fernando , she does not conclud e that all Brazilian s are
calle d Fernando . Simila r example s appea r in science . For example , whe n a freshma n
physic s studen t measure s the densit y and conductanc e of a sampl e of coppe r at a particula r
temperature , she is quit e confiden t in generalizin g thos e value s to all piece s of copper . Yet
when she measure s its mass , she does not even conside r the hypothesi s that all piece s of
coppe r have that mass . On the othe r hand , it woul d be quit e reasonabl e to mak e such a
generalizatio n over all pennies .
• Finally , conside r the case of a pharmacologicall y ignoran t but diagnosticall y sophisticate d
medica l studen t observin g a consultin g sessio n betwee n a patien t and an exper t internist .
After a serie s of question s and answers , the exper t tells the patien t to take a cours e of
a particula r antibiotic . Th e medica l studen t infer s the genera l rule that that particula r
antibioti c is effectiv e for a particula r type of infection .
These are all case s in whic h the use of background  knowledge  allows much  faster  learning  than
one might  expect  from  a pure  induction  program.
EXPLANATION ­
BASE D
LEARNIN GSome genera l scheme s
In each of the precedin g examples , one can appea l to prio r knowledg e to try to justif y the
generalization s chosen . We will now look at wha t kind s of entailmen t constraint s are operatin g
in each of these cases . The constraint s will involv e the Background  knowledge , in additio n to the
Hypothesis  and the observe d Descriptions  and Classifications.
In the case of lizar d toasting , the caveme n generaliz e by explaining  the succes s of the
pointe d stick : it support s the lizar d whil e keepin g the hand intact . Fro m this explanation , they
can infe r a genera l rule : that any long , thin , rigid , shar p objec t can be used to toas t small , soft ­
bodie d edibles . This kind of generalizatio n proces s has been calle d explanation­base d learning ,
or EBL . Notic e that the genera l rule follows  logically  from the backgroun d knowledg e possesse d
by the cavemen . Hence , the entailmen t constraint s satisfie d by EBL are the following :
Hypothesis  A Descriptions  |= Classifications
Background  \= Hypothesis
The firs t constrain t look s the sam e as Equatio n (21.1) , so EBL was initiall y though t to be a
bette r way to lear n from examples . Bu t becaus e it require s that the backgroun d knowledg e be
sufficien t to explai n the Hypothesis,  whic h in turn explain s the observations , the agen t does
not actuall y lear n anythin g factually  new  from the instance . The agen t could  have  derive d the
exampl e from wha t it alread y knew , althoug h that migh t have require d an unreasonabl e amoun t
of computation . EB L is now viewe d as a metho d for convertin g first­principle s theorie s into
useful , special­purpos e knowledge . We describ e algorithm s for EBL in Sectio n 21.2 .
The situatio n of our travelle r in Brazi l is quit e different . Fo r she canno t necessaril y
explai n why Fernand o speak s the way he does , unles s she know s her Papa l bulls . But the sam e
generalizatio n woul d be forthcomin g from a travelle r entirel y ignoran t of colonia l history . The
relevan t prio r knowledg e in this case is that , withi n any give n country , mos t peopl e tend to spea k
628 Chapte r 21 . Knowledg e in Learnin g
RELEVANC E
RELEVANCE­BASE D
LEARNIN G
KNOWLEDGE­BASE D
INDUCTIV E
LEARNIN G
INDUCTIV E LOGI C
PROGRAMMIN Gthe same language ; on the othe r hand , Fernand o is not assume d to be the nam e of all Brazilian s
becaus e this kind of regularit y does not hold for names . Similarly , the freshma n physic s studen t
also woul d be hard put to explai n the particula r value s that she discover s for the conductanc e and
densit y of copper . She does know , however , that the materia l of whic h an objec t is compose d and
its temperatur e togethe r determin e its conductance . In each case, the prior knowledg e Background
concern s the relevanc e of a set of feature s to the goal predicate . Thi s knowledge , together  with
the observations,  allow s the agen t to infe r a new , genera l rule that explain s the observations :
(21.2 )Hypothesis  A Descriptions  j= Classifications
Background  A Descriptions  A Classifications  |= Hypothesis
We call this kind of generalizatio n relevance­base d learning , or RBL (althoug h the nam e is not
standard) . Notic e that wherea s RBL does mak e use of the conten t of the observations , it does
not produc e hypothese s that go beyon d the logica l conten t of the backgroun d knowledg e and the
observations . It is a deductive  form of learning , and canno t by itsel f accoun t for the creatio n of
new knowledg e startin g from scratch . We discus s application s of RBL in Sectio n 21.3 .
In the case of the medica l studen t watchin g the expert , we assum e that the student' s prio r
knowledg e is sufficien t to infe r the patient' s diseas e D from the symptoms . This is not, however ,
enoug h to explai n the fact that the docto r prescribe s a particula r medicin e M. The studen t need s
to propos e anothe r rule , namely , that M generall y is effectiv e agains t D. Give n this rule , and
the student' s prio r knowledge , the studen t can now explai n why the exper t prescribe s M in this
particula r case. We can generaliz e this exampl e to com e up with the entailmen t constraint :
Background  A Hypothesis  A Descriptions  \= Classifications (21.3 )
That is, the background  knowledge  and the  new hypothesis  combine  to explain  the examples.  As
with pure inductiv e learning , the learnin g algorith m shoul d propos e hypothese s that are as simpl e
as possible , consisten t with this constraint . Algorithm s that satisf y constrain t 21.3 are calle d
knowledge­base d inductiv e learning , or KBIL , algorithms .
KBIL algorithms , whic h are describe d in detai l in Sectio n 21.4 , have been studie d mainl y
in the field of inductiv e logic programmin g or ILP. In ILP systems , prio r knowledg e play s two
key roles in reducin g the complexit y of learning :
1. Becaus e any hypothesi s generate d mus t be consisten t with the prio r knowledg e as well as
with the new observations , the effectiv e hypothesi s spac e size is reduce d to includ e only
those theorie s that are consisten t with wha t is alread y known .
2. For any give n set of observations , the size of the hypothesi s require d to construc t an
explanatio n for the observation s can be muc h reduced , becaus e the prio r knowledg e will
be availabl e to help out the new rule s in explainin g the observations . The smalle r the
hypothesis , the easie r it is to find .
In additio n to allowin g the use of prio r knowledg e in induction , ILP system s can formulat e
hypothese s in genera l first­orde r logic , rathe r than the restricte d language s used in Chapte r 18.
This mean s that they can learn in environment s that canno t be understoo d by simple r systems .
Sectio n 21.2 . Explanation­Base d Learnin g 62 9
21.2 EXPLANATION­BASE D LEARNING_________________ _
As we explaine d in the introductio n to this chapter , explanation­base d learnin g is a metho d for
extractin g genera l rule s from individua l observations . As an example , conside r the proble m
of differentiatin g and simplifyin g algebrai c expression s (Exercis e 10.4) . If we differentiat e an
expressio n such as X2 with respec t to X, we obtai n 2X. (Notic e that we use a capita l lette r for
the arithmeti c unknow n X, to distinguis h it from the logica l variabl e x.) In a logica l reasonin g
system , the goal migh t be expresse d as ASK(Derivative(X2 ,X) = d, KB),  with solutio n d = 2X.
We can see this solutio n "by inspection " becaus e we hav e man y year s of practic e in
solvin g such problems . A studen t encounterin g such problem s for the first time , or a progra m
with no experience , will have a muc h mor e difficul t job. Applicatio n of the standar d rule s of
differentiatio n eventuall y yield s the expressio n 1 x (2 x (X(2~ '*)), and eventuall y this simplifie s
to 2X. In the authors ' logi c programmin g implementation , this take s 136 proo f steps , of whic h
99 are on dead­en d branche s in the proof . Afte r such an experience , we woul d like the progra m
to solv e the sam e proble m muc h mor e quickl y the next time .
MEMOIZATIO N Th e techniqu e of memoizatio n has long been used in compute r scienc e to spee d up
program s by savin g the result s of computation . The basic idea of mem o function s is to accumulat e
a databas e of input/outpu t pairs ; whe n the functio n is called , it first check s the databas e to see if
it can avoi d solvin g the proble m from scratch . Explanation­base d learnin g take s this a good deal
further , by creatin g general  rule s that cove r an entir e class of cases . In the case of differentiation ,
memoizatio n woul d remembe r that the derivativ e of X2 with respec t to X is 2X, but woul d leav e
the agen t to calculat e the derivativ e of Z2 with respec t to Z from scratch . We woul d like to be able
to extrac t the genera l rule1 that for any arithmeti c unknow n u, the derivativ e of u2 with respec t to
u is 2u. In logica l terms , this is expresse d by the rule
ArithmeticUnknown(u)  =>• Derivative(u",u)  = 2u
If the knowledg e base contain s such a rule , then any new case that is an instanc e of this rule can
be solve d immediately .
This is, of course , merel y a trivia l exampl e of a very genera l phenomenon . Onc e somethin g
is understood , it can be generalize d and reuse d in othe r circumstances . It become s an "obvious "
step, and can then be used as a buildin g bloc k in solvin g still mor e comple x problems . Alfre d
North Whitehea d (1911) , co­autho r with Bertran d Russel l of Principia  Mathematica,  wrot e that
I i|i;~ "Civilization  advances  by extending  the number  of important  operations  that we can do without' LB' thinking  about  them,"  perhap s himsel f applyin g EBL to his understandin g of event s such as
Thag' s discovery . If you have understoo d the basic idea of the differentiatio n example , then your
brain is alread y busil y tryin g to extrac t the genera l principle s of explanation­base d learnin g from
it. Notic e that unles s you are a good deal smarte r than the authors , you hadn' t already  invente d
EBL befor e we showe d you an exampl e of it. Lik e the caveme n watchin g Thag , you (and we)
neede d an exampl e befor e we coul d generat e the basi c principles . Thi s is becaus e explaining
why somethin g is a good idea is muc h easie r than comin g up with the idea in the first place .
Of course , a genera l rule for u" can also be produced , but the curren t exampl e suffice s to mak e the point .
630 Chapte r 21 . Knowledg e in Learnin g
Extractin g genera l rule s from example s
The basi c idea behin d EBL is firs t to construc t an explanatio n of the observatio n usin g prio r
knowledge , and then to establis h a definitio n of the class of case s for whic h the same explanatio n
structur e can be used . Thi s definitio n provide s the basi s for a rule coverin g all of the case s in
the class . The "explanation " can be a logica l proof , but more generall y it can be any reasonin g
or problem­solvin g proces s whos e step s are well­defined . The key is to be able to identif y the
necessar y condition s for thos e same step s to appl y to anothe r case .
We will use for our reasonin g syste m the simpl e backward­chainin g theore m prove r de­
scribe d in Chapte r 9. The proo f tree for Derivative(X2,X) = d is too larg e to use as an example ,
so we will use a somewha t simple r proble m to illustrat e the generalizatio n method . Suppos e our
proble m is to simplif y 1 x (0 + X). The knowledg e base include s the followin g rules :
Rewrite(u,  v) A Simplify(v,  w) =>• Simplify(u,  w)
Primitive(u)  =>• Simplify(u,  u)
ArithmeticUnknown(u)  => Primitive(u)
Number(u)  => Primitive(u)
Rewrite(\  x u,u)
Rewrite(0  + u, u)
The proo f that the answe r isX is show n in the top half of Figur e 21.2 . The EBL metho d actuall y
construct s two proo f trees simultaneously . The secon d proo f tree uses a variabilized  goal in whic h
the constant s from the origina l goal are replace d by variables . As the origina l proo f proceeds , the
variabilize d proo f proceed s usin g exactly  the same  rule  applications.  Thi s may caus e som e of
the variable s to becom e instantiated . For example , in orde r to use the rule Rewrite(\  x u, u), the
variable s in the subgoa l Rewrite(x  x (y + z), v)  mus t be boun d to 1. Similarly , y mus t be boun d
to 0 in the subgoa l Rewrite(y  + z, v') in orde r to use the rule Rewrite(0  + u, it).
Once we have the generalize d proo f tree, we take the leave s (wit h the necessar y bindings )
and form a genera l rule for the goal predicate :
Rewrite(l  x (0 + z), 0 + z) A Rewrite(0  + z, z) A ArithmeticUnknown(z)
=> Simplify^  x (0 + z),z)
Notic e that the first two condition s on the left­han d side are true regardless of  the value  ofz. We
can therefore  drop them from the rule , yieldin g
ArithmeticUnknown(z)  => Simplify(\  x (0 + z),z )
In general , condition s can be droppe d from the fina l rule if they impos e no constraint s on the
variable s on the right­han d side of the rule , becaus e the resultin g rule will still be true and will
be mor e efficient . Notic e that we canno t drop the conditio n ArithmeticUnknown(z),  becaus e
not all possibl e value s of z are arithmeti c unknowns . Value s othe r than arithmeti c unknown s
migh t requir e differen t form s of simplification — for example , if z were 2x3 , then the correc t
simplificatio n of 1 x (0 + (2 x 3)) woul d be 6 and not 2x3 .
To recap , the basic EBL proces s work s as follows :
1. Give n an example , construc t a proo f that the goal predicat e applie s to the exampl e usin g
the availabl e backgroun d knowledge .
Sectio n 21.2 . Explanation­Base d Learnin g 631
Yes, /v/O+X/
Rewrite(0+X,v')
Yes, {v'/XJSimplifyfX,  w)
fw/Xj
Yes, I j
Figur e 21.2 Proo f trees for the simplificatio n problem . The first tree show s the proo f for the
origina l proble m instance . The secon d show s the proo f for a proble m instanc e with all constant s
replace d by variables .
2. In parallel , construc t a generalize d proo f tree for the variabilize d goal usin g the sam e
inferenc e steps as in the origina l proof .
3. Construc t a new rule whos e left­han d side consist s of the leave s of the proo f tree, and
whos e right­han d side is the variabilize d goal (afte r applyin g the necessar y binding s from
the generalize d proof) .
4. Dro p any condition s that are true regardles s of the value s of the variable s in the goal .
Improvin g efficienc y
Examinin g the generalize d proo f tree in Figur e 21.2 , we see that there is mor e than one generalize d
rule that can be extracte d from the tree. For example , if we terminate , or prune , the growt h of
the right­han d branc h in the proo f tree whe n it reache s the Primitive  step , we get the rule
Primitive(z)  => Simplify(\  x (0 + z),z )
This rule is equall y valid , but also more general  than the rule usmgArithmeticUnknown,  becaus e
it cover s case s wher e z is a number . We can extrac t a still mor e genera l rule by prunin g after the
632 Chapte r 21 . Knowledg e in Learnin g
step Simplify(y  + z, w), yieldin g the rule
Simplify(y  + z,w)  => Simplify(l  x (y + z),w)
In general , a rule can be extracte d from any partial  subtree  of the generalize d proo f tree. Now
we have a problem : whic h of these rules do we choose ?
The choic e of whic h rule't o generat e come s dow n to the questio n of efficiency . Ther e are
three factor s involve d in the analysi s of efficienc y gain s from EBL :
1. Addin g large number s of rules to a knowledg e base can slow dow n the reasonin g process ,
becaus e the inferenc e mechanis m mus t still chec k thos e rules even in case s wher e they do
not yield a solution . In othe r words , it increase s the branchin g facto r in the searc h space .
2. To compensat e for this, the derive d rules mus t offe r significan t increase s in spee d for the
cases that they do cover . Thi s mainl y come s abou t becaus e the derive d rule s avoi d dead
ends that woul d otherwis e be taken , as well as shortenin g the proo f itself .
3. Derive d rules shoul d also be as genera l as possible , so that they appl y to the larges t possibl e
set of cases .
OPERATIONALIT Y A  commo n approac h to ensurin g that derive d rule s are efficien t is to insis t on the operationalit y
of each subgoa l in the rule. A subgoa l is operational , roughl y speaking , if it is "easy " to solve .
For example , the subgoa l Primitive(z)  is easy to solve , requirin g at mos t two steps , wherea s
the subgoa l Simplify(y  + z, w) coul d lead to an arbitrar y amoun t of inference , dependin g on the
value s of y and z. If a test for operationalit y is carrie d out at each step in the constructio n of the
generalize d proof , then we can prun e the rest of a branc h as soon as an operationa l subgoa l is
found , keepin g just the operationa l subgoa l as a conjunc t of the new rule .
Unfortunately , ther e is usuall y a trade­of f betwee n operationalit y and generality . Mor e
specifi c subgoal s are usuall y easie r to solv e but cove r fewe r cases . Also , operationalit y is a
matte r of degree ; one or two steps is definitel y operational , but wha t abou t 10, or 100? Finally ,
the cost of solvin g a give n subgoa l depend s on wha t othe r rule s are availabl e in the knowledg e
base. It can go up or dow n as mor e rule s are added . Thus , EBL system s reall y face a very
comple x optimizatio n proble m in tryin g to maximiz e the efficienc y of a give n initia l knowledg e
base. It is sometime s possibl e to deriv e a mathematica l mode l of the effec t on overal l efficienc y
of addin g a give n rule , and to use this mode l to selec t the best rule to add. The analysi s can
becom e very complicated , however , especiall y whe n recursiv e rules are involved . One promisin g
approac h is to addres s the proble m of efficienc y empirically , simpl y by addin g severa l rales and
seein g whic h ones are usefu l and actuall y spee d thing s up.
The idea of empirica l analysi s of efficienc y is actuall y at the hear t of EBL . Wha t we have
been callin g loosel y the "efficienc y of a give n knowledg e base " is actuall y the average­cas e
complexit y on a populatio n of problem s that the agen t will have to solve . By generalizing  from
past example  problems,  EBL  makes  the knowledge  base  more  efficient  for the kind  of problems
that it is reasonable  to expect.  This work s as long as the distributio n of past example s is roughl y
the sam e as for futur e examples—th e sam e assumptio n used for PAC­learnin g in Sectio n 18.6 .
If the EBL syste m is carefull y engineered , it is possibl e to obtai n very significan t improvement s
on futur e problems . In a very larg e Prolog­base d natura l languag e syste m designe d for real­
time speech­to­speec h translatio n betwee n Swedis h and English , Samuelsso n and Rayne r (1991 )
repor t that EBL mad e the syste m mor e than 1200 time s faster .
Sectio n 21.3 . Learnin g Usin g Relevanc e Informatio n 63 3
21.3 LEARNIN G USIN G RELEVANC E INFORMATION____________ _
Our travelle r in Brazi l seem s to be able to mak e a confiden t generalizatio n concernin g the
languag e spoke n by othe r Brazilians . The inferenc e is sanctione d by her backgroun d knowledge ,
namely , that peopl e in a give n countr y (usually ) spea k the same language . We can expres s this in
first­orde r logic as follows:2
\/x,y,n,l  Nationality  (yi, ri) A Nationality^,  n) A Language(x,  I) => Language(y,  I) (2 1 .4)
(Litera l translation : "If x and y have a commo n nationalit y n and x speak s languag e /, then y also
speak s it.") It is not difficul t to show that, give n this sentenc e and the observatio n
Nationality(Femando,  Brazil)  A Language(Fernando,  Portuguese)
the conclusio n
V* Nationality(x,  Brazil)  => Language(x,  Portuguese)
follow s logicall y (see Exercis e 21.1) .
Sentence s such as (21 .4) expres s a stric t form of relevance : give n nationality , languag e is
fully determined . Put anothe r way : languag e is a functio n of nationality . Thes e sentence s are
DEPENDENCIE S calle d functiona l dependencie s or determinations . The y occu r so commonl y in certai n kind s
DETERMINATION S o f application s (e.g. , definin g databas e designs ) that a specia l synta x is used to writ e them . We
adop t the notatio n used by Davie s (1985) :
Nationality  (x, n) >­ Language(x,  I)
As usual , this is simpl y a syntacti c sugaring , but it make s it clear that the determinatio n is reall y
a relationshi p betwee n the predicates : nationalit y determine s language . The relevan t propertie s
determinin g conductanc e and densit y can be expresse d similarly :
Material(x,  m) A Temperature(x,  t) >­ Conductance(x,  p)
Material(x,  m) A Tempemture(x,  t) >­ Density(x,  d)
The correspondin g generalization s follo w logicall y from the determination s and observations .
Determinin g the hypothesi s spac e
Althoug h the determination s sanctio n genera l conclusion s concernin g all Brazilians , or all piece s
of coppe r at a give n temperature , they cannot , of course , yiel d a genera l predictiv e theor y for all
nationalities , or for all temperature s and materials , from a singl e example . Thei r main effec t can
be seen as limitin g the spac e of hypothese s that the learnin g agen t need consider . In predictin g
conductance , for example , one has only to conside r materia l and temperatur e and can ignor e
mass , ownership , day of the week , the curren t president , and so on. Hypothese s can certainl y
includ e term s that are in turn determine d by materia l and temperature , such as molecula r structure ,
I^ljjpi " therma l energy , or free­electro n density . Determinations  specify  a sufficient  basis  vocabulary
2 We assum e for the sake of simplicit y that a perso n speak s only one language . Clearly , the rule also woul d have to be
amende d for countrie s such as Switzerlan d or India .
634 Chapte r 21 . Knowledg e in Learnin g
from which to  construct hypotheses  concerning the target  predicate.  Thi s statemen t can be
prove d by showin g that a give n determinatio n is logicall y equivalen t to a statemen t that the
correc t definitio n of the targe t predicat e is one of the set of all definition s expressibl e usin g the
predicate s in the left­han d side of the determination .
Intuitively , it is clear that a reductio n in the hypothesi s spac e size shoul d mak e it easie r to
learn the targe t predicate . Usin g the basic result s of computationa l learnin g theor y (Sectio n 18.6) ,
we can quantif y the possibl e gains . First , recal l that for Boolea n functions , log(|H| ) example s
are require d to converg e to a reasonabl e hypothesis , wher e |H| is the size of the hypothesi s space .
If the learne r has n Boolea n feature s with whic h to construc t hypotheses , then , in the absenc e
of furthe r restrictions , H =  O(22"), so the numbe r of example s is O(2").  If the determinatio n
contain s d predicate s in the left­han d side , the learne r will requir e only 0(2'' ) examples , a
reductio n of O(2n~d). For biase d hypothesi s spaces , such as a conjunctivel y biase d space , the
reductio n will be less dramati c but still significant .
Learnin g and usin g relevanc e informatio n
As we state d in the introductio n to this chapter , prio r knowledg e is usefu l in learning , but it
also has to be learned . In orde r to provid e a complet e story of relevance­base d learning , we
must therefor e provid e a learnin g algorith m for determinations . The learnin g algorith m we now
presen t is base d on a straightforwar d attemp t to find the simples t determinatio n consisten t with
the observations . A determinatio n P >­ Q says that if any example s matc h on P, then they mus t
also matc h on Q. A determinatio n is therefor e consisten t with a set of example s if ever y pair that
matche s on the predicate s on the left­han d side also matche s on the targe t predicate , that is, has
the sam e classification . For example , suppos e we have the followin g example s of conductanc e
measurement s on materia l samples :
Sampl e
SI
SI
S2
S3
S3
S4Mass
12
12
24
12
12
24Temperatur e
26
100
26
26
100
26Materia l
Coppe r
Coppe r
Coppe r
Lead
Lead
LeadSize
3
3
6
2
2
4Conductanc e
0.59
0.57
0.59
0.05
0.04
0.05
The minima l consisten t determinatio n is Material  A Temperature  >­ Conductance.  Ther e is a
nonminima l but consisten t determination , namely , Mass  A Size  A Temperature  >­ Conductance.
This is consisten t with the example s becaus e mas s and size determin e density , and in our data
set, we do not have two differen t material s with the sam e density . As usual , we woul d need a
large r sampl e set in orde r to eliminat e a nearl y correc t hypothesis .
There are severa l possibl e algorithm s for rindin g minima l consisten t determinations . The
most obviou s approac h is to conduc t a searc h throug h the spac e of determinations , checkin g all
determination s with one predicate , two predicates , and so on, unti l a consisten t determinatio n is
found . We will assum e a simple  attribute­base d representation , like that used for decision­tre e
Sectio n 21.3 . Learnin g Usin g Relevanc e Informatio n 635
learnin g in Chapte r 18. A determinatio n d will be represente d by the set of attributes  on the
left­han d side , becaus e the targe t predicat e is assume d fixed . The basi c algorith m is outline d in
Figur e 21.3 .
functio n MINIMAL­CONSISTENT­DET(£ , A) return s a determinatio n
inputs : E, a set of example s
A, a set of attributes , of size n
for / <— 0, ... , n do
for each subse t A, of A of size /' do
if CONSISTENT­DET?(A,,F ) then retur n A,
end
end
functio n CONSISTENT­DET?(A , E) return s a truth­valu e
inputs : A, a set of attribute s
E, a set of example s
local variables : H, a hash table
for each exampl e e in E do
if som e exampl e in H has the sam e value s as e for the attribute s A
but a differen t classificatio n then retur n False
store the class of e in H, indexe d by the value s for attribute s A of the exampl e e
end
retur n True
Figur e 21.3 A n algorith m for findin g a minima l consisten t determination .
The tim e complexit y of this algorith m depend s on the size of the smalles t consisten t
determination . Suppos e this determinatio n hasp attribute s out of the n total attributes . The n the
algorith m will not find it unti l searchin g the subset s of A of size p. Ther e are (") = O(np) such
subsets , henc e the algorith m is exponentia l in the size of the minima l determination . It turn s
out that the proble m is NP­complete , so we canno t expec t to do bette r in the genera l case . In
most domains , however , ther e will be sufficien t loca l structur e (see Chapte r 15 for a definitio n of
locall y structure d domains ) such that/ 7 will be small .
Give n an algorith m for learnin g determinations , a learnin g agen t has a way to con ­
struc t a minima l hypothesi s withi n whic h to lear n the targe t predicate . W e can combin e
MINIMAL­CONSISTENT­DE T with the DECISION­TREE­LEARNIN G algorithm , for example , in orde r
to creat e a relevance­base d decision­tre e learnin g algorith m RBDTL :
functio n RBDTL(£,A,v ) return s a decisio n tree
returnDECISION­TREE­LEARNING(E,MlNIMAL­CONSISTENT­DET(£,A),v )
636 Chapte r 21 . Knowledg e in Learnin g
Unlik e DECISION­TREE­LEARNING , RBDT L simultaneousl y learn s and uses relevanc e informa ­
tion in orde r to minimiz e its hypothesi s space . We expec t that RBDTL' s learnin g curv e will show
some improvemen t over the learnin g curv e achieve d by DECISION­TREE­LEARNING , and this is
in fact the case . Figur e 21.4 show s the learnin g performanc e for the two algorithm s on randoml y
generate d data for a functio n that depend s on only 5 of 16 attributes . Obviously , in case s wher e
all the availabl e attribute s are relevant , RBDT L will show no advantage .
Figur e 21.4
generate d data1
0.9
p
t? 0.8
I 0.7o
8 0.6
0.5
0.4
(/"^
**^ RBDT L o
T DT L — ­ f ­
;
 % I ^ J~^'^¥J  " :
*
) 2 0 4 0 6 0 8 0 10 0 12 0 14 0
Trainin g set size
A performanc e compariso n RBDT L and DECISION­TREE­LEARNIN G on randoml y
for a targe t functio n that depend s on only 5 of 1 6 attributes .
DECLARATIV E BIAS Thi s sectio n has only scratche d the surfac e of the field of declarativ e bias , whic h aims to
understan d how prior knowledg e can be used to identif y the appropriat e hypothesi s spac e withi n
whic h to searc h for the correc t targe t definition . Ther e are man y unanswere d questions :
• How can the algorithm s be extende d to handl e noise ?
• How can othe r kind s of prior knowledg e be used , beside s determinations ?
• How can the algorithm s be generalize d to cove r any first­orde r theory , rathe r than just an
attribute­base d representation ?
Some of these are addresse d in the next section .
21.4 INDUCTIV E LOGI C PROGRAMMIN G
Inductiv e logi c programmin g (ILP ) is one of the newes t subfield s in AI. It combine s inductiv e
method s with the powe r of first­orde r representations , concentratin g in particula r on the repre ­
sentatio n of theorie s as logi c programs . Ove r the last five years , it has becom e a majo r part of
Sectio n 21.4 . Inductiv e Logi c Programmin g 63 7
the researc h agend a in machin e learning . Thi s has happene d for two reasons . First , it offer s a
rigorou s approac h to the genera l KBI L proble m mentione d in the introduction . Second , it offer s
complet e algorithm s for inducin g general , first­orde r theorie s from examples , whic h can therefor e
learn successfull y in domain s wher e attribute­base d algorithm s fail completely . ILP is a highl y
technica l field , relyin g on som e fairl y advance d materia l from the stud y of computationa l logic .
We therefor e cove r only the basi c principle s of the two majo r approaches , referrin g the reade r to
the literatur e for more details.3
An exampl e
Recal l from Equatio n (21.3 ) that the genera l knowledge­base d inductio n proble m is to "solve "
the entailmen t constrain t
Background  A Hypothesis  A Descriptions  \= Classifications
for the unknow n Hypothesis,  give n the Background  knowledg e and example s describe d by
Descriptions  and Classifications.  To illustrat e this, we will use the proble m of learnin g famil y
relationship s from examples . The observation s will consis t of an extende d famil y tree, describe d
in term s of Mother,  Father,  and Married  relations , and Male  and Female  properties . The targe t
predicate s will be such thing s as Grandparent,  BrotherlnLaw,  and Ancestor.  We will use the
famil y tree from Exercis e 7.6, show n here in Figur e 21.5 . Th e exampl e Descriptions  includ e
facts such as
Father(Philip,  Charles)  Father(Philip,Anne)
Mother(Mum,  Margaret)  Mother(Mum,  Elizabeth)
Married(Diana,Charles)  Married(Elizabeth,Philip)  ...
Male(Philip)  Male(Charles)
Female(Beatrice)  Female(Margaret)
The sentence s in Classifications  depen d on the targe t concep t bein g learned . If Q is Grandparent,
say, then the sentence s in Classifications  migh t includ e the following :
Grandparent(Mum,  Charles)  Grandparent(Elizabeth,  Beatrice)  ...
^Grandparent(Mum,  Harry)  ­^Grandparent(Spencer,  Peter)
The objec t of an inductiv e learnin g progra m is to com e up with a set of sentence s for
the Hypothesis  such that the entailmen t constrain t is satisfied . Suppose , for the moment , that
the agen t has no backgroun d knowledge : Background  is empty . The n one possibl e solutio n for
Hypothesis  is the following :
Grandparent(x,  >•) <= > [3 z Mother(x,  z) A Mother(z, }')]
V [ 3 z Mother(x,  z) A Father(z,  y)]
V [3z  Father(x,z)f\Mother(z,y)]
V [3z  Father(x,z)f\Father(z,y)}
3 We sugges t that it migh t be appropriat e at this poin t for the reade r to refer back to Chapte r 9 for some of the underlyin g
concepts , includin g Hor n clauses , conjunctiv e norma l form , unification , and resolution .
638 Chapte r 21 . Knowledg e in Learnin g
Georg e = Mum
Spence r = Kydd Elizabet h = Phili p Margare t
Diana = Charle s Ann e =  Mar k Andre w = Sara h Edwar d
Willia m Harr y Pete r Zar a Beatric e Eugeni e
Figur e 21.5 A  typica l famil y tree.
CONSTRUCTIV E
INDUCTIO NNotic e that an attribute­base d learnin g algorith m such as DECISION­TREE­LEARNIN G will get
nowher e in solvin g this problem . In order  to expres s Grandparent  as an attribut e (i.e., a unar y
predicate) , we woul d need to make  pairs  of peopl e into objects :
Grandparent((Mum,  Charles})...
Then we get stuc k in tryin g to represen t the exampl e descriptions . The only possibl e attribute s
are horribl e thing s such as
FirstElementIsMotherOfElizabeth(  (Mum,  Charles) )
The definitio n of Grandparent  in term s of these attribute s simpl y become s a large disjunctio n of
specifi c case s that does not generaliz e to new example s at all. Attribute­based  learning  algorithms
are incapable  of learning  relational  predicates.  Thus , one of the principa l advantage s of ILP
algorithm s is thei r applicabilit y to a muc h wide r rang e of problems .
The reade r will certainl y have notice d that a little bit of backgroun d knowledg e woul d help
in the representatio n of the Grandparent  definition . For example , if Background  include d the
sentenc e
Parent(x,  y) O [Mother(x,  y) V Father(x,  >•)]
then the definitio n of Grandparent  woul d be reduce d to
Grandparent(x,  y) ­O­ [3 z Parent(x,  z) A Parent(z,  >')]
This show s how backgroun d knowledg e can dramaticall y reduc e the size of hypothesi s require d
to explai n the observations .
It is also possibl e for ILP algorithm s to create  new predicate s in orde r to facilitat e the
expressio n of explanator y hypotheses . Give n the exampl e data show n earlier , it is entirel y
reasonabl e to propos e an additiona l predicate , whic h we woul d call "Parent,"  in orde r to simplif y
the definition s of the targe t predicates . Algorithm s that can generat e new predicate s are calle d
constructiv e inductio n algorithms . Clearly , constructiv e inductio n is a necessar y part of the
pictur e of cumulativ e learnin g sketche d in the introduction . It has been one of the hardes t problem s
in machin e learning , but som e ILP technique s provid e effectiv e mechanism s for achievin g it.
Sectio n 21.4 . Inductiv e Logi c Programmin g 639
In the rest of this chapter , we will stud y the two principa l approache s to ILP. The first
uses technique s base d on invertin g a resolutio n proof , and the secon d uses a generalizatio n of
decision­tre e methods .
INVERS E
RESOLUTIO NInvers e resolutio n
Invers e resolutio n is base d on the observatio n that if the exampl e Classifications  follo w from
Background  A Hypothesis  A Descriptions,  then one mus t be able to prov e this fact by resolutio n
(becaus e resolutio n is complete) . If we can "run the proo f backwards, " then we can find a
Hypothesis  such that the proo f goes through . Th e key , then , is to find a way to inver t the
resolutio n proces s so that we can run the proo f backwards .
Generatin g invers e proof s
The backwar d proo f proces s consist s of individua l backwar d steps . An ordinar y resolutio n step
takes two clause s C\ and C2 and resolve s them to produc e the resolven t C. An invers e resolutio n
step take s a resolven t C and produce s two clause s C\ and €2, such that C is the resul t of resolvin g
C\ and Ci\ or it takes C and C\ and produce s a suitabl e €2­
The earl y step s in an invers e resolutio n proces s are show n in Figur e 21.6 , wher e we
focus on the positiv e exampl e Grandparent(George,Anne).  The proces s begin s at the end of
the proof , that is, at the contradiction , and work s backwards . Th e negate d goa l claus e is
^Grandparent(George,Anne),  whic h is Grandparent(George,Anne)  => False  in implicativ e
norma l form . The first invers e step take s this and the contradictor y claus e True  =>• False,  and
generate s Grandparent(George,Anne).  Th e nex t step take s this claus e and the know n claus e
Parent(Elizabeth,Anne),  and generate s the claus e
Parent(Elizabeth,  y) => Grandparent(George,  y)
With one furthe r step , the invers e resolutio n proces s will generat e the correc t hypothesis .
Clearly , invers e resolutio n involve s a search . Eac h invers e resolutio n step is nondeter ­
ministic , becaus e for any C and C\, ther e can be severa l or even an infinit e numbe r of clause s
C2 that satisf y the requiremen t that whe n resolve d with C\ it generate s C. For example , in­
True  => Parent(Elizabeth,Anne)
^~~~~ — •— ­^^ ly/Annej ^  ~~~"^
Grandparent(George,An
 ^^^^•^
True ^ Falseie) =>  False
Figur e 21.6 Earl y step s in an invers e resolutio n process . The shade d clause s are generate d by
invers e resolutio n steps .
640 Chapte r 21 . Knowledg e in Learnin g ^
stead of Parent(Elizabeth,  y) => Grandparent(George,  y), the invers e resolutio n step migh t have
generate d the followin g sentences :
Parent(Elizabeth,Anne)  => Grandparent(George,Anne)
Parent(z,Anne)  => Grandparent(George,Anne)
Parent(z,y)  =?• Grandparent(George,y)
(See Exercise s 2 1 .4 and 2 1 .5.) Furthermore , the clause s Ci (and perhap s also C2) that participat e
in each step can be chose n from the Background  knowledge , from the exampl e Descriptions,
from the negate d Classifications,  or from hypothesize d clause s that have alread y been generate d
in the invers e resolutio n tree.
An exhaustiv e searc h proces s for invers e resolutio n woul d be extremel y inefficient . ILP
system s use a numbe r of restriction s to mak e the proces s mor e manageable , includin g the
eliminatio n of functio n symbols , generatin g only the mos t specifi c hypothese s possible , and the
use of Horn clauses . On e can also conside r invertin g the restricted  resolutio n strategie s that
were introduce d in Chapte r 9. Wit h a restricte d but complet e strategy , such as linea r resolution ,
the invers e resolutio n proces s will be mor e efficien t becaus e certai n clause s will be rule d out
as candidate s for C\ and €2­ Othe r usefu l constraint s includ e the fact that all the hypothesize d
clause s mus t be consisten t with each other , and that each hypothesize d claus e mus t agree with the
observations . This last criterio n woul d rule out the claus e Parent(z,y)  => Grandparent(George,y)
listed before .
Discoverin g new predicate s and new knowledg e
An invers e resolutio n procedur e that invert s a complet e resolutio n strateg y is, in principle , a
complet e algorith m for learnin g first­orde r theories . Tha t is, if som e unknow n Hypothesis
generate s a set of examples , then an invers e resolutio n procedur e can generat e Hypothesis  from
the examples . Thi s observatio n suggest s an interestin g possibility . Suppose , for example , that
the availabl e example s includ e a variet y of trajectorie s of fallin g bodies . Woul d an invers e
resolutio n progra m be theoreticall y capabl e of inferrin g the law of gravity ? The answe r is clearl y
yes, becaus e the law of gravit y allow s one to explai n the examples , give n suitabl e backgroun d
mathematics . Similarly , one can imagin e that electromagnetism , quantu m mechanics , and the
theor y of relativit y are also withi n the scop e of ILP programs . However , such imagining s are on
a par with the proverbia l monke y with a typewriter , at least unti l we find way s to overcom e the
very larg e branchin g factor s and the lack of structur e in the searc h spac e that characteriz e curren t
systems .
One thin g that invers e resolutio n system s will  do for you is inven t new predicates . Thi s
abilit y is ofte n seen as somewha t magical , becaus e computer s are ofte n though t of as "merel y
workin g with wha t they are given. " In fact , new predicate s fall directl y out of the invers e
resolutio n step. The simples t case arise s whe n hypothesizin g two new clause s C\ and €2, give n
a claus e C. The resolutio n of C\ and C2 eliminate s a litera l that the two clause s share , henc e
it is quit e possibl e that the eliminate d litera l containe d a predicat e that does not appea r in C.
Thus , whe n workin g backwards , one possibilit y is to generat e a new predicat e from whic h to
reconstruc t the missin g literal .
Sectio n 21.4 . Inductiv e Logi c Programmin g 641
Figur e 21.7 show s an exampl e in whic h the new predicat e P is generate d in the proces s
of learnin g a definitio n for Ancestor.  Onc e generated , P can be used in later invers e resolutio n
steps . For example , a later step migh t hypothesiz e that Mother(x,  >•) =>• P(x,y).  Thus , the new
predicat e P has its meanin g constraine d by the generatio n of hypothese s that involv e it. Anothe r
exampl e migh t lead to the constrain t Father(x,  y) => P(x,y).  In othe r words , the predicat e P is
what we usuall y thin k of as the Parent  relationship . As we mentione d earlier , the inventio n of
new predicate s can significantl y reduc e the size of the definitio n of the goal predicate . Hence , by
includin g the abilit y to inven t new predicates , invers e resolutio n system s can ofte n solve learnin g
problem s that are infeasibl e with othe r techniques .
Father(George,y)  =*• Ancestor(George,y)
Figur e 21.7 A n invers e resolutio n step that generate s a new predicat e P.
Some of the deepes t revolution s in scienc e com e from the inventio n of new predicate s
and functions—fo r example , Galileo' s inventio n of acceleratio n or Joule' s inventio n of therma l
energy . Onc e these term s are available , the discover y of new laws become s (relatively ) easy . The
difficul t part lies in realizin g that some new entity , with a specifi c relationshi p to existin g entities ,
will allow an entir e body of observation s to be explaine d with a muc h simple r and mor e elegan t
theor y than previousl y existed .
As yet, ILP system s have not been applie d to such difficul t tasks . It does appear , however ,
that the abilit y to use backgroun d knowledg e provide s significan t advantages . In severa l appli ­
cations , ILP technique s have outperforme d knowledge­fre e methods . For example , in molecula r
biology , it is usefu l to have backgroun d knowledg e abou t typica l molecular  bondin g patterns ,
valence s of atoms , bon d strengths , and so on. Usin g such knowledge , Stephe n Muggleton' s
GOLE M syste m has been able to generat e high­qualit y prediction s of both protei n structur e from
sequenc e informatio n (Muggleto n et al, 1992 ) and the therapeuti c efficac y of variou s drug s
based on their molecula r structure s (Kin g et al., 1992) . Thes e results , like Meta­DENDRAL's ,
were considere d sufficientl y interestin g in thei r own righ t to be publishe d in leadin g scientifi c
journals . The difference s betwee n GOLEM' S and Meta­DENDRAL' s performanc e are that (1) the
new domain s are muc h mor e difficult , and (2) GOLE M is a completel y general­purpos e progra m
that is able to mak e use of backgroun d knowledg e abou t any domai n whatsoever .
Top­dow n learnin g method s
The secon d approac h to ILP is essentiall y a generalizatio n of the technique s of decision­tre e
learnin g to the first­orde r case. Rathe r than startin g from the observation s and workin g backwards ,
we star t with a very genera l rule and graduall y specializ e it so that it fits the data . Thi s is
essentiall y wha t happen s in decision­tre e learning , wher e a decisio n tree is graduall y grow n unti l
642 Chapte r 21 . Knowledg e in Learnin g
it is consisten t with the observations . In the first­orde r case , we use first­orde r literal s instea d of
attributes , and the hypothesi s is a set of clause s instea d of a decisio n tree. This sectio n describe s
FOIL (Quinlan , 1990) , one of the first program s to use this approach .
Suppos e we are tryin g to lear n a definitio n of the Grandfather(x,  y) predicate , usin g the
same famil y data as before . As with decision­tre e learning , we can divid e the example s into
positiv e and negativ e examples . Positiv e example s are
(George,Anne],  (Philip,Peter],  (Spencer,flurry],  ...
and negativ e example s are
(George,Elizabeth],  (Harry,Zara],  (Charles,Philip],  ...
Notic e that each exampl e is a pair of objects , becaus e Grandfather  is a binar y predicate . In all,
there are 12 positiv e example s in the famil y tree, and 388 negativ e example s (all the othe r pairs
of people) .
FOIL construct s a set of Horn clause s with Grandfather(x,y)  as the head , such that the 12
positiv e example s are classifie d as instance s of the Grandfather(x,y)  relationship , wherea s the
other 388 example s are rule d out becaus e no claus e succeed s with thos e binding s for x and y. We
begin with a claus e with an empt y body :
=> Grandfather(x,  y)
This classifie s ever y exampl e as positive , so it need s to be specialized . Thi s is done by addin g
literal s one at a time to the left­han d side . Her e are thre e potentia l additions :
Father(x,  y)
Parent(x,z)
Father(x,  z)Grandfather(x,  y)
Grandfather(x,  y)
Grandfather(x,  y)
(Notic e that we are assumin g that a claus e definin g Parent  is alread y presen t as part of the';
backgroun d knowledge. ) Th e firs t of thes e thre e incorrectl y classifie s all of the 12 positiv e \
example s as negative , and therefor e can be ruled out. The secon d and third agre e with all of t
positiv e examples , but the secon d is incorrec t on a large r fractio n of the negativ e examples—twic e 1
as many , in fact, becaus e it allow s mother s as well as fathers . Hence , we prefe r the third clause .
Now we need to specializ e this claus e further , to rule out the case s in whic h x is the father |
of some z but z is not a paren t of y. Addin g the singl e litera l Parent(z,y)  give s the claus e
Father(x,  z) A Parent(z,  y) => Grandfather(x,  y)
whic h correctl y classifie s all the examples . FOI L will find and choos e this literal , thereb y solving |
the learnin g task.
The precedin g exampl e is a very simpl e illustratio n of how FOI L operates . A sketc h oil
the complet e algorith m is show n in Figur e 21.8 . Essentially , the algorith m repeatedl y constructs !
a clause , litera l by literal , unti l it agree s with som e subse t of the positiv e example s and none J
of the negativ e examples . The n the positiv e example s covere d by the claus e are remove d frorn l
the trainin g set, and the proces s continue s unti l no positiv e example s remain . The two main !
component s to be explaine d are NEW­LITERALS , whic h construct s all possibl e new literal s to add]
to the clause , and CHOOSE­LiTERAL , whic h select s a litera l to add.
Sectio n 21.4 . Inductiv e Logi c Programmin g 643
functio n FoiL(examples,target)  return s a set of Horn clause s
inputs : examples,  set of example s
target,  a litera l for the goal predicat e
local variables : clauses,  set of clauses , initiall y empt y
while examples  contain s positiv e example s do
clause  — NEW­CLAUSE(examples)
remov e example s covere d by clause  from examples
clauses^­  {clause\clauses}
retur n clauses
functio n NEW­CLAUSE(examples,  target)  return s a Hor n claus e
local variables : clause,  a claus e with target  as head and an empt y body
/, a litera l to be adde d to the claus e
extended­examples,  a set of example s with value s for new variable s
extended­examples  — examples
while extended­examples  contain s negativ e example s do
/ <— CHOOSE­LnERAL(NEW­LnERALS(clause),extended­examples)
appen d / to the body of clause
extended­examples  <— set of example s create d by applyin g EXTEND­ExAMPL E
to each exampl e in extended­examples
retur n clause
functio n ExTEND­ExAMPLE(example,literal ) return s
if example  satisfie s literal
then retur n the set of example s create d by extendin g example  with
each possibl e constan t valu e for each new variabl e in literal
else retur n the empt y set
Figur e 21.8 Sketc h of the FOI L algorith m for learnin g sets of first­orde r Horn clause s from
examples . NEW­EITERA L and CHOOSE­EITERA L are explaine d in the text.
NEW­LITERAL S take s a claus e and construct s all possibl e "useful " literal s that coul d be
added to the clause . Let us use as an exampl e the claus e
Father(x,z)  => Grandfather(x,y)
There are thre e kind s of literal s that can be added .
1. Literal s usin g predicates : the litera l can be negate d or unnegated , any existin g predicat e
(includin g the goal predicate ) can be used , and the argument s mus t all be variables . Any
variable s can be used for any argumen t of the predicate,  with one restriction : each litera l
must includ e at least  one variabl e from an earlie r litera l or from the head of the clause .
Literal s such as Mother(z,  u), Married(z,  z), ­>Male(y),  and Grandfather(v,  x) are allowed ,
wherea s Married(u,  v) is not. Notic e that the use of the predicat e from the head of the
claus e allow s FOIL to learn recursive  definitions .
644 Chapte r 21 . Knowledg e in Learnin g
2. Equalit y and inequalit y literals : thes e relat e variable s alread y appearin g in the clause . For
example , we migh t add zfa. Thes e literal s can also includ e user­specifie d constants . In the
famil y domain , there will not usuall y be any such "special " constants , wherea s in learnin g
arithmetic , we migh t use 0 and 1, and in list functions , the empt y list [ ].
3. Arithmeti c comparisons:  whe n dealin g with function s of continuou s variables , literal s such
as x > y and y < z can be added . As in decision­tre e learning , one can also use constan t
threshol d value s that are chose n to maximiz e the discriminator y powe r of the test.
All this adds up to a very large branchin g facto r in the searc h spac e (see Exercis e 21.6) . Imple ­
mentation s of FOIL may also use type informatio n to restric t the hypothesi s space . For example ,
if the domai n include d number s as well as people , type restriction s woul d preven t NEW­LITERAL S
from generatin g literal s such as Parent(x,  n), wher e x is a perso n and n is a number .
CHOOSE­LlTERA L uses a heuristi c somewha t simila r to informatio n gain (see page 541) to
decid e whic h litera l to add. The exac t detail s are not so importan t here , particularl y as a numbe r
of differen t variation s are currentl y bein g tried out. One interestin g additiona l featur e of FOIL is
the use of Ockham' s razo r to eliminat e som e hypotheses . If a claus e become s longe r (accordin g
to some metric ) than the total lengt h of the positiv e example s that the claus e explains , that claus e
is not considere d as a potentia l hypothesis . This techniqu e provide s a way to avoi d overcomple x
clause s that fit nois e in the data . For an explanatio n of the connectio n betwee n nois e and claus e
length , see Sectio n 19.6 .
FOIL and its relative s have been used to learn a wid e variet y of definitions . On e of the
most impressiv e demonstration s (Quinla n and Cameron­Jones , 1993 ) involve d solvin g a long
sequence s of exercise s on list­processin g function s from Bratko' s (1986 ) Prolo g textbook . In
each case , the progra m was able to learn a correc t definitio n of the functio n from a smal l set of
examples , usin g the previousl y learne d function s as backgroun d knowledge .
21.5 SUMMAR Y
This chapte r has investigate d variou s way s in whic h prio r knowledg e can help an agen t to learn
from new experiences .
• The use of prior knowledg e in learnin g lead s to a pictur e of cumulativ e learning , in whic h
learnin g agent s improv e their learnin g abilit y as they acquir e mor e knowledge .
• Prio r knowledg e help s learnin g by eliminatin g otherwis e consisten t hypothese s and by
"fillin g in" the explanatio n of examples , thereb y allowin g for shorte r hypotheses . Thes e
contribution s improv e both the sampl e complexit y and computatio n complexit y of learning .
• Understandin g the differen t logica l role s playe d by prio r knowledge , as expresse d by
entailmen t constraints , help s to defin e a variet y of learnin g techniques .
• Explanation­base d learnin g (EBL ) extract s genera l rule s from singl e example s by ex­
plaining  the example s and generalizin g the explanation . It provide s a deductiv e metho d to
turn first­principle s knowledg e into useful , efficient , special­purpos e expertise .
Sectio n 21.5 . Summar y 64 5
• Relevance­base d learnin g (RBL ) uses prior knowledg e in the form of determination s to
identif y the relevan t attributes , thereb y generatin g a reduce d hypothesi s spac e and speedin g
up learning . RBL also allow s deductiv e generalization s from singl e examples .
• Knowledge­base d inductiv e learnin g (KBIL ) find s inductiv e hypothese s that explai n sets
of observation s with the hejp of backgroun d knowledge .
• Inductiv e logic programmin g (ILP ) technique s perfor m KBI L usin g knowledg e expresse d
in first­orde r logic . ILP method s can lear n relationa l knowledg e that is not expressibl e in
attribute­base d systems .
• ILP method s naturall y generat e new predicate s with whic h concis e new theorie s can be
expressed , and show promis e as general­purpos e scientifi c theor y formatio n systems .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
The use of prio r knowledg e in learnin g from experienc e has had a surprisingl y brie f perio d
of intensiv e study . Fact,  Fiction,  and  Forecast,  by the philosophe r Nelso n Goodma n (1954) ,
refute d the earlie r suppositio n that inductio n was simpl y a matte r of seein g enoug h example s
of some universall y quantifie d propositio n and then adoptin g it as a hypothesis . Consider , for
GRUE example , the hypothesi s "All emerald s are grue, " wher e grue mean s "gree n if observe d befor e
time t, but blue if observe d thereafter. " At any time up to t, we migh t have observed  million s
of instance s confirmin g the rule that emerald s are grue , and no disconfirmin g instances , and yet
we are unwillin g to adop t the rule . This can only be explaine d by appea l to the role of relevan t
prior knowledg e in the inductio n process . Goodma n propose s a variet y of differen t kind s of prior
knowledg e that migh t be useful , includin g a versio n of determination s calle d overhypotheses .
Unfortunately , Goodman' s wor k was neve r take n up in early studie s of machin e learning .
EBL had its roots  in the technique s used by the STRIP S planne r (Fike s et ai, 1972) .
When a plan was constructed , a generalize d versio n of it was save d in a plan librar y and
used in late r plannin g as a macro­operator . Simila r idea s appeare d in Anderson' s ACT *
architecture , unde r the headin g of knowledg e compilatio n (Anderson , 1983) ; and in the SOA R
architectur e as chunkin g (Lair d et al, 1986) . Schem a acquisitio n (DeJong , 1981) , analytica l
generalizatio n (Mitchell , 1982) , and constraint­base d generalizatio n (Minton , 1984 ) wer e
immediat e precursor s of the rapi d growt h of interes t in EBL stimulate d by the publicatio n
of (Mitchel l etal,  1986 ; DeJon g and Mooney , 1986) . Hirs h (1987 ) introduce d the EBL algorith m
describe d in the text , showin g how it coul d be incorporate d directl y into a logic programmin g
system . Van Harmele n and Bund y (1988 ) explai n EBL as a varian t of the partia l evaluatio n
metho d used in progra m analysi s system s (Jone s et al., 1993) .
More recently , rigorou s analysi s and experimenta l wor k has led to a bette r understandin g
of the potentia l cost s and benefit s of EBL in term s of problem­solvin g speed . Minto n (1988 )
showe d that withou t extensiv e extr a work , EBL coul d easil y slow dow n a progra m significantly .
Tamb e et al. (1990 ) foun d a simila r proble m with chunking , and propose d a reductio n in the
expressiv e powe r of the rule languag e in orde r to minimiz e the cost of matchin g rule s agains t
workin g memory . Thi s wor k bear s stron g parallel s wit h recen t result s on the complexit y of
646 Chapte r 21 . Knowledg e in Learnin g
ANALOGICA L
REASONIN G
DISCOVER Y
SYSTEM Sinferenc e in restricte d version s of first­orde r logic (see Chapte r 10). Forma l probabilisti c analysi s
ofthe expecte d payof f of EBLca n be foun d in (Greiner , 1989 ; Subramania n and Feldman , 1990) .
An excellen t surve y appear s in (Dietterich , 1990) .
Instea d of usin g example s as foci for generalization , one can use them directl y to solv e
new problem s in a proces s know n as analogica l reasoning . This form of reasonin g range s from
a form of plausibl e reasonin g base d on degre e of similarit y (Gentner , 1983) , throug h a form
of deductiv e inferenc e base d on determination s (Davie s and Russell , 1987 ) but requirin g the
participatio n of the example , to a form of "lazy " EBL that tailor s the directio n of generalizatio n
of the old exampl e to fit the need s of the new problem . Thi s latte r form of analog y reasonin g
is foun d mos t commonl y in case­base d reasonin g (Kolodner , 1993 ) arid derivationa l analog y
(Velos o and Carbonell , 1993) .
Relevanc e informatio n in the form of functiona l dependencie s was first develope d in the
databas e community , wher e it is used to structur e large sets of attribute s into manageabl e subsets .
Functiona l dependencie s were used for analogica l reasonin g by Carbonel l and Collin s (1973) ,
and give n a mor e logica l flavo r by Bobro w and Raphae l (1974) . Dependencie s were indepen ­
dently  rediscovered , and give n a full logica l analysis , by Davie s (1985 ) and Russel l (1986a) ,
for the proble m of analogica l inferenc e (see also (Davie s and Russell , 1987)) . The y were used
for declarativ e bias by Russel l and Groso f (1987) . Th e equivalenc e of determination s to a
restricted­vocabular y hypothesi s spac e was prove d in (Russell , 1988) . Learnin g algorithm s for
determinations , and the improve d performanc e obtaine d by RBDTL , wer e first show n in the
FOCU S algorith m in (Almualli m and Dietterich , 1991) . Tadepall i (1993 ) describe s an ingeniou s
algorith m for learnin g with determination s that show s large improvement s in learnin g speed .
The stud y of method s for learnin g first­orde r logica l sentence s bega n with the remarkabl e
Ph.D . thesi s by Gordo n Plotki n (1971 ) at Edinburgh . Althoug h Plotki n develope d man y of the
theorem s and method s that are in curren t use in ILP, he was discourage d by some undecidabilit y
result s for certai n subproblem s in induction . MIS (Shapiro , 1981 ) reintroduce d the proble m
of learnin g logi c programs , but was mainl y seen as a contributio n to the theor y of automate d
debugging . The field was reinvigoratedb y Muggleto n and Buntin e (1988) , whos e CiGO L progra m
incorporate d a slightl y incomplet e versio n of invers e resolutio n and was capabl e of generatin g new
predicates.4 Mor e recen t system s includ e GOLE M (Muggleto n and Cao, 1990) , ITOU (Rouveiro l
and Puget , 1989 ) and CUN T (De Raedt , 1992) . A secon d threa d of ILP researc h bega n with
Quinlan' s FOI L system , describe d in this chapte r (Quinlan , 1990) . A forma l analysi s of ILP
method s appear s in (Muggleton , 1991) , and a large collectio n of paper s in (Muggleton , 1992) .
Early complexit y result s by Haussle r (1989 ) suggeste d that learnin g first­orde r sentence s
was hopelessl y complex . However , with bette r understandin g of the importanc e of variou s kind s
of syntacti c restriction s on clauses , positiv e result s have been obtaine d even for clause s with
recursio n (Dzerosk i et al, 1992) . A recen t pape r by Kiet z and Dzerosk i (1994 ) provide s an
excellen t surve y of complexit y result s in ILP.
Althoug h ILP now seem s to be the dominan t approac h to constructiv e induction , it has
not been the only approac h taken . So­calle d discover y system s aim to mode l the proces s of
scientifi c discover y of new concepts , usuall y by a direc t searc h in the spac e of concep t definitions .
Doug Lenat' s AM (Automate d Mathematician ) (Davi s and Lenat , 1982 ) used discover y heuristic s
The invers e resolutio n metho d also appear s in (Russell . 1986b) , wher e a complet e algorith m is mentione d in a footnote .
Sectio n 21.5 . Summar y 647
expresse d as exper t syste m rule s to guid e its searc h for concept s and conjecture s in elementar y
numbe r theory . In shar p contras t with mos t system s designe d for mathematica l reasoning ,
AM lacke d a concep t of proo f and coul d only mak e conjectures . It rediscovere d Goldbach' s
Conjectur e and the Uniqu e Prim e Factorizatio n Theorem . AM' s architectur e was generalize d
in the EURTSK O syste m (Lenat , 1983 ) by addin g a mechanis m capabl e of rewritin g the system' s
own discover y heuristics . EURISK O was applie d in a numbe r of area s othe r than mathematica l
discovery , althoug h with less succes s than AM. The methodolog y of AM and EURISK O has been
controversia l (Ritchi e and Hanna , 1984 ; Lena t and Brown , 1984) .
Anothe r clas s of discover y system s aim s to operat e with real scientifi c data to find new
laws. The system s DALTON , GLAUBER , and STAH L (Langle y et al, 1987 ) are rule­base d system s
that look for quantitativ e relationship s in experimenta l data from physica l systems ; in each
case, the syste m has bee n able to recapitulat e a well­know n discover y from the histor y of
science . AUTOCLAS S (Cheesema n et al, 1988 ) take s a more theoreticall y grounde d approac h to
concep t discovery : it uses Bayesia n probabilisti c reasonin g to partitio n give n data into the "mos t
likely " collectio n of classes . AUTOCLAS S has been applie d to a numbe r of real­worl d scientifi c
classificatio n tasks , includin g the discover y of new type s of stars from spectral  data and the
analysi s of protei n structur e (Hunte r and States , 1992) .
EXERCISE S
21.1 Show , by translatin g into conjunctiv e norma l form and applyin g resolution , that the con­
clusio n draw n on page 633 concernin g Brazilian s is sound .
21.2 Fo r each of the followin g determinations , writ e dow n the logica l representatio n and explai n
why the determinatio n is true (if it is):
a. Zip code determine s the state (U.S.) .
b. Desig n and denominatio n determin e the mas s of a coin .
c. For a give n program , inpu t determine s output .
d. Climate , food intake , exercise , and metabolis m determin e weigh t gain/loss .
e. Baldnes s is determine d by the baldnes s (or lack thereof ) of one' s materna l grandfather .
21.3 Woul d a probabilisti c versio n of determination s be useful ? Sugges t a definition .
21.4 Fil l in the missin g value s for the clause s C\ and/o r €2 in the followin g sets of clauses ,
given that C is the resolven t of C\ and €2­
a. C=  True  => P(A,B},  C\ = P(x,y)  =^ Q(x,y),  C2 =??.
b. C = True  => P(A,B),  C, =11,  C2 =??.
c. C = P(x,y)  => P(x,f(y)),  C{ =11,  C2 =11.
If there is mor e than one possibl e solution , provid e one exampl e of each differen t kind .
648 Chapte r 21 . Knowledg e in Learnin g
21.5 Suppos e one write s a logic progra m that carrie s out a resolutio n inferenc e step . Tha t is,
let Resolve(c\,  t*2,c)  succee d if c is the resul t of resolvin g c\ and ci. Normally , Resolve  woul d
be used as part of a theore m prove r by callin g it with c\ and ci instantiate d to particula r clauses ,
thereb y generatin g the resolven t c. Now suppos e instea d that we call it with c instantiate d and
c\ and €2 uninstantiated . Wil l this succee d in generatin g the appropriat e result s of an invers e
resolutio n step ? Woul d you need any specia l modification s to the logic programmin g syste m for
this to work ?
21.6 Suppos e that FOIL is considerin g addin g a litera l to a claus e usin g a binar y predicat e P,
and that previou s literal s (includin g the head of the clause ) contai n five differen t variables .
a. How man y functionall y differen t literal s can be generated ? Notic e that two literal s are
functionall y identica l if they diffe r only in the name s of the new variable s that they contain .
b. Can you find a genera l formul a for the numbe r of differen t literal s with a predicat e of arity
r whe n there are n variable s previousl y used ?
c. Wh y does FOIL not allow literal s that contai n no previousl y used variables ?
21.7 Usin g the data from the famil y tree in Figur e 21.5 , or a subse t thereof , appl y the FOI L
algorith m to learn a definitio n for the Ancestor  predicate .
I
Part VII
COMMUNICATING , PERCEIVING ,
AND ACTIN G
So far we have been concerne d with wha t happen s insid e an agent—fro m the
time it receive s a percep t to the time it decide s on an action . In this part , we
concentrat e on the interfac e betwee n the agen t and the environment . On one end,
we have perception:  vision , hearing , touch , and possibl y othe r senses . On the
other end, we have action:  the movemen t of a robo t arm, for example .
Also covere d in this part is communication.  A grou p of agent s can be mor e
successful—individuall y and collectively—i f they communicat e their belief s and
goals to each other . We look mos t closel y at huma n languag e and how it can be
used as a communicatio n tool.
r\ r\ AGENT S THA T
LL  COMMUNICAT E
In which  we see why  agents might  want  to exchange  information­carrying  messages
with each  other,  and how they can do so.
It is dusk in the savann a woodland s of Ambosel i Nationa l Park near the base of Kilimanjaro . A
group of verve t monkey s are foragin g for food . One verve t lets out a loud barkin g call and the
group respond s by scramblin g for the trees , neatl y avoidin g the leopar d that the first verve t had
seen hidin g in the bush . The verve t has successfull y communicate d with the group .
Communicatio n is such a widesprea d phenomeno n that it is hard to pin dow n an exac t
. ­ definition . In general , communication is the  intentional  exchange  of information  brought about
I ?: by  the production  and  perception  of signs  drawn  from  a shared  system  of conventional  signs.
Most animal s emplo y a fixe d set of sign s to represen t message s that are importan t to their survival :
food here , predato r nearby , approach , withdraw , let's mate . The verve t is unusua l in havin g a
variet y of calls for differen t predators : a loud bark for leopards , a shor t coug h for eagles , and
a chutte r for snakes . The y use one kind of grun t in exchange s with dominan t member s of their
own socia l group , anothe r kind with subordinat e members , and yet anothe r with vervet s in othe r
socia l groups .
The leopar d alarm (the loud bark ) is a conventiona l sign that both alert s other s that there is
dange r in the bush nearby , and signal s the actio n of escapin g to the trees . But conside r a verve t
that is too far away to hear the call, but can see the other s headin g for the trees . He too may clim b
the neares t tree, but he has not participate d in communicatio n becaus e he did not perceiv e any
intentionall y conveye d signs , but rathe r used his genera l power s of perceptio n and reasoning .
Human s use a limite d numbe r of conventiona l sign s (smiling , shakin g hands ) to communi ­
cate in muc h the same way as othe r animals . Human s have also develope d a complex , structure d
LANGUAG E syste m of sign s know n as languag e that enable s them to communicat e mos t of wha t they know
abou t the world . Althoug h chimpanzees , dolphins , and othe r mammal s have show n vocabularie s
of hundred s of sign s and som e aptitud e for stringin g them together , human s are the only specie s
that can reliabl y communicat e an unbounde d numbe r of qualitativel y differen t messages.1
1 Th e bee's tail­waggin g danc e specifie s the distanc e and angl e from the sun at whic h food can be found , so in one sens e
the bee can conve y an infinit e numbe r of messages , but we do not coun t this as unbounde d variation .
651
652 Chapte r 22 . Agent s that Communicat e
Of course , ther e are othe r attribute s that are uniquel y human : no othe r specie s wear s
clothes , create s representationa l art, or watche s four hour s of televisio n a day. But whe n Turin g
propose d his test (see Sectio n 1.1), he base d it on languag e becaus e languag e is intimatel y tied to
thinkin g in a way that, say, clothin g is not. In this chapter , we will explai n how a communicatin g
agent work s and presen t a simplifie d versio n of Englis h that is sufficien t to illustrat e the working s
of the agent' s majo r components .
22.1 COMMUNICATIO N AS ACTIO N
SPEEC H ACT
INDIREC T SPEEC H
ACTOne of the action s that is availabl e to an agen t is to produc e language . Thi s is calle d a speec h
act. "Speech " is used in the same sens e as in "free speech, " not "talking, " so typing , skywriting ,
and usin g sign languag e all coun t as speec h acts. Englis h has no neutra l wor d for an agen t that
produce s language , eithe r by speakin g or writin g or anythin g else. We will use speaker , hearer ,
and utteranc e as generi c term s referrin g to any mod e of communication . We will also use the
term word s to refer to any kind of conventiona l communicativ e sign .
Why woul d an agen t bothe r to perfor m a speec h act whe n it coul d be doin g a "regular "
action ? Imagin e a grou p of agent s are explorin g the wumpu s worl d together . The grou p gain s an
advantag e (collectivel y and individually ) by bein g able to do the following :
• Infor m each othe r abou t the part of the worl d each has explored , so that each agen t has
less explorin g to do. This is done by makin g statements : There's  a breeze  here  in 3 4.
• Quer y othe r agent s abou t particula r aspect s of the world . This is typicall y done by askin g
questions : Have  you smelled  the wumpus  anywhere?
• Answe r questions . This is a kind of informing . Yes,  I smelled  the wumpus  in 2 5.
• Reques t or comman d othe r agent s to perfor m actions : Please  help me  carry  the gold.  It
can be seen as impolit e to make a direc t requests , so often an indirec t speec h act (a reques t
in the form of a statemen t or question ) is used instead : / could use  some help  carrying  this
or Could  you help  me cany this?
• Promis e to do thing s or offe r deals : /'// shoot the  wumpus  if you let me share  the gold.
• Acknowledg e request s and offers : OK.
• Shar e feeling s and experience s with each other:  You  know,  old chap,  when  I get in a spot
like that,  I usually  go back to  the start  and head out  in another  direction,  or Man,  that
wumpus  sure  needs some deodorant!
Thes e example s show that speec h acts can be quit e usefu l and versatile . Som e kind s of speec h
acts (informing , answering , acknowledging , sharing ) hav e the intende d effec t of transferrin g
informatio n to the hearer . Other s (requesting , commanding , querying , leopar d alarm ) have the
intende d effec t of makin g the heare r take som e action . A dual purpos e of communicatio n is to
establis h trus t and buil d socia l ties (jus t as primate s groom  each othe r both to remov e fleas and
to build relationships) . This help s to explai n wha t is communicate d in exchange s such as "Hello ,
how are you ? Fine , how are you ? Not bad. "
Sectio n 22.1 . Communicatio n as Actio n 65 3
THE EVOLUTIO N OF LANGUAGE
In 1866 , the Societ e de Linguistiqu e de Pari s passe d a by­la w bannin g all debat e
on the origi n of language . Outsid e the hall s of that body , the debat e goes on. One
proposa l (Chomsky , 1980 ; Fodor , 1983 ) is that there is a languag e modul e that exist s
only in the brai n of humans . Ther e is now considerabl e evidenc e (Dingwell , 1988 )
that languag e use is mad e possibl e by a large repertoir e of differen t skill s that did not
develo p in isolatio n of genera l cognitiv e capabilities , and that man y of the precursor s
of huma n languag e use can be seen in the othe r primate s and in the fossi l record .
At least seve n researcher s claim to have taugh t primate s over 100 words . Kok o
the gorill a is the vocabular y quee n with over a thousand . Althoug h som e argu e that
the researcher s becom e too attache d to the animal s and attribut e abilitie s to them that
are not reall y there , it seem s safe to say that primate s can learn word s spontaneousl y
(from huma n trainer s or from each other ) and can use them to infor m other s of wha t
they know and wha t they want . The y can produc e and understan d sentence s involvin g
abstrac t concept s in a limite d way , such as referrin g to object s that are not present . For
example , chimpanzee s can correctl y respon d to "get the ball that is outside " or "brin g
the red ball over to the blue ball. " They can even  tell lies: a verve t who is losin g a figh t
may give the leopar d alarm , causin g the figh t to be calle d off whil e everyon e head s
for the trees . Som e have claime d that such behavio r is evidenc e that animal s form
model s of the world , includin g model s of how other agent s act. Unfortunately , presen t
psychologica l methodologie s do not allow us to distinguis h betwee n behavior s that
are mediate d by interna l model s and thos e that are merel y stimulus­respons e patterns .
Also , althoug h primate s clearl y learn some rules for orderin g words , there appea r to be
limitation s in the way they use synta x to produc e unbounde d sequences . Despit e some
impressiv e accomplishments , no primat e has duplicate d the explosio n of languag e use
that all norma l huma n childre n accomplis h by age four .
Languag e and though t reinforc e each other , but it is not know n if human s evolve d
to use languag e wel l becaus e they are smart , or if they are smar t becaus e they use
languag e well . One theor y (Jerison , 1991 ) is that huma n languag e stem s primaril y
from a need for bette r cognitiv e map s of the territory . Canine s and othe r socia l
carnivore s rely heavil y on scen t markin g and thei r olfactor y syste m to decid e both
wher e they are and wha t othe r animal s have been there—jus t as the wumpu s worl d
agent s use the presenc e of a smel l or breez e to help map out that world . But the early
hominid s (monkey s and apes of 30 millio n year s ago) did not have a well enoug h
develope d olfactor y syste m to map out the worl d this way , so they substitute d voca l
sound s for scen t marking . Thus , the rest of this chapter , in Jerison' s view , is devote d
to the way we human s compensat e for our inadequat e noses .
654 Chapte r 22 . Agent s that Communicat e
The hard part for an agen t is to decid e when  a speec h act of som e kind is calle d for,
and to decid e which  speec h act, out of all the possibilities , is the righ t one. At one level ,
this is just the familia r plannin g problem—a n agen t has a set of possibl e action s to choos e
from , and mus t someho w try to choos e action s that achiev e the goal of communicatin g som e
...A:­•­.< • informatio n to anothe r agent . All  the difficulties  that  make  planning  hard  (see Chapter  12)
•sfsS$ apply  to planning  speech  acts.  It woul d be impossibl y comple x to plan Englis h conversation s
at the leve l of individua l movement s of the mout h and tongue , so we need to plan with severa l
level s of hierarchica l abstraction—words , phrase s and sentences , at least . Anothe r proble m is
nondeterminism . Wherea s most  action s in the wumpu s worl d are deterministic , speec h acts are
not. Conside r the actio n Speak("Turn  Right!").  If anothe r agen t perceive s the words , and if the
agen t interpret s it as a comman d to turn right , then that agen t may do so. Or then again , the agen t
may ignor e the comman d and choos e anothe r action . The nondeterminis m mean s that we will
need conditiona l plans . Instea d of plannin g a conversatio n from beginnin g to end, the best we
can do is construc t a genera l plan or polic y for the conversation , generat e the first sentence , stop
to perceiv e the reply , and react to it.
UNDERSTANDIN G Th e proble m of understandin g speec h acts is muc h like othe r understandin g problems ,
such as imag e understandin g or medica l diagnosis . We are give n a set of ambiguou s inputs ,
and from them we have to wor k backward s to decid e wha t state of the worl d coul d have create d
the inputs . Par t of the speec h act understandin g proble m is specifi c to language . We need . j
to kno w somethin g abou t the synta x and semantic s of a languag e to determin e why anothe r
agen t performe d a give n speec h act. The understandin g proble m also include s the mor e genera l
PLAN RECOGNITIO N proble m of plan recognition . If we observ e an agen t turnin g and movin g towar d the gold , we
can understan d the action s by formin g a mode l of the agent' s belief s that says the agen t has the
goal of gettin g the gold . A simila r kind of menta l mode l buildin g is require d to understan d the
agent who turn s towar d the gold and says , "I'm goin g to grab it." Eve n thoug h there may be othe r
object s nearby , it is fairl y clear that "it" refer s to the gold .
Part of the understandin g proble m can be handle d by logica l reasoning . We will see that
logica l implication s are a good way of describin g the way s that word s and phrase s combin e to
form large r phrases . Anothe r part of the understandin g proble m can only be handle d by uncertai n
reasonin g techniques . Usually , ther e will be severa l state s of the worl d that coul d all lead to the
same speec h act, so the understande r has to decid e whic h one is mor e probable .
Now that we have seen how communicatio n fits into our genera l agen t design , we can turn |
our attentio n to languag e itself . As we focu s mor e and mor e on the way languag e actually  is,
rathe r than on the genera l propertie s of communicatio n methods , we will find ourselve s movin g |
into the realm of natura l science—tha t is, scienc e that work s by findin g out thing s abou t the real
worl d rathe r than abou t program s or othe r artifacts . Natura l languag e understandin g is one of the
few area s of AI that has this property .
FORMA L LANGUAGE S
NATURA L
LANGUAGE SFundamental s of languag e
We distinguis h betwee n forma l languages —the ones like Lisp and first­orde r logi c that are
invente d and rigidl y defined—an d natura l languages —the one s like Chinese , Danish , and
Englis h that human s use to talk to one another . Althoug h we are primaril y intereste d in natura l
Sectio n 22.1 . Communicatio n as Actio n
STRING S
TERMINA L SYMBOL Slanguages , we will mak e use of all the tools of forma l languag e theory , startin g with the Backus ­
Naur form (BNF ) notation , whic h is describe d in Appendi x B on page 854.
A forma l languag e is define d as a set of strings , wher e each strin g is a sequenc e of symbol s
taken from a finit e set calle d the termina l symbols . For English , the termina l symbol s includ e
word s like a, aardvark,  aback,  abacus,  and abou t 400,00 0 more .
One of the confusin g thing s in workin g with both forma l and natura l language s is that
there are so man y differen t formalism s and notation s for writin g grammar s (see the Historica l
Note s sectio n for this chapter) . However , mos t of them are simila r in that they are base d on
the idea of phras e structure —tha t string s are compose d of substring s calle d phrases , whic h
come in differen t categories . For example , the phrase s "the wumpus, " "the king, " and "the agen t
in the corner " are all example s of the categor y noun phras e (or NP for short) . Ther e are two
reason s for identifyin g phrase s in this way . First , phrase s are convenien t handle s on whic h we can
attac h semantics . Second , categorizin g phrase s help s us to describ e the allowabl e string s of the
language . We can say that any of the noun phrase s can combin e with a verb phras e (or VP) such
as "is dead " to form a phras e of categor y sentenc e (or 5). Withou t the intermediat e notion s of noun
phras e and verb phrase , it woul d be difficul t to explai n why "the wumpu s is dead " is a sentenc e
wherea s "wumpu s the dead is" is not. Grammatica l categorie s are essentiall y posite d as part of a
scientifi c theor y of languag e that attempt s to accoun t for the differenc e betwee n grammatica l and
ungrammatica l categories . It is theoreticall y possible , althoug h perhap s unlikely , that in som e
futur e theor y of the Englis h language , the NP and VP categorie s may not exist .
Categorie s such as NP, VP,  and S are calle d nontermina l symbols . In the BNF notation ,
rewrit e rule s consis t of a singl e nontermina l symbo l on the left­han d side , and a sequenc e of
terminal s or nonterminal s on the right­han d side. The meanin g of a rule such asPHRAS E STRUCTUR E
NOUN PHRAS E
VERB PHRAS E
SENTENC E
NONTERMINA L
SYMBOL S
REWRIT E RULE S
5" — NP VP
is that we can take any phras e categorize d as a NP, appen d to it any phras e categorize d as a VP,
and the resul t will be a phras e categorize d as an S.
The componen t step s of communicatio n
A typica l communicatio n episode , in whic h speake r S want s to conve y propositio n P to heare r H
using word s W, is compose d of seve n processes . Thre e take plac e in the speaker :
Intention : S  want s H to believ e P (wher e S typicall y believe s P)
Generation : S choose s the word s W (becaus e they expres s the meanin g P)
Synthesis : 5 utter s the word s W (usuall y addressin g them to H)
Four take plac e in the hearer :
Perception : H  perceive s W (ideall y W' = W, but misperceptio n is possible )
Analysis : H  infer s that W has possibl e meaning s P\,...,  P,, (word s and
phrase s can have severa l meanings )
Disambiguation : H infer s that S intende d to conve y P, (wher e ideall y Pj = P,
but misinterpretatio n is possible )
Incorporation : H  decide s to believ e P, (or reject s it if it is out of line with
what H alread y believes )
656 Chapte r 22 . Agent s that Communicat e
GENERATIV E CAPACIT Y
Grammatica l formalism s can be classifie d by thei r generativ e capacity : the set of
language s they can represent . Chomsk y (1957 ) describe s four classe s of grammatica l
formalism s that diffe r only in the form of the rewrit e rules . The classe s can be arrange d
in a hierarchy , wher e each class can be used to describ e all the language s that can be
describe d by a less powerfu l class , as well as som e additiona l languages . Here we list
the hierarchy , mos t powerfu l class first :
Recursivel y enumerabl e grammar s use unrestricte d rules : bot h side s of the
rewrit e rule s can hav e any numbe r of termina l and nontermina l symbols . Thes e
grammar s are equivalen t to Turin g machine s in their expressiv e power .
Context­sensitiv e grammar s are restricte d only in that the right­han d hand side
must contai n at leas t as man y symbol s as the left­han d side . Th e nam e context ­
sensitiv e come s from the fact that a rule such as ASB  — AXB  says that an S can be
rewritte n as an X in the contex t of a precedin g A and a followin g B.
In context­fre e grammar s (or CFGs) , the left­han d side consist s of a singl e
nontermina l symbol . Thus , each rule license s rewritin g the nontermina l as the right ­
hand side in any context . CFG s are popula r for natura l languag e grammars , althoug h
it is now widel y accepte d that at leas t som e natura l language s are not context­fre e
(Pullum , 1991) .
Regula r grammar s are the mos t restricte d class . Ever y rule has a singl e nontermi ­
nal on the left­han d side, and a termina l symbo l optionall y followe d by a nontermina l
on the right­han d side . Regula r grammar s are equivalen t in powe r to finite­stat e
machines . The y are poorl y suite d for programmin g language s because , for example ,
they canno t represen t construct s such as balance d openin g and closin g parentheses .
To give you an idea of whic h language s can be handle d by whic h classes , the
languag e a"b"  (a sequenc e of n copie s of a followe d by the sam e numbe r of b) can
be generate d by a context­fre e grammar , but not a regula r grammar . The languag e
a"b"c"  require s a context­sensitiv e grammar , wherea s the languag e a*b*  (a sequenc e
of any numbe r of a followe d by any numbe r of b) can be describe d by any of the four
classes . A summar y of the four classe s follows .
Class Sampl e Rule Sampl e Lan;
Recursivel y enumerabl e
Context­sensitiv e
Context­fre e
Regula rA B — C an y
A B ­+ B A a"b"c"
S^aSb  a"b"
S^aS  a'b*
Sectio n 22.1 . Communicatio n as Actio n 657
INTENTIO N
GENERATIO N
SYNTHESI S
PERCEPTIO NLet us look at these seve n processe s in the contex t of the exampl e show n in Figur e 22.1 .
Intention . Somehow , the speake r decide s that there is somethin g that is wort h sayin g to
the hearer . Thi s ofte n involve s reasonin g abou t the belief s and goal s of the hearer , so that the
utteranc e will have the desire d effect . For our example , the speake r has the intentio n of havin g
the heare r know that the wumpu s is no longe r alive .
Generation . The speake r uses knowledg e abou t languag e to decid e wha t to say. In man y
ways , this is harde r than the invers e proble m of understandin g (i.e., analysi s and disambiguation) .
Generatio n has not been stresse d as muc h as understandin g in AI, mainl y becaus e we human s
are anxiou s to talk to machines , but are not as excite d abou t them talkin g back . For now , we just
assum e the heare r is able to choos e the word s "The wumpu s is dead. "
Synthesis . Mos t language­base d AI system s synthesiz e type d outpu t on a scree n or paper ,
whic h is a trivia l task . Speec h synthesi s has been growin g in popularity , and som e system s are
beginnin g to soun d human . In Figur e 22.1 , we show the agen t synthesizin g a strin g of sound s
writte n in the phoneti c alphabe t define d on page 758: "[thaxwahmpahsihzdeyd]. " The detail s of
this notatio n are unimportant ; the poin t is that the sound s that get synthesize d are differen t from
the word s that the agen t generates . Als o note that the word s are run together ; this is typica l of
quickl y spoke n speech .
Perception . Whe n the mediu m is speech , the perceptio n step is calle d speec h recognition ;
when it is printing , it is calle d optica l characte r recognition . Bot h have move d from bein g
Intention :
Know(H,­>  Alive(Wumpus,S 3»Generation :
"The wumpu s is dead "SPEAKE R
Synthesis :
[thaxwahmpahsihzdeyd ]
Perception :
"The wumpu s is dead "Analysis :
(Parsing) :
NP VP
Article
The
(Semanti c Interpretation) :Noun  Verb  Adjective
I I  I
wumpu s is dea d
AIive(  Wumpus,Now)
Tired(  Wumpus, Now)
(Pragmati c Interpretation) : ­, Alive(Wumpus,S 3)
Tired(  Wumpus,  S3)Disambiguation :
~~i Alive(Wumpus,S 3)
Incorporation :
TELL(  KB,
~~i Alive(Wumpus,S 3)
Figur e 22.1 Seve n processe s involve d in communication , usin g the exampl e sentenc e "The
wumpu s is dead. "
658 Chapte r 22 . Agent s that Communicat e
ANALYSI S
PARSIN G
PARS E TRE E
SEMANTI C
INTERPRETATION
PRAGMATI C
INTERPRETATIO N
DISAMBIGUATIO N
INCORPORATIO Nesoteri c to bein g commonplac e withi n the last five years . For the example , let us assum e that the
heare r perceive s the sound s and recover s the spoke n word s perfectly . (In Chapte r 24 we see how
this migh t be done. )
Analysis . We divid e analysi s into two main parts : syntacti c interpretatio n (or parsing ) and
semanti c interpretation . Semanti c interpretatio n include s both understandin g the meaning s of
word s and incorporatin g knowledg e of the curren t situatio n (also calle d pragmati c interpretation) .
The wor d parsin g is derive d from the the Lati n phras e pars  orationis,  or "part of speech, "
and refer s to the proces s of assignin g a part of speec h (noun , verb , and so on) to each wor d in a
sentenc e and groupin g the word s into phrases . One way of displayin g the resul t of a syntacti c
analysi s is with a pars e tree, as show n in Figur e 22.1 . A pars e tree is a tree in whic h interio r
node s represent s phrases , link s represen t application s of gramma r rules , and leaf node s represen t
words . We defin e the yield of a node as the list of all the leave s belo w the node , in left­to­righ t
order , then we can say that the meanin g of a pars e tree is that each node with labe l X assert s that
the yiel d of the node is a phras e of categor y X.
Semanti c interpretatio n is the proces s of extractin g the meanin g of an utteranc e as an
expressio n in som e representatio n language . In Figur e 22.1 we show two possibl e semanti c
interpretations : that the wumpu s is not alive , and that it is tired (a colloquia l meanin g of dead).
Utterance s with severa l possibl e interpretation s are said to be ambiguous . We use logi c as the
representatio n language , but othe r representation s coul d be used . Pragmati c interpretatio n is
the part of semanti c interpretatio n that take s the curren t situatio n into account.2 In the example ,
all that pragmatic s does is replac e the constan t Now  with the constan t 83, whic h stand s for the
curren t situation .
Disambiguation . Mos t speaker s are not intentionall y ambiguous , but most utterance s have
severa l lega l interpretations . Communicatio n work s becaus e the heare r does the work of figurin g
out whic h interpretatio n is the one the speake r probabl y mean t to convey . Notic e that this is the
first time we have used the word  probably,  and disambiguatio n is the first proces s that depend s
heavil y on uncertai n reasoning . Analysi s generate s possibl e interpretations ; if mor e than one
interpretatio n is found , then disambiguatio n choose s the one that is best.
Incorporation . A totall y naiv e agen t migh t believ e everythin g it hears , but a sophisticate d
agent treat s the word s W and the derive d interpretatio n P, as additiona l piece s of evidenc e that
get considere d alon g with all othe r evidenc e for and agains t P,.
Note that it only make s sens e to use languag e whe n ther e are agent s to communicat e
with who (a) understan d a commo n language , (b) have a share d contex t on whic h to base the
conversation , and (c) are at least somewha t rational . Communicatio n does not work whe n agent s
are completel y irrational , becaus e there is no way to predic t how an irrationa l agen t will reac t to a
speec h act. Interestingly , communicatio n can wor k whe n agent s are uncooperative . Eve n if you
believ e that anothe r wumpu s worl d explore r woul d lead you astra y in orde r to get the gold all for
itself , you can still communicat e to help each othe r kill the wumpu s or perfor m som e othe r task
that is helpfu l to both agents . Returnin g to Afric a for anothe r example , whe n an antelop e sees a
2 Thus ,us, pragmati c interpretatio n associate s meaning s with utterances  mad e i
semanti c interpretatio n associate s meaning s with strings  in isolation . Thi s is controversial , anu ouie r auinoi ^ uiaw mt
line betwee n semantic s and pragmatic s in differen t places , or just grou p them together . Also , som e author s use the term
parsin g to encompas s all of wha t we call analysis .•ranees  mad e in specifi c contexts , wherea s the rest of
controversial , and othe r author s draw the
Sectio n 22.2 . Type s of Communicatin g Agent s 659
predato r at a safe distance , it will stot, or leap high into the air. Thi s not only communicate s to
other antelope s that dange r is near , but also communicate s to the predato r "I see you , and I am
health y enoug h to run away , so don' t even bothe r chasin g me." So even thoug h the two animal s
are enemie s with no commo n goal , the communicatio n save s both of them from wastin g time and
energ y on a fruitles s chase .
Two model s of communicatio n
Our stud y of communicatio n center s on the way that an agent' s belief s are turne d into word s and
back into belief s in anothe r agent' s knowledg e base (or head) . Ther e are two way s of lookin g at
ENCODE D MESSAG E th e process . The encode d messag e mode l says that the speake r has a definit e propositio n P in
mind , and encode s the propositio n into the word s (or signs ) W. The heare r then tries to decod e
the messag e W to retriev e the origina l propositio n P (cf. Mors e code) . Unde r this model , the
meanin g in the speaker' s head , the messag e that gets transmitted , and the interpretatio n that the
heare r arrive s at all ideall y carry the same content . Whe n they differ , it is becaus e of noise in the
communicatio n channe l or an erro r in encodin g or decoding .
SITUATE D LANGUAG E Limitation s of the encode d messag e mode l led to the situate d languag e model , whic h says
that the meanin g of a messag e depend s on both the word s and the situatio n in whic h the word s
are uttered . In this model , just as in situatio n calculus , the encodin g and decodin g function s take
an extr a argumen t representin g the curren t situation . Thi s account s for the fact that the sam e
word s can have very differen t meaning s in differen t situations . "I am here now " represent s one
fact whe n spoke n by Pete r in Bosto n on Monday , and quit e anothe r fact whe n spoke n by Stuar t in
Berkele y on Tuesday . Mor e subtly , "You mus t read this book " is a suggestio n whe n writte n by a
critic in the newspaper , and an assignmen t whe n spoke n by an instructo r to the class . "Diamond "
mean s one thin g whe n the subjec t is jewelry , and anothe r whe n the subjec t is baseball .
The situate d languag e mode l point s out a possibl e sourc e of communicatio n failure : if the
speake r and heare r have differen t idea s of wha t the curren t situatio n is, then the messag e may
not get throug h as intended . For example , suppos e agent s X, Y, and Z are explorin g the wumpu s
world together . X and Y secretly  meet and agree that whe n they smel l the wumpu s they will both
shoo t it, and if they see the gold , they will grab it and run, and try to keep Z from sharin g it. Now
suppos e X smell s the wumpus , whil e Y in the adjacen t squar e smell s nothing , but sees the gold .
X yells "Now!, " intendin g it to mea n that now is the time to shoo t the wumpus , but Y interpret s
it as meanin g now is the time to grab the gold and run.
22.2 TYPE S OF COMMUNICATIN G AGENT S
In this chapter , we conside r agent s that communicat e in two differen t ways . Firs t are agent s who
share a commo n interna l representatio n language ; they can communicat e withou t any externa l
languag e at all. The n come agent s that mak e no assumption s abou t each other' s interna l language ,
but shar e a communicatio n languag e that is a subse t of English .
660
TELEPATHI C
COMMUNICATIO NChapte r 22 . Agent s that Communicat e
Communicatin g usin g Tell and Ask
In this section , we stud y a form of communicatio n in whic h agent s shar e the same interna l repre ­
sentatio n languag e and have direc t acces s to each other' s knowledg e base s throug h the TEL L and
ASK interface . Tha t is, agen t A can communicat e propositio n P to agen t B with TELL(KBg,  "P"),
just as A woul d add P to its own­knowledg e base with TELL(KB A, "P"). Similarly , agen t A can find
out if B know s Q with AsK(KB B, "Q").  We will call this telepathi c communication . Figur e 22.2
show s a schemati c diagra m in whic h each agen t is modifie d to have an input/outpu t port to its
knowledg e base , in additio n to the perceptio n and actio n ports .
Human s are lackin g in telepath y powers , so they canno t mak e use of this kind of commu ­
nication , but it is feasibl e to progra m a grou p of robot s with a commo n interna l representatio n
languag e and equi p them with radio or infrare d link s to transmi t interna l representation s directl y
to each other' s knowledg e bases . The n if agen t A wante d to tell agen t B that there is a pit in
locatio n [2,3] , all A woul d have to do is execute :
TELL(KBv,  "Pit(P Ai) A At(P Ai, [2,3],5 A9)")
wher e 5^9 is the curren t situation , and PM is A's symbo l for the pit. For this to work , the
agent s have to agre e not just on the forma t of the interna l representatio n language , but also on a
great man y symbols . Som e of the symbol s are static : they have a fixe d denotatio n that can be
easil y agree d on ahea d of time . Example s are the predicate s At and Pit, the constant s A and B
representin g the agents , and the numberin g schem e for locations .
Othe r symbol s are dynamic : the y are create d afte r the agent s star t explorin g the world .
Example s includ e the constan t PA\ representin g a pit, and 5/19 representin g the curren t situation .
The hard part is synchronizin g the use of these dynami c symbols . Ther e are three difficulties :
1. Ther e has to be a namin g polic y so that A and B do not simultaneousl y introduc e the same
symbo l to mea n differen t things . We have adopte d the polic y that each agen t include s its
own nam e as part of the subscrip t to each symbo l it introduces .
2. Ther e has to be som e way of relatin g symbol s introduce d by differen t agents , so that an
agent can tell whethe r PA \ and, say, PB2 denot e the same pit or not. In part, this is the same
proble m that a singl e agen t faces . An agen t that detect s a breez e in two differen t square sI
Communication  with  Tell  and Ask
Figur e 22.2 Tw o agent s with a share d interna l languag e communicatin g directl y with each
other' s knowledg e base s throug h TEL L and ASK .
Sectio n 22.2 . Type s of Communicatin g Agent s 66 1
migh t introduc e the symbol s P\ and PI to denot e the two pits that cause d the breezes , and
must then do som e reasonin g to decid e if PI = PI. But the proble m is harde r for multipl e
agent s becaus e they shar e fewe r symbol s to begi n with , and thus have less commo n groun d
to reaso n with . In particular , agen t B has the proble m of decidin g how the situatio n S^g
relate s to its own situatio n symbols . Thi s proble m can be lessene d by minimizin g the
numbe r of new symbols . For example , A coul d tell B that there is a pit in [2,3] withou t
introducin g any new dynami c symbol s with the followin g action :
TELL(KB B, "3/7 , s Pit(p)  A At(p,  [2,3] , j)")
This sentenc e is weake r than the one containin g P&\ and 5^9 becaus e it does not say whic h
pit is in [2,3] , nor whe n it was there , but it turn s out that these facts do not reall y matte r for
the wumpu s world . Not e that this sentenc e is simila r to the Englis h sentenc e "Ther e is a
pit in 2,3," whic h also fails to uniquel y identif y the pit and the time .
3. The fina l difficult y is in reconcilin g the difference s betwee n differen t agents ' knowledg e
bases . If communicatio n is free and instantaneous , then all agent s can adop t the polic y of
broadcastin g each new fact to everyon e else as soon as they learn it. Tha t way everyon e
will have all the same knowledge . But in mos t applications , the bandwidt h betwee n agent s
is limited , and they are ofte n completel y out of touc h with each othe r for period s of time .
When they come back into contact , they have the proble m of decidin g wha t new informatio n
is wort h communicating , and of discoverin g wha t interestin g facts the othe r agen t knows .
Anothe r proble m with telepathi c agent s as we have describe d them is that they are vulnerabl e to
sabotage . Anothe r agen t coul d TEL L lies directl y into the knowledg e base and mak e our naiv e
telepathi c agen t believ e anything .
Communicatin g usin g forma l languag e
Becaus e of the sabotag e problem , and becaus e it is infeasibl e for everyon e to have the sam e
interna l language , mos t agents  communicat e throug h languag e rathe r than throug h direc t acces s
to knowledg e bases . Figur e 22.3 show s a diagra m of this type of communication . Agent s
can perfor m action s that produc e language , whic h othe r agent s can perceive . Th e externa l
communicatio n languag e can be differen t from the interna l representatio n language , and the
agent s can each have differen t interna l languages . The y need not agre e on any interna l symbol s
at all as long as each one can map reliabl y from the externa l languag e to its own interna l symbols .
An externa l communicatio n languag e bring s with it the problem s of generatio n and analysis ,
and muc h of the effor t in natura l languag e processin g (NLP ) has gone into devisin g algorithm s
for thes e two processes . But the hardes t part of communicatio n with languag e is still proble m
(3) from the previou s section : reconcilin g the difference s betwee n differen t agents ' knowledg e
bases . Wha t agen t A says , and how agen t B interpret s A's statemen t depend s cruciall y on wha t A
and B alread y believ e (includin g wha t they believ e abou t each other' s beliefs) . This mean s that
agent s that have the same interna l and externa l languag e woul d have an easy time with generatio n
and analysis , but they woul d still find it challengin g to decid e wha t to say to each other .
662 Chapte r 22 . Agent s that Communicat e
Language
Figur e 22.3 Tw o agent s communicatin g with language .
An agen t that communicate s
We will now look at an exampl e of communication , in the form of a wumpu s worl d agen t that
acts as a robo t slave that can be commande d by a master . On each turn , the slave describe s its
percept s in Englis h (actuall y in a restricte d subset) , wait s for a comman d from the master , and
then interpret s the comman d and execute s it. Here is a fragmen t of a typica l dialogue :
ROBO T SLAV E MASTE R
I feel a breeze . G o to 1 2.
Nothin g is here . G o north .
I feel a breez e and I smel l a stenc h
and I see a glitter . Gra b it.
The agen t progra m is show n in Figur e 22.4 . The rest of this chapte r fills in the missin g routine s
in the agen t program . First , Sectio n 22.3 define s the subse t of Englis h in whic h the agent s
communicate . Then , Sectio n 22.4 show s how to implemen t the PARS E functio n to syntacticall y
analyz e string s of the language . Sectio n 22.7 show s how to recove r the meanin g of a strin g
(the functio n SEMANTICS) , and Sectio n 22.8 show s wha t to do whe n there are severa l possibl e
meaning s (the functio n DISAMBIGUATE) . The functio n GENERATE­DESCRIPTIO N is not covere d in
depth , but in genera l muc h the sam e knowledg e that is used for understandin g can also be used
for producin g language .
22.3 A FORMA L GRAMMA R FOR A SUBSE T OF ENGLIS H
In this section , we defin e a forma l gramma r for a smal l subse t of Englis h that is suitabl e for
makin g statement s abou t the wumpu s world . We will call this languag e £Q. In definin g the
languag e this way , we are implicitl y claimin g that forma l languag e technique s are appropriat e for
dealin g with natura l language . In man y ways , they are appropriate : natura l language s mak e use
Sectio n 22.3 . A  Forma l Gramma r for a Subse t of Englis h 663
functio n SIMPLE­COMMUNICATING­AGENT(perc<?/7 0 return s action
static : KB,  a knowledg e base
/, a counter , initiall y 0, indicatin g time
TELL(KB,  MAKE­PERCEPT­SENTENCE(/fB , t))
words  <— SPEECH­PART ( percept)
semantics  <— DlSAMBIGUATE(SEMANTICS(PARSE(word.s)) )
if T\PE[semantics]  = Command  then
actio n <— CONTENT S [semantics]
else if 1\PE[semantics]  = Statement  then
TELL(AT5 , CONTENTS|,semanri«] )
actio n <­ ASK(KB,  MAKE­ACTlON­QUERY(pe rcept , t))
else if T\PE[semantics]  = None  then
actio n — ASK(KB,  MAKE­AcnON­QuERY(perce/rt , t))
descriptio n ^­ GENERATE­DESCRIPTION(perrepr )
retur n COMPOUND­ACT:iON(SAY(description),  Do(action))
Figur e 22.4 A  communicatin g agen t that accept s command s and statements , and return s a
compoun d actio n that both describe s its percept s and does somethin g else.
of a fixe d set of letter s (for writte n language ) or sound s (for spoke n language ) that combin e into
a relativel y fixe d set of words . Eithe r leve l can be considere d the symbol s of a forma l language .
The symbol s are forme d into strings , and it is clea r that string s such as "Thi s is a sentence " are
part of the Englis h language , and string s such as "A is sentenc e this" are not. We can com e up
with a set of gramma r rule s that cove r at leas t part of English , and compos e phrase s to form
arbitraril y comple x sentences .
However , ther e are way s in whic h natura l language s diffe r from forma l ones . It is hard to
characteriz e a natura l languag e as a set of string s for four reasons . First , not all speaker s agre e on
what is in the language . Som e Canadian s end sentence s with "eh?, " som e U.S. southerner s use
"y'all, " and one can start argument s in som e circle s by askin g whethe r "Thi s ain't a sentence " is.
Second , the languag e change s over time—th e Englis h of Shakespeare' s day is quit e differen t from
what we spea k today . Third , som e utterance s that are clearl y ungrammatica l are nevertheles s
understandable . Fourth , grammaticalit y judgment s are ofte n grade d rathe r than absolute . Tha t
mean s that ther e are som e sentence s that do not soun d quit e right , but are not clearl y wrong ,
either . The followin g sentence s (for mos t speakers ) rang e from good to bad:
To who m did you send the letter ?
Next to who m did you stand ?
Of who m did you mee t a friend ?
Of who m did you see the deale r that bough t the pictur e that Vincen t painted ?
Even if we coul d agre e on exactl y whic h sentence s are Englis h and whic h are not, that woul d be
only a smal l part of natura l languag e processing . The reall y hard parts are semanti c interpretatio n
and disambiguation . Speec h act interpretatio n (whic h take s plac e acros s the syntactic , semantic ,
and pragmati c levels ) also complicate s the picture . In programmin g languages , ever y statemen t
664 Chapte r 22 . Agent s that Communicat e
is a command , but in natura l languag e the heare r has to determin e if an utteranc e is a command ,
question , statement , promise , or whatever . Forma l languag e theor y provide s a framewor k withi n
whic h we can addres s thes e more difficul t problems , but it does not answe r them on its own . In
the remainde r of this section , we defin e a forma l languag e for our Englis h subset , £Q­
The Lexico n of £0
LEXICO N Th e first step in definin g a gramma r is to defin e a lexicon , or list of allowabl e vocabular y words .
The word s are groupe d into the categorie s or parts of speec h familia r to dictionar y users : nouns ,
pronouns , and name s to denot e things , verb s to denot e events , adjective s to modif y nouns , and
ARTICLE S adverb s to modif y verbs . Categorie s that may  be less familia r to som e reader s are article s (suc h
PREPOSITION S a s the),  preposition s (in),  and conjunction s (and)?  Figur e 22.5 show s a smal l lexicon .
CONJUNCTION S Eac h of the categorie s ends in ... to indicat e that ther e are othe r word s in the category .
However , it shoul d be note d that there are two distinc t reason s for the missin g words . For nouns ,
verbs , adjectives , and adverbs , it is in principl e infeasibl e to list them all. Not only are there
thousand s or tens of thousand s of members  in each class , but new ones are constantl y bein g
added . For example , "fax " is now a very commo n nou n and verb , but it was coine d only a few
OPEN CLASSE S year s ago. Thes e four categorie s are calle d open classes . The othe r categorie s (pronoun , article ,
CLOSE D CLASSE S preposition , and conjunction ) are calle d close d classes . The y have a smal l numbe r of word s (a
few to a few dozen ) that coul d in principl e be enumerated . Close d classe s chang e over the cours e
of centuries , not months . For example , "thee " and "thou " were commonl y used pronoun s in the
seventeent h century , on the declin e in the nineteenth , and are seen only in poetr y and regiona l
dialect s today .
The Gramma r of £0
The next step is to combin e the word s into phrases . We will use five nontermina l symbol s to defin e
the differen t kind s of phrases : sentenc e (S), nou n phras e (NP),  verb phras e (VP),  prepositiona l
phras e (PP),  and relativ e claus e (RelClause).4 Figur e 22.6 show s a gramma r for £Q with an
exampl e for each rewrit e rule .
22.4 SYNTACTI C ANALYSI S (PARSING )
PARS E FORES TThere are man y algorithm s for parsing —recoverin g the phras e structur e of an utterance , give n a
grammar . In Figur e 22.7 , we give a very simpl e algorith m that nondeterministicall y choose s one
possibl e pars e tree, if one exists.  It treat s the list of word s as a pars e forest : an ordere d list of
parse trees . On each step throug h the main loop , it find s som e subsequenc e of element s in the
3 Th e term conjunctio n here mean s somethin g that joins two phrase s together . It additio n to and,  it include s but, since,
while,  or, and because.  Do not be confuse d by the fact that or is logicall y a disjunctio n but syntacticall y a conjunction .
4 A relativ e claus e follow s and modifie s a nou n phrase . It consist s of a relativ e pronou n (suc h as "who " or "that" )
followe d by a verb phras e (and sometime s a whol e sentence) . An exampl e of a relativ e claus e is that  gave  me the gold  in
"The agen t that gave  me the gold  is in 2,2."
Sectio n 22.4 . Syntacti c Analysi s (Parsing ) 665
Noun  — stench  \ breeze  \ glitter  \ nothing
| wumpus  | pit | pits  \ gold  \ east  \ ...
Verb  —­ is \ see  \ smell  \ shoot  \ feel  \ stinks
\ go | grab  | carry  \ kill  \ turn  \ ...
Adjective  —» right  \ left  \ east  \ south  \ back  \ smelly
Adverb  —* • here  \ there  \ nearby  \ ahead
right  | left  \ east  \ south  \ back  \ ...
Pronoun  —+ me \ you  \ I \ it \ ...
Name  —>• John  \ Mary  \ Boston  \ Aristotle  ...
Article  —> the \ a \ an \ ...
Preposition  —> to \ in \ on \ near  \ ...
Conjunction  —> and  \ or \ but  \ ...
Digit  ­>0|1|2|3|4|5|6|7|8| 9
Figur e 22.5 Th e lexico n for £o­
NP —
VP ­>
PP
RelClauseNPVP
S Conjunction  S
Pronoun
Noun
Article  Noun
Digit  Digit
NPPP
NP RelClause
Verb
VPNP
VP Adjective
VP PP
VP Adverb
Preposition  NP
that VPI + feel a breez e
I feel a breez e + and +1 smel l a wumpu s
I
pits
the + wumpu s
34
the wumpu s + to the east
the wumpu s + that is smell y
stink s
feel + a breez e
is + smell y
turn + to the east
go + ahea d
to + the east
that + is smell y
Figur e 22.6 Th e gramma r for £Q, with exampl e phrase s for each rule .
666 Chapte r 22 . Agent s that Communicat e
fores t that matc h the right­han d side of one of the gramma r rules . It then replace s the subsequenc e
with a singl e parse tree whos e categor y is the left­han d side of the rule, and whos e childre n are
the node s in the origina l subsequence . In Figur e 22.8 , we show a trace of the executio n in parsin g
the strin g "the wumpu s is dead. " Ever y choic e is a good one, so there is no backtracking .
There are man y possibl e parsin g algorithms . Som e operat e top­down , startin g with an S
and expandin g it accordin g to the gramma r rule s to matc h the word s in the string . Som e use
a combinatio n of top­dow n and bottom­up , and som e use dynami c programmin g technique s to
avoid the inefficiencie s of backtracking . We cove r one efficien t algorith m for parsin g context­fre e
grammar s in Sectio n 23.2 . But first , we will show how the parsin g proble m can be interprete d as
a logica l inferenc e problem , and thus can be handle d by genera l inferenc e algorithms .
functio n BOTTOM­UP­PARSE ( words , grammar)  return s a pars e tree
forest  — words
loop do
if LENGTH(/bretf ) = 1 and CATEGORY(fcraf [ 1 ]) = START(grammar ) then
retur n forest\  1)
else
;' — choos e from {1. . .LENGTH(/ore.5/) }
rule  <— choos e from RULES(grammar )
n — LENGTH(RUEE­RHS(rw/e) )
subsequence  <— SUBSEQUENCE(/c>re.sr , /', i+n­\)
if MATCH(subsequence,  RUEE­RHSOw/e) ) then
forest[i...i+n­l ] —
else fail
end
Figur e 22.7 Nondeterministi c bottom­u p parsin g algorith m for context­fre e grammars . It
picks one parse to return . Each node in a pars e tree has two fields : CATEGOR Y and CHILDREN .
forest
The wumpu s is dead
Article  wumpu s is dead
Article  Noun  is dead
NP is dead
NP Verb  dead
NP Verb  Adjective
NP VP Adjective
NPVP
Ssubsequence
The
wumpu s
Article  No/in
is
dead
Verb
VP Adjective
NPVP
Figur e 22.8 Trac e of BOTTOM­UP­PARS E on the strin grule
Article  — the
Noun  — wumpus
NP — Article  Noun
Verb — is
Adjective  — > dead
VP — Verb
VP — VP Adjective
S — NPVP
"The wumpu s is dead. "
Sectio n 22.5 . Definit e Claus e Gramma r (DCG ) 667
22.5 DEFINIT E CLAUS E GRAMMA R (DCG )
LOGI C GRAMMA R
DEFINIT E CLAUS E
GRAMMA R
DCGThere are two problem s with BNF . First , we are intereste d in usin g languag e for communication ,
so we need some way of associatin g a meanin g with each string , and BNF only talks abou t strings ,
not meanings . Second , we will wan t to describ e grammar s that are context­sensitive , wherea s
BNF is strictl y context­free . In this section , we introduc e a formalis m that can handl e both of
these problems .
The idea is to use the full powe r of first­orde r logic to talk abou t string s and their meanings .
Each nontermina l symbo l become s a one­plac e predicat e that is true of string s that are phrase s
of that category . For example , Noun("  stench")  is a true logica l sentence , wherea s Noun("the")  is
false. It is easy to writ e BNF rule s as logica l implicatio n sentence s in first­orde r logic :
BNF First­Orde r Logi c
S — NPVP  NP(si)  A VP(s 2) => S(Append(s ],s2))
Noun  —stenc h ...  (s  = "stench"  V ...) =>• Noun(s)
The first of these rule s says that if there is a strin g s\ that is a nou n phras e and a strin g s2 that is
a verb phrase , then the strin g forme d by appendin g them togethe r is a sentence . The secon d rule
says that if s is the strin g "stench " (or one of the othe r word s not shown) , then the strin g s is a
noun . A gramma r writte n with logica l sentence s is calle d a logic grammar . Sinc e unrestricte d
logica l inferenc e is computationall y expensive , mos t logic grammar s insis t on a restricte d format .
The mos t commo n is definit e claus e gramma r or DCG , in whic h ever y sentenc e mus t be a
definit e clause.5
The DCG  formalism  is attractiv e becaus e it allow s us to describ e grammar s in term s of
somethin g we understan d well : first­orde r logic . Unfortunately , it has the disadvantag e of bein g
more verbos e than BNF notation . We can hav e the best of both by definin g a specia l DCG
notation  that is an extensio n of BNF , but retain s the well­founde d DCG semantics . Fro m now
on, whe n we say "definit e claus e grammar, " we mea n a gramma r writte n in this specia l notation ,
whic h is define d as follows :
• The notation X — Y Z ... translate s as Y(s\)  A Z(s 2) A ...
• The notatio n X — word  translate s as X(["word"]).
• The notatio n X — Y ZX(Append(s\,  S2,.. .)•
... translate s as Y'(s)  V Z'(s)  V ... => X(s),  wher e Y' is the
translatio n into logi c of the DCG expressio n Y.
Thes e thre e rule s allo w us to translat e BNF into definit e clauses . We now see how to exten d the
notatio n to incorporat e grammar s that can not be expresse d in BNF .
• Nontermina l symbol s can be augmente d with extra arguments . In simpl e BNF notation , a
nontermina l such as NP gets translate d as a one­plac e predicate s wher e the singl e argumen t
represent s a string : NP(s).  In the augmente d DCG notation , we can writ e NP(sem),  for
example , to expres s "an NP with semantic s sem."  Thi s gets translate d into logi c as the
two­plac e predicat e NP(sem,  s).
5 A definit e claus e is a type of Horn claus e that , whe n writte n as an implication , has exactl y one atom in its consequent ,
and a conjunctio n of zero or mor e atom s in its antecedent , for example , ^41 A ^2 A ... => C\.
668 Chapte r 22 . Agent s that Communicat e
• A variabl e can appea r on the right­han d side of a DCG rule. The variabl e represent s a
singl e symbo l in the inpu t string , withou t sayin g wha t it is. We can use this to defin e the
nontermina l categor y Double  as the set of string s consistin g of a word repeate d twice . Her e
is the definitio n in both DCG notatio n and norma l first­orde r logic notation :
DCG ,  First­Orde r Logi c
Double  — w w (s\  = [w] A. $2 = [w])  =>• Double(Append(s\,S2))
• An arbitrar y logica l test can appea r on the right­han d side of a rule. Suc h tests are enclose d
in curl y brace s in DCG notation .
As an exampl e usin g all thre e extensions , here are rule s for Digit  and Number  in the gramma r
of arithmeti c (see Appendi x B). Th e nonterminal s take an extr a argumen t representin g thei r
semanti c interpretation :
DCG First­Orde r Logi c
Digit(sem)  — sent {0 < sem < 9} (s  = [sem\)  =>• Digit(sem,s)
Number(sem)  —^ Digit(sem)  Digit(sem,  s) =3­ Number(sem,  s)
Number(sem)  — Number(sem\)  Digit(semi)  Number(sem,s\ ) A Digit(sem,ST)
{sem=  10 x semi  + sem^}  f\sem=  10 x sem\  + semi  =>
Number(sem,Append(s\ , 53) )
The first rule can be read as "a phras e of categor y Digit  and semanti c valu e sem can be forme d
from a symbo l sem betwee n 0 and 9." The secon d rule says that a numbe r can consis t of a singl e
digit , and has the sam e semantic s as that digit . The third can be read as sayin g "a numbe r with
semanti c valu e sem can be forme d from a numbe r with semanti c valu e sem\  followe d by a digit
with semanti c valu e sem­i,  wher e sem=  10 x sem\  + sem^"
There are now five thing s that can appea r on the right­han d side of a rule in DCG nota ­
tion: an un­augmente d nontermina l (Digit),  an augmente d nontermina l (Digit(sem)),  a variabl e
representin g a termina l (sem),  a constan t termina l (word),  or a logica l test ({0 < sem < 9}).  The
left­han d side mus t consis t of a singl e nonterminal , with or withou t augmentation .
22.6 AUGMENTIN G A GRAMMA R
OVERGENERATE S
CASE S
AGREEMEN TThe simpl e gramma r for £$ generate s man y sentence s of English , but it also overgenerates —
generate s sentence s that are not grammatical—suc h as "Me smell s a stench. " Ther e are two
problem s with this string : it shoul d be "I," not "me, " and it shoul d be "smell, " not "smells. " To
fix thes e problems , we will first determin e wha t the fact s of Englis h are, and then see how we
can get the gramma r to reflec t the facts .
Many language s divid e thei r noun s into differen t cases , dependin g on thei r role in the
sentence . Thos e who have take n Lati n are familia r with this, but in Englis h there is no notio n
of case on regula r nouns , only on pronouns . We say that pronoun s like "I" are in the subjectiv e
(or nominative ) case , and pronoun s like "me " are in the objectiv e (or accusative ) case . Man y
language s hav e a dativ e case for word s in the indirec t objec t position . Man y language s also
requir e agreemen t betwee n the subjec t and mai n verb of a sentence . Here , too, the distinction s
Sectio n 22.6 . Augmentin g a Gramma r 669
are minima l in Englis h compare d to mos t languages ; all verb s excep t "be" have only two forms .
One form (e.g. , "smells" ) goes with third perso n singula r subject s such as "he" or "the wumpus. "
The othe r form (e.g. , "smell" ) goes with all othe r subjects , such as "I" or "you " or "wumpuses. "
Now we are read y to fix our grammar . We will conside r noun case s first . The proble m is
that with the distinction s we have,mad e so far (i.e., the nonterminal s we have defined) , Englis h
is not context­free . It is not the case that any NP can combin e with a VP to form a sentence , for
example , the NP "I" can, but "me" cannot . If we wan t to stick with a context­fre e gramma r we will
have to introduc e new categorie s such as NP S and NP 0, to stand for noun phrase s in the subjectiv e
and objectiv e case , respectively . We woul d also need to split the categor y Pronoun  into the two
categorie s Pronouns  (whic h include s "I") and Pronouno  (whic h include s "me") . The necessar y
change s to the gramma r are outline d in Figur e 22.9 . Notic e that all the NP rules are duplicated ,
S ­> NP S VP | ...
NPs  —» • Pronouns  \ Noun  \ Article  Noun
NPo — Pronouno  \ Noun  \ Article  Noun
VP — VPNPo
PP — Preposition  NP O
Pronouns  — I \ you  \ he \ she
Pronouno  — me \ you  \ him  \ her
Figur e 22.9 Th e change s neede d in £o to handl e subjectiv e and objectiv e cases .
once for NPs  and once for NPo­  It woul d not be too bad if the numbe r of rules double d once ,
but it woul d doubl e agai n whe n we change d the gramma r to accoun t for subject/ver b agreement ,
and agai n for the next distinction . Thus , the size of the gramma r can grow exponentiall y with the
numbe r of distinction s we need to make .
AUGMEN T A n alternativ e approac h is to augmen t the existin g rule s of the gramma r instea d of intro ­
ducin g new rules . The resul t is a concise , compac t grammar . We start by parameterizin g the
categorie s NP and Pronoun  so that they take a paramete r indicatin g thei r case . In the rule for 5,
the NP mus t be in the subjectiv e case , wherea s in the rule s for VP and PP, the NP mus t be in
the objectiv e case . The rule for NP take s a variabl e as its argument . Thi s use of a variable —
avoidin g a decisio n wher e the distinctio n is not important—i s wha t keep s the size of the rule set
manageable . Figur e 22.1 0 show s the augmente d grammar , whic h define s a languag e we call £\.
The proble m of subject/ver b agreemen t coul d also be handle d with augmentations , but we
delay showin g this unti l the next chapter . Instead , we addres s a slightl y harde r problem : ver b
subcategorization .
Verb Subcategorizatio n
The £\ languag e is an improvemen t over £Q, but it still allow s ungrammatica l sentences . One
proble m is in the way verb phrase s are put together . We wan t to accep t verb phrase s like "giv e
me the gold " and "go to 1,2." All thes e are in £\, but unfortunatel y so are "go me the gold "
670 Chapte r 22 . Agent s that Communicat e
S
NP(case)
VP
PP
Pronoun(Subjective)
Pmnoun(Objective)— NP(Subjective)  VP
Pronoun(case)
VP NP(Objective)
Preposition  NP(Objective)
I | you  | he \ she
me | you  \ him  \ her  \Noun  I Article  Noun
Figur e 22.1 0 Th e gramma r of £\ usin g augmentation s to represen t noun cases .
SUBCATEGORIZATIO N
COMPLEMENT S
SUBCATEGORIZATIO N
LISTand "giv e to 1,2." To eliminat e these , the gramma r mus t state whic h verb s can be followe d by
whic h othe r categories . We call this the subcategorizatio n informatio n for the verb . Eac h verb
has a list of complements —obligator y phrase s that follo w the verb withi n the verb phrase . So in
"Give the gold to me," the NP "the gold " and the PP "to me" are complement s of "give."6
A subcategorizatio n list is a list of complemen t categorie s that the verb accepts , so "give "
has the subcategorizatio n list [NP,  PP] in this example . It is possibl e for a verb to have mor e
than one subcategorizatio n list, just as it is possibl e for a word to have mor e than one category ,
or for a pronou n to have mor e than one case.7 In fact, "give " also has the subcategorizatio n list
[NP, NP],  as in "Giv e me the gold. " We can treat this like any othe r kind of ambiguity . It is also
importan t to kno w wha t subcategorizatio n lists a verb does not take . The fact that "give " does
not take the list [PP]  mean s that "giv e to me" is not by itsel f a valid verb phrase . Figur e 22.1 1
gives som e example s of verb s and their subcategorizatio n lists , or subcat s for short .
Verb
give
smell
is
died
believ eSubcat s
[NP, PP]
[NP,NP]
[NP]
[Adjective]
[PP]
[Adjective]
[PP]
[NP)
[]
[S]Exampl e Verb Phras e
give the gold in 3 3 to me
give me the gold
smell a wumpu s
smell awfu l
smel l like a wumpu s
is smell y
is in 2 2
is a pit
died
believ e the smell y wumpu s in 2 2 is dead
Figur e 22.11 Example s of verbs  with thei r subcategorizatio n frames .
6 Thi s is one definitio n of complement,  but othe r author s have differen t terminology . Som e say that the subjec t of the
verb is also a complement . Other s say that only the prepositiona l phras e is a complement , and the noun phras e shoul d be
calle d an argument .
7 Fo r example , "you " can be used in eithe r subjectiv e or objectiv e case .
Sectio n 22.6 . Augmentin g a Gramma r 671
ADJUNCT STo integrat e verb subcategorizatio n into the grammar , we do thre e things . The first step is
to augmen t the categor y VP to take a subcategorizatio n argumen t that indicate s the complement s
that are neede d to form a complet e VP. For example , "give " can be mad e into a complet e VP by
addin g [NP,PP],  "giv e the gold " can be mad e complet e by addin g [PP],  and "giv e the gold to
me" is alread y complete ; its subcategorizatio n list is []. Tha t give s us these rules :
VP(subcat)  —  VP([\'P\subcat])  NP(Objective)
VP([Adjective\subcai])  Adjective
VP([PP\subcat])  PP
Verb(subcat)
The first line can be read as "A VP with a give n subca t list, subcat,  can be forme d by a VP
followe d by a NP in the objectiv e case , as long as that VP has a subca t list that start s with the
symbo l NP and is followe d by the element s of the list subcatr
The secon d step is to chang e the rule for S to say that it require s a verb phras e that has all
its complements , and thus has a subca t list of []. This mean s that "He died" is a legal sentence ,
but "You give " is not. The new rule ,
5 —  NP(Subjective)  VP([])
can be read as "A sentenc e can be compose d of a NP in the subjectiv e case , followe d by a VP
whic h has a null subca t list." Figur e 22.1 2 show s a pars e tree usin g this grammar .
The thir d step is to remembe r that in additio n to complements , verb phrase s (and othe r
phrases ) can also take adjuncts , whic h are phrase s that are not license d by the individua l verb but
rathe r may appea r in any verb phrase . Phrase s representin g time and plac e are adjuncts , becaus e
almos t any actio n or even t can have a time or place . For example , the adver b "now " in "I smel l
a wumpu s now " and the PP "on Tuesday " in "giv e me the gold on Tuesday " are adjuncts . Her e
are two rules to allow adjuncts :
VP(subcat)  — VP(subcat)PP
| VP(subcat)  Adverb
Notic e that we now have two rule s with VP PP on the right­han d side , one as an adjunc t and
one as a complement . Thi s can lead to ambiguities . For example , "I walke d Sunday " is usuall y
interprete d as an intransitiv e VP followe d by a time adjunc t meanin g "I move d mysel f with my
feet on Sunday. " But if Sunda y is the nam e of a dog, then Sunda y is an NP complement , and the
meanin g is that I took the dog for a walk at som e unspecifie d time .
RULE SCHEM AGenerativ e Capacit y of Augmente d Grammar s
The generativ e capacit y of augmente d grammar s depend s on the numbe r of value s for the
augmentations . If ther e is a finit e number , then the augmente d gramma r is equivalen t to a
context­fre e grammar . To see this, conside r each augmente d rule as a rule schema , whic h stand s
for a set of rules , one for each possibl e combinatio n of value s forth e augmente d constituents . If we
replac e each rule schem a with the complet e set of rules , we end up with a finit e (althoug h perhap s
exponentiall y large ) set of context­fre e rules . But in the genera l case , augmente d grammar s go
672 Chapte r 22 . Agent s that Communicat e
VP(J])
VP([NP])^~^
VP([NP,NP])  NP  NP
Pronoun  Verb([NP,NP])  Pronoun  Article  Noun
You giv e m e th e gol d
Figur e 22.12 Pars e tree for "You give me the gold " showin g subcategorizatio n of the verb and
verb phrase .
beyon d context­free . For example , the context­sensitiv e languag e anbnc" can be represente d with
the followin g augmente d gramma r (wher e e represent s the empt y string) :
S(n) — A(n)  B(n)  C(n)
A(0)^ e A(n+l)­>aA(n )
5(0) — e B(n+l)—bB(n)
—e C(«+l ) — c C(n)
22.7 SEMANTI C INTERPRETATIO N
COMPOSITIONA L
SEMANTIC SThroughou t Part III, we gaine d experienc e in translatin g betwee n Englis h and first­orde r logic .
There it was done informally , to get a feelin g for wha t statement s of first­orde r logic mean . Her e
we will do the sam e thin g in a mor e carefull y controlle d way , to defin e wha t statement s of Si
mean . Late r on we will investigat e the meanin g of othe r type s of speec h acts beside s sentences .
Befor e movin g on to natura l languages , we will conside r the semantic s of forma l languages .
It is easy to deal with meaning s of expression s like "X + K" in arithmetic  or X A Y in logic becaus e
they have a compositiona l semantics . This mean s that the semantic s of any phras e is a functio n
of the semantic s of its subphrases ; it does not depen d on any othe r phras e before , after , or
encompassin g the give n phrase . So if we kno w the meanin g of X and Y (and +), then we know
the meanin g of the whol e phrase .
Thing s get more complicate d whe n we go beyon d the simpl e languag e of arithmetic . For
example , the programmin g languag e expressio n "10+10 " migh t have the semanti c interpretatio n
4 whe n it appear s in the large r strin g "BASE(2) ; x <— 10+10. " However , we can still salvag e a
compositiona l semantic s out of this if we say that the semanti c functio n associate d with "x+y"  is
to add x and y in the curren t base . The grea t advantag e of compositiona l semantic s is the same as
the advantag e of context­fre e grammars : it lets us handl e an infinit e gramma r with a finit e (and
often small ) set of rules .
Sectio n 22.7 . Semanti c Interpretatio n 673
At first glance , natura l language s appea r to have a noncompositiona l semantics . In "The
batte r hit the ball," we expec t the semanti c interpretatio n of "batter " to be one who  swings a  bat
and of "ball " to be spherical sporting equipment.  But in "The chef mixe d the batte r to be serve d
at the ball, " we expec t the two word s to have differen t meanings . This suggest s that the meanin g
of "batter " and "ball " depend s noncompositionall y on the surroundin g context . However , these
semanti c interpretation s are only the expected , preferre d ones , not the only possibl e ones . It
is possible  that "The batte r hit the ball" refer s to a cake mixtur e makin g a gran d appearanc e at
a forma l dance . If you wor k hard enough , you can inven t a stor y wher e that is the preferre d
reading . In short , semantic  interpretation alone cannot be  certain  of the right  interpretation
of a phrase  or sentence.  So we divid e the work—semanti c interpretatio n is responsibl e for
combinin g meaning s compositionall y to get a set of possibl e interpretations , and disambiguatio n
is responsibl e for choosin g the best one.
There are othe r construction s in natura l languag e that pose problem s for the composi ­
tiona l approach . The proble m of quantifie r scop e (pag e 678) is one example . But overall , the
compositiona l approac h is the one most favore d by moder n natura l languag e systems .
Semantic s as DCG Augmentation s
On page 668 we saw how augmentation s coul d be used to specif y the semantic s of number s and
digits . In fact, it is not difficul t to use the same idea to specif y the semantic s of the complet e
languag e of arithmetic , as we do in Figur e 22.13 . Figur e 22.14 show s the parse tree for 3 + (4 ­=­ 2)
accordin g to this grammar , with the semanti c augmentations . The strin g is analyze d as Exp(5),
an expressio n whos e semanti c interpretatio n is 5.
Exp(sem)  — Exp(sem\)  Operator(op)  Exp(sem 2) {sem  = Apply(op,sem\,sem2)}
Exp(sem)  —> ( Exp(sem)  )
Exp(sem)  —* Number(sem)
Digit(sem)  —> sem {0 < sem < 9}
Number(sem)  —> • Digit(sem)
Number(sem)  —> Number(sem\)  Digit(sem2)  {sem=  10 x sem\
Operator(sem)  —> sem {sem  G {+, —,­;­ , x }}
Figur e 22.1 3 A  gramma r for arithmeti c expressions , with semantics .
The semantic s of "Joh n love s Mary "
We are now read y to writ e a gramma r with semantic s for a very smal l subse t of English . As
usual , the first step is to determin e wha t the facts are—wha t semanti c representation s we wan t
to associat e with wha t phrases . We will look at the simpl e sentenc e "Joh n love s Mary " and
associat e with it the semanti c interpretatio n Loves(John,Mary).  It is trivia l to see whic h part s
of the semanti c interpretatio n com e from whic h word s in the sentence . The complicate d part is
674 Chapte r 22 . Agent s that Communicat e
Exp(5)
Digit(4)  Operator^)  Digit(2)
Figur e 22.1 4 Pars e tree with semanti c interpretation s for the strin g "3 + (4 ­H 2)"
decidin g how the parts fit together , particularl y for intermediat e phrase s such as the VP "love s
Mary. " Not e that the semanti c interpretatio n of this phras e is neithe r a logica l term nor a
complet e logica l sentence . Intuitively , "love s Mary " is a descriptio n that may or may not appl y to
a particula r perso n (in this case , it applie s to John) . This mean s that "love s Mary " is a predicat e
that, whe n combine d with a term that represent s a perso n (the perso n doin g the loving) , yield s a
complet e logica l sentence . Usin g the A­notatio n (see page 195) , we can represen t "love s Mary "
as the predicat e
\x Loves(x,  Mary)
The NP "Mary " can be represente d by the logica l constan t Mary.  Tha t sets us up to defin e a rule
that says "an NP with semantic s obj followe d by a VP with semantic s rel yield s a sentenc e whos e
semantic s is the resul t of applyin g the relatio n rel to the objec t obj:"
S(rel(obj))  — NP(obj)  VP(rel)
The rule tells us that the semanti c interpretatio n of "Joh n love s Mary " is
(\x Loves(x,  Mary))(John)
whic h is equivalen t to Loves(John,  Mary).
The rest of the semantic s follow s in a straightforwar d way from the choice s we have mad e
so far. Becaus e VPs  are represente d as predicates , it is a good idea to be consisten t and represen t
verbs as predicate s as well . The verb "loves " is represente d as \y \x Loves(x,y),  the predicat e
that, whe n give n an argumen t such as Mary,  return s the predicat e \x Loves(x,Mary).
The VP  —> Verb  NP rule applie s the predicat e that is the semanti c interpretatio n of the
verb to the objec t that is the semanti c interpretatio n of the NP to get the semanti c interpretatio n
of the whol e VP. We end up with the gramma r show n in Figur e 22.1 5 and the pars e tree show n
in Figur e 22.16 .
Sectio n 22.7 . Semanti c Interpretatio n 675
S(rel(obj))  — NP(obj)  VP(rel)
VP(rd(obj})  — Verb(rel)  NP(obj)
NP(obj)  — Name(obj)
Name(John)  — John
Name(Mary)  —* • Mary
Verb(\x  \y Loves(x,  y)) —• loves
Figur e 22.1 5 A  gramma r that can deriv e a pars e tree and semanti c interpretatio n for "Joh n
loves Mary " (and three othe r sentences) .
S(Loves(John,Mary))
es(x,Mary)
/~
NP(John) /  NP(Mary)
Name(John)  Verb(hyXx  Loves(x,y))  Name(Mary)
John love s Mar y
Figur e 22.16 A  pars e tree with semanti c interpretation s for the strin g "Joh n love s Mary "
The semantic s of £\
We had no proble m with "Joh n love s Mary, " but thing s get more complicate d whe n we conside r all
of £\. Immediatel y we are faced with all the choice s of Chapte r 8 for our semanti c representation ;
for example , how do we represen t time , events , and substances ? Our first choic e will be to use
the even t calculu s notatio n of Sectio n 8.4. In this notation , the sentenc e "Ever y agen t smell s a
wumpus " can be expresse d as:
Va Agent(a)  => 3w  Wumpus(w)  A 3 e e £ Perceive(a,  w,Nose)  A During(Now,  e)
We coul d have used a Smell  predicat e instea d of Perceive,  but we wante d to be able to emphasiz e
the similaritie s betwee n smelling , hearing , feeling , touching , and seeing .
Our task is to build up our desire d representatio n from the constituent s of the sentence . We
first brea k the sentenc e into NP and VP phrases , to whic h we can assig n the followin g semantics :
Every agen t NP(Va  Agent(a)  =3­ ...)
smell s a wumpu s VP(3  w Wumpus(w)  A
Be (e e Perceive(...  , w,Nose)  ADuring(Now,e) )
Righ t away there are two problems . First , the semantic s of the entir e sentenc e appear s to be the
semantic s of the NP with the semantic s of the VP fillin g in the ... part. Tha t mean s that we canno t
form the semantic s of the sentenc e with rel(obj).  We coul d do it with obj(rel),  whic h seem s a
676 Chapte r 22 . Agent s that Communicat e
INTERMEDIAT E FOR M
QUASI­LOGICA L
FORM
QUANTIFIE D TERMlittle odd (at leas t at first glance) . The secon d proble m is that we need to get the variabl e a as
an argumen t to the relatio n Perceive.  In othe r words , the semantic s of the sentenc e is forme d by
pluggin g the semantic s of the VP into the right argumen t slot of the NP, whil e also pluggin g the
variabl e a from the NP into the right argumen t slot of the semantic s of the VP. It look s as if we
need two functiona l compositions , and promise s to be rathe r confusing . The complexit y stem s
from the fact that the semanti c structur e is very differen t from the syntacti c structure .
To avoi d this confusion , man y moder n grammar s take a differen t tack . The y defin e an
intermediat e form to mediat e betwee n synta x and semantics . The intermediat e form has two
key properties . First , it is structurall y simila r to the synta x of the sentence , and thus can be easil y
constructe d throug h compositiona l means . Second , it contain s enoug h informatio n so that it can
be translate d into a regula r first­orde r logica l sentence . Becaus e it sits betwee n the syntacti c
and logica l forms , it is sometime s calle d a quasi­logica l form.8 In this chapter , we will use a
quasi­logica l form that include s all of first­orde r logic and is augmente d by lambd a expression s
and one new construction , whic h we will call a quantifie d term . The quantifie d term that is the
semanti c interpretatio n of "ever y agent " is writte n
[V a Agent(a)]
This look s like a logica l sentence , but it is used in the same way that a logica l term is used .
In the followin g example , we see quantifie d term s as argument s to the relatio n Perceive  in the
interpretatio n of "Ever y agen t smell s a wumpus" :
3 e (e 6 Perceive^  a Agent(a)],  [3 w Wumpus(w)],  Nose)  A During(Now,  e))
We will writ e our gramma r so that it generate s this quasi­logica l form . In Sectio n 22.7 we will
see how to translat e this into regula r first­orde r logic .
It can be difficul t to writ e a comple x gramma r that alway s come s up with the righ t se­
manti c interpretation , and everyon e has their own way of attackin g the problem . We sugges t a
methodolog y base d on these steps :
1. Decid e on the logica l or quasi­logica l form you wan t to generate . Writ e dow n som e
exampl e sentence s and their correspondin g logica l forms . One  such exampl e sentenc e is
at the top of Figur e 22.17 .
2. Mak e one­word­at­a­tim e modification s to your exampl e sentences , and stud y the corre ­
spondin g logica l forms . For example , the semantic s of "Ever y agen t smelle d a wum ­
pus" is the same as our exampl e sentence , excep t that During(Now,  e) is replace d with
After(Now,  e). Thi s suggest s that During  is part of the semantic s of "smells " and After  is
part of the semantic s of "smelled. " Similarly , changin g the wor d "every " to "an" migh t
resul t in a chang e of V to 3 in the logica l form . This give s you a hint abou t the semanti c
interpretatio n of "an" and "every. "
3. Eventuall y you shoul d be able to write dow n the basic logica l type of each lexica l categor y
(noun , verb , and so on), alon g with som e word/logica l form pairs . Thi s is motivate d in
part by exampl e sentence s and in part by you r intuitions . For example , it seem s clear
enoug h that the pronou n "I" shoul d denot e the objec t Speaker  (whic h happen s to be a
fluent , dependen t on the situation) . Onc e we decid e that one word in a categor y is of a
8 Som e quasi­logica l form s have the third propert y that they can succinctl y represen t ambiguitie s that coul d only be
represente d in logica l form by a long disjunction .
Sectio n 22.7 . Semanti c Interpretatio n 67 7
certai n semanti c type , then we know that everythin g in the categor y is of the same type .
Otherwise , the compositionalit y woul d not work out right . See the middl e of Figur e 22.1 7
for type s and example s of all the lexica l categories .
4. Now conside r phrase­at­a­tim e modification s to your exampl e sentence s (e.g. , substitutin g
"ever y stinkin g wumpus " for "I"). You shoul d be able to determin e example s and type s for
constituen t phrases , as in the botto m of Figur e 22.17 . In Figur e 22.18 , we see the complet e
parse tree for a sentence .
5. Onc e you know the type of each category , it is not too hard to attac h semanti c interpretatio n
augmentation s to the gramma r rules . Som e of the rules have only one right­han d side
constituen t and only need to copy up the semantic s of that constituent :
NP(sem)  —> Pronoun(sem)
6. Othe r times , the right­han d side of a rule will contai n a semanti c interpretatio n that is a
predicat e (or function) , and one or more that are objects . To get the semantic s of the whol e
phrase , just appl y the relatio n (or function ) to the object(s) :
S(rel(obj))  ­+ NP(obj)  VP(rel)
1. Sometime s the semantic s is buil t up by concatenatin g the semantics  of the constituents ,
possibl y with som e connector s wrappe d aroun d them :
NP([sem\,seni2\)  —> Digit(sem\)  Digit(semi)
8. Finally , sometime s you need to take apar t one of the constituent s befor e puttin g the
semantic s of the whol e phras e back together . Here is a comple x example :
VP(Xx  reli(x)  A AE/ 2(EVENT­VAR(rc?/i)) ) ­^ VP(rel\)  Adverb(rel 2)
The inten t here is that the functio n EVENT­VA R pick s out the even t variabl e from the
intermediat e form expressio n rel\.  The end resul t is that a verb phras e such as "saw me
yesterday " gets the interpretation :
\x 3 e e£  Sees(x,  Speaker)  A After(Now,  e) A During(e,  Yesterday)
By followin g these steps , we arriv e at the gramma r in Figur e 22.19 . To actuall y use this grammar ,
we woul d augmen t it furthe r with the case and subcategorizatio n informatio n that we worke d out
previously . Ther e is no difficult y in combinin g such thing s with semantics , but the gramma r is
easie r to understan d when we look at one type of augmentatio n at a time .
Convertin g quasi­logica l form to logica l form
The fina l step of semanti c interpretatio n is to conver t the quasi­logica l form into real first­orde r
logic . For our quasi­logica l form , that mean s turnin g quantifie d term s into real terms . Thi s is
done by a simpl e rule : For each quantifie d term [qx  P(x)]  withi n a quasi­logica l form QLF,
replac e the quantifie d term with x, and replac e QLF  with qx P(x)  op QLF,  wher e op is =>
when q is V, and is A whe n q is 3 or 3!. For example , the sentenc e "Ever y dog has a day" has the
quasi­logica l form :
3e e£  Has([V  d Dog(d)},  [3 a Day(a)],  Now)
678 Chapte r 22. Agent s that Communicat e
Categor y
S
Adjective
Adverb
Article
Conjunction
Digit
Noun
Preposition
Pronoun
Verb
NP
PP
RelClause
VPType
Sentenc e
object  — ­> sentence
event  — > sentence
Quantifie r
sentence2 — <• sentence
Numbe r
object  — > sentence
object2 — sentence
Objec t
object"  — * sentence
Objec t
object  —  <• sentence
object  — * sentence
object11 — > sentenceExampl e
I sleep .
smell y
today
the
and
7
wumpu s
in
I
eats
a dog
in [2,2]
that sees me
sees meQuas i ­Logica l Form
3 e e£  (Sleep,  Speaker)
A During(Now,  e)
\x Smelly(x)
Xe During(e,  Today)
3!
Xp,q(pf \ q)
1
\x Wumpus(x)
\x \y In(x,  y)
Speaker
Xy Xx 3 e e£ Eats(x,  y)
A During(Now,  e)
[3dDog(d)]
\x In(x,  [2, 2])
Xx 3 e e £ Sees(x,  Speaker)
A During(Now,  e)
Xx 3 e e G Sees(x,  Speaker)
A During(Now,  e)
Figur e 22.1 7 Tabl e showin g the type of quasi­logica l form expressio n for each syntacti c
category . The notatio n t — * r denote s a functio n that take s an argumen t of type t and return s a
resul t of type r.
We did not specif y whic h of the two quantifie d term s gets pulle d out first , so there are actuall y
two possibl e interpretations :
Md Dog(d)  => 3 a Day(a)f\3e  e£Has(d,a,Now)
3a Day(a)/\Md  Dog(d)  => 3e  e&Has(d,a,Now)
The first one says that each dog has his own day, whil e the secon d says there is a specia l day that
all dogs share . Choosin g betwee n them  is a job for disambiguation . Ofte n the left­to­righ t order
of the quantifie d term s matche s the left­to­righ t orde r of the quantifiers , but other factor s com e
into play. The advantag e of quasi­logica l form is that it succinctl y represent s all the possibilities .
The disadvantag e is that it doesn' t help you choos e betwee n them ; for that we need the full powe r
of disambiguatio n usin g all source s of evidence .
Pragmati c Interpretatio n
We have show n how an agen t can perceiv e a strin g of word s and use a gramma r to deriv e a set of
possibl e semanti c interpretations.  Now we addres s the proble m of completin g the interpretatio n
by addin g informatio n abou t the curren t situation , informatio n that is noncompositiona l and
context­dependent .
Sectio n 22.7 . Semanti c Interpretatio n 679
S( 3e e6 Perceive([Va  Agent(a)],  [3w  Wumpus(w)],  Nose))
r\During(Now,e)
VP( hx 3e e£ Perceive(x,  [3w  Wumpus(w)],  Nose))
/\During(Now,e)
NP([Ma  Agent(a)]) NP([3w  Wumpus(w)])
Article(^i)  Noun(Agent)Verb(hy\  x 3e ee Perceive(x,  y, Nose))
f\During(Now,e)
Ever y agen t smell sArticle(3)  Noun(Wumpus)
wumpu s
Figur e 22.1 8 Pars e tree for the sentenc e "Ever y agen t smell s a wumpus, " showin g both
syntacti c structur e and semanti c interpretations .
INDEXICAL S Th e mos t obviou s need for pragmati c informatio n is in resolvin g the meanin g of indexicals ,
whic h are phrase s that refe r directl y to the curren t situation . For example , in the sentenc e "I
am in Bosto n today, " the interpretatio n of the indexical s "I" and "today " depen d on who uttere d
the sentenc e when . We represen t indexical s by Skole m constant s (suc h as Speaker),  whic h are
interprete d as fluents . Th e heare r who perceive s a speec h act shoul d also perceiv e who the
speake r is, and use this informatio n to resolv e the indexical . For example , the heare r migh t know
T((Speaker  = Agents),  Now).
ANAPHOR A Anothe r importan t concer n is anaphora , the occurrenc e of phrase s referrin g to object s that
have been mentione d previously . Conside r the passage :
"John was hungry . He entere d a restaurant. "
To understan d that "he" in the secon d sentenc e refer s to John , we need to have processe d the
first sentenc e and used it as part of the situationa l knowledg e in interpretin g the secon d sentence .
Anaphori c referenc e can also be mad e wit h definit e nou n phrase s like "the man. " In fact ,
the patter n of referenc e can be rathe r complicated , requirin g a thoroug h understandin g of the
discourse . Conside r the followin g sentence :
"Afte r John propose d to Marsha , they foun d a preache r and got married . For the
honeymoon , they wen t to Hawaii. "
Here the definit e nou n phras e "the honeymoon " refer s to somethin g that was only implicitl y
allude d to by the verb "married. " The pronou n "they " refer s to a grou p that was not explicitl y
mentione d before : John and Marsh a (but not the preacher) .
Whe n pronoun s are used to refer to thing s withi n the sam e sentence , we at least get som e
help from syntax . For example , in "He saw him in the mirror " the two pronoun s mus t refe r to
680 Chapte r 22 . Agent s that Communicat e
S(rel(obj))  ­> NP(obj)  VP(rel)
S(conj(semi,  sem 2)) — > S(sem\)  Conjunction(conj)  S(sem 2)
NP(sem)  — ^ Pronoun(sem)
NP(sem)  — > Name(sem)
NP([q  x sem(x)])  ­^ Article(q)  Noun(sem)
NP([qx  obj A rel(x)])  ­>• NP([qx  obj])  PP(rel)
NP([qx  obj A rel(x)})  ­> NP([qx  obj})  RelClause(rel)
NP([sem\,sem 2}) —>• Digit(sem\)  Digit(sem 2)
/* VP rules  for subcategorization:  * I
VP(sem)  —> Verb(sem)
VP(rel(obj))  — VP(rel)  NP(obj)
VP(sem\(sem2)  ­^ VP(sem\)  Adjective(sem2)
VP(semi(sem 2)) — VP(sem l) PP(sem 2)
I * VP rules  for adjuncts:  * I
VP(\x  sem\(x)  A sem 2 (EVENT ­ V/&(sem\)))  —> VP(sem\)  PP(sem 2)
VP(\x  sem\(x)  A seni2(EVEN'T­VA.R(sem\)))  — VP(sem\  ) Adverb(sem 2)
RelClause(sem)  — that  VP(sem)
PP(Xx  rel(x,  obj))  —  > Preposition(rel)  NP(obj)
Figur e 22.1 9 A  gramma r for £2 with semantics .
differen t people , wherea s in "He saw himself, " they refer to the same person . But mos t of the
time, there are no stric t rules on on anaphori c reference . So decidin g whic h referenc e is the right
one is a part of disambiguation , althoug h the disambiguatio n is certainl y guide d by pragmati c
(i.e., context­dependent ) information .
22.8 AMBIGUIT Y AND DISAMBIGUATIO N
In the idea l communicativ e exchange , the speake r has a propositio n P in min d and perform s a
speec h act that may have severa l interpretations , but whic h in the curren t situatio n can best be
interprete d as communicatin g P. The heare r realize s this, and so arrive s at P as the interpretation .
We say that the heare r has disambiguate d or resolve d the ambiguity . Occasionally , the heare r
may be confuse d and need to ask for clarification , but it woul d be tiresom e if this happene d too
often , or if the heare r aske d for clarificatio n on the clarification . Unfortunately , ther e are man y
ways in whic h communicatio n can brea k down . A speake r who does not spea k loudl y enoug h
Sectio n 22.8 . Ambiguit y and Disambiguatio n 681
LEXICA L AMBIGUIT Y
SYNTACTI C
AMBIGUIT Y
SEMANTI C
AMBIGUIT Y
REFERENTIA L
AMBIGUIT Y
PRAGMATI C
AMBIGUIT Y
LOCA L AMBIGUIT Y
VAGU Ewill not be heard.  But the biggest  problem  is that most  utterances  are ambiguous.  Her e are som e
example s take n from newspape r headlines :
Squa d help s dog bite victim .
Red­ho t star to wed astronomer .
Helicopte r powere d by huma n flies .
Once­saggin g cloth diape r industr y save d by full dumps .
and the Worl d War II favorite :
America n pushe s bottl e up Germans .
The simples t type of ambiguit y is lexica l ambiguity , wher e a word has more than one meaning .
For example , the adjectiv e "hot" can mean warm or spicy or electrifie d or radioactiv e or vehemen t
or sexy or popula r or stolen . Lexica l ambiguit y can cut acros s categories : "back " is an adver b in
"go back, " an adjectiv e in "bac k door, " a nou n in "the back of the room, " and a verb in "bac k up
your files. "
Syntacti c ambiguit y (also know n as structura l ambiguity ) can occu r with or withou t
lexica l ambiguity . For example , the strin g "I smelle d a wumpu s in 2,2" has two parses : one
wher e the propositiona l phras e modifie s the noun , and one wher e it modifie s the verb . The
syntacti c ambiguit y leads to a semanti c ambiguity , becaus e one parse mean s the wumpu s is in
2,2 and the othe r mean s that a stenc h is in 2,2. In this case, gettin g the wron g interpretatio n coul d
be a deadl y mistake . The lexica l ambiguitie s of the previou s paragrap h also lead to semanti c
ambiguities . On the othe r hand , semanti c ambiguit y can occu r even  in phrase s with no lexica l
or syntacti c ambiguity . For example , the nou n phras e "cat person " can be someon e who likes
feline s or the lead of the movi e Attack  of the Cat People.  A "coas t road " can be a road that
follow s the coast , or a road that leads to the coast .
One pervasiv e form of semanti c ambiguit y is referentia l ambiguity . Anaphori c expres ­
sions such as "it" can refe r to almos t anything . Referentia l ambiguit y occur s becaus e natura l
language s consis t almos t entirel y of word s for categories , not for individua l objects . Ther e is no
word for the­apple­I­had­for­lunch­today,]\\si  categorie s like apple.
One type of pragmati c ambiguit y occur s whe n the speake r and heare r disagre e on wha t
the curren t situatio n is. If the speake r says "I'll mee t you next Friday " thinkin g that they'r e
talkin g abou t the 17th , and the heare r think s that they are talkin g abou t the 24th , then ther e is
miscommunication . The exampl e on page 659 abou t "Now! " also involve s pragmati c ambiguity .
Sometime s a phras e or sentenc e has loca l ambiguity , wher e a substrin g can be parse d
severa l ways , but only one of thos e way s fits into the large r contex t of the whol e string . For
example , in the C programmin g language , the strin g *c mean s "pointe r to c" whe n it appear s in
the declaratio n cha r *c; but it mean s "multipl y by c" whe n it appear s in the expressio n 2 *c.
In English , "the radio broadcasts " is a noun phras e in "the radio broadcast s inform " and a noun
phras e followe d by a verb in "the radi o broadcast s information. " It is possibl e for a phras e or
sentenc e to be syntacticall y ambiguou s but semanticall y unambiguous . For example , "S\ and £2
and ^3" has two differen t parses . But they both have the same meanin g (at least in £2),  becaus e
conjunctio n is associative .
Natura l language s are also vague . Whe n we say, "It's hot outside, " it says somethin g abou t
the temperature , but it is open to a wide rang e of interpretatio n becaus e "hot " is a vagu e term . To
some it migh t mea n the temperatur e is abov e 75°F , and to other s it migh t mea n 90°F .
682 Chapte r 22 . Agent s that Communicat e
Finally , there can be ambiguit y abou t wha t speec h act has been performed . A heare r who
says, "yes " whe n asked , "Do you know wha t time it is?" has successfull y interprete d the sentenc e
as if it were question , but mos t likel y it was actuall y intende d as a reques t for information .
Disambiguatio n
As we said before , disambiguation  is a question  of diagnosis.  The heare r maintain s a mode l of
the worl d and, upon hearin g a new speec h act, adds the possibl e interpretation s of the speec h act
to the mode l as hypotheses . The uncertai n reasonin g technique s of Part V can then be used to
decid e whic h interpretatio n is best . To do this well , the mode l mus t includ e a lot of informatio n
abou t the world . For example , to correctl y resolv e the syntacti c ambiguit y in "Chri s saw the
Gran d Canyo n flyin g to New York, " one need s to know that it is mor e likel y that Chri s is doin g
the flyin g than that the Gran d Canyo n is. Similarly , to understan d "Donal d keep s his mone y in the
bank, " it help s to know that mone y is kept in saving s institution s more ofte n than in snowbanks .
One also need s a good mode l of the belief s of speake r and hearer , in orde r to decid e
what the speake r will bothe r to say. For example , the norma l interpretatio n of the statemen t "I
am not a crook " is that the speake r is not a criminal . Thi s is true even thoug h an alternativ e
interpretation—tha t the speake r is not a hooke d shepherd' s staff—ha s a highe r probabilit y of
being true . Similarly , "Howar d doesn' t keep his mone y in the bank " probabl y refer s to savin g
institutions , becaus e it woul d not be wort h remarkin g that he did not keep his mone y in a
snowbank . In general , disambiguatio n require s the combinatio n of four models :
1. The worl d model : the probabilit y that a fact occur s in the world .
2. The menta l model : the probabilit y that the speake r form s the intentio n of communicatin g
this fact to the hearer , give n that it occurs.9 (Thi s combine s model s of wha t the speake r
believes , wha t the speake r believe s the heare r believes , and so on.)
3. The languag e model : the probabilit y that a certai n strin g of word s will be chosen , give n
that the speake r has the intentio n of communicatin g a certai n fact.
4. The acousti c model : the probabilit y that a particula r sequenc e of sound s will be generated ,
given that the speake r has chose n a give n strin g of words . This will be take n up whe n we
conside r perceptio n in Chapte r 24.
The fina l reaso n why it is hard to pick the righ t interpretatio n is that there may be severa l righ t
ones. Joke s rely on the fact that the heare r will entertai n two interpretation s simultaneously . In
"She criticize d his apartmen t so he knocke d her flat, " we have thre e lexica l and one syntacti c
ambiguity . But the joke woul d be lost on a heare r who simpl y accepte d the best interpretatio n
and ignore d the other . Poetry , advertising , politica l rhetoric , and murde r mysterie s are othe r
genre s that make use of deliberat e ambiguity . Mos t languag e understandin g program s ignor e this
possibility , just as man y diagnosi s system s ignor e the possibilit y of multipl e causes .
Context­fre e grammar s do not provid e a very usefu l languag e mode l (eve n whe n augmenta ­
tions are included) . The proble m is that the gramma r does not say whic h string s are more probabl e
than others—i t simpl y divide s the string s into two classes : grammatica l and agrammatical .
9 We shoul d also conside r the possibilit y that the speake r intend s to conve y som e informatio n give n that it did not occur ,
that is, that the speake r is mistake n or lying .
Sectio n 22.9 . A  Communicatin g Agen t 683
The simples t way to provid e a probabilit y distributio n is to use a probabilisti c context­fre e
CONTEXTLFREE gramma r or PCFG. ' ° In the PCF G languag e model , each rewrit e rule has a probabilit y associate d
GRA wit h it, such that the sum for all rules with the same left­han d side is 1—fo r example ,
S^NPVP  (0.9 )
5 — 5 Conjunction  S '(0.1 )
In the PCF G model , the probabilit y of a string , P(words),  is just the sum of the probabilitie s of
its pars e trees—on e such tree for an unambiguou s string , no trees for an ungrammatica l string ,
and severa l trees for an ambiguou s string . The probabilit y of a give n tree is the produc t of the
probabilitie s of all the rules that mak e up the node s of the tree.
The proble m with PCFG s is that they are context­free . Tha t mean s that the differenc e
betwee n P("I ate a banana" ) and P("I ate a bandana" ) depend s only on P("banana" ) versu s
Pf'bandana") , and not on the relatio n betwee n "ate" and the respectiv e nouns . To get at that
kind of relationship , we will need som e kind of context­sensitiv e model . Othe r probabilisti c
languag e model s that includ e contex t sensitivit y have been propose d (see the Historica l Note s
section) . The proble m of combinin g the four model s into one is take n up whe n we discus s speec h
recognitio n in Sectio n 24.7 .
22.9 A COMMUNICATIN G AGEN T
We have now seen how to go all the way from string s to meaning s usin g syntacti c and semanti c
analysi s and disambiguation . The fina l step is to show how this fits in to an agen t that can
communicate . We start with the simpl e wumpu s worl d robo t slave describe d on page 662.
The first step in buildin g the communicatin g agen t is to exten d the gramma r to accep t
command s such as "Go east. " So far, the languag e £2 has only one type of speec h act: the
statement . We will exten d it with command s and acknowledgment s to yield the languag e £3.
The new word s and gramma r rules are not complicated . A comman d can be forme d from
a VP, wher e the subjec t is implicitl y the hearer . For example , "Go to 2 2" is a command , and
it alread y is a VP accordin g to the £2 grammar . The semantic s of the comman d is derive d by
applyin g the semantic s of the VP to the object  Hearer.  Now that we have severa l kind s of speec h
acts, we will identif y the kind (i.e, comman d or statement ) as part of the quasi­logica l form . Here
are the rules for command s and statements :
S(Command(rel(Hearer))  — VP(rel)
S(Statement(rel(obj)}  ­> NP(obj)  VP(rel)
So the quasi­logica l form for "Go to 2 2" is:''
Command(3  e e G Go(Hearer,  [2, 2]))
10 PCFG s are also know n as stochasti c context­fre e grammar s or SCFGs .
11 Not e that the quasi­logica l form for a comman d does not includ e the time of the even t (e.g. , During(Now,e)).  Tha t
is becaus e command s are tenseless . We can' t tell that by lookin g at command s with "go," but conside r that the correc t
form of a comman d is "[you ] be good, " (usin g the untense d form "be" ) not "[you ] are good. "
684 Chapte r 22 . Agent s that Communicat e
Acknowledgment s are even simpler—the y consis t of a singl e word : "yes " or "OK " to positivel y
acknowledge , and "no" to negativel y acknowledge . Her e are the rules :
S(Acknowledge(sem))  —> Ack(sem)
Ack(True)  —> yes
Ack(True)  ­> OK
Ack(False)  —>• no
It is up to the maste r (or whoeve r hear s an acknowledgment ) to do pragmati c interpretatio n to
realiz e that "OK, " whic h gets the interpretatio n Ack(True),  mean s that the agen t has agree d to
follo w out a comman d (usuall y the mos t recen t command) . The agen t progra m for the robo t
slave was show n in Figur e 22.4 (pag e 663) . But in a sens e we do not need a new agen t program ;
all we need to do is use an existin g agen t (suc h as a logica l plannin g agent ) and give it the goal s
of understandin g the inpu t and respondin g to it properly .
22.10 SUMMAR Y
We have seen why it is usefu l to communicate , and how languag e can be interprete d by agent s in
a situation . Natura l languag e processin g is difficul t for thre e reasons . First , one has to have a lot
of specifi c knowledg e abou t the word s and gramma r rule s of the language . Second , one mus t be
able to integrat e this knowledg e with othe r knowledg e abou t the world . Third , languag e involve s
an additiona l complicatio n that we have not deal t with so far: that there are othe r agent s in the
worl d who have their own beliefs , goals , and plans . Thi s chapte r make s the followin g point s in
addressin g these difficulties :
• Agent s send signal s to each othe r to achiev e certai n purposes : to inform , to warn , to elici t
help, to shar e knowledge , or to promis e something . Sendin g a signa l in this way is calle d
a speec h act. Ultimately , all speec h acts are an attemp t to get anothe r agen t to believ e
somethin g or do something .
• All animal s use som e conventiona l sign s to communicate , but human s use languag e in a
more sophisticate d way that enable s them to communicat e muc h more .
• Forma l languag e theor y and phras e structur e grammar s (and in particular , context­fre e
grammar ) are usefu l tools for dealin g with some aspect s of natura l language .
• Communicatio n involve s three step s by the speaker : the intentio n to conve y an idea , the
menta l generatio n of words , and their physica l synthesis . The heare r then has four steps :
perception , analysis , disambiguation , and incorporatio n of the meaning .
• The encode d messag e mode l of communicatio n state s that a speake r encode s a represen ­
tation of a propositio n into language , and the heare r then decode s the messag e to uncove r
the proposition . The situate d languag e mode l state s that the meanin g of a messag e is a
functio n of both the messag e and the situatio n in whic h it occurs .
• It is convenien t to augmen t a gramma r to handl e such problem s as subject/ver b agreement ,
pronou n case , and semantics . Definit e Claus e Gramma r (DCG ) is an extensio n of BNF
that allow s for augmentations .
Sectio n 22.10 . Summar y 68 5
• Ther e are man y algorithm s for parsin g strings . We showe d a simpl e one. It is also possibl e
to feed DCG rule s directl y to a logic programmin g syste m or theore m prover .
• Pragmati c interpretatio n take s the curren t situatio n into accoun t to determin e the effec t of
an utteranc e in context .
• Disambiguatio n is the proces s of decidin g whic h of the possibl e interpretation s is the one
that the speake r intende d to convey .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
The idea of languag e as actio n stem s from twentieth­centur y linguisticall y oriente d philosoph y
(Wittgenstein , 1953 ; Grice , 1957 ; Austin , 1962 ) and particularl y from the book Speech  Acts
(Searle , 1969) . A precurso r to the idea of speec h acts was Protagoras' s distinctio n of four type s
of sentence : prayer , question , answer , and injunction . Hobb s et al. (1987 ) describ e a mor e
practica l applicatio n of the situate d model .
Like semanti c networks , context­fre e grammar s (also know n as phras e structur e grammars )
are a reinventio n of a techniqu e first used by the ancien t India n grammarian s (especiall y Panini ,
c. 350 B.C. ) studyin g Shastri c Sanskri t (Ingerman , 1967) . In moder n times , they were reinvente d
by Noa m Chomsk y for the analysi s of Englis h synta x (Chomsky , 1956 ) and independentl y by
John Backu s for the analysi s of Algol­6 0 syntax . Nau r (Naur , 1963 ) extende d Backus' s notation ,
and is now credite d with the "N" in BNF , whic h originall y stoo d for "Backu s Norma l Form. "
GTRTARMMUAR Knut h (1968 ) denne d a kind of augmente d gramma r calle d attribut e grammar .
There have been man y attempt s to writ e forma l grammar s of natura l languages , both in
"pure " linguistic s and in computationa l linguistics . The Linguisti c Strin g Projec t at New York
Universit y (Sager , 1981 ) produce d a large gramma r for the machin e parsin g of English , usin g
essentiall y context­fre e rewrit e rules with som e restriction s base d on subcategorization . A good
exampl e of a moder n syste m usin g unificatio n gramma r is the Core Languag e Engin e (Alshawi ,
1992) . Ther e are severa l comprehensiv e but informa l grammar s of Englis h (Quir k et al., 1985 ;
Huddleston , 1988) . Goo d textbook s on linguistic s includ e Bake r (1989 ) and Chierchi a and
McConnell­Gine t (1990) . McCawley' s (1993 ) text concentrate s on logic for linguists . Definit e
claus e grammar s wer e introduce d by Colmeraue r (1975 ) and develope d and popularize d by
Pereir a and Warre n (1980) .
Forma l semanti c interpretatio n of natura l language s originate s withi n philosoph y and for­
mal logi c and is especiall y closel y relate d to Alfre d Tarski' s (1935 ) wor k on the semantic s of
forma l languages . Bar­Hille l was the first to conside r the problem s of pragmatic s and propos e
that they coul d be handle d by forma l logic . For example , he introduce d C.S. Peirce' s (1902 ) term
indexical  into linguistic s (Bar­Hillel , 1954) . Richar d Montague' s essay "Englis h as a forma l lan­
guage " (1970 ) is a kind of manifest o for the logica l analysi s of language , but the book by Dowty ,
Wall , and Peter s (1991 ) and the articl e by Lewi s (1972 ) are more readable . A complet e collectio n
of Montague' s contribution s has been edite d by Thomaso n (1974) . In artificia l intelligence , the
work of McAlleste r and Giva n (1992 ) continue s the Montagovia n tradition , addin g man y new
technica l insights .
686 Chapte r 22 . Agent s that Communicat e
TRANSFORMATIONA L
GRAMMA R
DEEP STRUCTUR E
SURFAC E
STRUCTUR E
AUGMENTE D
TRANSITIO N
NETWOR KThe idea of an intermediat e or quasi­logica l form to handl e problem s such as quantifie r
scopin g goes back to Wood s (1978) , and is presen t in man y recen t system s (Alshawi , 1992 ;
Hwan g and Schubert , 1993) . Van Lehn (1978 ) give s a surve y of huma n preference s for quantifie r
scope disambiguation .
Linguist s have dozen s of differen t formalisms ; hundred s if you coun t all the mino r modi ­
fication s and notationa l variants . We have stuc k to a singl e approach—definit e claus e gramma r
with BNF­styl e notation—bu t the histor y of the other s is interesting . Sell s (1985 ) offer s a good
compariso n of some curren t formalisms , but Geral d Gazdar' s (1989 ) analysi s is more succinct :
Here is the histor y of linguistic s in one sentence : once upon a time linguist s (i.e., syntacticians )
used augmente d phras e structur e grammars , then they wen t over to transformationa l grammars ,
and then som e of them starte d usin g augmente d phras e structur e grammar s again , <space
for moml>.  Whils t we are in this carefu l scholarl y mode , let us do the sam e servic e for
computationa l linguistics : once upon a time computationa l linguistic s (i.e., builder s of parsers )
used augmente d phras e structur e grammars , then they wen t over to augmente d transitio n
networks , and then man y of them starte d usin g augmente d phras e structur e grammar s again ,
<spacefor  moral>.
We can characteriz e the differen t formalism s accordin g to four dichotomies : transformationa l
versu s monostratal , unificatio n versu s assignment , lexica l versu s grammatical , and syntacti c
categorie s versu s semanti c categories .
The dominan t formalis m for the quarte r centur y startin g in 195 6 was transformationa l
gramma r (Chomsky , 1957 ; Chomsky , 1965) . In this approach , the commonalit y in meanin g
betwee n sentence s like "Ma n bite s dog" and "Do g is bitte n by man " is capture d by a context ­
free gramma r that generate s proto­sentence s in a canonica l form calle d the deep structure .
"Man bites dog" and "Dog is bitte n by man " woul d have the sam e deep structure , but differen t
surfac e structure . A separat e set of rule s calle d transformation s map betwee n deep and
surfac e structure . The fact that ther e are two distinc t level s of analysi s and two sets of rules
make s transformationa l gramma r a multistrata l theory . Computationa l linguist s have turne d
away from transformationa l grammar , becaus e it is difficul t to writ e parser s that can inver t the
transformation s to recove r the deep structure .
The augmente d transitio n networ k (ATN ) grammar s mentione d in the Gazda r quot e were
invente d as a way to go beyon d context­fre e gramma r whil e maintainin g a monostrata l approac h
that is computationall y tractable . They were invente d by Thorn e (1968 ) but are mostl y associate d
with the wor k of Wood s (1970) . The rule s in an ATN gramma r are represente d as a directe d
graph , but one can easil y transliterat e betwee n transitio n network s and context­fre e grammars ,
so choosin g one over the othe r is mostl y a matte r of taste . The othe r big differenc e is that DCG s
are augmente d with unificatio n assertions , wherea s ATN s are augmente d with assignmen t
statements . This make s DCG s close r to standar d first­orde r logic , but more importantly , it allow s
a DCG gramma r to be processe d by a variet y of algorithms . Any orde r of applicatio n of the rules
will arriv e at the same answer , becaus e unificatio n is commutative . Assignment , of course , is not
commutative . GPS G or Generalize d Phras e Structur e Gramma r (Gazda r et al, 1985 ) and HPS G
or Head­drive n Phras e Structur e Gramma r (Pollar d and Sag, 1994 ) are two importan t example s
of unification­base d grammars . Shiebe r (1986 ) survey s them and others .
Since the mid­1980s , ther e has also been a tren d towar d puttin g mor e informatio n in the
lexico n and less in the grammar . For example , rathe r than havin g a gramma r rule to transfor m
Sectio n 22.10 . Summar y 68 7
activ e sentence s into passive , man y moder n grammar s place the burde n of passive s on the lexicon .
The gramma r woul d have one or more rules sayin g a verb phras e can be a verb optionall y precede d
by an auxiliar y verb and followe d by complements . The lexica l entr y for "bitten " woul d say that
it is precede d by a form of the auxiliar y verb "be" and followe d by a prepositiona l phras e with
the prepositio n "by."
In the 1970 s it was felt that puttin g this kind of informatio n in the lexico n woul d be missin g
an importan t generality—tha t mos t transitiv e verb s have passiv e forms.12 The curren t view is
that if we can accoun t for the way the passive s of new verb s are learned , then we have not lost
any generalities . Puttin g the informatio n in the lexico n rathe r than the gramma r is just a kind of
compilation—i t can mak e the parser' s job easie r at run time . LFG or lexical­functiona l gramma r
(Bresnan , 1982 ) was the first majo r gramma r of Englis h and formalis m to be highl y lexicalized .
If we carry lexicalizatio n to an extreme , we end up with categoria l grammar , in whic h ther e
can be as few as two gramma r rules , or dependenc y gramma r (Melcu k and Polguere , 1988) , in
whic h there are no phrases , only words . TAG or Tree­Adjoinin g Gramma r (Joshi , 1985 ) is not
strictl y lexical , but it is gainin g popularit y in its lexicalize d form (Schabe s et al, 1988) .
A majo r barrie r to the widesprea d use of natura l languag e processin g is the difficult y of
tunin g an NLP syste m to perfor m well in a new domain , and the amoun t of specialize d trainin g (in
linguistic s and compute r science ) neede d to do the tuning . One way to lowe r this barrie r is to throw
out the specialize d terminolog y and methodolog y of linguistic s and base the system' s gramma r
more directl y on the proble m domain . This is achieve d by replacin g abstrac t syntacti c categorie s
GRAMMA R w'tn domain­specifi c semanti c categories . Suc h a gramma r is calle d a semanti c grammar . For
example , an interfac e to an airlin e reservatio n syste m coul d have categorie s like Location  and
Fly­To  instea d of NP and VP.  See Birnbau m and Selfridg e (1981 ) for an implementatio n of a
syste m base d on semanti c grammars .
There are two main drawback s to semanti c grammars . First , they are specifi c to a particula r
domain . Ver y little of the wor k that goes into buildin g a syste m can be transferre d to a differen t
domain . Second , they mak e it hard to add syntacti c generalizations . Handlin g construction s
such as passiv e sentence s mean s addin g not just one new rule , but one rule for each verb­lik e
category . Gettin g it righ t is time­consumin g and error­prone . Semanti c grammar s can be used to
get a smal l applicatio n workin g quickl y in a limite d domain , but they do not scale up well .
The othe r approac h to knowledg e acquisitio n for NLP is to use machin e learning . Gol d
(1967 ) set the groundwor k for this field , and Fu and Boot h (1986a ; 1986b ) give a tutoria l of
recen t work . Stolck e (1993 ) give s an algorith m for learnin g probabilisti c context­fre e grammars ,
and Blac k et al. (1992 ) and Magerman  (1993 ) show how to learn mor e comple x grammars .
Researc h on languag e learnin g by human s is surveye d by Wanne r and Gleitma n (1982 )
and by Bloo m (1994) . Pinke r (1989 ) give s his take on the field . A variet y of machin e learnin g
experiment s have tried to duplicat e huma n languag e learnin g (Clark , 1992 ; Siskind , 1994) .
Disambiguatio n has alway s been one of the hardes t parts of NLP . In part , this is becaus e
of a lack of help from othe r fields . Linguistic s consider s disambiguatio n to be largel y outsid e
its domain , and literar y criticis m (Empson , 1953 ; Hobbs , 1990 ) is ambiguou s abou t whethe r
12 It is now know n that passivit y is a featur e of sentences , not verbs . For example , "Thi s bed was slep t in by Georg e
Washington " is a good sentence , but "The stars were slep t unde r by Fred " is not (eve n thoug h the two correspondin g
activ e sentence s are perfectl y good) .
688 Chapte r 22 . Agent s that Communicat e
ambiguit y is somethin g to be resolve d or to be cherished . Som e of the earlies t work on disam ­
biguatio n was Wilks ' (1975 ) theor y of preferenc e semantics , whic h tried to find interpretation s
that minimiz e the numbe r of semanti c anomalies . Hirs t (1987 ) describe s a syste m with simila r
aims that is close r to the compositiona l semantic s describe d in this chapter . Som e problem s with
multipl e interpretation s are addresse d by Norvi g (1988) .
Probabilisti c technique s for disambiguatio n have been predominan t in recen t years , partl y
becaus e of the availabilit y of larg e corpor a of text from whic h to gathe r statistics , and partl y
becaus e the field is evolvin g toward s a mor e scientifi c methodology . Researc h involvin g large
corpor a of text is describe d in the specia l issu e of Computational  Linguistics  (Volum e 19,
Number s 1 and 2, 1993 ) and the book by Garsid e et al. (1987) . The statistica l approac h to
languag e is covere d in a book by Charnia k (1993) . This subfiel d starte d whe n NLP researcher s
notice d the succes s of probabilisti c model s in informatio n retrieva l (Salton , 1989 ) and speec h
recognitio n (Rabiner , 1990) . Thi s lead to the developmen t of probabilisti c model s for wor d
sense disambiguatio n (Yarowsky , 1992 ; Resnik , 1993 ) and eventuall y to the full parsin g task ,
as in the wor k by Churc h (1988 ) and by Chitra o and Grishma n (1990) . Som e recen t wor k
casts the disambiguatio n proble m as belie f networ k evaluatio n (Charnia k and Goldman , 1992 ;
Goldma n and Charniak , 1992 ; Wu, 1993) .
The Associatio n for Computationa l Linguistic s (ACL ) hold s regula r conferences ; muc h
curren t researc h on natura l languag e processin g is publishe d in thei r proceedings , and in the
ACL' s journa l Computational  Linguistics.  Readings  in Natural  Language  Processing  (Groszet
al., 1986 ) is an antholog y containin g man y importan t paper s in the field . The leadin g textboo k
is Natural  Language  Understanding  (Alien , 1995) . Pereir a and Sheibe r (1987 ) and Covingto n
(1994 ) offe r concis e overview s base d on implementation s in Prolog . The Encyclopedia of  Al
has man y usefu l article s on the field ; see especiall y "Computationa l Linguistics " and "Natura l
Languag e Understanding. "
EXERCISE S
22.1 Outlin e the majo r difference s betwee n Pasca l (or any othe r compute r languag e with whic h
you are familiar ) and English , and the "understanding " proble m in each case . Thin k abou t such
thing s as grammar , syntax , semantics , pragmatics , compositionality , context­dependence , lexica l
ambiguity , syntacti c ambiguity , reference­findin g (includin g pronouns) , backgroun d knowledge ,
and wha t it mean s to "understand " in the first place .
22.2 Whic h of the followin g are reasons  for introducin g a quasi­logica l form ?
a. To mak e it easie r to writ e simpl e compositiona l gramma r rules .
b. To exten d the expressivenes s of the semanti c representatio n language .
c. To be able to represen t quantifie r scopin g ambiguitie s (amon g others ) in a succinc t form .
d. To mak e it easie r to do semanti c disambiguation .
Sectio n 22.10 . Summar y 68 9
22.3 Determin e wha t semanti c interpretatio n woul d be give n to the followin g sentence s by the
gramma r in this chapter :
a. It is a wumpus .
b. The wumpu s is dead .
c. The wumpu s is in 2,2.
Woul d it be a good idea to have the semanti c interpretatio n for "It is a wumpus " be simpl y
3x Wumpus(x)r! Conside r alternativ e sentence s such as "It was a wumpus. "
22.4 Augmen t the gramma r from this chapte r so that it handle s the following :
a. Pronou n case.
b. Subject/ver b agreement .
c. Article/nou n agreement : "agents " is an NP but "agent " is not. In general , only plura l noun s
can appea r withou t an article .
22.5 Thi s exercis e concern s grammar s for very simpl e languages .
a. Writ e a context­fre e gramma r for the languag e a"b".
b. Writ e a context­fre e gramma r for the palindrom e language : the set of all string s whos e
secon d half is the revers e of the first half.
c. Writ e a context­sensitiv e gramma r for the languag e a"b"c".
d. Writ e a context­sensitiv e gramma r for the duplicat e language : the set of all string s whos e
secon d half is the same as the first half.
? 22. 6 Thi s exercis e continue s the exampl e of Sectio n 22.9 by makin g the slave mor e intelligent .
On each turn , the slave describe s its percept s as before , but it also says wher e it is (e.g. , "I am in
1,1") and report s any relevan t facts it has deduce d abou t the neighborin g square s (e.g. , "Ther e is
a pit in 1,2" or "2,1 is safe") . You need not do any fanc y languag e generation , but you do have
to addres s the intentio n problem : decidin g whic h facts are wort h mentioning . In addition , you
shoul d give your slave a sens e of self­preservation . If it is commande d to ente r a deadl y square ,
it shoul d politel y refuse . If commande d to ente r an unsaf e square , it can ask for confirmation ,
but if commande d again , it shoul d obey . Run this slave in the wumpu s environmen t a few times .
How mus h easie r is it to work with this slave than the simpl e one from Sectio n 22.9 ?
22.7 Conside r the sentenc e "Someon e walke d slowl y to the supermarket " and the followin g set
of context­fre e rewrit e rules whic h give the grammatica l categorie s of the word s of the sentence :
Pronoun  ­^someon e V ­^walke d
Adv ­^slowl y Prep  —>t o
Det ­^th e Noun  ­^supermarke t
Whic h of the followin g thre e sets of rewrit e rules , whe n adde d to the precedin g rules , yiel d
context­fre e grammar s that can generat e the abov e sentence ?
690 Chapte r 22 . Agent s that Communicat e
(A):
S —NP  VP
NP —Pronoun
NP ­­Del  Noun
VP—VPPP
VP—VPAdvAdv
VP—V
PP —Prep  NP
NP —Noun(B): (C):
S —NP  VP S  —NP  VP
NP —Pronoun  NP  —Pronou n
NP —Noun  NP  —Det  NP
NP ­^Det  NP VP  —VAdv
VP—V  Vmod  Adv  —Adv  Adv
Vmod  —Adv  Vmod  Adv  —PP
Vmod  —Adv  PP  —Prep  NP
Adv —PP  NP  —Noun
PP ­,prep  NP
Write dow n at least one othe r Englis h sentenc e generate d by Gramma r (B). It shoul d be signifi ­
cantl y differen t from the abov e sentence , and shoul d be at least six word s long . Do not use any of
the word s from the precedin g sentence ; instead , add grammatica l rule s of your own , for instance ,
Noun  —» bottle . Sho w the pars e tree for your sentence .
22.8 Thi s exercis e concern s a languag e we call Buffalo",  whic h is very muc h like Englis h excep t
the only wor d in its lexico n is buffalo.  (Th e languag e is due to Barton , Berwick , and Ristad. )
Here are two sentence s from the language :
• Buffal o buffal o buffal o Buffal o buffalo .
• Buffal o Buffal o buffal o buffal o buffal o Buffal o buffalo .
In case you don' t believ e thes e are sentences , here are two Englis h sentence s with correspondin g
syntacti c structure :
• Dalla s cattl e bewilde r Denve r cattle .
• Chef s Londo n critic s admir e cook Frenc h food .
Write a gramma r for Buffalo".  The lexica l categorie s are adjective , noun , and (transitive ) verb ,
and there shoul d be one gramma r rule for sentence , one for verb phrase , and three rules for noun
phrase : raw noun , adjectiv e modifier , and reduce d relativ e claus e (i.e., a relativ e claus e withou t
the word "that") . Tabulat e the numbe r of possibl e parse s for Buffalo"  for n up to 10.
PRACTICA L NATURA L
LANGUAG E PROCESSIN G
In which we  see how  to scale  up from  toy domains  like the  wumpus  world  to practical
systems  that  perform  useful  tasks  with  language.
In Chapte r 22, we saw that agent s can gain by communicatin g with each other . We also saw som e
technique s for interpretin g sentence s from simpl e subset s of English . In this chapter , we show
how far beyon d the wumpu s worl d one can go by elaboratin g on thos e techniques . The topic s
covere d are as follows :
0 Practica l applications : task s wher e natura l languag e has prove d useful .
<> Discours e processing : the proble m of handlin g more than one sentence .
0 Efficien t parsing : algorithm s for parsin g and interpretin g sentence s quickly .
0 Scalin g up the lexicon : dealin g with unusua l and even unknow n words .
<J> Scalin g up the grammar : dealin g with complicate d syntax .
<> Semanti c interpretation : som e problem s that mak e semanti c interpretatio n mor e than
just a matte r of composin g simpl e functions .
0 Disambiguation : how to choos e the righ t interpretation .
23.1 PRACTICA L APPLICATION S
We start by surveyin g successfu l system s that put natura l languag e to practica l use. The successfu l
system s shar e two properties : the y are focuse d on a particula r domain  rathe r than allowin g
discussio n of any topic , and they are focuse d on a particula r task  rathe r than attemptin g to
understan d languag e completely . We will look at five tasks .
Machin e translatio n
In the early 1960s , ther e was grea t hop e that computer s woul d be able to translat e from one
natura l languag e to another , just as Turing' s projec t "translated " code d message s into intelligibl e
691
692 Chapte r 23 . Practica l Natura l Languag e Processin g
German . But by 1966 , it becam e clear that translatio n require s an understandin g of the meanin g
of the messag e (and henc e detaile d knowledg e abou t the world) , wherea s code breakin g depend s
only on the syntacti c propertie s of the messages .
Althoug h there has been no fundamenta l breakthroug h in machin e translation , ther e has
been real progress , to the poin t that ther e are now dozen s of machin e translatio n system s in
everyda y use that save mone y over fully manua l techniques . One of the mos t successfu l is the
TAUM­METE O system , develope d by the Universit y of Montreal , whic h translate s weathe r report s
from Englis h to French . It work s becaus e the languag e used in these governmen t weathe r report s
is highl y stylize d and regular .
In more open domains , the result s are less impressive . A representativ e syste m is SPANA M
(Vasconcello s and Leon , 1985) , whic h can translat e a Spanis h passag e into Englis h of this quality :
The extensio n of the coverag e of the healt h service s to the underserve d or not serve d populatio n
of the countrie s of the regio n was the centra l goal of the Ten­Yea r Plan and probabl y that of
greate r scop e and transcendence . Almos t all the countrie s formulate d the purpos e of extendin g
the coverag e althoug h coul d be appreciate d a diversit y of approache s for its attack , whic h is
understandabl e in view of the differen t nationa l policie s that had acted in the configuratio n of
the healt h system s of each one of the countries .
This is mostl y understandable , but not alway s grammatica l and rarel y fluent . Standin g on its
own, unrestricte d machin e translatio n is still inadequate . But whe n a huma n translato r is give n a
text like this as an initia l guideline , the huma n is able to work two to four times faster . Sometime s
a monolingua l huma n can post­edi t the outpu t withou t havin g to read the original . This save s
mone y becaus e such editor s can be paid less than bilingua l translators .
Anothe r possibilit y is to inves t the huma n effor t on pre­editin g the origina l document . If
the origina l documen t can be mad e to confor m to a restricte d subse t of Englis h (or whateve r the
origina l languag e is), then it can sometime s be translate d withou t the need for post­editing . This
approac h is particularl y cost­effectiv e whe n there is a need to translat e one documen t into man y
languages , as is the case for legal document s in the Europea n Community , or for companie s that
sell the sam e produc t internationally . Restricte d language s are sometime s calle d "Caterpilla r
English, " becaus e Caterpilla r was the first firm to try writin g their manual s in this form . The
first reall y successfu l use of this approac h was mad e by Xerox . The y define d a languag e for
their maintenanc e manual s that was simpl e enoug h that it coul d be translate d by the SYSTRA N
syste m into all the language s Xero x deals with . As an adde d benefit , the origina l Englis h manual s
becam e cleare r as well .
There is a substantia l start­u p cost to any machin e translatio n effort . To achiev e broa d
coverage , translatio n system s have lexicon s of 20,00 0 to 100,00 0 word s and grammar s of 100 to
10,00 0 rules , the number s varyin g greatl y dependin g on the choic e of formalism .
Translatio n is difficul t because , in the genera l case , it require s in­dept h understandin g of
the text, and that require s in­dept h understandin g of the situatio n that is bein g communicated .
This is true even for very simpl e texts—eve n "texts " of one word . Conside r the wor d "Open "
on the door of a store. ' It communicate s the idea that the store is acceptin g customer s at the
moment . Now conside r the sam e wor d "Open " on a large banne r outsid e a newl y constructe d
store . It mean s that the stor e is now in dail y operation , but reader s of this sign woul d not feel
1 Thi s exampl e is due to Marti n Kay .
Sectio n 23.1 . Practica l Application s 69 3
misle d if the store close d at nigh t withou t removin g the banner . The two sign s use the identica l
word to conve y differen t meanings . In som e othe r languages , the same word or phras e woul d be
used in both cases , but in German , the sign on the door woul d be "Offen " whil e the banne r woul d
read "Ne u Eroffnet. "
The proble m is that differen t language s categoriz e the worl d differently . A majorit y of the
situation s that are covere d by the Englis h wor d "open " are also covere d by the Germa n wor d
"offen, " but the boundarie s of the categor y diffe r acros s languages . In English , we exten d the
basic meanin g of "open " to cove r open markets , open questions , and open job offerings . In
German , the extension s are different . Job offering s are "freie, " not open , but the concept s of
loose ice, privat e firms , and blan k check s all use a form of "offen. "
To do translatio n well, a translato r (huma n or machine ) mus t read the origina l text, under ­
stand the situatio n to whic h it is referring , and find a correspondin g text in the targe t languag e that
does a good job of describin g the same or a simila r situation . Ofte n this involve s a choice . For
example , the Englis h word "you " can be translate d into Frenc h as eithe r the forma l "vous " or the
informa l "tu." Ther e is just no way that one can refer to the concep t of "you " in Frenc h withou t
also makin g a choic e of forma l or informal . Translator s (bot h machin e and human ) sometime s
find it difficul t to mak e this choice .
Databas e acces s
The first majo r succes s for natura l languag e processin g (NLP ) was in the area of databas e access .
Circa 1970 , ther e were man y database s on mainfram e computers , but they coul d be accesse d
only by writin g complicate d program s in obscur e programmin g languages . The staff in charg e
of the mainframe s coul d not keep up with all the request s of users who neede d to get at this data ,
and the users understandabl y did not wan t to learn how to progra m their own requests . Natura l
languag e interface s provide d a solutio n to this dilemma .
The first such interfac e was the LUNA R system , a prototyp e built by Willia m Wood s (1973 )
and his team for the NAS A Manne d Spacecraf t Center . It enable d a geologis t to ask question s
abou t the chemica l analysi s data of luna r rock and soil sample s brough t back by the Apoll o
missions . Th e syste m was not put into real operationa l use, but in one test it successfull y
answere d 78% of querie s such as
What is the averag e moda l plagioclas e concentratio n for luna r sample s that contai n
rubidium ?
Fernand o Pereira' s CHA T syste m (Pereira , 1983 ) is at a simila r level of complexity . It generate s
the followin g answer s to question s abou t a geographica l database :
Q: Whic h countrie s are bordere d by two seas ?
A: Egypt , Iran , Israel , Saud i Arabi a and Turke y
Q: Wha t are the countrie s from whic h a river flow s into the Blac k sea?
A: Romania , Sovie t Unio n
Q: Wha t is the total area of countrie s sout h of the equato r and not in Australasia ?
A: 10,228,00 0 square  mile s
Q: Wha t is the ocea n that border s Africa n countrie s and that border s Asia n countries ?
A: India n Ocea n
694 Chapte r 23 . Practica l Natura l Languag e Processin g
The advantage s of system s like this are obvious . The disadvantag e is that the user neve r know s
whic h wording s of a quer y will succee d and whic h are outsid e the system' s competence . For
example , CHA T handle s "sout h of the equator " and "wit h latitud e less than zero, " but not "in the
souther n hemisphere. " Ther e is no principle d reaso n why this last paraphras e shoul d not work ; it
just happen s that "hemisphere' ' is not in the dictionar y (nor is this sens e of "in") . Similarly , the
final sampl e questio n coul d not be phrase d as "Wha t ocea n border s both Africa n countrie s and
Asian? " becaus e the gramma r does not allow that kind of conjunction .
Over the last decade , som e commercia l system s have buil t up large enoug h grammar s and
lexicon s to handl e a fairl y wid e variet y of inputs . The main challeng e for curren t system s is to
follo w the contex t of an interaction . The user shoul d be able to ask a serie s of question s wher e
some of them implicitl y refer to earlie r question s or answers :
Wha t countrie s are nort h of the equator ?
How abou t south ?
Show only the ones outsid e Australasia .
Wha t is their total area?
Some system s (e.g. , TEA M (Gros z et al., 1987) ) handl e problem s like this to a limite d degree ,
We retur n to the proble m in Sectio n 23.6 .
In the 1990s , companie s such as Natura l Languag e Inc. and Symante c are still sellin g
databas e acces s tools that use natura l language , but customer s are less likel y to make their buyin g
decision s base d on the strengt h of the natura l languag e componen t than on the graphica l user
interfac e or the degre e of integratio n of the databas e with spreadsheet s and wor d processing .
Natura l languag e is not alway s the mos t natura l way to communicate : sometime s it is easie r to
point and click with a mous e to expres s an idea (e.g. , "sum that colum n of the spreadsheet") .
The emphasi s in practica l NLP has now shifte d awa y from databas e acces s to the broa d
INTERPRETATIO N ^e'^ °^ ^ex^ interpretation . In part , this is a reflectio n of a chang e in the compute r industry .
In the early 1980s , mos t onlin e informatio n was store d in database s or spreadsheets . Now the
majorit y of onlin e informatio n is text: email , news , journa l articles , reports , books , encyclopedias .
Most compute r user s find there is too muc h informatio n available , and not enoug h time to sort
throug h it. Text interpretatio n program s help to retrieve , categorize , filter , and extrac t informatio n
from text. Tex t interpretatio n system s can be split into three types : informatio n retrieval , text
categorization , and data extraction .
Informatio n retrieva l
In informatio n retrieva l (IR) , the task is to choos e from a set of document s the ones that are
relevan t to a query . Sometime s a documen t is represente d by a surrogate , such as the title and a
list of keyword s and/o r an abstract . Now that so muc h text is online , it is more commo n to use
the full text, possibl y subdivide d into section s that each serv e as a separat e documen t for retrieva l
purposes . The quer y is normall y a list of word s type d by the user . In early informatio n retrieva l
systems , the quer y was a Boolea n combinatio n of keywords . For example , the quer y "(natura l
and language ) or (computationa l and linguistics) " woul d be a reasonabl e quer y to find documents
relate d to this chapter . However , user s foun d it difficul t to get good result s with Boolea n queries .
Whe n a quer y find s no documents , for example , it is not clea r how to rela x the quer y to find
Sectio n 23.1.  Practica l Application s 695
some . Changin g an "and " to an "or" is one possibility ; addin g anothe r disjunctio n is another , but
users foun d there were too man y possibilitie s and not enoug h guidance .
VECTOR­SPAC E Mos t moder n IR system s have switche d from the Boolea n mode l to a vector­spac e model ,
in whic h ever y list of word s (both documen t and query ) is treate d as a vecto r in n­dimensiona l
space , wher e n is the numbe r of ^istinc t token s in the documen t collection . In this model , the
query woul d simpl y be "natura l languag e computationa l linguistics, " whic h woul d be treate d as
a vecto r with the valu e 1 for these four word s (or terms , as they are calle d in IR) and the valu e
0 for all the othe r terms . Findin g document s is then a matte r of comparin g this vecto r agains t
a collectio n of othe r vector s and reportin g the ones that are close . The vecto r mode l is mor e
flexibl e than the Boolea n mode l becaus e the document s can be ranke d by their distanc e to the
query , and the closes t ones can be reporte d first.
There are man y variation s on this model . Som e system s are equippe d with morphologica l
analyzer s that matc h "linguisti c computation " with "computationa l linguistics. " Som e allow the
query to state that two word s mus t appea r near each othe r to coun t as a match , and other s use a
thesauru s to automaticall y augmen t the word s in the quer y with their synonyms . Onl y the mos t
naive system s coun t all the term s in the vector s equally . Mos t system s ignor e commo n word s like
"the" and "a," and man y system s weigh t each term differently . A good way to do this is to give
a term a large r weigh t if it is a good discriminator : if it appear s in a smal l numbe r of document s
rathe r than in man y of them .
This mode l of informatio n retrieva l is almos t entirel y at the wor d level . It admit s a
minuscul e amoun t of synta x in that word s can be require d to be near each other , and allow s
a similarl y tiny role for semanti c classe s in the form of synony m lists . You migh t thin k that
IR woul d perfor m muc h bette r if it used som e mor e sophisticate d natura l languag e processin g
techniques . Man y peopl e have though t just that, but surprisingly , none has been able to show a
significan t improvemen t on a wide rang e of IR tasks . It is possibl e to tune NLP technique s to a
particula r subjec t domain , but nobod y has been able to successfull y appl y NLP to an unrestricte d
range of texts .
The mora l is that mos t of the informatio n in a text is containe d in the words . The IR
approac h does a good job of applyin g statistica l technique s to captur e mos t of this information .
It is as if we took all the word s in a document , sorte d them alphabetically , and then very
carefull y compare d that list to anothe r sorte d list. Whil e the sort loses a lot of informatio n abou t
the origina l document , it ofte n maintain s enoug h to decid e if two sorte d lists are on simila r
topics . In contrast , the NLP technolog y we have toda y can sometime s pick out additiona l
information—disambiguatin g word s and determinin g the relation s betwee n phrases—bu t it often
fails to recove r anythin g at all. We are just beginnin g to see hybri d IR/NL P system s that combin e
the two approaches .
Text categorizatio n
NLP technique s have prove n successfu l in a relate d task : sortin g text into fixed topic categories .
There are severa l commercia l service s that provid e acces s to new s wire storie s in this manner .
A subscribe r can ask for all the new s on a particula r industry , company , or geographi c area ,
for example . The provider s of these service s have traditionall y used huma n expert s to assig n
694 Chapte r 23 . Practica l Natura l Languag e Processin g
The advantage s of system s like this are obvious . The disadvantag e is that the user neve r know s
whic h wording s of a quer y will succee d and whic h are outsid e the system' s competence . For
example , CHA T handle s "sout h of the equator " and "wit h latitud e less than zero, " but not "in the
souther n hemisphere. " Ther e is no principle d reaso n why this last paraphras e shoul d not work ; it
just happen s that "hemisphere",i s not in the dictionar y (nor is this sens e of "in") . Similarly , the
final sampl e questio n coul d not be phrase d as "Wha t ocea n border s both Africa n countrie s and
Asian? " becaus e the gramma r does not allow that kind of conjunction .
Over the last decade , som e commercia l system s have buil t up large enoug h grammar s and
lexicon s to handl e a fairl y wid e variet y of inputs . The main challeng e for curren t system s is to
follo w the contex t of an interaction . The user shoul d be able to ask a serie s of question s wher e
some of them implicitl y refer to earlie r question s or answers :
Wha t countrie s are nort h of the equator ?
How abou t south ?
Show only the ones outsid e Australasia .
What is their total area?
Some system s (e.g. , TEA M (Gros z et ai, 1987) ) handl e problem s like this to a limite d degree .
We retur n to the proble m in Sectio n 23.6 .
In the 1990s , companie s such as Natura l Languag e Inc. and Symante c are still sellin g
databas e acces s tools that use natura l language , but customer s are less likel y to make their buyin g
decision s base d on the strengt h of the natura l languag e componen t than on the graphica l user
interfac e or the degre e of integratio n of the databas e with spreadsheet s and wor d processing .
Natura l languag e is not alway s the mos t natura l way to communicate : sometime s it is easie r to
point and click with a mous e to expres s an idea (e.g. , "sum that colum n of the spreadsheet") .
The emphasi s in practica l NLP has now shifte d awa y from databas e acces s to the broa d
field of text interpretation . In part, this is a reflectio n of a chang e in the compute r industry .
In the early 1980s , mos t onlin e informatio n was store d in database s or spreadsheets . Now the
majorit y of onlin e informatio n is text: email , news , journa l articles , reports , books , encyclopedias .
Most compute r user s find ther e is too muc h informatio n available , and not enoug h time to sort
throug h it. Text interpretatio n program s help to retrieve , categorize , filter , and extrac t informatio n
from text. Tex t interpretatio n system s can be split into three types : informatio n retrieval , text
categorization , and data extraction .
Informatio n retrieva l
In informatio n retrieva l (IR) , the task is to choos e from a set of document s the ones that are
relevan t to a query . Sometime s a documen t is represente d by a surrogate , such as the title and a
list of keyword s and/o r an abstract . Now that so muc h text is online , it is mor e commo n to use
the full text, possibl y subdivide d into section s that each serv e as a separat e documen t for retrieva l
purposes . The quer y is normall y a list of word s type d by the user . In early informatio n retrieva l
systems , the quer y was a Boolea n combinatio n of keywords . For example , the quer y "(natura l
and language ) or (computationa l and linguistics) " woul d be a reasonabl e quer y to find document s
relate d to this chapter . However , user s foun d it difficul t to get good result s with Boolea n queries .
When a quer y find s no documents , for example , it is not clea r how to rela x the quer y to find
Sectio n 23.1 . Practica l Application s 69 5
some . Changin g an "and " to an "or" is one possibility ; addin g anothe r disjunctio n is another , but
users foun d there were too man y possibilitie s and not enoug h guidance .
VECTOR­SPAC E Mos t moder n IR system s have switche d from the Boolea n mode l to a vector­spac e model ,
in whic h ever y list of word s (bot h documen t and query ) is treate d as a vecto r in ^­dimensiona l
space , wher e n is the numbe r of distinc t token s in the documen t collection . In this model , the
query woul d simpl y be "natura l languag e computationa l linguistics, " whic h woul d be treate d as
a vecto r with the valu e 1 for these four word s (or terms , as they are calle d in IR) and the valu e
0 for all the othe r terms . Findin g document s is then a matte r of comparin g this vecto r agains t
a collectio n of othe r vector s and reportin g the ones that are close . The vecto r mode l is mor e
flexibl e than the Boolea n mode l becaus e the document s can be ranke d by their distanc e to the
query , and the closes t ones can be reporte d first .
There are man y variation s on this model . Som e system s are equippe d with morphologica l
analyzer s that matc h "linguisti c computation " with "computationa l linguistics. " Som e allow the
query to state that two word s mus t appea r near each othe r to coun t as a match , and other s use a
thesauru s to automaticall y augmen t the word s in the quer y with their synonyms . Onl y the most
naive system s coun t all the term s in the vector s equally . Mos t system s ignor e commo n word s like
"the" and "a," and man y system s weigh t each term differently . A good way to do this is to give
a term a large r weigh t if it is a good discriminator : if it appear s in a smal l numbe r of document s
rathe r than in man y of them .
This mode l of informatio n retrieva l is almos t entirel y at the wor d level . It admit s a
minuscul e amoun t of synta x in that word s can be require d to be near each other , and allow s
a similarl y tiny role for semanti c classe s in the form of synony m lists . Yo u migh t thin k that
IR woul d perfor m muc h bette r if it used som e mor e sophisticate d natura l languag e processin g
techniques . Man y peopl e have though t just that, but surprisingly , none has been able to show a
significan t improvemen t on a wide rang e of IR tasks . It is possibl e to tune NLP technique s to a
particula r subjec t domain , but nobod y has been able to successfull y appl y NLP to an unrestricte d
range of texts .
The mora l is that mos t of the informatio n in a text is containe d in the words . The IR
approac h does a good job of applyin g statistica l technique s to captur e mos t of this information .
It is as if we took all the word s in a document , sorte d them alphabetically , and then very
carefull y compared  that list to anothe r sorte d list. Whil e the sort lose s a lot of informatio n abou t
the origina l document , it ofte n maintain s enoug h to decid e if two sorte d lists are on simila r
topics . In contrast , the NLP technolog y we have toda y can sometime s pick out additiona l
information—disambiguatin g word s and determinin g the relation s betwee n phrases—bu t it often
fails to recove r anythin g at all. We are just beginnin g to see hybri d IR/NL P system s that combin e
the two approaches .
Text categorizatio n
NLP technique s have prove n successfu l in a relate d task : sortin g text into fixed topi c categories .
There are severa l commercia l service s that provid e acces s to new s wire storie s in this manner .
A subscribe r can ask for all the new s on a particula r industry , company , or geographi c area ,
for example . The provider s of thes e service s have traditionall y used huma n expert s to assig n
696 Chapte r 23 . Practica l Natura l Languag e Processin g
the categories . In the last few years , NLP system s have prove n to be just as accurate , correctl y
categorizin g over 90% of the new s stories . The y are also far faste r and more consistent , so there
has been a switc h from human s to automate d systems .
Text categorizatio n is amenabl e to NLP technique s wher e IR is not becaus e the categorie s
are fixed , and thus the syste m builder s can spen d the time tunin g their progra m to the problem .
For example , in a dictionary , the primar y definitio n of the word "crude " is vulgar , but in a large
sampl e of the Wall  Street  Journal,  "crude " refer s to oil 100% of the time .
Extractin g data from text
The task of data extractio n is to take on­lin e text and deriv e from it som e assertion s that can be
put into a structure d database . For example , the SciSO R syste m (Jacob s and Rau , 1990 ) is able to
take the followin g Dow Jone s New s Servic e story :
PILLSBUR Y SURGE D 3 3­4 TO 62 IN BIG BOAR D COMPOSIT E TRADIN G OF 3.1 MIL ­
LION SHARE S AFTE R BRITAIN' S GRAN D METROPOLITA N RAISE D ITS HOSTIL E
TENDE R OFFE R BY $3 A SHAR E TO $63. TH E COMPAN Y PROMPTL Y REJECTE D
THE SWEETENE D BID , WHIC H CAM E AFTE R THE TWO SIDE S COULDN' T AGRE E
TO A HIGHE R OFFE R ON FRIENDL Y TERM S OVE R THE WEEKEND .
and generat e this templat e to add to a database :
Corp-Takeover-Core :
Subevent: Increase d Offer, Rejecte d Offer
Type: Hostile
Target: Pillsbur y
Suitor: Grand Metropolita n
Share-Price : 63
Stock-Exchange : NYSE
Volume: 3.1M
Effect­On­Stock : (U p Increment : 3 3­4 , To : 62 )
23.2 EFFICIEN T PARSIN G
Conside r the followin g two sentences :
Have the student s in sectio n 2 of Compute r Scienc e 101 take the exam .
Have the student s in sectio n 2 of Compute r Scienc e 101 take n the exam ?
Even thoug h they shar e the first ten words , thes e sentence s have very differen t parses , becaus e
the first is a comman d and the secon d is a question . A left­to­righ t parsin g algorith m like the one
in Sectio n 22.4 that nondeterministicall y tries to buil d the righ t structur e woul d have to gues s if
the first wor d is part of a comman d or a question , and will not be able to tell if the gues s is correc t
until at leas t the elevent h word , "take/taken. " If the algorith m guesse d wrong , it will have to
Sectio n 23.2 . Efficien t Parsin g 697
CHAR T
PACKE D FORES T
VERTICE S
EDGE Sbacktrac k all the way to the first word . This kind of backtrackin g is inevitable , but if our parsin g
algorith m is to be efficient , it mus t avoi d reanalyzin g "the student s in sectio n 2 of Compute r
Scienc e 101" as an NP each time it backtracks .
In this section , we look at efficien t parsin g algorithms . At the broadest level,  there  are
three  main things  we can  do improve  efficiency:
1. Don' t do twic e wha t you can do once .
2. Don' t do once wha t you can avoi d altogether .
3. Don' t represen t distinction s that you don' t need .
To be more specific , we will desig n a parsin g algorith m that does the following :
1. Onc e we discove r that "the student s in sectio n 2 of Compute r Scienc e 101" is an NP, it is a
good idea to recor d that resul t in a data structur e know n as a chart . Algorithm s that do this
are calle d char t parsers . Becaus e we are dealin g with context­fre e grammars , any phras e
that was foun d in the contex t of one branc h of the searc h spac e can wor k just as well in
any othe r branc h of the searc h space . Recordin g result s in the char t is a form of dynami c
programmin g that avoid s duplicat e work .
2. We will see that our chart­parsin g algorith m uses a combinatio n of top­dow n and bottom­u p
processin g in a way that mean s it neve r has to conside r certai n constituent s that coul d not
lead to a complet e parse . (Thi s also mean s it can handl e grammar s with both left­recursiv e
rules and rule s with empt y right­han d sides withou t goin g into an infinit e loop. )
3. The resul t of our algorith m is a packe d fores t of pars e tree constituent s rathe r than an
enumeratio n of all possibl e trees . We will see later why this is important .
The char t is a data structur e for representin g partia l result s of the parsin g proces s in such a way
that they can be reuse d later on. The char t for an n­wor d sentenc e consist s of n + 1 vertice s and
a numbe r of edge s that connec t vertices . Figur e 23.1 show s a char t with 6 vertice s (circles) , and
3 edge s (lines) . For example , the edge labelle d
[0,5, S^NPVP»]
mean s that an NP followe d by a VP combin e to mak e an S that span s the strin g from 0 to 5. The
symbo l • in an edge separate s wha t has been foun d so far from wha t remain s to be found.2 Edge s
with the • at the end are calle d complet e edges . The edge
[0,2, S ­> NP • VP]
says that an NP span s the strin g from 0 to 2, and if we coul d find a VP to follo w it, then we woul d
have an S. Edge s like this with the dot befor e the end are calle d incomplet e edges,3 and we say
that the edge is lookin g for a VP. We have alread y seen two way s to look at the parsin g process .
In BOTTOM­UP­PARS E on page 666, we describe d parsin g as a proces s of buildin g word s into
trees, backtrackin g whe n necessary . Wit h Definit e Claus e Grammar , we describe d parsin g as a
form of logica l inferenc e on strings . Backtrackin g was used whe n severa l rule s coul d deriv e the
2 It is becaus e of the • that edge s are ofte n calle d dotte d rules . We thin k this term is a little confusing , becaus e there
can be man y dotte d rules correspondin g to the sam e gramma r rule .
3 Som e author s call thes e activ e edges . In som e paper s (Earley , 1970) , edge s are calle d states ; the idea is that an
incomplet e edge mark s an intermediat e state in the proces s of findin g a complet e constituent .
698 Chapte r 23 . Practica l Natura l Languag e Processin g
(
Figur e
shown ,/ [0,  2 S
A The ^o) — — (j­»• NP • VP]
>. agen t[0, 5 S ­» NP
^. feel s
\t)VP
[2,
<7•7
5V/3­* Verb  NP •] 1
. a ^^ breez e }L
) ——— — ©— — ^ ——— — ©
23.1 Par t of the char t for the sentenc e "The agen t feels a breeze. " All 6 vertice s are
but only three of the edge s that woul d mak e up a complet e parse .
INITIALIZE R
PREDICTO R
COMPLETE R
SCANNE Rsame predicate . Now we will see a third approach , chart­parsing . Unde r this view , the proces s
of parsin g an n­ word sentenc e consist s of formin g a char t with n + 1 vertice s and addin g edge s to
the char t one at a time , tryin g to produc e a complet e edge that span s from verte x 0 to n and is of
categor y S. Ther e is no backtracking ; everythin g that is put in the char t stay s there .
There are four way s to add an edge to the chart , and we can give each one a name : The
initialize r adds an edge to indicat e that we are lookin g for the start symbo l of the grammar , S,
startin g at positio n 0, but that we have not foun d anythin g yet. The predicto r takes an incomplet e
edge that is lookin g for an X and adds new incomplet e edge s that, if completed , woul d build an
X in the right place . The complete r takes an incomplet e edge that is lookin g for an X and ends at
verte x j and a complet e edge that begin s at 7 and has X as the left­han d side, and combine s them
to mak e a new edge wher e the X has been found . Finally , the scanne r is simila r to the completer ,
excep t that it uses the inpu t word s rathe r than existin g complete  edge s to generat e the X. Tha t is,
if there is an edge endin g at verte x j that is lookin g for a Noun,  and if the yth word in the inpu t
string has a Noun  entr y in the lexicon , then the scanne r will add a new edge that incorporate s the
word , and goes to verte x j + 1.
We will show two version s of chart­parsin g algorithms . Figur e 23.2 treat s the char t as a set
of edge s and at each step adds one new edge to the set, nondeterministicall y choosin g betwee n
the possibl e additions . This algorith m uses the operato r pick rathe r than choos e to indicat e that it
has no backtrac k points . Any orde r of choice s lead s to the same resul t in the end. The algorith m
terminate s whe n none of the four method s can add a new edge . We use a sligh t trick to start : we
add the edge [0,0 , S' —> »S] to the chart , wher e S is the grammar' s start symbol , and S' is a new
symbo l that we just invented . This edge make s the PREDICTO R add an edge for each gramma r
rule with S on the left­han d side, whic h is just wha t we need to start .
Figure s 23.3 and 23.4 show a char t and trace of the algorith m parsin g the sentenc e "I feel
it." Thirtee n edge s (labelle d a­m) are recorde d in the chart , includin g five complet e edge s (show n
abov e the vertice s of the chart ) and eigh t incomplet e ones (belo w the vertices) . Not e the cycl e
of predictor , scanner , and complete r actions . For example , the predicto r uses the fact that edge
(a) is lookin g for an S to licens e the predictio n of an NP (edg e b) and a Pronoun  (edg e c). The n
the scanne r recognize s that ther e is a Pronoun  in the righ t plac e (edg e d), and the complete r
combine s the incomplet e edge b with the complet e edge d to yield a new edge , e. Not e that the
name COMPLETE R is misleadin g in that the edge s it produce s (like e) are not necessaril y complete .
We use the nam e becaus e it has a long history , but a bette r nam e migh t have been EXTENDER .
Sectio n 23.2 . Efficien t Parsin g 699
functio n NoNDETERMlNlSTlC­CHART­PARSE(sfrmg , grammar)  return s chart
INITIALIZER :
chart*­[0,0,  S' — •  S]
while new edge s can still be adde d do
edge  <— choos e [i,j, A —> a » S /?] in c/iar f
choos e one of the three method s that will succeed :
PREDICTOR :
choos e (B ­^  ~i) in R\JLES[grammar]
add \j,j, B ­^ • 7] to chart
COMPLETER :
choos e \j,k, B —> F •] in c/iar t
add [/, k, A — a B • /?] to c/wr t
SCANNER :
if string\j  + 1 ] is of categor y B then
add \j,j + 1, A —> a S • /?] to c/iar t
end
retur n c/wr t
Figur e 23.2 Nondeterministi c char t parsin g algorithm . 5 is the start symbo l and S' is a new
nontermina l symbol . The Gree k letter s matc h a strin g of zero or more symbols . The variabl e edge
is an edge lookin g for a B. The predicto r adds an edge that will form a B, the complete r choose s
a complet e edge with B on the left­han d side and adds a new edge that is just like edge  excep t the
dot is advance d past B. The scanne r advance s the dot if the next wor d is of categor y B.
An importan t featur e of our chart­parsin g algorith m is that it avoid s buildin g som e edge s
that coul d not possibl y be part of an S spannin g the whol e string . Conside r the sentenc e "The
ride the hors e gave was wild. " Som e algorithm s woul d parse "ride the horse " as a VP, and then
discar d it whe n it is foun d not to fit into a large r S. But if we assum e that the gramma r does
not allow a VP to follo w "the, " then the chart­parsin g algorith m will neve r predic t a VP at that
point , and thus will avoi d wastin g time buildin g the VP constituen t there . Algorithm s that have
LEFT­CORNE R thi s propert y are calle d left­corne r parsers , becaus e they build up a parse tree that start s with the
grammar' s start symbo l and extend s dow n to the left­mos t word in the sentenc e (the  left corner) .
An edge is adde d to the char t only if it can serv e to exten d this pars e tree. See Figur e 23.5 for an
exampl e of this.
Our algorith m has the constrain t that the edge s are adde d in left­to­righ t order . Tha t is, if
edge [i,j, A —> B] is adde d befor e [i',f , C —> D], then it mus t be that ) < /. Figur e 23.6 show s
a deterministi c implementatio n that obey s this constraint . To get efficiency , we inde x edge s in
the char t by their endin g verte x number . The notatio n chart\j]  mean s the set of edge s that end at
verte x j. Additiona l indexin g of edge s may lead to furthe r efficiency : the loop in SCANNE R coul d
be eliminate d if we indexe d edge s at a verte x by the termina l symbo l they are lookin g for, and
the loop in COMPLETE R coul d be eliminate d if we indexe d the complet e edge s at a verte x by their
left­han d side. The algorith m also indexe s rules so that REWRiTES­FOR(Ar,G) return s all rules in
G whos e left­han d side is X.
700 Chapte r 23 . Practica l Natura l Languag e Processin g
m:S
a:SVS
b:S/NP  VP
c:NP/Pronounf:VP/Verb
g:VP/VPNPj:NP/Pronoun
Figur e 23.3 Char t for a pars e of "o I i feel 2 it 3." The notatio n m:S mean s that edge m has an
S on the left­han d side , whil e the notation / VP/Verb  mean s that edge / has a VP on the left­han d
side, but it is lookin g for a Verb.  Ther e are 5 complet e edge s abov e the vertices , and 8 incomplet e
edges below .
Edge
a
b
c
d
e
f
g
h
i
j
k
1
mProcedur e
INITIALIZE R
PREDICTOR(a )
PREDlCTOR(b )
SCANNER(C )
COMPLETER(b,d )
PREDICTOR(e )
PREDICTOR(e )
SCANNER(f )
COMPLETER(g,h )
PREDICTOR(g )
SCANNE R (j)
COMPLETER(i,k )
COMPLETER(eJ )Derivatio n
[0,0, 5' — »S]
[0, 0, S — »NP  VP]
[0, 0, NP ­+ •Pronoun]
[0, 1, NP — Pronoun*]
[0,1, S — NP*  VP]
[1,1, VP^*Verb]
[1,1, VP ­> *VP  NP]
[1,2, VP—  Verb*]
[1,2, VP—  VP»NP]
[2, 2, NP — •Pronoun]
[2, 3, NP  —? Pronoun*]
[1,3, VP ­» VPMP M
[0, 3, 5 — W W»]
Figur e 23.4 Trac e of a pars e of "o I i feel 2 it 3." For each edge a­m, we show the procedur e
used to deriv e the edge from othe r edge s alread y in the chart .
Sectio n 23.2 . Efficien t Parsin g 701
The rid e th e hors e gav e wa s wild
Figur e 23.5 A  left­corne r parsin g algorith m avoid s predictin g a VP startin g with "ride, " but
does predic t a VP startin g with "was, " becaus e the gramma r expect s a VP followin g an NP.
The triangl e over "the hors e gave " mean s that the word s have a pars e as a RelClause,  but with
additiona l intermediat e constituent s that are not shown .
Extractin g parse s from the chart : Packin g
When the chart­parsin g algorith m finishes , it return s the entir e chart , but wha t we reall y wan t is a
parse tree (or trees) . Dependin g on how the parse r is used , we may wan t to pick out one or all the
parse trees that span the entir e input , or we may wan t to look at som e subtree s that do not span
the whol e input . If we have an augmente d grammar , we may only wan t to look at the semanti c
augmentation , ignorin g the syntacti c structure . In any case , we need to be able to extrac t parse s
from the chart .
The easies t way to do that is to modif y COMPLETE R so that whe n it combine s two chil d
edges to produc e a paren t edge , it store s in the paren t edge the list of childre n that compris e it.
Then , whe n we are don e with the parse , we need only look in chart[n]  for an edge that start s
at 0, and recursivel y look at the childre n lists to reproduc e a complet e pars e tree . Th e only
complicatio n is decidin g wha t to do abou t ambiguou s parses . To see why this is a problem , let
us look at an example . The sentenc e
Fall leave s fall and sprin g leave s sprin g
is highl y ambiguou s becaus e each word (excep t "and" ) can be eithe r a nou n or a verb , and "fall "
and "spring " can be adjective s as well . Altogethe r the sentenc e has four parses:4
[S [S [NP Fall leaves ] fall] and [S [NP sprin g leaves ] spring ]
[S [S [NP Fall leaves ] fall ] and [S sprin g [VP leave s spring] ]
[S [S Fall [VP leave s fall] ] and [S [NP sprin g leaves ] spring ]
[S [S Fall [VP leave s fall] ] and [S sprin g [VP leave s spring] ]
4 Th e pars e [S Fall [VP leave s fall] ] is equivalen t to "Autum n abandon s autumn. "
702 Chapte r 23 . Practica l Natura l Languag e Processin g
functio n CHART­PARSE(i?n'«g , grammar)  return s chart
chart[Q]^{[0,0,S'  — • S]}
for v —  from 1 to LENGTH(srrmg ) do
SCANNER(v , string[v])
end
retur n chart
procedur e ADD­EDGE(edge )
if edge  in c/iarf[END(edge) ] then do nothin g
else
push edge  on chart[END(edge)]
if COMPLETE?(edge ) then COMPLETER(edge )
else PREDlCTOR(edge )
procedur e SCANNER(/ , word )
for each [i,j , A — a • B /?] in chart\j]  do
if word is of categor y B then
ADD­EDGE([/,;+I , A ­­ o B • /?])
end
procedur e PREDiCTOR([i'J , A ­^ a • B /?])
for each (B —  ­y) in REWR1TES­FOR(B , grammar)  do
ADD­EDGE([/,,/ , B ­> • ­/])
end
procedur e COMPLETER([/ , k, B —> 7 •])
for each [/,_/, /I —> a • B' /3] in chart\j]  do
if B = S' then
ADD­EDGE([; , k, A ­> «B' • /^])
end
Figur e 23.6 Deterministi c versio n of the chart­parsin g algorithm . S is the start symbo l and
S' is a new nontermina l symbol . The functio n ADD­EDG E adds an edge to the chart , and eithe r
complete s it or predict s from it.
The ambiguit y can be divide d into two independen t parts : eac h of the two subsentence s is
ambiguou s in two ways . If we had a sentenc e with n such subsentence s joine d by conjunctions ,
then we woul d get one big sentenc e with 2" parses . (Ther e also woul d be ambiguit y in the way
the subsentence s conjoi n with each other , but that is anothe r story , one that is told quite well by
Churc h and Patil (1982). ) An exponentia l numbe r of parse s is a bad thing , and one way to avoi d
the proble m is to represen t the parse s implicitly . Conside r the followin g representation :
[S[S[NP Fall leaves ] [VP fallj
[NP Fall] [VP leave s fall]]and [S[NP sprin g leaves ] [VP spring ]
[NP spring ] [VP leave s spring ]
Instea d of multiplyin g out the ambiguit y to yield 2" separat e pars e trees , we have one big "tree "
with ambiguou s subpart s represente d by curl y braces . Of course , whe n n = 2, ther e is not muc h
Sectio n 23.3 . Scalin g Up the Lexico n 703
differenc e betwee n 2" and 2n, but for large n, this representatio n offer s considerabl e saving . The
representatio n is calle d a packe d forest , becaus e it is the equivalen t to a set of trees (a forest) ,
but they are efficientl y packe d into one structure .
To implemen t the packe d fores t representation , we modif y COMPLETE R to keep track of
lists of possibl e children , and we modif y ADD­EDG E so that whe n we go to add an edge that is
alread y in the chart , we merg e its list of possibl e childre n with the list that is alread y there .
We end up with a parsin g algorith m that is O(«3) in the wors t case (wher e n is the numbe r
of word s in the input) . This is the best that can be achieve d for a context­fre e grammar . Not e
that withou t the packe d forest , the algorith m woul d be exponentia l in the wors t case , becaus e it
is possibl e for a sentenc e to have 0(2" ) differen t pars e trees . In practice , one can expec t a good
implementatio n of the algorith m to pars e on the orde r of 100 word s per second , with variatio n
dependin g on the complexit y of the gramma r and the input .
23.3 SCALIN G UP THE LEXICO N
TOKENIZATIO N
MORPHOLOGICA L
ANALYSI S
INFLECTIONA L
MORPHOLOG Y
DERIVATIONA L
MORPHOLOG Y
COMPOUNDIN GIn Chapte r 22, the inpu t was a sequenc e of words . In real text­understandin g systems , the inpu t
is a sequenc e of character s from whic h the word s mus t be extracted . Mos t system s follo w a
four­ste p proces s of tokenization , morphologica l analysis , dictionar y lookup , and error recovery .
Tokenizatio n is the proces s of dividin g the inpu t into distinc t tokens—word s and punctu ­
ation marks . In Japanese , this is difficul t becaus e there are no space s betwee n words . Language s
like Englis h are easier , but not trivial . A hyphe n at the end of the line may be an inter ­ or intrawor d
dash. In som e type s of text, font changes , underlines , superscripts , and othe r contro l sequence s
must be accounte d for. Tokenizatio n routine s are designe d to be fast, with the idea that as long as
they are consisten t in breakin g up the inpu t text into tokens , any problem s can alway s be handle d
at som e later stage of processing .
Morphologica l analysi s is the proces s of describin g a wor d in term s of the prefixes ,
suffixes , and root form s that compris e it. Ther e are three way s that word s can be composed :
0 Inflectiona l morpholog y reflect s the change s to a wor d that are neede d in a particula r
grammatica l context . For example , mos t noun s take the suffi x "s" whe n they are plural .
0 Derivationa l morpholog y derive s a new wor d from anothe r wor d that is usuall y of a
differen t category . For example , the noun "shortness " is derive d from the adjectiv e "short "
togethe r with the suffi x "ness. "
<) Compoundin g take s two word s and puts them together . For example , "bookkeeper " is a
compoun d of "book " and "keeper. " (Th e nou n "keeper " is in turn derive d from the verb
"keep " by derivationa l morphology. )
Even in a morphologicall y simpl e languag e like English , ther e can be morphologica l am­
biguities . "Walks " can be eithe r a plura l nou n or a third­perso n singula r verb . "Unionizable "
can be analyze d as "un­ion­izable " or "union­izable, " and "untieable " can be "un­(tie­able) " or
"(un­tie)­able. " Man y language s mak e mor e use of morpholog y than English . In German , it
is not uncommo n to see word s like "Lebensversicherungsgesellschaftsangestellter " (life insur ­
704 Chapte r 23 . Practica l Natura l Languag e Processin g
ance compan y employee) . Language s such as Finish , Turkish , Inuit , and Yupi k have recursiv e
morphologica l rales that can generat e an infinit e numbe r of infinitel y long words .
DICTIONAR Y LOOKU P Dictionar y looku p is performe d on ever y toke n (excep t for specia l ones such as punctu ­
ation) . It may be mor e efficien t to store morphologicall y comple x word s like "walked " in the
dictionary , or it may be bette r to do morphologica l analysi s first : a morphologica l rule applie s
to the inpu t and says that we strip off the "ed" and look up "walk. " If we find that it is a verb
that is not marke d as bein g irregular , then the rule says that "walked " is the past tens e of the root
verb. Eithe r way , the task of dictionar y looku p is to find a word in the dictionar y and retur n its
definition . Thus , any implementatio n of the table  abstrac t data type can serv e as a dictionary .
Good choice s includ e hash tables , binar y trees , b­trees , and tries . The choic e depend s in part on
if there is room to fit the dictionar y in primar y storage , or if it reside s in a file.
ERRO R RECOVER Y Erro r recover y is undertake n whe n a word is not foun d in the dictionary . Ther e are at
least four type s of error recovery . First , morphologica l rule s can gues s at the word' s syntacti c
class : "smarply " is not in the dictionary , but it is probabl y an adverb . Second , capitalizatio n is a
clue that a word (or sequenc e of words ) is a prope r name . Third , othe r specialize d format s denot e
dates , times , socia l securit y numbers , and so forth . Thes e are ofte n domain­dependent .
Finally , spellin g correctio n routine s can be used to find a wor d in the dictionar y that is
close to the inpu t word . Ther e are two popula r model s of "closeness " betwee n words . In the
letter­base d model , an erro r consist s of insertin g or deletin g a singl e letter , transposin g two
adjacen t letters , or replacin g one lette r with another . Thus , a 10­lette r wor d is one erro r awa y
from 555 othe r words : 10 deletions , 9 swaps , 10 x 25 replacements , and 11 x 26 insertions .
Exercis e 23.1 1 discusse s the implication s of this for dictionar y implementation . Thi s mode l is
good for correctin g slips of the finger , wher e one key on the keyboar d is hit instea d of another .
In the sound­base d model , word s are translate d into a canonica l form that preserve s most of
informatio n neede d to pronounc e the word , but abstract s awa y som e of the details . For example ,
the word "attention " migh t be translate d into the sequenc e [a,T,a,N,SH,a,N] , wher e "a" stand s
for any vowel . Th e idea is that word s such as "attension " and "atennshun " translat e to the
same sequence . If no othe r word in the dictionar y translate s to the sam e sequence , then we can
unambiguousl y correc t the spellin g error . Not e that the letter­base d approac h woul d work just as
well for "attension, " but not for "atennshun, " whic h is 5 error s awa y from "attention. "
Practica l NLP system s have lexicon s with from 10,00 0 to 100,00 0 root word forms . Build ­
ing a lexico n of this size is a big investmen t in time and money , and one that dictionar y publisher s
and companie s with NLP program s have not been willin g to share . An exceptio n is Wordnet , a
freel y availabl e dictionar y of roughl y 100,00 0 word s produce d by a grou p at Princeto n led by
Georg e Miller . Figur e 23.7 give s som e of the informatio n on the word "ride " in Wordnet .
As usefu l as dictionarie s like Wordne t are, they do not provid e all the lexica l informatio n
you woul d like . The two missin g piece s are frequenc y informatio n and semanti c restrictions .
Frequenc y informatio n tells us that the teasin g sens e of ride is unusual , whil e the othe r sense s are
common , or that the femal e swa n sens e of "pen " is very rare, whil e the othe r sense s are common .
Semanti c restriction s tell us that the direc t objec t of the first sens e of ride is a horse , camel , or
simila r animal , whil e the direc t objec t of the secon d kind is a mean s of conveyanc e such as a
car, bus, skateboard , or airplane . Som e frequenc y informatio n and semanti c restriction s can be
capture d with the help of a larg e corpu s of text.
Sectio n 23.4 . Scalin g Up the Gramma r 705
Noun: ride
=> mechanica l device (device based on mechanica l principles )
Noun: drive, ride
=> journey (the act of traveling )
Verb: ride, ride an animal (of animals)
=> travel, go, move, change location , locomot e
*> Somebody rides
*> Somebody rides somethin g
Verb: ride, travel in a conveyanc e
=> travel, go, move, change location , locomot e
OP walk, go on foot, foot, leg it, hoof, hoof it
*> Somebody rides
*> Somebody rides somethin g
Verb: tease, cod, tantalize , bait, taunt, twit, rally, ride
=> mock, bemock, treat with contempt
*> Somebody rides somebody
Other words containin g "ride":
rider, joyride , ride piggyback , ride the bench,
phencyclidin e hydrochlorid e ...
Figur e 23.7 Par t of Wordnet' s informatio n for "ride. " Wordne t distinguishe s two nou n and
three verb senses , and lists the superclas s of each sense . (Th e user has the optio n of followin g
superclas s link s all the way up or dow n the tree. ) Ther e is also one opposit e liste d (OP) , man y
superclas s relation s (=>) , and for each verb , a list of subcategorizatio n frame s (*>) . Finally , a
few of the othe r entrie s with "ride " are liste d to give an idea of the breadt h of coverage . Not e
that expression s like "ride piggyback " are include d in additio n to individua l words . You can get
Wordne t by anonymou s ftp from clarit y .princeto n . edu.
23.4 SCALIN G UP THE GRAMMA R
Figur e 23.8 show s two example s of real­lif e language . Thes e example s contras t with our simpl e £2
languag e in man y ways . In tokenization , we have to deal with hyphenatio n (e.g. , smal ­ lest) ,
unusua l punctuatio n convention s (e.g. , fnct l ()), and formattin g (the unindente d DESCRIP ­
TION indicatin g a sectio n header) . Lexically , we have nove l word s like cmd and FD_CLOEXEC .
Syntactically , we have an odd sort of conjunctio n (read/write) ; appositio n of two nou n phrase s
(the argumen t fd); the "greate r than or equa l to" construction , whic h can be treate d eithe r as
an idio m or an unusua l disjunctio n of post­nomina l modifiers ; and the fact that "the sam e file
pointer " is the objec t of "shares, " even thoug h ther e are a doze n word s betwee n them . And then
there is the 67­wor d allege d sentence , whic h is actuall y a convolute d subordinat e claus e at best .
To mak e any sens e at all out of these real­lif e example s require s muc h mor e sophisticatio n
at ever y step of the languag e interpretatio n process . In this section , we exten d £2 to yiel d £3, a
languag e that cover s muc h more , but still has a long way to go.
706 Chapte r 23 . Practica l Natura l Languag e Processin g
DESCRIPTIO N
fcntl() perform s a variety of function s on open descriptors .
The argumen t fd is an open descripto r used by cmd as fol-
lows :
F_DUPFD Returns a new descriptor , which has the smal-
lest value greater than or equal to arg. It
refers to the same object as the origina l
descriptor , and has the same access mode
(read, write or read/write) . The new
descripto r shares descripto r status flags
with fd, and if the object was a file, the
same file pointer . It is also associate d
with a FD_CLOEXE C (close-on-exec ) flag set to
remain open across execve(2V ) system calls.
"And sinc e I was not informed—a s a matte r of fact, sinc e I did not know that there were
exces s fund s until we, ourselves , in that checku p after the whol e thing blew up, and that
was, if you'l l remember , that was the inciden t in whic h the attorne y genera l cam e to me and
told me that he had seen a mem o that indicate d that there were no more funds. "
Figur e 23.8 Tw o example s of real­lif e language : one from the UNI X manua l entry for f cntl ,
and one from a statemen t mad e by Ronal d Reagan , printe d in the May 4, 1987 , Roll Call.
Nomina l compound s and appositio n
Technica l text is full of nomina l compounds : string s of noun s such as "POSTSCRIP T languag e
code inpu t file." The first thin g we have to do to handl e nomina l compound s is realiz e that we
are dealin g with nouns , not NPs.  It is not the case that "the input " and "the file" combin e to form
"the inpu t the file. " Rather , we have two noun s combinin g to form a large r unit that still can
combin e with an articl e to form a NP. We will call the large r unit a noun5 and thus will need a
rule of the form :
Noun  —+ Noun  Noun
For our exampl e compound , the pars e that lead s to a semanticall y sensibl e interpretatio n is
[Noun  [NOUH  [NOUH  POSTSCRIP T language ] code ] [Noun inpu t file] ]
The hardest  part abou t nomina l compound s is specifyin g the semantics . Ou r exampl e can be
paraphrase d as "a file that is used for inpu t and that consist s of code writte n in a languag e
name d POSTSCRIPT. " Clearly , there is a wide variet y in the meanin g associate d with a nomina l
compound . We will use the generi c relatio n NN to stan d for any of the semanti c relation s that
Some grammar s introduc e intermediat e categorie s betwee n nou n and NP.
Sectio n 23.4 . Scalin g Up the Gramma r 707
APPOSITIO NExampl e Nomina l Compoun d
input file
code file
languag e code
POSTSCRIP T languag e
basketbal l shoe s
baby shoe s
alligato r shoe s
designe r shoe s
brake shoe sRelatio n
UsedFor
ConsistsOf
Writtenln
Named
UsedFor
UsedBy
MadeOf
MadeBy
PartOfSemanti c Rule
Vx,y  UsedFor(y,x)  ^ NN(x,y)
Vx,y  ConsistsOf(y,x)  =>  NN(x,y)
\/x,y Writtenln(y,x)  => NN(x,y)
\/x,y  Named(y,x)  => NN(x,y)
Vx,y  UsedFor(y,x)  => NN(x,y)
Vx,y  UsedBy(y,x)  =>  NN(x,y)
Vx,y  MadeOf  (y\x)  =>  NN(x,y}
Vx,y  MadeBy(y,x)  => NN(x,y)
\/x,y  PartOf  (y,x)  => • NN(x,y)
Figur e 23.9 Nomina l compound s and the correspondin g semanti c relationships .
can hold betwee n two noun s in a nomina l compound . We can then writ e logica l rule s that say,
for example , that whe n a UsedFor  relatio n hold s betwee n two objects , we can infe r that a NN
relatio n holds . Som e example s are show n in Figur e 23.9 .
We woul d like a noun phras e like "ever y file input " to get the semanti c interpretation :
[V/ 3 z Input(i)  A File(f)  A AW(z,/) ]
Give n wha t we have alread y decide d abou t representin g the semantic s of NPs and articles , we
can get this if we arrang e for the semantic s of the nomina l compoun d "file input " to be
A/ 3 z Inputii)  A File(f)  A NN(i,f)
And we can get that with the rule
Noun(\y  3x sem\(x)  A seni2(y)  A NN(x,y))  —' Noun(sem\)  Noun(sem­i)
Anothe r complicatio n is that two noun phrase s (not two Nouns)  can be concatenate d togethe r
in a constructio n calle d apposition , in whic h both noun phrase s refer to the same thing . Two
example s are "[ NP the argument ] [NP fd]" and "[ NP the language ] [NP POSTSCRIPT]. " In thes e
examples , the secon d NP is a name , but it need not alway s be. In "[NP Davi d McDonald ] [NP  the
CMU grad ] wrot e abou t nomina l compounds " we are usin g "the CMU grad " to distinguis h this
Davi d McDonal d from the othe r one.6 A simplifie d rule for appositio n is
NP([qx  sem\  A sem 2]) —> NP([qx  sem\])  NP([qxsem 2])
Adjectiv e Phrase s
In £2, adjectives  were only allowe d as complement s to a verb , as in "the wumpu s is smelly"
But of cours e adjectives  also appea r as modifier s befor e a noun , as in "a smelly  wumpus. " The
semantic s of a phras e like this can be forme d by a conjunctio n of the semantic s contribute d by
the adjectiv e and by the noun :
3 w Smelly(w)  A Wumpus(w)
6 Thi s is calle d a restrictiv e appositio n becaus e it restrict s the set of possibl e references . Nonrestrictiv e apposition s just
add new information . The y are ofte n set off by comma s or dashes , as in "Tarzan , lord of the jungle. "
708 Chapte r 23 . Practica l Natura l Languag e Processin g
This is calle d intersectiv e semantics , becaus e the meanin g of "smell y wumpus " is the intersectio n
of smell y thing s and thing s that are wumpuses . Bot h noun s and adjective s are represente d by
predicate s that defin e categories ; this was the first schem e for representin g categorie s show n in
Sectio n 8.4. If all adjective s had intersectiv e semantics , it woul d be easy to handl e them . We
woul d add a rule for Noun,  sayin g it can be compose d of an Adjective  and anothe r Noun:
Noun(Xx  sem\(x)  A seni2(x))  —> Adjective(sem\)  Noun(sem­i)
Unfortunately , the semanti c relatio n betwee n adjectiv e and nou n is ofte n more complicate d than
just intersection . For example :
• A "red book " has a red cover , but not red page s or lettering . A "red pen" has red ink or a
red body . In general , colo r adjective s refer to a major , salient , visibl e part of the object .
• "Re d hair" is orangish , not red. This implie s that the modificatio n is dependen t on both
the noun and the adjective .
• A "red herring " is an irrelevan t distraction , not a fish nor somethin g red. In this case , the
phras e has an idiomati c meanin g that is completel y independen t of its parts .
• A "smal l moon " is bigge r than a "larg e molecule " and is clearl y not the intersectio n of
small thing s and thing s that are moons .
• A "mer e child " is the sam e as a child : you canno t take a grou p of childre n and separat e
them into the childre n and the mere children . The adjectiv e "mere " refer s to the speaker' s
attitud e abou t the noun , and not to actua l propertie s of the noun at all.
• In "allege d murderer, " the adjectiv e agai n says somethin g abou t the attitud e of som e
perso n (not necessaril y the speaker) , but the phras e make s no commitmen t as to whethe r
the referen t actuall y is a murderer .
• "Rea l leather " is no differen t from "leather, " but the adjectiv e is used whe n the listene r
migh t expec t artificia l leather .
• A "fak e gun " is not a gun at all. Its appearanc e is simila r to a gun's , but it lack s the
functiona l properties .
These kind s of semanti c relation s can be handle d by reifyin g the categorie s that were formerl y
represente d as predicates . Thi s is the secon d schem e for representin g categorie s in Sectio n 8.4.
For example , instead  of the predicat e Gun,  we will use the objec t Guns  to represen t the class of all
guns . The n "a gun" is represente d by 3 g (g 6 Guns)  and "a fake gun" is 3 g (g G Fake(Guns).
Determiner s
In Chapte r 22, we showe d how article s such as "a" and "the " can be part of nou n phrases .
DETERMINE R Article s are just one type of the more genera l class of determiner , whic h we abbreviat e as Det.
Determiner s can becom e quit e complicated , as in "[Det my brother' s neighbor's ] dog" and "[oet
all but three of her many ] good friends. " For our £3 language , we allow only one new kind of
determiner , a number , as in "thre e dogs. " We will use the quasi­logica l form [3* Dog(x)]  to
represen t this. This give s us the followin g gramma r rules :
Det(q)  —> Article(q)
Det(q)  —<• Number(q)
NP([qx  noun(x)])  —*• Det(q)  Noun(noun)
Sectio n 23.4 . Scalin g Up the Gramma r 70 9
So far, the rules are simple , and the mappin g from string s to quasi­logica l form is easy . The hard
part is translatin g from quasi­logica l form to logica l form . Her e are two example s of the logica l
form we woul d like to end up with :
Three wome n carrie d a piano .
3 s Cardinality(s)  = 3 A V,w ((w  £ s =>• Woman(w))
A 3/7 Piano(p)/\3e  (e £ Carry\(s,p,Past)))
(There  is a set of  3 women;  this set carried the  piano.)
Three wome n carrie d a baby .
3 s Cardinality(s)  = 3 A V w ((w G s =>• Woman(w))
A 3£ Baby(b)  A 3e (e <E Carry 2(w,p,Past)))
(There  is a set  of 3 women;  each woman  in the  set carried a  baby.)
These example s are ambiguous . In the mos t likel y interpretations , it is the set of women , s,
who are carryin g the pian o togethe r in the first example , wherea s in the secon d example , each
woma n is separatel y carryin g a differen t baby . The subscript s on Carry  indicat e differen t senses .
To accoun t for these two differen t interpretations , we will have to mak e the rule for translatin g
[3 w Woman(w)]  capabl e of equatin g eithe r the variabl e denotin g the set (s) or the variabl e
denotin g the element s of the set (w) with the variabl e representin g the subjec t of the verb phrase .
Noun phrase s revisite d
Now we look at the rules for noun phrases . The rule for pronoun s is unchange d from Chapte r 22,
and the old rule for Article  plus Noun  coul d be update d simpl y by changin g Article  to Del and
includin g case informatio n and agreemen t in perso n and number , yieldin g the followin g rule:
NP(case,  Person(3),  number,  [qx sem(x)])  —> Det(number,  q) Noun(number,  sem)
We stick to the conventio n that the semantics  is alway s the last argument . The case  variabl e
is unbound , indicatin g that the NP can be used in eithe r the subjectiv e or objectiv e case . The
number  variabl e can take on the value s Singular  or Plural,  and the rule says that the Del and Noun
must have the sam e number . For example , "a dog" and "thos e dogs " are grammatica l becaus e
they agre e in number , but "a dogs " and "thos e dog" are ungrammatica l becaus e they disagree .
Note that som e determiner s (like "the" ) and som e noun s (like "sheep" ) can be eithe r singula r or
plural .
Beside s playin g a role insid e the nou n phrase , the number  variabl e is also importan t
externally : in the S —* NP VP rule , the subjec t nou n phras e mus t agre e with the verb phras e
in number  and person.  All noun s are in the third person , whic h we have denote d Person(3).
Pronoun s have more variety ; the pronou n "you, " for example , is in the secon d perso n and "I" is in
the third . Verb s are also marke d for perso n and number . For example , the verb "am" is Singular
and Person(l),  and therefor e "I am" is grammatical , whil e "you am" is not. Her e is a rule that
enforce s subject/ver b agreement :
S(rel(obj))  —> NP(Subject,  person,  number,  obj)  VP(person,  number,  rel)
It is also possibl e to form a noun phras e from a noun with no determiner . Ther e are severa l way s
to do this. One is with a name , such as "John " or "Berkeley. " Ther e are severa l choice s for the
710 Chapte r 23 . Practica l Natura l Languag e Processin g
semantic s of names . The simples t choic e is to represen t "John " with the constan t John.  The n
the representatio n of "Joh n slept " is 3 e e £ Sleep(John,Past).  But this simpl e approac h does
not work in a worl d with more than one thin g calle d John . A bette r representatio n is therefor e
3 e,x e e Sleep([3 ! x Name(x)  = John],  Past).  Her e are rule s that deriv e that representation :
NP(case,Person($),  number,  [3l x Name(x)  = name])  — Name(number,name)
Name(Singular,  John)  — John
A nou n also need s no determine r if it is a mas s nou n (e.g. , "water" ) or a generi c plura l (e.g. ,
"Dog s like bones") . We leav e thes e rules as exercises .
Clausa l complement s
In £2, all the verb s took only nou n phrase s and prepositiona l phrase s as complements . Bu t
some verb s accep t clause s (i.e., sentence s or certai n type s of verb phrases ) as complements . For
example , in "I believ e [he has left], " the objec t of "believe " is a sentence , and in "I wan t [to
go there], " the objec t of wan t is an infinitiv e verb phrase . We can handl e this with the sam e
subcategorizatio n mechanis m we have been usin g (here show n withou t the othe r augmentations) :
VP(subcat)  ­> VP([S\subcat})  S
VP(subcat)  ­^ VP([VP\subcat])  VP
Verb([S])  ­+ believe
Verb([VP])  — want
GAP
FILLE R
LONG­DISTANC E
DEPENDENC YRelativ e clause s
The gramma r of £2 allows  relativ e clause s such as "the perso n [tha t saw me], " in whic h the
relativ e claus e consist s of the word "that " followe d by a VP, and the interpretatio n is that the
head nou n phras e (the person ) is the subjec t of the embedde d VP. In English , it is also possibl e
to have relativ e clause s such as "the perso n [that I saw u]," in whic h a sentenc e follow s the word
"that, " but the sentenc e is missin g an object . The u symbol , whic h we call a gap7 indicate s the
place wher e the head noun phras e (the person ) logicall y woul d appea r to complet e the sentence .
We say that the head noun phras e is the filler of the gap. In this example , the gap is in the direc t
objec t position , but it coul d be neste d farthe r into the relativ e clause . For example , it coul d be
the objec t of a prepositio n (e.g. , "the perso n [tha t I looke d [PP at u]]") or the objec t of a deepl y
neste d clause :
the perso n [that [5 you said [5 you though t [s I gave the book to u]]]]
So far, all the syntacti c relationship s we have seen have been at a singl e leve l in the pars e tree.
The filler­ga p relationshi p is calle d a long­distanc e dependenc y becaus e it reache s dow n a
potentiall y unbounde d numbe r of node s into the pars e tree. The subscript s (i) on pars e node s are
used to show that ther e is an identit y relationship—tha t the recipien t of the book givin g is the
same as "the person " that is the head nou n phrase :
[the person] , [tha t [s you said [s you though t [s I gave the book to u,]]]]
7 Th e gap is calle d a trac e in som e theories .
Sectio n 23.4 . Scalin g Up the Gramma r 71 1
To represen t filler­ga p relationships , we augmen t mos t of the categor y predicate s with a gap
argument . Thi s argumen t says wha t is missin g from the phrase . For example , the sentenc e "I
saw him " has no gaps , whic h we represen t as S(Gap(None)).  The phrase s "usaw him " and "I
saw u" are both represente d as S(Gap(NP)).
To defin e relativ e clauses , all we have to do is say that an NP can be modifie d by followin g
it with a relativ e clause , and that a relativ e claus e consist s of a relativ e pronou n followe d by a
sentenc e that contain s an NP gap. Thi s allow s us to handl e both "the perso n that I saw u" and
"the perso n that usaw him. " Her e are the rules with all othe r augmentation s removed :
NP(gap)  ­> NP(gap)  RelClause
RelClause  ­> Pronoun(Relative)  S(Gap(NP))}
We also have to say that the empt y string , e, comprise s an NP with an NP gap in it:
NP(Gap(NP))  — 6
The rest of the gramma r has to cooperat e by passin g the gap argumen t along ; for example :
S(Gap(Concat(g\,g2)))  — NP(Gap(g {)) VP(Gap(g 2))
Here Concat(g\,g 2) mean s g\ and §2 together . If g\ and g2 are both Gap(None),  then the S as a
whol e has no gap. But if, say, gi is Gap(NP)  and g2 is Gap(None},  then the S has an NP gap.
Question s
In English , there are two main type s of questions :
0 Yes/No : Did you see that ?
<> Wh (gapped) : Wha t did you see u ?
A yes/n o question , as the nam e implies , expect s a yes or no as answer . It is just like a declarativ e
sentence , excep t that it has an auxiliar y verb that appear s befor e the subjec t NP. We call this
PNUvBERsioNUX subject­au x inversio n and use the categor y Sinv  to denot e a sentenc e that has it. If there are
severa l auxiliar y verbs , only the first one come s befor e the subject : "Shoul d you have been seein g
that? " Thus , the gramma r rule s coverin g simpl e yes/n o sentence s are as follows :
S —* Question
Question  —> Sinv
Sinv — Aux NP VP
Wh question s (pronounce d "double­ U H") expec t a noun phras e as an answer . In the simples t
case, they star t with an interrogativ e pronou n (who , what , when , where , why , how) , whic h is
followe d by a gappe d Sinv:
Question  — Pronoun(Interrogative)  Sinv(Gap(NP))
There are also som e less commo n questio n constructions , as these example s show :
0 Echo : You saw what!
0 Raisin g intonation : You see something ?
<C> Yes/N o with "be" : Is it safe ?
0 Wh subject : Wha t is the frequency , Kenneth ?
712 Chapte r 23 . Practica l Natura l Languag e Processin g
0 Wh NP: [Wha t book ] did you read u ?
0 Wh PP: [Wit h what ] did you see it u ?
<C> Wh PP: [Whence ] did he com e u ?
Echo question s can also be answere d with a noun phrase , but they are normall y used rhetoricall y
to expres s the speaker' s amazemen t at wha t was just said . For example , "I saw a 30­foot­hig h
purpl e cow" is answere d by "You saw whatT  Sentence s with norma l declarativ e structur e can
be mad e into question s with the use of raisin g intonatio n at the end of the sentence . Thi s is
uncommo n in writte n text. The verb "to be" is the only verb that can stan d by itsel f (withou t
anothe r verb ) in an inverte d yes/n o question . Tha t is, we can say "Is it safe? " but not "Seem s it
safe? " or "Did they it?" In som e dialects , "have " can be used , as in "Hav e you the time? " or
"Hav e you any wool? " Finally , it is possibl e to have an Sinv  with a prepositiona l phras e gap by
prefacin g it with a prepositiona l wh phras e like "from where " or "whence. "
Handlin g agrammatica l string s
No matte r how thoroug h the grammar , there will alway s be string s that fall outsid e it. It doesn' t
much matte r if this happen s becaus e the strin g is a mistak e or becaus e the gramma r is missin g
something . Eithe r way , the speake r is tryin g to communicat e somethin g and the syste m mus t
proces s it in som e way . Thus , it is the hearer' s job to interpre t a strin g somehow , even if it is
not completel y grammatical . For example , if a characte r in a myster y nove l suddenl y keels over
but manage s to gasp "Poison—Butler—Martini, " mos t peopl e woul d be able to com e up with
a good interpretatio n of these dyin g words : the butle r is the agen t of the poisonin g actio n of
whic h the speake r is the victi m and the martin i is the instrument . We arriv e at this interpretatio n
by considerin g the possibl e semanti c relation s that coul d link each componen t together , and
choosin g the best one. It woul d not do to say "I'm sorry , that' s not a grammatica l sentence ; coul d
you pleas e rephras e it?"
23.5 AMBIGUIT Y
In this chapter , we have extende d the rang e of syntacti c construction s and semanti c representation s
that we can handle . Thi s help s us cove r a wide r rang e of language , but it also make s the job
of disambiguatio n harder , becaus e ther e are mor e possibilitie s to choos e from . Finding  the
right  interpretation  involves  reasoning  with uncertainty  usin g the evidenc e provide d by lexical ,
syntactic , semantic , and pragmati c sources .
Historically , mos t approache s to the disambiguatio n proble m have been base d on logica l
inferenc e with no quantitativ e measure s of certainty . In the last few years , the trend has been
towar d quantitativ e probabilisti c model s such as belie f network s and hidde n Marko v models .
Belie f network s provid e an answe r to one hard part of the problem—ho w to combin e
evidenc e from differen t sources . Bu t we are still left with two mor e problems : decidin g wha t
evidenc e to put into the network , and decidin g wha t to do with the answer s that com e back . We
Sectio n 23.5 . Ambiguit y 71 3
• ­ftig'''  star t by lookin g at severa l source s of evidence . It is important  to distinguish  between  sources  of
' 1S* evidence  and sources  of ambiguity.  It is quit e possibl e that a syntacti c ambiguit y is resolve d by
semanti c evidence . For example , in "I ate spaghett i and meatball s and salad, " there is a syntacti c
ambiguit y (is it [NP spaghett i and meatballs ] or [NP meatball s and salad]? ) tha t is resolve d by
nonsyntacti c evidence : that spaghett i and meatball s is a commo n dish , wherea s meatball s and
salad is not.
Syntacti c evidenc e
Modifier s such as adverb s and prepositiona l phrase s caus e a lot of ambiguity , becaus e they can
attach to severa l differen t heads . For example , in
Lee aske d Kim to tell Toby to leave on Saturday .
the adver b "Saturday " can modif y the asking , the telling , or the leaving . Psychologica l studie s
show that in the absenc e of othe r evidence , ther e is a preferenc e to attac h such modifier s to the
most recen t constituen t (in this case , the leaving) . So this is a syntacti c ambiguit y that can be
resolve d by syntacti c evidence .
Lexica l evidenc e
Many word s are ambiguous , but not all sense s of a wor d are equall y likely . Whe n aske d wha t
the wor d "pen " means , mos t peopl e will say first that it is a writin g instrument . However , it has
two othe r fairl y commo n sense s meanin g an enclosur e for animal s and a penitentiary . The sense s
meanin g a femal e swa n and the interna l shel l of a squi d are mor e obscure , know n mostl y to
specialist s in biolog y (or specialist s in ambiguity) . As anothe r example , conside r the following :
Lee positione d the dres s on the rack .
Kim wante d the dress on the rack .
Here "on the rack " is best interprete d as modifyin g the verb in the first sentenc e and the dres s in
the second . Tha t is, Lee is puttin g the dres s on the rack (not movin g a dres s that is on the rack) ,
and Kim want s to have a dres s that is on the rack (and does not wan t the dres s to be on the rack) .
In both cases , eithe r interpretatio n is semanticall y plausible . So this is a syntacti c ambiguit y that
is resolve d by lexica l evidence—b y the preferenc e of the verb for one subcategorization .
Semanti c evidenc e
The a prior i probabilit y of a word sens e is usuall y less importan t than the conditiona l probabilit y
in a give n context . Conside r the following :
ball, diamond , bat, base
Even withou t any syntacti c structur e to spea k of, it is easy to pick out the basebal l sense s of
each of thes e word s as bein g the intende d interpretation . Thi s is true even thoug h the a prior i
probabilit y for "diamond " favor s the jewe l interpretation . So this is a case of lexica l ambiguit y
resolve d by semanti c evidence .
714 Chapte r 23 . Practica l Natura l Languag e Processin g
As anothe r example , here are five sentences , alon g with the mos t likel y interpretatio n of the
relatio n represente d by the word "with. " Eac h of the sentence s has a lexica l ambiguit y ("with "
has severa l senses ) and a syntacti c ambiguit y (the PP can attac h to eithe r "spaghetti " or "ate") ,
but each is resolve d by semanti c evidence .
Sentenc e
I ate spaghett i with meatballs .
I ate spaghett i with salad .
I ate spaghett i with abandon .
I ate spaghett i with a fork .
I ate spaghett i with a friend .Relatio n
(ingredien t of spaghetti )
(side dish of spaghetti )
(manne r of eating )
(instrumen t of eating )
(accompanie r of eating )
It is certainl y possible  to use meatball s as a utensi l with whic h to eat spaghetti , but it is unlikel y
(not to mentio n messy) , so we prefe r interpretation s that refer to mor e likel y events . Of course ,
likelines s of event s or situation s is not the only factor . In Chapte r 22, we saw that the righ t
interpretatio n of "I am not a crook " is not the mos t likel y situation . It is perfectl y coheren t to
use languag e to talk abou t unlikel y or even impossibl e situations : "I ran a mile in one minute " or
"This squar e is a triangle. "
Metonym y
METONYM Y A  metonym y is a figur e of speec h in whic h one objec t is used to stan d for another . Whe n we
hear "Chrysle r announce d a new model, " we do not interpre t it as sayin g that companie s can talk;
rathe r we understan d that a spokesperso n representin g the compan y mad e the announcement .
Metonym y is commo n in man y kind s of text , and ofte n goes unnotice d by huma n readers .
Unfortunately , our gramma r as it is writte n is not so facile . To handl e the semantic s of metonym y
properly , we need to introduc e a whol e new leve l of ambiguity . We do this by providin g two
object s for the semanti c interpretatio n of ever y phras e in the sentence : one for the objec t that
the phras e literall y refer s to (Chrysler) , and one for the metonymi c referenc e (the spokesperson) .
We then have to say that there is a relatio n betwee n the two. In our curren t grammar , "Chrysle r
announced " gets interprete d as
3 x, e Chrysler(x)  A e € Announce(x,  Past)
We need to chang e that to
3m,x,e  Chrysler(x)  A Metonymy(m,  x) A e E Announce(m,  Past)
This give s a representatio n for the problem , but not a solution . The next step is to defin e wha t
kinds of metonym y can occur , that is, to defin e constraint s on the Metonymy  relation . Th e
simples t case is whe n there is no metonym y at all—th e litera l objec t x and the metonymi c objec t
m are identical :
V m, x (m = x) =^ Metonymy(m,  x)
For the Chrysle r example , a reasonabl e generalizatio n is that an organizatio n can be used to stand
for a spokesperso n of that organization :
V m, x Organization(x)  A Spokesperson(m,  x) => Metonymy(m,  x)
Sectio n 23.6 . Discours e Understandin g 71 5
Othe r metonymie s includ e the autho r for the work s (I read Shakespeare ) or more generall y the
produce r for the produc t (I drive a Honda ) and the part for the whol e (The Red Sox need a stron g
arm) . Othe r example s of metonymy , such as "The ham sandwic h on Tabl e 4 want s anothe r beer, "
are somewha t harde r to classify .
Metapho r
METAPHO R A  metapho r is a figur e of speec h in whic h a phras e with one litera l meanin g is used to sugges t
a differen t meanin g by way of an analogy . Mos t peopl e thin k of metapho r as a tool used by
poets that does not play a large role in everyda y text . However , ther e are a numbe r of basic
metaphor s that are so commo n that we do not even recogniz e them as such . One such metapho r
is the idea that more  is up. Thi s metapho r allow s us to say that price s have risen , climbed , or
skyrocketed , that the temperatur e has dippe d or fallen , that one' s confidenc e has plummeted , or
that a celebrity' s popularit y has jumpe d or soared .
There are two way s to approac h metaphor s like this. One is to compil e all knowledg e of
the metapho r into the lexicon—t o add new sense s of the word s rise, fall, climb , and so on, that
describ e them as dealin g with quantitie s on any scal e rathe r than just altitude . Thi s approac h
suffice s for man y applications , but it does not captur e the generativ e characte r of the metapho r
that allows  human s to use new instance s such as "nose­dive " withou t fear of misunderstanding .
The secon d approac h is to includ e explici t knowledg e of commo n metaphor s and use them to
interpre t new uses as they are read . For example , suppos e the syste m know s the "mor e is up"
metaphor . Tha t is, it know s that logica l expression s that refer to a poin t on a vertica l scale can be
interprete d as bein g abou t correspondin g point s on a quantit y scale . The n the expressio n "sale s
are high " woul d get a litera l interpretatio n alon g the lines of Altitude(Sales,  High),  whic h coul d
be interprete d metaphoricall y as Quantity(Sales,Much).
23.6 DISCOURS E UNDERSTANDIN G
DISCOURS E I n the technica l sense , a discours e or text is any strin g of language , usuall y one that is more than
TEXT on e sentenc e long . Novels , weathe r reports , textbooks , conversations , and almos t all nontrivia l
uses of languag e are discourses . So far we hav e largel y ignore d the problem s of discourse ,
preferrin g to dissec t languag e into individua l sentence s that can be studie d in vitro.  In this
section , we stud y sentence s in their nativ e habitat .
To produc e discourse , a speake r mus t go throug h the standar d step s of intention , genera ­
tion, and synthesis . Discours e understandin g include s perception , analysi s (and thus syntactic ,
semantic , and pragmati c interpretation) , disambiguation , and incorporation . The hearer' s state of
knowledg e play s a crucia l part in arrivin g at an understanding—tw o agent s with differen t knowl ­
edge may well understan d the sam e text differently . Discours e understandin g can be modelle d
crudel y by the followin g equation :
KB' = DlSCOURSE­UNDERSTANDING^etf,/^ )
716 Chapte r 23 . Practica l Natura l Languag e Processin g
wher e KB is the hearer' s knowledg e base , and KB' is the hearer' s knowledg e base afte r incorpo ­
ratin g the text. The differenc e betwee n KB and KB' is the hearer' s understandin g of the text. At
least six type s of knowledg e come into play in arrivin g at an understanding :
1. Genera l knowledg e abou t the world .
2. Genera l knowledg e abou t the structur e of coheren t discourse .
3. Genera l knowledg e abou t synta x and semantics .
4. Specifi c knowledg e abou t the situatio n bein g discussed .
5. Specifi c knowledg e abou t the belief s of the characters .
6. Specifi c knowledg e abou t the belief s of the speaker .
Let us look at an exampl e discourse :
John wen t to a fanc y restaurant .
He was please d and gave the waite r a big tip.
He spen t $50.
A prope r understandin g of this discours e woul d includ e the fact that John ate a fanc y mea l at the
restaurant , that the waite r was employe d by the restaurant , and that John paid som e of the $50 to
the waite r and mos t of it to the restaurant . We'l l call this understandin g (a). All the inference s
seem obvious , but to get them righ t we need to know a lot and appl y the knowledg e in just the
right way. To understan d why this is hard , we give an alternativ e understandin g (b):
John ducke d into a fanc y restauran t to ask directions . He was please d that they were
able to help him. Bac k on the street , John bumpe d into a man that he had met at
a party the othe r night . All John coul d remembe r was that the man was a waite r at
anothe r restauran t and that he was intereste d in gettin g a new radio . Joh n gave the
man a tip that there was a grea t sale goin g on at the stere o store dow n the block . The
man spen t $50 on a radio .
This is a situatio n that coul d conceivabl y be describe d by our three­sentenc e discourse . It is
far­fetche d for two reasons : First , the situatio n in (a) has a highe r a prior i probability—peopl e
go to restaurant s to eat more ofte n than to ask for directions . Second , the situatio n in (a) is more
probabl e give n the text . A rationa l speake r woul d not expec t a heare r to extrac t understandin g
(b) from the discourse . To see why (a) is better , let us look at it piec e by piece :
• Joh n ate a fanc y meal at the restaurant .
To get this require s goin g beyon d disambiguatio n and into incorporation . Ther e is certainl y
no part of the discours e that mention s the eatin g explicitly , but it still shoul d be part of
the update d knowledg e base that the heare r come s awa y with . To get it, the heare r has to
know that restaurant s serv e meals , and thus that a reaso n for goin g to a restauran t is to eat.
The heare r also know s that fanc y restaurant s serv e fanc y meals , and that $50 is a typica l
price for such a meal , and that payin g and leavin g a tip are typicall y done afte r eatin g a
restauran t meal . Beside s this genera l knowledg e abou t the world , it also help s if the heare r
know s that discourse s are commonl y structure d so that they describ e som e steps of a plan
for a character , but leav e out step s that can be easil y inferre d from the othe r steps.  Fro m
this, the heare r can infe r from the first sentenc e that John has adopte d the eat­at­restauran t
plan, and that the eat­mea l step probabl y occurre d even if it was not mentioned .
Sectio n 23.6 . Discours e Understandin g 717
The waite r was employe d by the restaurant .
Again , thi s is a mixtur e of genera l worl d knowledge—tha t restaurant s emplo y waiters—an d
knowledg e abou t discours e conventions—tha t the definit e articl e "the" is used for object s
that have been mentione d before , or at least  have been implicitl y allude d to; in this case ,
by the eat­at­restauran t plan .
John paid som e of the $50 to the waite r and mos t of it to the restaurant .
This is anothe r exampl e of identifyin g a step in a currentl y activ e plan that matche s a
sentenc e in the text , and unifyin g them to arriv e at the interpretatio n that "He" is John , and
that the recipient s of the deal are the restauran t and waiter .
The structur e of coheren t discours e
In logic , conjunctio n is commutative , so there is no differenc e betwee n P A Q A R and R A P A Q.
But this is certainl y not true of natura l languages . Conside r the followin g two discourses , whic h
contai n the same sentence s in differen t order :
SEGMENT S
COHERENC E
RELATIO NDiscours e (A)
I visite d Paris .
I bough t you som e expensiv e cologne .
Then I flew home .
I wen t to Kmart .
I bough t some underwear .Discours e (B)
I visite d Paris .
Then I flew home .
I wen t to Kmart .
I bough t you som e expensiv e cologne .
I bough t som e underwear .
In (A), the preferre d interpretatio n is that the cologn e come s from Paris , wherea s in (B) it come s
from Kmart . Both discourse s have a sequentia l structur e in whic h the sentence s that come later in
the discours e also occu r later in time . But there is mor e to the understandin g of these discourse s
than just tempora l ordering . In (A), we understan d that a (or the) reaso n for goin g to Kmar t was
to buy the underwear . Similarly , we understan d that visitin g Paris enable d the speake r to buy
cologn e there , but that was not necessaril y the purpos e of the trip.
We need a theor y of how discourse s are put together . We will say that discourse s are
compose d of segments , wher e a segmen t can be a clause , a complet e sentence , or a grou p of
severa l consecutiv e sentences . Ther e are severa l theorie s buil t on the idea that each segmen t of
the discours e is relate d to the previou s one by a coherenc e relatio n that determine s the role that
each segmen t play s in the unfoldin g discourse . For example , the coherenc e relatio n betwee n the
first two sentence s of (A) is one of enablement . Coherence  relations  serve  to bind  the discourse
together.  A speake r know s that she can exten d a discours e by addin g a sentenc e that stand s in a
coherenc e relatio n to the existin g text, and the heare r know s that in additio n to interpretin g and
disambiguatin g each sentence , he is suppose d to recove r the coherenc e relation s that bind the
segment s together . This mean s that the heare r of a discours e is doin g more work than the heare r
in Chapte r 22, who only had to worr y abou t interpretin g and disambiguatin g a singl e sentence .
But it also mean s that the heare r of a discours e has som e help , becaus e the coherenc e relation s
constrai n the possibl e meaning s of each sentence . So even thoug h the individua l sentence s may
have man y possibl e meanings , the discours e as a whol e will have few meaning s (preferabl y one) .
718 Chapte r 23 . Practica l Natura l Languag e Processin g
We presen t the theor y by Hobb s (1990),  whic h start s with the observatio n that the speake r
does four thing s in puttin g togethe r a discourse :
• The speake r want s to conve y a message .
• The speake r has a motivatio n or goal in doin g this.
• The speake r want s to mak e it easy for the heare r to understand .
• The speake r mus t link the new informatio n to wha t the heare r alread y knows .
A sentenc e is a coheren t extensio n of a discours e if it can be interprete d by the heare r as bein g
in servic e of one of thes e four points . Let us look at som e example s of each of the four as they
appea r in the followin g discourse :
(1) A funn y thin g happene d yesterday .
(2) John wen t to a fanc y restaurant .
(3) He ordere d the duck .
(4) The bill cam e to $50.
(5) John got a shoc k whe n the waite r cam e to collec t the bill.
(6) He realize d he didn' t have a cent on him .
(7) He had left his walle t at home .
(8) The waite r said it was all righ t to pay later .
(9) He was very embarrasse d by his forgetfulness .
Sentenc e (1) describe s wha t the speake r want s to talk about— a funn y thing . It is a meta­comment ,
an evaluatio n by the speake r of wha t the comin g messag e holds . Onc e the speake r has mad e it
clear that she has the goal of describin g this , it is coheren t to add (2), whic h start s to describ e
the funn y thing . The coherenc e relatio n betwee n (1) and (2) is that utterin g (2) is part of the
speaker' s plan that was implicitl y allude d to by (1). Mor e formally , we can say that two adjacen t
discours e segment s 5, and S/ stan d in the evaluatio n coherenc e relation , if from S1, we can infe r
that Sj is a step in the speaker' s plan for achievin g som e discours e goal .
Sentence s (3) and (4) are coheren t becaus e they can be interprete d as step s in John' s plan
of eatin g a meal . Sentence s (2) and (3) stan d in the enablemen t coherenc e relation , and (3)
and (4) are in the causa l relation . Bot h relation s arise from the speaker' s goal of conveyin g a
messag e abou t the world . Thus , we see that understandin g discours e involve s two level s of plan
recognition —recognizin g the speaker' s plan s and the characters ' plans .
Sentence s (5) and (6) overla p in their content : the shoc k is the realization , but it is describe d
in a differen t way. We say that (5) and (6) stand in the elaboratio n relation . This is an exampl e
of the speake r makin g it easie r for the heare r by lessenin g the amoun t of inferenc e the heare r
has to make . Sentenc e (6) coul d well have been left out of the discourse , but then the jum p from
(5) to (7) woul d have been a little harde r to make . Not e that once we recogniz e that (6) is an
elaboratio n of (5), we can infe r that "he" in (6) refer s to John . Withou t the coherenc e relation ,
we migh t be tempte d to assum e that "he" refer s to the waiter , who was mentione d more recently .
The relatio n betwee n (6) and (7) is one of explanation : the reaso n John didn' t have a cent
on him is becaus e he left his walle t at home . Thi s is one case wher e a sentenc e that appear s
later in the discours e actuall y occur s earlie r in the real world . This is an exampl e of the speake r
linkin g a new explanatio n to the hearer' s existin g knowledge .
Sentenc e (9) stand s in a causa l relatio n with the discours e segmen t comprise d of (5) and
(6). Recognizin g this allow s us to interpre t "he" in (9) as John , not the waiter .
Sectio n 23.7 . Summar y 71 9
There have been severa l catalog s of coherenc e relations . Man n and Thompso n (1983 )
give a mor e elaborat e one that include s solutionhood , evidence , justification , motivation , reason ,
sequence , enablement , elaboration , restatement , condition , circumstance , cause , concession ,
background , and thesis­antithesis .
The theor y of Gros z and Sidne r (1986 ) also account s for wher e the speake r and hearer' s
ATTENTIO N attentio n is focuse d durin g the discourse . Thei r theor y include s a pushdow n stac k of focu s
spaces . Certai n utterance s caus e the focu s to shif t by pushin g or poppin g element s off the stack .
For example , in Discours e (A), the sentenc e "I visite d Paris " cause s a new focu s to be pushe d
on the stack.  Withi n that focus , the speake r can use definit e description s to refe r to, say, "the
museums " or "the cafes. " The sentenc e "The n I flew home " woul d caus e the focu s on Paris to be
poppe d from the stack ; from that poin t on, the speake r woul d need to use an indefinit e descriptio n
such as "the cafe I wen t to in Paris " rathe r than a simpl e definit e description . Ther e is also a more
specifi c notio n of loca l focu s that affect s the interpretatio n of pronouns .
23.7 SUMMAR Y
In this chapter , we have seen wha t can be done with state­of­the­ar t natura l languag e processing ,
and wha t is involve d in gettin g there . The main point s were as follows :
• Natura l languag e processin g technique s mak e it practica l to develo p program s that mak e
querie s to a database , extrac t informatio n from texts , retriev e relevan t document s from a
collection , translat e from one languag e to another , or recogniz e spoke n words . In all thes e
areas , ther e exis t program s that are useful , but ther e are no program s that do a thoroug h
job in an open­ende d domain .
• It is possibl e to pars e sentence s efficientl y usin g an algorith m that is carefu l to save its
work , avoi d unnecessar y work , and pack the result s into a fores t rathe r than an exhaustiv e
list of trees .
• In recen t years , ther e has bee n a shif t of emphasi s from the gramma r to the lexicon .
Buildin g a lexico n is a difficul t task.
• Natura l language s have a huge variet y of syntacti c forms . Nobod y has yet capture d them
all, but we can exten d the simpl e gramma r of the last chapte r to give mor e complet e
coverag e of Englis h sentences .
• Choosin g the righ t interpretatio n of a sentenc e in the situatio n in whic h it was uttere d
require s evidenc e from man y sources , includin g syntactic , lexical , and semantic .
• Mos t of the interestin g languag e come s in connecte d discours e rathe r than in isolate d
sentences . Coherenc e relation s constrai n how a discours e is put together , and thus constrai n
the possibl e interpretation s it has.
720 Chapte r 23 . Practica l Natura l Languag e Processin g
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
Lock e and Boot h (1955 ) presen t the very early protohistor y of machin e translation , and captur e the
enthusias m of the early days . Late r disillusionmen t with machin e translatio n is describe d by Bar­
Hille l (1960) ; Lindsa y (1963 ) also point s out some of the obstacle s to machin e translatio n havin g
to do with the interactio n betwee n synta x and semantics , and the need for worl d knowledge .
Quinla n and O'Brie n (1992 ) describ e TAUM­METEO . Voorhee s (1993 ) report s som e recen t
application s base d on Wordnet .
One proble m with the prototyp e natura l languag e interface s we describe—LUNA R (Woods ,
1972) , CHA T (Pereira , 1983) , and TEA M (Gros z et al, 1987) , is that they are necessaril y incom ­
plete . In the mid­1980s , severa l companie s investe d the effor t to build seriou s natura l languag e
databas e acces s programs . Artificia l Intelligenc e Corporation' s INTELLEC T (Harris , 1984 ) is one
of the best known . Wit h a decad e of tunin g it perform s bette r than a prototype , but still has
the difficul t proble m of revealin g to the user its rang e of competence . The NL­MEN U syste m
(Tennan t et al., 1983 ) mitigate s this proble m by usin g a mod e of interactio n in whic h the user
build s a quer y by selectin g word s and phrase s from a serie s of on­scree n menu s instea d of typing .
In othe r words , the syste m reveal s wha t it is capabl e of acceptin g rathe r than attemptin g to handl e
anythin g the user throw s at it.
The fundamenta l advance s in efficient , genera l parsin g method s were mad e in the late
1960s , with a few twist s sinc e then (Kasami , 1965 ; Younger , 1967 ; Barley , 1970 ; Graha m et al.,
1980) . Maxwel l and Kapla n (1993 ) show how char t parsin g with non­context­fre e grammar s can
be mad e efficien t in the averag e case. Churc h and Patil (1982 ) introduc e some refinement s in the
resolutio n of syntacti c ambiguity . A numbe r of researcher s have attempte d to use quantitativ e
measure s of the goodnes s of syntacti c and semanti c fit in parsin g and semanti c interpretation .
This can eithe r be done withi n a basicall y deductiv e framewor k (Hobb s et al., 1993 ) or withi n
the framewor k of Bayesia n belie f networ k reasonin g (Charnia k and Goldman , 1992 ; Wu, 1993) .
Nunber g (1979 ) gives an outlin e of a computationa l mode l of metonymy . Lakof f and John ­
son (1980 ) give an engagin g analysi s and catalo g of commo n metaphor s in English . Orton y (1979 )
present s a collectio n of article s on metaphor ; Helma n (1988 ) focuse s on analogica l reasoning , but
also contain s a numbe r of article s abou t the phenomeno n of metaphor . Marti n (1990 ) present s a
computationa l approac h to metapho r interpretation .
Kukic h (1992 ) survey s the literatur e on spellin g correction .
Hobb s (1990 ) outline s the theor y of discours e coherenc e on whic h this chapter' s expositio n
is based . Man n and Thompso n (1983 ) give a simila r catalo g of coherenc e relations . Grime s
(1975 ) lays the groundwor k for muc h of this work . Gros z and Sidne r (1986 ) presen t a theor y of
discours e coherenc e base d on shiftin g focu s of attention . Joshi , Webber , and Sag (1981 ) collec t
importan t earl y wor k in the field . Webbe r present s a mode l of the interactin g constraint s of
synta x and discours e on wha t can be said at any poin t in the discours e (1983 ) and of the way verb
tense interact s with discours e (1988) . The idea of trackin g the characters ' goal s and plan s as a
mean s of understandin g storie s was first studie d by Wilensk y (Wilensky , 1983) . A plan­base d
mode l of speec h acts was suggeste d first by Cohe n and Perraul t (1979) . Cohen , Morgan , and
Pollac k (1990 ) collec t more recen t wor k in this area .
Sectio n 23.7 . Summar y 721
EXERCISE S
23.1 Rea d the followin g text once for understandin g and remembe r as muc h of it as you can.
There will be a test later .
The procedur e is actuall y quit e simple . Firs t you arrang e thing s into differen t groups . Of
course , one pile may be sufficien t dependin g on how muc h there is to do. If you have to go
somewher e else due to lack of facilitie s that is the next step , otherwis e you are prett y well
set. It is importan t not to overd o things . Tha t is, it is bette r to do too few thing s at once
than too many . In the shor t run this may not seem importan t but complication s can easil y
arise. A mistak e is expensiv e as well . At first the whol e procedur e will seem complicated .
Soon , however , it will becom e just anothe r facet of life. It is difficul t to forese e any end to the
necessit y for this task in the immediat e future , but then one can neve r tell. Afte r the procedur e
is complete d one arrange s the materia l into differen t group s again . The n they can be put into
their appropriat e places . Eventuall y they will be used once mor e and the whol e cycl e will
have to be repeated . However , this is part of life.
23.2 Describ e how a simpl e pseudo­natural­languag e (template­based ) explanatio n facilit y can
be buil t for a vanilla , backward­chaining , logica l reasonin g system . The explanatio n facilit y
shoul d be writte n as a progra m WH Y that generate s an explanatio n after ASK has answere d a
questio n from the user . The explanatio n shoul d consis t of a descriptio n of the premise s and
inferenc e metho d used to reac h the conclusion ; the user shoul d be able to furthe r quer y the
premise s to see how they were derived .
23.3 Withou t lookin g back at Exercis e 23.1 , answe r the followin g questions :
• Wha t are the four steps that are mentioned ?
• Wha t step is left out?
• Wha t is "the material " that is mentione d in the text?
• Wha t kind of mistak e woul d be expensive ?
• Is it bette r to do too few or too many ?
• Why ?
23.4 Ope n any boo k or magazin e to a rando m pag e and writ e dow n the firs t 20 nomina l
compound s you find . Characteriz e the semanti c relation s (e.g. , made­of , used­for , etc.) .
23.5 Ope n any book or magazin e to a rando m page and copy dow n the first 10 sentences . How
many of them are in £3 ? Sho w the parse s of the sentence s that are, and explai n wha t wen t wron g
for the sentence s that are not.
23.6 Wor k out the gramma r rule s for possessiv e nou n phrases . Ther e are three part s to this, (a)
Write a rule of the form Del  —> NP(Case(Possessive)).  (b) Mak e sure this rule combine s with
the NP  —»• Det Noun  rule to produc e the righ t semantics , (c) Writ e a rule that build s a possessiv e
722 Chapte r 23 . Practica l Natura l Languag e Processin g
NP usin g the 's ending . You will first have to decid e if 's attache s to a noun or NP. To help you
do this, conside r at least the followin g examples :
the Quee n of England' s speec h
the attorne y general' s decisio n
the man I saw yesterday's,nam e
the man and woman' s hous e
Russel l and Norvig' s text
23.7 Collec t som e example s of time expressions , suc h as "two o'clock, " "midnight, " and
"12:46. " Als o thin k up som e example s that are ungrammatical , such as "thirtee n o'clock " or
"half past two fifteen. " Writ e a gramma r for the time language .
23.8 In this exercise , you will writ e rule s for nou n phrase s consistin g of a nou n with no
determiner . Conside r these examples :
a. Dog s like bones .
b. I ate rice.
c. Gol d is valuable .
d. I saw som e gold in 2,2.
e. I saw gold in 2,2.
Writ e dow n the semantic s for each of these sentences . The n use that to writ e dow n lexica l entrie s
for "gold " and "rice, " and to writ e a rule (or rules ) of the form NP ­^ Noun.
23.9 W e said that 3 e, x e E Sleep(John,  Past)  A Name(x)  = John  was a plausibl e interpretatio n
for "Joh n slept. " But it is not quit e right , becaus e it blur s the distinctio n betwee n "Joh n slept "
and "Som e perso n name d John slept. " The poin t is that the forme r sentenc e presuppose s that
speake r and heare r agre e on whic h John is bein g talke d about . Writ e gramma r and lexica l rules
that will distinguis h the two examples .
23.10 Writ e gramma r rule s for the categor y Adjp,  or adjectiv e phrase , usin g reifie d categories .
Show how to deriv e 3 g (g £ Fake(Guns)  as the semantic s of "a fake gun. " An adjectiv e phras e
can be eithe r a lone adjectiv e (big),  a conjunctio n (big and  dumb),  or an adjectiv e phras e modifie d
by an adjectiv e or adver b (light  green  or very  dumb).
23.11 On e way to defin e the task of spellin g correctio n is this : give n a misspelle d wor d and a
dictionar y of correctl y spelle d words , find the word(s ) in the dictionar y that can be transforme d
into the misspelle d wor d in one insertion , deletion , substitution , or transposition . Give n a
dictionar y of w word s and a misspelle d wor d that is k letter s long , give the averag e case time
complexit y of spellin g correctio n for a dictionar y implemente d as (a) a hash table , (b) a b­tree ,
and (c) a trie.
23.12 W e forgo t to mentio n that the title of the text in Exercis e 23.1 is "Washin g Clothes. " Go
back and rerea d the text, and answe r the question s in Exercis e 23.3 . Did you do bette r this time ?
Bransfor d and Johnso n (1973 ) used this text in a better­controlle d experimen t and foun d that the
title helpe d significantly . Wha t does this tell you abou t discours e comprehension ?
Sectio n 23.7 . Summar y 72 3
23.13 Implemen t a versio n of the chart­parsin g algorith m that return s a packe d tree of all edge s
that span the entir e input .
23.14 Implemen t a versio n of the chart­parsin g algorith m that return s a packe d tree for the
longes t leftmos t edge , and then if that edge does not span the whol e input , continue s the pars e
from the end of that edge . Sho w why you will need to call PREDIC T befor e continuing . The fina l
resul t is a list of packe d trees such that the list as a whol e span s the input .
24PERCEPTIO N
In which  we connect  the computer  to the raw,  unwashed  world.
24.1 INTRODUCTIO N
Perceptio n provide s agent s with informatio n abou t the worl d they inhabit . Perceptio n is initiate d
SENSOR S b y sensors . A senso r is anythin g that can chang e the computationa l state of the agen t in respons e
to a chang e in the state of the world . It coul d be as simpl e as a one­bi t senso r that detect s whethe r
a switc h is on or off, or as comple x as the retin a of the huma n eye, whic h contain s more than a
hundre d millio n photosensitiv e elements .
There are a variet y of sensor y modalitie s that are availabl e to artificia l agents . Thos e they
share with human s includ e vision , hearing , and touch . In this chapter , our focu s will be on vision ,
becaus e this is by far the mos t usefu l sens e for dealin g with the physica l world . Hearin g in the
contex t of speec h recognitio n is also covere d briefl y in Sectio n 24.7 . Touch , or tactil e sensing ,
is discusse d in Chapte r 25, wher e we examin e its use in dextrou s manipulatio n by robots .
We will not have all that muc h to say abou t the desig n of sensor s themselves . The main
focus will be on the processin g of the raw informatio n that they provide . The basi c approac h
taken is to first understan d how sensor y stimul i are create d by the world , and then to ask the
l»­jSi p followin g question : if sensory  stimuli  are produced  in such  and such  a way  by the world,  then
I'*"S what  must  the world  have  been  like to produce  this particular  stimulus?  We can writ e a crud e
mathematica l analogu e of this question . Let the sensor y stimulu s be S, and let W be the worl d
(wher e W will includ e the agen t itself) . If the function / describe s the way in whic h the worl d
generate s sensor y stimuli , then we have
S =f(W)
Now , our questio n is: given / and S, wha t can be said abou t W? A straightforwar d approac h
woul d try to calculat e wha t the worl d is like by invertin g the equatio n for generatin g the stimulus :
W=f­\S)
724
Sectio n 24.2 . Imag e Formatio n 725
MANIPULATIO N
NAVIGATIO N
OBJEC T
RECOGNITIO NUnfortunately, / does not have a prope r inverse . For one thing , we canno t see aroun d corners ,
so we canno t recove r all aspect s of the curren t worl d state from the stimulus . Moreover , even
the part we can see is enormousl y ambiguous . A key aspec t of the stud y of perceptio n is to
understan d wha t additiona l informatio n can be brough t to bear to resolv e ambiguity .
A second , and perhap s mor e important , drawbac k of the straightforwar d approac h is that
it is tryin g to solv e too difficul t a problem . In man y cases , the agen t does not need  to know
everythin g abou t the world . Sometimes , just one or two predicate s are needed—suc h as "Is there
any obstacl e in fron t of me?" or "Is that an electrica l outle t over there? "
In orde r to understan d wha t sorts of processin g we will need to do, let us look at som e of
the possibl e uses for vision :
<> Manipulation : grasping , insertion , and so on, need loca l shap e informatio n and feedbac k
("gettin g closer , too far to the right,... ) for moto r control .
<> Navigation : findin g clear paths , avoidin g obstacles , and calculatin g one' s curren t velocit y
and orientation .
0 Objec t recognition : a usefu l skill for distinguishin g betwee n tasty mice and dangerou s
carnivores ; edibl e and inedibl e objects ; clos e relation s and strangers ; ordinar y vehicles ,
Volvos . and polic e cars.
None of these application s require s the extractio n of complet e description s of the environment .
This chapte r is organize d as follows . In Sectio n 24.2 , we stud y the proces s of imag e
formation . We cove r both the geometr y of the process , whic h dictate s wher e a give n poin t
will be imaged , and the photometr y of the process , whic h dictate s how brigh t the poin t will
appear . Sectio n 24.3 treat s the basi c image­processin g operation s commonl y used in earl y
vision . They set the stage for later analysi s that extract s the informatio n neede d for task s such as
manipulation , navigation , and recognition . In Sectio n 24.4 , we stud y variou s cues in the imag e
that can be harnesse d to this end, includin g motion , stereopsis , texture , shading , and contour .
Sectio n 24.5 discusse s the informatio n neede d for visuall y guide d manipulatio n and navigation ,
and Sectio n 24.6 cover s variou s approache s to objec t recognition . Finally , Sectio n 24.7 addresse s
the proble m of perceptio n in the contex t of speec h recognition , thereb y helpin g to pinpoin t the
issue s that arise in perceptio n in general .
24.2 IMAG E FORMATIO N
SCEN E
IMAG EVisio n work s by gatherin g light scattere d from object s in the scen e and creatin g a 2­D image . In
order to use this imag e to obtai n informatio n abou t the scene , we have to understan d the geometr y
of the process .
PINHOL E CAMER APinhol e camer a
The simples t way to form an imag e is to use a pinhol e camer a (Figur e 24.1) . Let P be a poin t
in the scene , with coordinate s (X, Y,  Z), and P' be its imag e on the imag e plane , with coordinate s
726 Chapte r 24 . Perceptio n
image
plane
Figur e 24.1 Geometr y of imag e formatio n in the pinhol e camera .
(x,y,z) . If/ is the distanc e from the pinhol e O to the imag e plane , then by simila r triangles , we
can deriv e the followin g equations :
­*_ X ­y y _  ­fX ­/ y
PERSPECTIV E
PROJECTIO N
VANISHIN G POIN T
SCALE D
ORTHOGRAPHIC
PROJECTIO NNote that the imag e is inverted,  both left­righ t and up­down , compare d to the scen e as indicate d
in the equation s by the negativ e signs . Thes e equation s defin e an imag e formatio n proces s know n
as perspectiv e projection .
Equivalently , we can mode l the perspectiv e projectio n proces s with the projectio n plan e
being at a distance / in front  of the pinhole . This devic e of imaginin g a projectio n surfac e in fron t
was first recommende d to painter s in the Italia n Renaissanc e by Albert i in 1435 as a techniqu e for
constructin g geometricall y accurat e depiction s of a three­dimensiona l scene . For our purposes ,
the mai n advantag e of this mode l is that it avoid s latera l inversio n and thereb y eliminate s the
negativ e sign s in the perspectiv e projectio n equations .
Unde r perspectiv e projection , paralle l lines appea r to converg e to a poin t on the horizo n —
think of railwa y tracks . Let us see why this mus t be so. We know from vecto r calculu s that an
arbitrar y poin t P\ on the line passin g throug h (X 0, YO,ZQ) in the directio n (U, V, W) is give n by
(Xo + \U, Y0 + XV,  ZQ + AW) , with A varyin g betwee n +00 and ­.30. The projectio n of FA on the
imag e plan e is give n by
•\U , V•xvf{Q+An
As A —  DC. or A — ­ DC , this become s 77^ = (f'U/WJV/W)  if W^O . We call p^ the vanishin g
point associate d with the famil y of straigh t line s with directio n (U, V, W). It does not depen d on
the poin t (X (), Y0,ZQ) throug h whic h the straigh t line in the scen e passes , only on the direction .
If the objec t is relativel y shallo w compare d to its distanc e from the camera , we can ap­
proximat e perspectiv e projectio n by scale d orthographi c projection . The idea is the following .
If the dept h Z of point s on the objec t varie s in som e rang e Zo ± AZ, with AZ <C ZQ, then the
Sectio n 24.2 . Imag e Formatio n 72 7
perspectiv e scalin g facto r fIZ can be approximate d by a constan t s = f/Z 0. The equation s for
projectio n from the scen e coordinate s (X, Y,Z)  to the imag e plan e becom e x = sX and y = sY.
Note that scale d orthographi c projectio n is an approximatio n valid only for part s of the scen e
with not muc h interna l dept h variation . It shoul d not be used to stud y propertie s "in the large. "
An exampl e to convinc e you of the need for caution : unde r orthographi c projection , paralle l lines
stay paralle l instea d of convergin g to a vanishin g point !
Lens system s
LENS Vertebrat e eyes and real cameras  use a lens . A lens is muc h wide r than a pinhole , enablin g it to
let in mor e light . Thi s is paid for by the fact that not all the scen e can be in shar p focu s at the
same time . The imag e of an objec t at distanc e Z in the scen e is produce d at a fixed distanc e from
the lens Z', wher e the relatio n betwee n Z and Z' is give n by the lens equatio n
1 _L ­Iz + z* ~]'
where / is the foca l lengt h of the lens . Give n a certai n choic e of imag e distanc e Z'0 betwee n the
noda l poin t of the lens and the imag e plane , scen e point s with depth s in a rang e aroun d ZQ, wher e
ZQ is the correspondin g objec t distance , will be image d in reasonabl y shar p focus . This rang e of
DEPTH OF FIEL D depth s in the scen e is referre d to as the dept h of field .
Note that becaus e the objec t distanc e Z is typicall y muc h greate r than the imag e distanc e
Z' or/, we ofte n mak e the followin g approximation :
I _L ~ _L J _ ~ 1
~Z + Z1~Zi ^ Z7 ~/
Thus , the imag e distanc e Z' K, f. We can therefor e continu e to use the pinhol e camer a perspectiv e
projectio n equation s to describ e the geometr y of imag e formatio n in a lens system .
In orde r to focu s object s at differen t distance s Z, the lens in the eye (see Figur e 24.2 )
change s shape , wherea s the lens in a camer a move s in the Z­direction .
The imag e plan e is coate d with photosensitiv e material :
• Silve r halide s on photographi c film .
• Rhodopsi n and variant s in the retina .
• Silico n circuit s in the CCD (charge­couple d device ) camera . Eac h site in a CCD integrate s
the electron s release d by photo n absorptio n for a fixed time period .
PIXEL S I n the eye and CCD camera , the imag e plan e is subdivide d into pixels : typicall y 512x51 2
(0.25 x 106) in the CCD camera , arrange d in a rectangula r grid ; 120 x 106 rods and 6 x 106
cones in the eye, arrange d in a hexagona l mosaic .
In both cases , we can mode l the signa l detecte d in the imag e plan e by the variatio n in imag e
brightnes s over time : I(x,y,t).  Figur e 24.3 show s a digitize d imag e of a staple r on a desk , and
Figur e 24.4 show s an arra y of imag e brightnes s value s associate d with a 12 x 12 bloc k of pixel s
extracte d from the staple r image . A compute r progra m tryin g to interpre t the imag e woul d have
to start from such a representation .
728 Chapte r 24 . Perceptio n
_^­ \ Opti c Nerv e
Figur e 24.2 Horizonta l cross­sectio n of the huma n eye.
Figur e 24.3 A  photograp h of a staple r on a table . The outline d box has been magnifie d and
displaye d in Figur e 24.4 .
Sectio n 24.2 . Imag e Formatio n 729
195
210
164
167
162
153
126
103
095
093
093
095209
236
172
164
167
157
135
107
095
093
093
093221
249
180
171
166
160
143
118
097
093
093
093235
254
192
170
169
162
147
125
101
093
093
093249
255
241
179
169
169
156
133
115
095
093
093251
254
251
189
170
170
157
145
124
099
093
093254
225
255
208
176
168
160
151
132
105
095
093255
226
255
244
185
169
166
156
142
118
097
093250
212
255
254
196
171
167
158
117
125
101
093241
204
255
255
232
176
171
159
122
135
109
093247
236
235
251
249
185
168
163
124
143
119
093248
211
190
234
254
218
170
164
161
119
132
119
(b)
Figur e 24.4 (a ) Magnifie d view of a 12 x 12 bloc k of pixel s from Figur e 24.3 . (b) The
associate d imag e brightnes s values .
Photometr y of imag e formatio n
The brightnes s of a pixe l p in the imag e is proportiona l to the amoun t of light directe d towar d the
camer a by the surfac e patc h Sp that project s to pixe l p. Thi s in turn depend s on the reflectanc e
propertie s of Sp, the positio n and distributio n of the light sources . Ther e is also a dependenc e
on the reflectanc e propertie s of the rest of the scen e becaus e othe r scen e surface s can serv e as
indirec t light source s by reflectin g light receive d by them onto Sp.
The ligh t reflecte d from an objec t is characterize d as bein g eithe r diffusel y or specularl y
reflected . Diffusel y reflecte d light is light that has penetrate d belo w the surfac e of an object , been
absorbed , and then re­emitted . The surfac e appear s equall y brigh t to an observe r in any direction .
Lambert' s cosin e law is used to describ e the reflectio n of ligh t from a perfectl y diffusin g or
LAMBERTIA N Lambertiai i surface . The intensit y E of light reflecte d from a perfect  diffuse r is give n by
E = pEo cos 9
wher e E0 is the intensit y of the light source ; p is the albedo , whic h varie s from 0 (for perfectl y
black surfaces ) to 1 (for pure whit e surfaces) ; and 0 is the angl e betwee n the light directio n and
the surfac e normal .
Specularl y reflecte d ligh t is reflecte d from the oute r surfac e of the object . Her e the energ y
of reflecte d ligh t is concentrate d primaril y in a particula r direction—th e one wher e the reflecte d
ray is in the same plan e as the inciden t ray and satisfie s the conditio n that the angl e of reflectio n
is equa l to the angl e of incidence . This is the behavio r of a perfec t mirror .
In real life, surface s exhibi t a combinatio n of diffus e and specula r properties . Modellin g
this on the compute r is the brea d and butte r of compute r graphics . Renderin g realisti c image s
is usuall y done by ray tracing , whic h aims to simulat e the physica l proces s of ligh t originatin g
730 __ _ Chapte r 24 . Perceptio n
from ligh t source s and bein g reflecte d and rereflecte d multipl e times . The shape­from­shadin g
proble m in compute r visio n is aime d at invertin g the process , that is, startin g from the "rendered "
imag e and figurin g out the layou t of the three­dimensiona l scen e that gave rise to it. We will talk
abou t this in mor e dept h in Sectio n 24.4 .
Spectrophotometr y of imag e formatio n
We have been talkin g of imag e intensit y I(x, y, t), merril y ignorin g the fact that visibl e light come s
in a rang e of wavelengths—rangin g from 400 nm on the viole t end of the spectru m to 700 nm on
the red end. Give n that there is a continuu m of wavelengths , wha t does it mean that we have three
primar y colors ? The explanatio n is that colo r is quit e literall y in the eye of the beholder . Ther e
are three differen t cone type s in the eye with three differen t spectra l sensitivit y curve s RkW­  The
outpu t of the Mi cone at locatio n (x,y)  at time t then is Ik(x,y,  t) = f I(x,y,  t, \)R k(X) d\. The
infinit e dimensiona l wavelengt h spac e has been projected  to a three­dimensiona l colo r space .
This mean s that we ough t to thin k of / as a three­dimensiona l vecto r at (x,y,  t). Becaus e the eye
maps man y differen t frequenc y spectr a into the sam e colo r sensation , we shoul d expec t that there
METAMER S exis t metamers —differen t ligh t spectr a that appea r the sam e to a human .
24.3 IMAGE­PROCESSIN G OPERATION S FOR EARL Y VISIO N
Figur e 24.5 show s an imag e of a scen e containin g a staple r restin g on a tabl e as wel l as the
EDGE S edge s detecte d on this image . Edge s are curve s in the imag e plan e acros s whic h ther e is a
"significant " chang e in imag e brightness . The ultimat e goal of edge detectio n is the constructio n
of an idealize d line drawin g such as Figur e 24.6 . The motivatio n is that edge contour s in the
imag e correspon d to importan t scen e contours . In the example , we have dept h discontinuities ,
labelle d 1; surface­orientatio n discontinuities , labelle d 2; a reflectanc e discontinuity , labelle d 3;
and an illuminatio n discontinuit y (shadow) , labelle d 4.
As you can see, ther e is a big differenc e betwee n the outpu t of an edge detecto r as show n
in Figur e 24.5(b ) and an ideal line drawing . Typically , there are missin g contour s (suc h as the top
edge of the stapler) , as well as nois e contour s that do not correspon d to anythin g of significanc e
in the scene . Late r stage s of processin g base d on edge s have to take thes e error s into account .
How do we detec t edge s in an image ? Conside r the profil e of imag e brightnes s alon g a 1­D
cross­sectio n perpendicula r to an edge , for example , the one betwee n the left edge of the tabl e
and the wall . It look s somethin g like wha t is show n in Figur e 24.1  (a). The locatio n of the edge
correspond s to x = 50.
Becaus e edge s correspon d to location s in image s wher e the brightnes s undergoe s a shar p
change , a naiv e idea woul d be to differentiat e the imag e and look for place s wher e the magnitud e
of the derivativ e I'(x) is large . Well , that almos t works . In Figur e 24.7(b ) we see that althoug h
there is a peak at x = 50, ther e are also subsidiar y peak s at othe r location s (e.g. , x =  75) that
could potentiall y be mistake n as true edges . Thes e arise becaus e of the presenc e of nois e in the
SMOOTHIN G image . We get muc h bette r result s by combinin g the differentiatio n operatio n with smoothing .
Sectio n 24.3 . Image­Processin g Operation s for Earl y Visio n 731
(b)
Figur e 24.5 (a ) Photograp h of a stapler , (b) Edge s compute d from (a).
Figur e 24.6 Differen t kind s of edges : (1) dept h discontinuities ; (2) surfac e orientatio n discon ­
tinuities ; (3) reflectanc e discontinuities ; (4) illuminatio n discontinuitie s (shadows) .
The resul t is Figur e 24.7(c) , in whic h the centra l peak at x = 50 remain s and the subsidiar y peak s
are muc h diminished . This allow s one to find the edge s withou t gettin g confuse d by the noise .
CONVOLUTIO N T o understan d thes e idea s better , we need the mathematica l concep t of convolution . Man y
usefu l image­processin g operation s such as smoothin g and differentiatio n can be performe d by
convolvin g the imag e with suitabl e functions .
732 Chapte r 24 . Perceptio n
10 20 30 40 50 60 70 80 90 100
10 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 10 0
10 20 30 40 50 60 70 80 90 10 0
Figur e 24.7 (a ) Intensit y profil e l(x) alon g a 1 ­D sectio n acros s a step edge , (b) Its derivativ e
/'(.v). (c) The resul t of the convolutio n R(x) = I * G' a. Lookin g for large value s in this functio n is
a good way to find edge s in (a).
Convolutio n with linea r filter s
The resul t of convolvin g two functions / and g is the new functio n h, denote d as h =f * g, whic h
is define d by
f(u)g(x­u)du  an d h(x)=
for continuou s and discret e domain s respectively . Typically , the functions / and g that we work
with take nonzer o value s only in some finit e interval , so the summatio n can be easil y performe d
on a computer .
The generalizatio n to function s define d on two dimension s (suc h as images ) is straightfor ­
ward . We replac e the 1 ­D integral s (or sums ) by 2­D integral s (or sums) . The resul t of convolvin g
two functions / and g is the new functio n h, denote d as h =f * g, whic h is define d by
/+OC /­+O O
/ f(u,  v)g(x—  u,y — v)dudv
­co J — oo
or if we take the domain s of the two function s to be discret e
Sectio n 24.3 . Image­Processin g Operation s for Earl y Visio n 73 3
Edge detectio n
Let us go back to our 1­D exampl e in Figur e 24.7 . We wan t to mak e the notio n of imag e smoothin g
more precise . One standar d form of smoothin g is to convolv e the imag e with a Gaussia n functio n
Ga(x) =
Now it can be show n that for any functions / and g, f * g' = (f * g)', so smoothin g the imag e
by convolvin g with a Gaussia n Ga and then differentiatin g is equivalen t to convolvin g the imag e
with G'CT(JC), the first derivativ e of a Gaussian :
So, we have a simpl e algorith m for 1 ­D edge detection :
1. Convolv e the imag e / with G'a to obtai n R.
2. Find the absolut e valu e of R.
3. Mar k thos e peak s in \\R\\  that are abov e som e prespecifie d threshol d Tn. The threshol d is
chose n to eliminat e spuriou s peak s due to noise .
In two dimensions , we need to cope with the fact that the edge may be at any angl e 0. To detec t
vertica l edges , we have an obviou s strategy : convolv e with G' a(x)G a(y). In the y­direction , the
effec t is just to smoot h (becaus e of the Gaussia n convolution) , and in the ^­direction , the effec t is
that of differentiatio n accompanie d with smoothing . The algorith m for detectin g vertica l edge s
then is as follows :
1. Convolv e the imag e l(x,y)  withf v(x,y)  ­ G' c,(x)G (r(y) to obtai n Rv(x,y).
2. Find the absolut e valu e of RV(X,  y).
3. Mar k thos e peak s in | \RV\ \(x,y)  that are abov e som e prespecifie d threshol d Tn.
In orde r to detec t an edge at an arbitrar y orientation , we need to convolv e the imag e with two
filter s fv = G' a(x)G a(y) and// / = G' a(y)G a(x), whic h is just fv rotate d by 90°. The algorith m for
detectin g edge s at arbitrar y orientation s is
1. Convolv e the imag e I(x, y) with/y(* , y) andf H(x, y) to obtai n Rv(x, y) and RH(X,  y), respec ­
tively . Defin e R(x,y)  = R2
v(x,y) + R2
H(x,y)
2. Fin d the absolut e valu e of R(x, y).
3. Mar k thos e peak s in \\R\  \(x,y)  that are abov e som e prespecifie d threshol d Tn.
Once we have marke d edge pixel s by this algorithm , the next stag e is to link thos e pixel s that
belon g to the sam e edge curves . This can be done by assumin g that any two neighborin g pixel s
that are both edge pixel s with consisten t orientation s mus t belon g to the sam e edge curve .
We have just outline d the basic procedur e used in the Cann y edge detecto r (Canny , 1986) ,
whic h is a standar d algorith m widel y used for detectin g edge s in images .
734 Chapte r 24 . Perceptio n
24.4 EXTRACTIN G 3­D INFORMATIO N USIN G VISIO N
SEGMENTATIO N
POSE
SLAN T
TILT
SHAP EWe need to extrac t 3­D informatio n for performin g certai n task s such as manipulation , navigation ,
and recognition . Ther e are thret e aspect s of this:
1. Segmentatio n of the scen e into distinc t objects .
2. Determinin g the positio n and orientatio n of each objec t relativ e to the observer .
3. Determinin g the shap e of each object .
Segmentatio n of the imag e is a key step toward s organizin g the array of imag e pixel s into
region s that woul d correspon d to semanticall y meaningfu l entitie s in the scene . For recognition ,
we woul d like to know wha t feature s belon g togethe r so that one coul d compar e them with store d
models ; to gras p an object , one need s to know wha t belong s togethe r as an object .
Edge detection , as discusse d in the last section , is a usefu l first step towar d imag e and scen e
segmentation , but not adequat e by itself . Thi s is becaus e of two reasons : (a) som e fractio n of
the edge curve s that correspon d to surfac e boundarie s may be low contras t and not get detected ;
(b) man y of the edge curve s that are detecte d may correspon d to noise , surfac e markings , or
shadows . Segmentatio n is best viewe d as part of extractio n of 3­D informatio n abou t the scene .
Determinin g the positio n and orientatio n of an objec t relativ e to the observe r (the so­calle d
pose of the object ) is mos t importan t for manipulatio n and navigatio n tasks . To mov e aroun d
in a grocer y store , one need s to know the location s of the obstacles , so that one can plan a path
avoidin g them . If one want s to pick up and gras p an object , one need s to know its positio n relativ e
to the hand so that an appropriat e trajector y of move s coul d be generated . Manipulatio n and
navigatio n action s typicall y are done in a contro l loop setting—th e sensor y informatio n provide s
feedbac k to modif y the motio n of the robot , or the robo t arm. In fact, ofte n we may be intereste d
more in relativ e chang e of position .
Let us specif y positio n and orientatio n in mathematica l terms . The positio n of a poin t P in
the scen e is characterize d by three numbers , the (X, Y, Z) coordinate s of P in a coordinat e fram e
with its origi n at the pinhol e and the Z­axi s alon g the optica l axis (Figur e 24.1) . Wha t we have
availabl e is the perspectiv e projectio n of the poin t in the imag e (x, y). This specifie s the ray from
the pinhol e alon g whic h P lies; wha t we do not know is the distance . The term "orientation "
could be used in two senses :
1. The orientatio n of the objec t as a whole . Thi s can be specifie d in term s of a three­
dimensiona l rotatio n relatin g its coordinat e fram e to that of the camera .
2. The orientatio n of the surfac e of the objec t at P. Thi s can be specifie d by a vecto r n that
specifie s the directio n of the unit surfac e norma l vector , whic h is locall y perpendicula r to
the surface . Ofte n we expres s the surfac e orientatio n usin g the variable s slan t and tilt.
Slant is the angl e betwee n the Z­axi s and n. Tilt is the angl e betwee n the X­axi s and the
projectio n of n on the imag e plane .
Whe n the camer a move s relativ e to an object , both the object' s distanc e and its orientatio n change .
Wha t is preserve d is the shap e of the object.  If the object  is a cube , that fact is not change d whe n
the objec t moves . Geometer s hav e been attemptin g to formaliz e shap e for centuries—th e basi c
concep t bein g that shap e is wha t remain s unchange d unde r som e grou p of transformations , for
Sectio n 24.4 . Extractin g 3­D Informatio n Usin g Visio n 735
example , combination s of rotation s and translations . The difficult y lies in findin g a representatio n
of globa l shap e that is genera l enoug h to deal with the wide variet y of object s in the real world —
not just simpl e form s like cylinders , cones , and spheres—an d yet can be recovere d easil y from
the visua l input . Th e proble m of characterizin g the local  shap e of a surfac e is muc h bette r
understood . Essentially , this can'b e done in term s of curvature—ho w does the surfac e norma l
chang e as one move s in differen t direction s on the surface . For a plane , ther e is no chang e at
all. For a cylinder , if one move s paralle l to the axis there is no change , but in the perpendicula r
direction , the surfac e norma l rotate s at a rate inversel y proportiona l to the radiu s of the cylinder .
And so on. All this is studie d in the subjec t of differentia l geometry .
The shap e of an objec t is relevan t for som e manipulatio n tasks , for example , decidin g
wher e to gras p an object . But its mos t significan t role is in objec t recognition , wher e geometri c
shape alon g with colo r and textur e provid e the mos t significan t cues to enabl e us to identif y
objects , classif y what  is in the imag e as an exampl e of some class one has seen before , and so on.
The fundamenta l questio n is the following : Give n the fact that durin g perspectiv e projec ­
tion, all point s in the 3­D worl d alon g a ray from the pinhol e have been projecte d to the same
point in the image , how do we recove r 3­D information ? Ther e are a numbe r of cues availabl e
in the visua l stimulu s for this, includin g motion , binocula r stereopsis , texture , shading , and
contour . Eac h of these cues relie s on backgroun d assumption s abou t physica l scene s in orde r to
provid e unambiguou s interpretations . We discus s each of these cues in the followin g subsections .
OPTICA L FLOW
SUM OF SQUARE D
DIFFERENCE SMotio n
If the camer a move s relativ e to the three­dimensiona l scene , the resultin g apparen t motio n in the
imag e is calle d optica l flow . This describe s the directio n and spee d of motio n of feature s in the
image  as a resul t of relativ e motio n betwee n the viewe r and the scene . In Figur e 24.8 , we show
two frame s from a vide o of a rotatin g Rubik' s cube . Figur e 24.9 show s the optica l flow vector s
compute d from these images . The optica l flow encode s usefu l informatio n abou t scene structure .
For example , whe n viewe d from a movin g car, distan t object s have muc h slowe r apparen t motio n
than close objects ; thus , the rate of apparen t motio n can tell us somethin g abou t distance .
The optica l flow vecto r field can be represente d by its component s vx(x, y) in the ^­directio n
and \\­(x,y)  in the y­direction . To measur e optica l flow , we need to find correspondin g point s
betwee n one time fram e and the next . We exploi t the fact that imag e patche s aroun d correspondin g
point s have simila r intensit y patterns . Conside r a bloc k of pixel s centere d at pixe l p, Oo,yo ) at
time to. Thi s bloc k of pixel s is to be compare d with pixe l block s centere d at variou s candidat e
pixel s q­, at (x0 + A,.>' o + A) at time to +  Dt. One possibl e measur e of similarit y is the sum of
square d difference s (SSD) :
SSD(D f, A ) = , y, 0 ­ I(x + A , v + D,,t + A))2
CROSS ­
CORRELATIO NHere (x, y) rang e over pixel s in the bloc k centere d at (xo,yo)  • We find the (A, A) that minimize s
the SSD . The optica l flow at (x 0,y0) is then (v x,vy) = (A/A , A/A) ­ Alternatively , one can
maximiz e the cross­correlation :
Correlation(D x, A ) = , y, t)I(x + Dx, y + Dv,t + A)
736 Chapte r 24 . Perceptio n
EGOMOTIO N
FOCU S OF
EXPANSIO NFigur e 24.8 (a ) A Rubik' s cube on a rotatin g turntable , (b) The sam e cube , show n 19/3 0
second s later . (Imag e courtes y of Richar d Szeliski. )
Cross­correlatio n work s best when there is textur e in the scene , resultin g in window s containin g
significan t brightnes s variatio n amon g the pixels . If one is lookin g at a unifor m whit e wall , then
the cross­correlatio n is goin g to be nearl y the same for the differen t candidat e matche s q, and the
algorith m is reduce d to makin g a blind guess .
Suppos e that the viewe r has translationa l velocit y T and angula r velocit y u> (whic h thus
describ e the egomotion) . One can deriv e an equatio n relatin g the viewe r velocities , the optica l
flow, and the position s of object s in the scene . In fact, assuming / = 1,
T,
Z(x,y)
T,
— U) 7X+ <[ Z(x,y)
T,
Z(x,y)
wher e Z(x, y) give s the z­coordinat e of the scen e poin t correspondin g to the imag e poin t at (x, y).
One can get a good intuitio n by considerin g the case of pure translation . In that case, the
flow field become s
­Tx+xT zvy(x,y) =~Ty+yT z
Z(x,y)  ' '­vv"­" Z(x,y)
One can observ e som e interestin g properties . Bot h component s of the optica l flow , vx(x,y) and
vy(x,y),  are zero at the poin t x = Tx/T,,y = Ty/Tz. This poin t is calle d the focu s of expansio n of
the flow field . Suppos e we chang e the origi n in the x­y plan e to lie at the focu s of expansion ;
then the expression s for optica l flow take on a particularl y simpl e form . Let (X,/ ) be the new
coordinate s define d by x1 = x ­ TJT Z, y' = y ­ TyITz. The n
Tc'T  V 'T
Vx(x',y') = *'z ••  <­•'  ­'­­ y '*
Sectio n 24.4 . Extractin g 3­D Informatio n Usin g Visio n 737
* * t * *
* * *
* * *»».­.•»­ * ^':^_ r­'­' ~~z£­
~** ^* ^*^*  ~*~~*~*  ~*~^~*  * ­*»*­t­TJ­*­*^*J'^'
^•^*~"^*^*^*^""**1*^ . » * » ­t ­t­» ­ *  •*­s. ^­«"»^»^ * ­*­*—*** * ^  ^ ^  ^
­V^T . •
Figur e 24.9 Flo w vector s calculate d by comparin g the two image s in Figur e 24.8 . (Courtes y
of Joe Webe r and Jitendr a Malik. )
This equatio n has som e interestin g applications . Suppos e you are a fly tryin g to land on wall ,
and you wan t to know the time to contac t at the curren t velocity . Thi s time is give n by Z/T Z.
Note that althoug h the instantaneou s optica l flow field canno t provid e eithe r the distanc e Z or the
velocit y componen t Tz, it can provid e the ratio of the two, and can therefore  be used to contro l
the landin g approach . Experiment s with real flies show that this is exactl y wha t they use.
To recove r depth , one shoul d mak e use of multipl e frames . If the camer a is lookin g at a
rigid body,  the shap e does not chang e from fram e to fram e and thus  we are able  to bette r deal
with the inherentl y nois y optical  flow measurements . Result s from one such approac h due to
Tomas i and Kanad e (1992 ) are show n in Figure s 24.1 0 and 24.11 .
DISPARIT YBinocula r stereopsi s
The idea here is rathe r simila r to motio n parallax , excep t that instea d of usin g images  over time ,
we use two (or more ) images  separate d in space , such as are provide d by the forward­facin g eyes
of humans . Becaus e a give n featur e in the scen e will be in a differen t place relativ e to the z­axi s
of each imag e plane , if we superpos e the two images , there will be a disparit y in the locatio n of
738 Chapte r 24 . Perceptio n
Figur e 24.1 0 (a ) Four frame s from a vide o sequenc e in whic h the camer a is move d and rotate d
relativ e to the object , (b) The first fram e of the sequence , annotate d with smal l boxe s highlightin g
the feature s foun d by the featur e detector . (Courtes y of Carl o Tomasi. )
(a)
Figur e 24.1 1 (a ) 3­D reconstructio n of the location s of the imag e feature s in Figur e 24.10 ,
show n from above , (b) The real house , take n from the sam e position .
the imag e featur e in the two images . You can see this clearl y in Figur e 24.12 , wher e the neares t
point of the pyrami d is shifte d to the left in the righ t imag e and to the righ t in the left image .
Let us wor k out the geometrica l relationshi p betwee n disparit y and depth . First , we will
conside r the case whe n both the eyes (or cameras ) are lookin g forwar d with thei r optica l axes
L
Sectio n 24.4 . Extractin g 3­D Informatio n Usin g Visio n 739
FIXAT E
BASELIN EPerceive d objec t
Righ t imag e
Figur e 24.1 2 Th e idea of stereopsis : differen t camer a position s resul t in slightl y differen t 2­D
view s of the sam e 3­D scene .
parallel . The relationshi p of the righ t camer a to the left camer a is then just translatio n alon g the
.r­axi s by an amoun t b, the baseline . We can use the optica l flow equation s from the previou s
sectio n to comput e the horizonta l and vertica l disparit y as H = vx At. V = vv At, give n that
rv = b/At  and Ty = T­ = 0. The rotationa l parameter s wv, u> v, and u>­ are zero . On e obtain s
H = b/Z,  V = 0. In words , the horizonta l disparit y is equa l to the ratio of the baselin e to the
depth , and the vertica l disparit y is zero .
This is, of course , the simples t viewin g geometr y (relationshi p betwee n the two cameras )
that we coul d consider . Unde r norma l viewin g conditions , human s fixate ; that is, ther e is som e
point in the scen e at whic h the optica l axes of the two eyes intersect . Figur e 24.1 3 show s two eyes
fixate d at a poin t PQ, whic h is at a distanc e Z from the midpoin t of the eyes . For convenience , we
will comput e the angular  disparity , measure d in radians . The disparit y at the poin t of fixatio n PQ
is zero . For som e othe r poin t P in the scen e that is 6Z furthe r away , we can comput e the angula r
displacement s of the left and righ t image s of P, whic h we will call PL and PR. If each of thes e is
displace d by an angl e 60/2  relativ e to PQ, then the displacemen t betwee n PL and PR, whic h is the
disparit y of P, is just 60. From simpl e geometry , we have
— ­ ^L
~6Z ~ Z2"
In humans , b (the baseline ) is abou t 6 cm. Suppos e that Z is abou t 100 cm. Th e smalles t
detectabl e 69 (corresponding  to the pixe l size ) is abou t 5 second s of arc, or 2.42 x 10~5 radians ,
givin g a 6Z of abou t 0.4 mm. ForZ = 30cm( l ft), we get the impressivel y smal l valu e 6Z = 0.03 6
mm. Statin g this in words , at a distanc e of 30 cm, human s can discriminat e depth s that diffe r by
as little as 0.036 mm, enablin g us to threa d needle s and the like .
740 Chapte r 24 . Perceptio n
Figur e 24.1 3 Th e relatio n betwee n disparit y and dept h in stereopsis .
Note that unlik e the case of motion , we have assume d that we know the viewin g geometry ,
or the relativ e orientatio n betwee n the eyes . This is often a reasonabl e assumption . In the case of
the eyes , the brain has commande d a particula r state of the ocula r muscle s to mov e the eyes , and
hence the position s of the eyes relativ e to the head are known . Similarly , in a binocula r camer a
system , one know s the relativ e configuration .
Knowledg e of the viewin g geometr y is very usefu l in tryin g to measur e disparity . As
in the case of optica l flow , we can try to find correspondin g point s betwee n the left and righ t
image s by maximizin g som e measur e of similarity . However , one does not have to searc h in
EPIPOLA R LINE S a  two­dimensiona l region . Correspondin g point s mus t alway s lie alon g epipola r lines in the
image s (see Figur e 24.14) . Thes e line s correspon d to the intersection s of an epipola r plan e (the
plane throug h a poin t in the scen e and the noda l point s of the two eyes ) with the left and righ t
imag e planes . Exploitin g this epipola r constrain t reduce s an initiall y two­dimensiona l searc h to
a one­dimensiona l one. Obviousl y determinatio n of the epipola r lines require s a knowledg e of
the viewin g geometry .
A simple­minde d approac h for findin g disparit y woul d be to searc h alon g epipola r lines
lookin g to maximiz e the cross­correlation , just as in the case of optica l flow . Give n a poin t pi
in the left view , its correspondin g poin t q­t in the righ t view is obtaine d by searchin g alon g the
associate d epipola r line in the othe r view . For each of the possibl e matche s q, the cross­correlatio n
betwee n window s centere d at pt and q is computed . The correspondin g poin t is declare d to be
the pixe l g, for whic h the cross­correlatio n is maximized .
One can do bette r by exploitin g som e additiona l constraints :
1. Uniqueness : a poin t in one imag e can correspon d to at mos t one poin t in the othe r image .
We say at mos t one, becaus e it is possibl e that the poin t may be occlude d in the othe r view .
2. Piecewis e continuit y of surface s in the scene : the fact the worl d is usuall y piecewis e contin ­
uous mean s that nearb y point s in the scen e have nearb y value s of depth , and consequentl y
of disparity , excep t acros s objec t boundarie s and occlusions .
An exampl e of a syste m that exploit s these multipl e constraint s is the work of Belhumeu r (1993) .
Belhumeur' s result s for an obliqu e view of a box (Figur e 24.15 ) are show n in Figur e 24.16 . His
Sectio n 24.4 . Extractin g 3­D Informatio n Usin g Visio n 741
Left imag e
Left
Cente r of Projectio nRigh t imag e
Righ t
Cente r of Projectio n
Figur e 24.1 4 Epipola r geometry .
(a)
Figur e 24.1 5 Th e figur e show s an imag e of a Q­tip s box standin g on end with a long vertica l
creas e protrudin g towar d the camera . Behin d the box is a flat surface .
algorith m uses the orderin g constrain t and deal s with dept h discontinuitie s wher e the window s at
correspondin g point s in the imag e are not samplin g correspondin g patche s in the scene . Becaus e
of occlusio n in one of the views , ther e is a strip seen only in one eye. Not e also the use of
stereopsi s for scen e segmentatio n as demonstrate d in Figur e 24.16(c) .
A standar d criticis m of area­base d matchin g approache s is that they are susceptibl e to
error s whe n correspondin g patche s in the two image s do not look similar . This can happe n for a
variet y of reasons : (1) the surfac e reflectanc e functio n has an appreciabl e specula r component ,
742 Chapte r 24 . Perceptio n
Figur e 24.1 6 Result s from processin g the Q­tip s stere o pair, (a) Imag e of depth , (b) Imag e
of smoothe d slope , (c) Objec t boundarie s (white ) and surfac e crease s (gray) , (d) Wire fram e of
depth . (Image s courtes y of Pete r Belhumeur. )
so the brightnes s of a poin t in the scen e is a functio n of viewpoint ; (2) there is a differin g amoun t
of foreshortenin g in the two views , becaus e the patc h make s differen t angle s to the optica l axes
in the two views .
Anothe r famil y of approache s is base d on first findin g edge s and then lookin g for matches .
Edge s are deeme d compatibl e if they are near enoug h in orientatio n and have the sam e sign of
contras t acros s the edge . Correspondin g edge s are usuall y assume d to obey the same left­to­righ t
orderin g in each image , whic h allow s one to restric t the numbe r of possibl e matche s and lend s
itself to efficien t algorithm s base d on dynami c programming . Wit h any edge­base d approach ,
however , the resultin g dept h informatio n is sparse , becaus e it is availabl e only at edge locations .
Thus , a furthe r step is neede d to interpolat e dept h acros s surface s in the scene .
Textur e gradient s
TEXTUR E Textur e in everyda y languag e is a propert y of surface s associate d with the tactil e qualit y they
sugges t (textur e has the same root as textile) . In computationa l vision , it refer s to a closel y relate d
concept , that of a spatiall y repeatin g patter n on a surfac e that can be sense d visually . Example s
Sectio n 24.4 . Extractin g 3­D Informatio n Usin g Visio n 74 3
includ e the patter n of window s on a building , the stitche s on a sweater , the spots on a leopard' s
skin, blade s of grass on a lawn , pebble s on a beac h or a crow d of peopl e in a stadium . Sometime s
the arrangemen t is quite periodic , as in the stitche s on a sweater ; in othe r instances , as in pebble s
on a beac h the regularit y is only in a statistica l sense—th e densit y of pebble s is roughl y the same
on differen t parts of the beach . ,
Wha t we just said is true in the scene.  In the image , the apparen t size, shape , spacing , and
TEXEL S s o on, of the textur e element s (the texels ) do indee d vary , as illustrate d in Figur e 24.17 . The tiles
are identica l in the scene . Ther e are two main cause s for this variatio n in the projecte d size and
shape of the tiles :
1. Varyin g distance s of the differen t texel s from the camera . Recal l that unde r perspectiv e
projection , distan t object s appea r smaller . The scalin g facto r is 1/Z.
2. Varyin g foreshortenin g of the differen t texels . This is relate d to the orientatio n of the texel
relativ e to the line of sight from the camera . If the texel is perpendicula r to the line of sight ,
there is no foreshortening . The magnitud e of the foreshortenin g effec t is proportiona l to
cos (7, wher e <r is the slan t of the plan e of the texel .
After som e mathematica l analysi s (Carding , 1992) , one can comput e expression s for the rate of
chang e of variou s imag e texe l features , such as area, foreshortening , and density . Thes e textur e
GRADIENT S gradient s are function s of the surfac e shap e as well as its slant and tilt with respec t to the viewer .
To recove r shap e from texture , one can use a two­ste p process : (a) measur e the textur e
gradients ; (b) estimat e the surfac e shape , slant , and tilt that woul d give rise to the measure d textur e
gradients . We show the result s of an algorith m develope d by Mali k and Rosenholtz(1994 ) in
Figure s 24.1 7 and 24.18 .
Shadin g
Shading—variatio n in the intensit y of ligh t receive d from differen t portion s of a surfac e in the
scene—i s determine d by scen e geometr y and reflectanc e propertie s of the surfaces . In compute r
graphics , the objectiv e is to determin e the imag e brightnes s I(x,y)  give n the scen e geometr y and
reflectanc e properties . In compute r vision , our hope migh t be to inver t the process , that is, recove r
the scen e geometr y and reflectanc e propertie s give n the imag e brightnes s I(x, y).  This has prove d
difficul t to do in anythin g but the simples t cases .
Let us star t with an exampl e of a situatio n wher e we can in fact solv e for shap e from
shading . Conside r a Lambertia n surfac e illuminate d by a distan t poin t ligh t source . We will
assum e that the surfac e is distan t enoug h from the camer a so that we coul d use orthographi c
projectio n as an approximatio n to perspectiv e projection . The imag e brightnes s is
l(x,y)  = kn(x,y).s
wher e k is a scalin g constant , n is the unit surfac e norma l vector , and s is the unit vecto r in the
directio n of the light source . Becaus e n and s are unit vectors , their dot produc t is just the cosin e
of the angl e betwee n them . Surfac e shap e is capture d in the variatio n of the surfac e norma l n
along the surface . Let us assum e that k and s are known . Ou r proble m then is to recove r the
surfac e norma l n(x,y)  give n the imag e intensit y I(x,y).
744
Chapte r 24 . Perceptio n
————————— — —  —
Figur e 24.17 A  scen e illustratin g textur e gradient . Assumin g that the real textur e is unifor m
allow s recover y of the surfac e orientation . Th e compute d surfac e orientatio n is indicate d by
overlayin g a whit e circl e and pointer , transforme d as if the circl e were painte d on the surfac e at
that point . (Imag e courtes y of Jitendr a Mali k and Ruth Rosenholtz. )
Figur e 24.1 8 Recover y of shap e from textur e for a curve d surface .
Sectio n 24.4 . Extractin g 3­D Informatio n Usin g Visio n 745
The first observatio n to mak e is that the proble m of determinin g n, give n the brightnes s / at
a give n pixe l (x,y),  is underdetermine d locally . We can comput e the angl e that n make s with the
light sourc e vector , but that only constrain s it to lie on a certai n cone of direction s with axis s and
apex angl e 0 = cos"1 (//£)• To procee d further , note that n canno t vary arbitraril y from pixe l to
pixel . It correspond s to the norma l vecto r of a smoot h surfac e patc h and consequentl y mus t also
INTEGRABILIT Y var y in a smoot h fashion—th e technica l term for the constrain t is integrability . Severa l differen t
technique s have been develope d to exploi t this insight . One techniqu e is simpl y to rewrit e n in
terms of the partia l derivative s Zt and Zy of the dept h Z(x, y). Thi s result s in a partia l differentia l
equatio n for Z that can be solve d to yield the dept h Z(x, y),  give n appropriat e boundar y conditions .
One can generaliz e the approac h somewhat . It is not necessar y for the surfac e to be
Lambertia n nor for the light sourc e to be a poin t source . As long as one is able to determin e
REFLECTANC E MAP th e reflectanc e map /?(n) , whic h specifie s the brightnes s of a surfac e patc h as a functio n of its
surfac e norma l n, essentiall y the same kind of technique s can be used .
The real difficult y come s in dealin g with interreflections . If we conside r a typica l indoo r
scene , such as the object s insid e an office , surface s are illuminate d not only by the light sources ,
but also by the ligh t reflecte d from othe r surface s in the scen e that effectivel y serv e as secondar y
light sources . Thes e mutua l illuminatio n effect s are quit e significant . Th e reflectanc e map
formalis m completel y fails in this situation—imag e brightnes s depend s not just on the surfac e
normal , but also on the comple x spatia l relationship s amon g the differen t surface s in the scene .
Human s clearl y do get some perceptio n of shap e from shading , so this remain s an interestin g
proble m in spite of all thes e difficulties .
Contou r
Whe n we look at a line drawing , such as Figur e 24.19 , we get a vivi d perceptio n of 3­D shap e
and layout . How ? Afte r all, we saw earlie r that there is an infinit y of scen e configuration s that
can give rise to the sam e line drawing . Not e that we get even a perceptio n of surfac e slan t and
tilt. It coul d be due to a combinatio n of high­leve l knowledg e abou t typica l shape s as well as
some low­leve l constraints .
We will conside r the qualitativ e knowledg e availabl e from a line drawing . As discusse d
earlier , lines in a drawin g can have multipl e significance s (see Figur e 24.6 and the accompanyin g
text) . The task of determinin g the actua l significanc e of each line in an imag e is calle d line
LINE LABELLIN G labelling , and was one of the first task s studie d in compute r vision . For now , let us deal with a
simplifie d mode l of the worl d wher e the object s have no surfac e mark s and wher e the lines due
to illuminatio n discontinuitie s like shado w edge s and specularitie s have been remove d in som e
preprocessin g step, enablin g us to limi t our attentio n to line drawing s wher e each line correspond s
eithe r to a dept h or orientatio n discontinuity .
LIMB Eac h line then can be classifie d as eithe r the projectio n of a limb (the locu s of point s on
the surfac e wher e the line of sigh t is tangen t to the surface ) or as an edge (a surfac e norma l
discontinuity) . Additionally , each edge can be classifie d as a convex , concave , or occludin g edge .
For occludin g edge s and limbs , we woul d like to determin e whic h of the two surface s borderin g
the curv e in the line drawin g is neare r in the scene . Thes e inference s can be represente d by givin g
LINE LABEL S eac h line one of 6 possibl e line label s as illustrate d in Figur e 24.20 .
746 Chapte r 24 . Perceptio n
TRIHEDRA L
CRACK SFigur e 24.1 9 A n evocativ e line drawing . (Courtes y of Isha Malik. )
1. "+" and "—" label s represen t conve x and concav e edges , respectively . Thes e are associate d
with surfac e norma l discontinuitie s wher e both surface s that meet alon g the edge are visible .
2. A "<— " or a "—»• " represent s an occludin g conve x edge . Whe n viewe d from the camera ,
both surfac e patche s that meet alon g the edge lie on the same side, one occludin g the other .
As one move s in the directio n of the arrow , thes e surface s are to the right .
3. A "^^ " or a "^^ " represent s a limb . Her e the surfac e curve s smoothl y aroun d to
occlud e itself . As one move s in the directio n of the twin arrows , the surfac e lies to the
right . The line of sigh t is tangentia l to the surfac e for all point s on the limb . Limb s mov e
on the surfac e of the objec t as the viewpoin t changes .
Of the 6" combinatoriall y possibl e labe l assignment s to the n lines in a drawing , only a smal l
numbe r are physicall y possible . The determinatio n of these labe l assignment s is the line labellin g
problem . Note that the proble m only make s sens e if the label is the same all the way alon g a line.
This is not alway s true, becaus e the label can chang e alon g a line for image s of curve d objects .
In this section , we will deal only with polyhedra l objects , so this is not a concern .
Huffma n (1971 ) and Clowe s (1971 ) independentl y attempte d the first systemati c approac h
to polyhedra l scen e analysis . Huffma n and Clowe s limite d their analysi s to scene s with opaqu e
trihedra l solids—object s in whic h exactl y three plan e surface s com e togethe r at each vertex .
For scene s with multipl e objects , they also rule d out objec t alignment s that woul d resul t in a
violatio n of the trihedra l assumption , such as two cube s sharin g a commo n edge . Cracks , that
is, "edges " acros s whic h the tangen t plane s are continuous , were also not permitted . For the
trihedra l world , Huffma n and Clowe s mad e an exhaustiv e listin g of all the differen t verte x type s
and the differen t way s in whic h they coul d be viewe d unde r genera l viewpoint . The genera l
viewpoin t conditio n essentiall y ensure s that if there is a smal l movemen t of the eye, non e of the
junction s change s character . For example , this conditio n implie s that if three line s intersec t in
the image , the correspondin g edge s in the scen e mus t also intersect .
Sectio n 24.4 . Extractin g 3­D Informatio n Usin g Visio n 747
Figur e 24.2 0 Differen t kind s of line labels .
Figur e 24.21 Th e four kind s of trihedra l vertices .
The four way s in whic h three plan e surface s can com e togethe r at a verte x are show n in
Figur e 24.21 . Thes e case s have been constructe d by takin g a cube and dividin g it into eigh t
OCTANT S octants . We wan t to generat e the differen t possibl e trihedra l vertice s at the cente r of the cube
by fillin g in variou s octants . The verte x labele d 1 correspond s to 1 fille d octant , 3 to 3 fille d
octants , and so on. Reader s shoul d convinc e themselve s that these are indee d all the possibilities .
For example , if one fills two octant s in a cube , one canno t construc t a valid trihedra l verte x at
the center . Not e also that thes e four case s correspon d to differen t combination s of conve x and
concav e edge s that meet at the vertex .
The thre e edge s meetin g at the verte x partitio n the surroundin g spac e into eigh t octants .
A verte x can be viewe d from any of the octant s not occupie d by solid material . Movin g the
viewpoin t withi n a singl e octan t does not resul t in a pictur e with differen t junctio n types . The
verte x labele d 1 in Figur e 24.2 1 can be viewe d from any of the remainin g seve n octant s to give
the junctio n label s in Figur e 24.22 .
748 Chapte r 24 . Perceptio n
Figur e 24.22 Th e differen t appearance s of verte x labele d 1 from Figur e 24.21 .
T
Figur e 24.2 3 Th e Huffman­Clowe s labe l set.
An exhaustiv e listin g of the differen t way s each verte x can be viewe d result s in the possi ­
bilitie s show n in Figur e 24.23 . We get four differen t junctio n type s whic h can be distinguishe d
in the image : L­, Y­, arrow , and T­junctions . L­junction s correspon d to two visibl e edges . Y­
and arrow junction s correspon d to a tripl e of edges—i n a Y­junctio n none of the three angle s is
greate r than TT. T­junction s are associate d with occlusion . Whe n a nearer , opaqu e surfac e block s
the view of a mor e distan t edge , one obtain s a continuou s edge meetin g a half edge . The four
T­junctio n label s correspon d to the occlusio n of four differen t type s of edges .
Whe n usin g this junctio n dictionar y to find a labelin g for the line drawing , the proble m is
to determin e whic h junctio n interpretation s are globall y consistent . Consistenc y is force d by the
rule that each line in the pictur e mus t be assigne d one and only one labe l alon g its entir e length .
Sectio n 24.5 . Usin g Visio n for Manipulatio n and Navigatio n 74 9
Walt z (1975 ) propose d an algorith m for this proble m (actuall y for an augmente d versio n with
shadows , cracks , and separabl y concav e edges ) that was one of the first application s of constrain t
satisfactio n in AI (see Chapte r 3). In the terminolog y of CSPs , the variable s are the junctions ,
the value s are labelling s for the junctions , and the constraint s are that each line has a singl e
label . Althoug h Kirousi s and Parjadimitrio u (1988 ) have show n that the line­labellin g proble m
for trihedra l scene s is NP­complete , standar d CSP algorithm s perfor m well in practice .
Althoug h the Huffman­Clowes­Walt z labelin g schem e was limite d to trihedra l objects ,
subsequen t wor k by Mackworth(1973 ) and Sugihar a (1984 ) resulte d in a generalizatio n for
arbitrar y polyhedr a and wor k by Mali k (1987 ) for piecewis e smoot h curve d objects .
24.5 USIN G VISIO N FOR MANIPULATIO N AND NAVIGATIO N
One of the principa l uses of visio n is to provid e informatio n for manipulatin g objects—pickin g
them up, grasping , twirling , and so on—a s well as navigatin g in a scen e whil e avoidin g obstacles .
The capabilit y to use visio n for thes e purpose s is presen t in the mos t primitiv e of anima l
visua l systems . Perhap s the evolutionar y origi n of the visio n sens e can be trace d back to the
presenc e of a photosensitiv e spot on one end of an organis m that enable d it to orien t itsel f towar d
(or away from ) the light . Flie s use visio n base d on opti c flow to contro l their landin g responses .
Mobil e robot s movin g aroun d in an environmen t need to kno w wher e the obstacle s are,
wher e free spac e corridor s are available , and so on. Mor e on this in Chapte r 25.
Let us stud y the use of visio n in drivin g in detail . Conside r the task s for the drive r of the
car in the botto m left corne r of Figur e 24.24 .
1. Kee p movin g at a reasonabl e spee d VQ.
2. Latera l control—ensur e that the vehicl e remain s securel y withi n its lane , that is, keep
di = d,~.
3. Longitudina l control—ensur e that there is a safe distanc e di betwee n it and the vehicl e in
front of it.
4. Monito r vehicle s in neighborin g lane s (at distance s d\ and d^) and be prepare d for appro ­
priat e maneuver s if one of them decide s to chang e lanes .
The proble m for the drive r is to generat e appropriat e steering , actuatio n or brakin g action s to
best accomplis h thes e tasks . Focusin g specificall y on latera l and longitudina l control , wha t
informatio n is neede d for thes e tasks ?
For latera l control , one need s to maintai n a representatio n of the positio n and orientatio n
of the car relativ e to the lane . The view of the road from a camer a mounte d on a car is show n in
Figur e 24.25 . We detec t edge s correspondin g to the lane marke r segment s and then fit smoot h
curve s to these . The parameter s of these curve s carr y informatio n abou t the latera l positio n of the
car, the directio n it is pointin g relativ e to the lane , and the curvatur e of the lane . This information ,
along with the dynamic s of the car, is all that is neede d by the steerin g contro l system . Not e
also that becaus e from ever y fram e to the next fram e ther e is only a smal l chang e in the positio n
750 Chapte r 24 . Perceptio n
Figur e 24.24 Th e informatio n neede d for visua l contro l of a vehicl e on a freeway .
of the projectio n of the lane in the image , one know s where  to look for the lane marker s in the
image—i n the figure , this is indicate d by showin g the searc h windows .
For longitudina l control , one need s to know distance s to the vehicle s in front . This can be
accomplishe d usin g binocula r stereopsi s or optica l flow . Bot h these approache s can be simplifie d
by exploitin g the domai n constraint s derive d from the fact that one is drivin g on a plana r surface .
Usin g these techniques , Dickmann s and Zapp (1987 ) have demonstrate d visuall y controlle d car
drivin g on freeway s at high speeds . Pomerlea u (1993 ) achieve d simila r performanc e usin g a
neura l networ k approac h (see discussio n on page 586) .
The drivin g exampl e make s one poin t very clear : for a specific  task,  one  does not  need
to recover all  the information  that  in principle  can be recovered  from  an image.  On e does
not need to recove r the exac t shap e of ever y vehicle , solv e for shape­from­textur e on the gras s
surfac e adjacen t to the freeway , and so on. The need s of the task requir e only certai n kind s of
informatio n and one can gain considerabl e computationa l spee d and robustnes s by recoverin g
only that informatio n and fully exploitin g the domai n constraints . Our purpos e in discussin g the
genera l approache s in the previou s sectio n was that they form the basi c theory , whic h one can
specializ e for the need s of particula r tasks .
Sectio n 24.6 . Objec t Representatio n and Recognitio n 751
Figur e 24.25 Imag e of a road take n from a camer a insid e the car.
24.6 OBJEC T REPRESENTATIO N AND RECOGNITIO N
The object  recognitio n proble m can be define d as follows . Give n
1. a scen e consistin g of one or more object s chose n from a collectio n of object s O\, O
know n a priori , and
2. an imag e of the scen e take n from an unknow n viewe r positio n and orientation ,
determin e the following :
1. Whic h of the object s O\, O2, • • •, On are presen t in the scene .
2. For each such object , determin e its positio n and orientatio n relativ e to the viewer .
Obviously , this determinatio n require s prio r storag e of suitabl e description s of the objects .
This proble m is clearl y of considerabl e industria l importance . For example , in an assembl y
process , the robo t has to identif y the differen t component s and comput e their position s and orien ­
tation s in orde r to generat e a graspin g or pick­and­plac e strategy . In additio n to the engineerin g
standpoint , the proble m is of grea t scientifi c interest . Human s have the impressiv e abilit y to rec­
ogniz e thousand s of object s almos t instantaneousl y even whe n the object s are partiall y occlude d
or presente d in highl y simplifie d line drawings .
The two fundamenta l issue s that any objec t recognitio n schem e mus t addres s are the
representatio n of the model s and the matchin g of model s to images .
752 Chapte r 24 . Perceptio n
GENERALIZE D
CYLINDER SFirst, let us conside r the representatio n problem . The two mos t popula r representation s of
3­D object s in compute r visio n have been polyhedra l approximation s and generalize d cylinders .
Polyhedra l description s are general , but they get painfull y long if a high accurac y is desire d
in modelin g curve d objects . The y are also very cumbersom e for user s to input . Generalize d
cylinder s (Figur e 24.26 ) provid e compac t description s for a wide class of object s and have been
used in a numbe r of objec t recognitio n systems .
A generalize d cylinde r is define d by a plana r cross section , an axis (whic h may be curved) ,
and a sweepin g rule whic h describe s how the cros s sectio n change s alon g the axis . Man y shape s
can be buil t up usin g generalize d cylinder s as parts . Generalize d cylinder s are not alway s idea l
for representin g arbitrar y objects ; the objec t may have to be decompose d into man y parts , each of
whic h is a generalize d cylinder . In such situations , there can be man y alternativ e decomposition s
into part s with each part describabl e as a generalize d cylinde r in severa l ways . Thi s lead s to
difficultie s at matchin g time . In general , the proble m of effectiv e shap e representatio n for curve d
object s is largel y unsolved .
Figur e 24.2 6 Som e example s of generalize d cylinders . Eac h of thes e shape s has a principa l
axis and a plana r cross­sectio n whos e dimension s may vary alon g the axis .
The alignmen t metho d
We wi 11 conside r one particula r versio n of the proble m in greate r detail—w e are asked to identif y a
three­dimensiona l objec t from its projectio n on the imag e plane . For convenience , the projectio n
proces s is modelle d as a scale d orthographi c projection . We do not kno w the pose of the
object—it s positio n and orientatio n with respec t to the camera .
The objec t is represente d by a set of m feature s or distinguishe d point s //,, /i2,..., /^m in
three­dimensiona l space—perhap s vertice s for a polyhedra l object . Thes e are measure d in som e
coordinat e syste m natura l for the object . Th e point s are then subjecte d to an unknow n 3­D
rotatio n R, followe d by translatio n by unknow n amoun t t and projectio n to give rise to imag e
Sectio n 24.6 . Objec t Representatio n and Recognitio n 753
featur e point s p \ ,p2, ...,/? „ on the imag e plane . In general , n^m  becaus e som e mode l point s may
be occluded . The featur e detecto r in the imag e also may miss true feature s and mark false ones
due to noise . We can expres s this as
for 3­D mode l poin t /», and correspondin g imag e point/;, . Here FI denote s perspectiv e projectio n
or one of its approximation s such as scale d orthographi c projection . We can summariz e this by
the equatio n /?, = Q\i;  wher e Q is the (unknown ) transformatio n that bring s the mode l point s in
alignmen t with the image . Assumin g the objec t is rigid , the transformatio n Q is the same  for all
the mode l points .
One can solv e for Q give n the 3­D coordinate s of thre e mode l point s and thei r 2­D
projections . The intuitio n is as follows : one can writ e dow n equation s relatin g the coordinate s of
Pi to thos e of Hi. In thes e equations , the unknow n quantitie s correspon d to the parameter s of the
rotatio n matri x R and the translatio n vecto r t. If we have sufficientl y man y equations , we ough t
to be able to solv e for Q. We will not give any proo f here , but merel y state the followin g resul t
(Huttenloche r and Ullman , 1990) :
Give n thre e noncollinea r point s //i, //2, and //j in the model , and their projection s
on the imag e plane , p\, p2, and pj unde r scale d orthographi c projection , ther e exis t
exactl y two transformation s from the three­dimensiona l mode l coordinat e fram e to
a two­dimensiona l imag e coordinat e frame .
These transformation s are relate d by a reflectio n aroun d the imag e plan e and can be compute d by
a simpl e closed­for m solution . We will just assum e that there exist s a functio n FlND­TRANSFORM ,
as show n in Figur e 24.27 .
If we coul d identif y the correspondin g mode l feature s for thre e feature s in the image ,
we coul d comput e Q, the pose of the object . Th e proble m is that we do not kno w thes e
correspondences . Th e solutio n is to operat e in a generate­and­tes t paradigm . W e have to
guess an initia l correspondenc e of an imag e triple t with a mode l triple t and use the functio n
FIND­TRANSFOR M to hypothesiz e Q. If the guesse d correspondenc e was correct , then Q will be
correct , and whe n applie d to the remainin g mode l point s will resul t in predictio n of the imag e
points . If the guesse d correspondenc e was incorrect , then Q will be incorrect , and whe n applie d
to the remainin g mode l point s woul d not predic t the imag e points .
functio n FIND­TRANSFORM^;,/^ , 773, /«i, V2, /'a) return s a transfor m Q such that
GO'i )= pi
Q(V>2)=P2
2(/<3) = j03
inputs : p\,p2,pj,  imag e featur e point s
AM, /*2, /<3, mode l featur e point s
Figur e 24.2 7 Th e definitio n of the transformation­findin g process . We omi t the algorith m
(Huttenloche r and Ullman , 1990) .
754 Chapte r 24 . Perception
functio n ALIGN(Image  feature  points,  Model  feature  points)  return s a solutio n or failur e
loop do
choos e an untrie d triple t p,,, p,2, p,3 from imag e
if no untrie d triplet s left then retur n failur e
while there are still untrie d mode l triplet s left do
choos e an untrie d triple t /*/, , p,j 2, /// 3 from mode l
g<­FlND­TRANSFORM(p,­| , ph, p,,,/*,, , ft,,  [ij })
if projectio n accordin g to Q explain s imag e then
retur n (success , Q)
end
end
Figur e 24.28 A n informa l descriptio n of the alignmen t algorithm .
This is the basis of the algorith m ALIGN , whic h seek s to find the pose for a give n mode l
and retur n failur e otherwis e (see Figur e 24.28) . The worst­cas e time complexit y of the algorith m
is proportiona l to the numbe r of combination s of mode l triplet s and imag e triplets—thi s give s the
numbe r of time s Q has to be computed—time s the cost of verification . Thi s give s (") (™) time s
the cost of verification . The cost of verificatio n is m logn , as we mus t predic t the imag e positio n
of each of m mode l points , and find the distanc e to the neares t imag e point , a log n operatio n if
the imag e point s are arrange d in an appropriat e data structure . Thus , the worst­cas e complexit y
of the alignmen t algorith m is O(m4rr' log n), wher e m and n are the numbe r of mode l and imag e
points , respectively .
One can lowe r the time complexit y by a numbe r of ideas . On e simpl e techniqu e is to
hypothesiz e matche s only betwee n pairs of imag e and mode l points . Give n two imag e point s
and the edge s at thes e points , a third virtua l poin t can be constructe d by extendin g the edge s
and findin g the intersection . This lower s the complexit y to O(m3n2 logn) . Technique s base d on
using pose clusterin g in combinatio n with randomizatio n (Olson , 1994 ) can be used to brin g the
complexit y dow n to O(mn3). Result s from the applicatio n of this algorith m to the staple r imag e
are show n in Figur e 24.29 .
GEOMETRI C
INVARIANT SUsin g projectiv e invariant s
Alignmen t usin g outlin e geometr y and recognitio n is considere d successfu l if outlin e geometr y
in an imag e can be explaine d as a perspectiv e projectio n of the geometri c mode l of the object . A
disadvantag e is that this involve s tryin g each mode l in the mode l library , resultin g in a recognitio n
complexit y proportiona l to the numbe r of model s in the library .
A solutio n is provide d by usin g geometri c invariant s as the shap e representation . Thes e
shape descriptor s are viewpoint  invariant,  that is, they have the sam e valu e measure d on the objec t
or measure d from a perspectiv e imag e of the object , and are unaffecte d by objec t pose . Th e
simples t exampl e of a projectiv e invarian t is the "cross­ratio " of four point s on a line, illustrate d
Sectio n 24.6 . Objec t Representatio n and Recognitio n 755
Figur e 24.29 (a ) Corner s foun d in the staple r image , (b) Hypothesize d reconstructio n overlai d
on the origina l image . (Courtes y of Clar k Olson. )
in Figur e 24.30 . Unde r perspectiv e projection , the ratio s of distance s are not preserved—thin k
of the spacin g of sleeper s on an imag e of a recedin g railwa y track . The spacin g is constan t in the
world , but decrease s with distanc e from the camer a in an image . However , the ratio of ratio of
distance s on a line is preserved , that is, it is the same measure d on the objec t or in the image .
INDEX FUNCTION S Invariant s are significan t in visio n becaus e they can be used as inde x functions , so that a
value measure d in an imag e directl y indexe s a mode l in the library . To take a simpl e example ,
suppos e there are three model s {A,B,  C] in the library , each with a correspondin g and distinc t
invarian t valu e {/(A),/(fi),/(C)} . Recognitio n proceed s as follows : Afte r edge detectio n and
grouping , invariant s are measure d from imag e curves . If a valu e / = I(B) is measured , then there
is evidenc e that objec t B is present . It is not necessar y to conside r object s A and C any further . It
may be that for a large mode l base , all invariant s are not distinc t (i.e., severa l model s may share
invarian t values) . Consequently , whe n an invarian t measure d in the imag e correspond s to a valu e
in the library , a recognitio n hypothesi s is generated . Recognitio n hypothese s correspondin g to
the sam e objec t are merge d if compatible . The hypothese s are verified  by back projectin g the
outlin e as in the alignmen t method . An exampl e of objec t recognitio n usin g invariant s is give n
in Figur e 24.31 .
Anothe r advantag e of invarian t shap e representatio n is that model s can be acquire d directl y
from images . It is not necessar y to mak e measurement s on the actua l object , becaus e the shap e
descriptor s have the sam e valu e whe n measure d in any image . Thi s simplifie s and facilitate s
automatio n of mode l acquisition . It is particularl y usefu l in application s such as recognitio n from
satellit e images .
Althoug h the two approache s to objec t recognitio n that we have describe d are usefu l in
practice , it shoul d be note d that we are far awa y from huma n competence . Th e generatio n
of sufficientl y rich and descriptiv e representation s from images , segmentatio n and groupin g to
identif y thos e feature s that belon g together , and the matchin g of thes e to objec t model s are
difficul t researc h problem s unde r activ e investigation .
756 Chapte r 24 . Perceptio n
Figur e 24.3 0 Invarianc e of the cross­ratio : AD.BC/AB.C D = A'D'.B'C'/A'B'.C'D' . Exer ­
cise 24.7 asks you to verif y this fact.
Figur e 24.3 1 (a ) A scen e containin g a numbe r of objects , two of whic h also appeari n the mode l
library . Thes e are recognize d usin g invariant s base d on line s and conies . The imag e show s 100
fitted line s and 27 fitte d conie s superimpose d in white . Invariant s are forme d from combination s
of lines and conies , and the value s inde x into a mode l library . In this case , there are 35 model s in
the library . Not e that man y line s are cause d by texture , and that som e of the conie s correspond  to
edge data over only a smal l section , (b) The two object s from the librar y are recognize d correctly .
The lock strike r plate is matche d with a singl e invarian t and 50.9 % edge match , and the spanne r
with thre e invariant s and 70.7 % edge match . Courtes y of Andre w Zisserman .
Sectio n 24.7 . Speec h Recognitio n 757
24.7 SPEEC H RECOGNITIO N
SPEEC H
RECOGNITIO N
PHONE S
HOMOPHONE S
SEGMENTATIO NIn this section , we turn from visio n to anothe r type of percept—speech . Speec h is the dominan t
modalit y for communicatio n betwee n humans , and promise s to be importan t for communicatio n
betwee n human s and machines , if it can just be mad e a little more reliable . Speec h recognitio n
is the task of mappin g from a digitall y encode d acousti c signa l to a strin g of words . Speec h
understandin g is the task of mappin g from the acousti c signa l all the way to an interpretatio n of
the meanin g of the utterance . A speec h understandin g syste m mus t answe r three questions :
1. Wha t speec h sound s did the speake r utter ?
2. Wha t word s did the speake r inten d to expres s with thos e speec h sounds ?
3. Wha t meanin g did the speake r inten d to expres s with thos e words ?
To answe r questio n 1, we have to firs t decid e wha t a speec h soun d is. It turn s out that all
huma n language s use a limite d repertoir e of abou t 40 or 50 sounds , calle d phones . Roughl y
speaking , a phon e is the soun d that correspond s to a singl e vowe l or consonant , but ther e are
some complications : combination s of letter s such as "th" and "ng" produc e singl e phones , and
some letter s produc e differen t phone s in differen t context s (for example , the "a" in rat and rate.
Figur e 24.3 2 lists all the phone s in Englis h with an exampl e of each . Onc e we kno w wha t the
possibl e sound s are, we need to characteriz e them in term s of feature s that we can pick out of the
acousti c signal , such as the frequenc y or amplitud e of the soun d waves .
Questio n 2 is conceptuall y muc h simpler . You can thin k of it as lookin g up word s in a
dictionar y that is arrange d by pronunciation . We get a sequenc e of three phones , [k],  [ce],  and
ft], and find in the dictionar y that this is the pronunciatio n for the word "cat. " Two thing s mak e
this difficult . The first is the existenc e of homophones , differen t word s that soun d the same ,
like "two " and "too."1 The secon d is segmentation , the proble m of decidin g wher e one wor d
ends and the next begins . Anyon e who has tried to learn a foreig n languag e will appreciat e this
problem ; at first all the word s seem to run together . Gradually , one learn s to pick out word s from
the jumbl e of sounds . In this case , first impression s are correct ; a spectrographi c analysi s show s
that in fluen t speech , the word s reall y do run togethe r with no silenc e betwee n them . We learn to
identif y wor d boundarie s despit e the lack of silence .
Questio n 3 we alread y kno w how to answer—us e the parsin g and analysi s algorithm s
describe d in Chapte r 22. Som e speec h understandin g system s extrac t the mos t likel y strin g of
word s and pass them directl y to an analyzer . Othe r system s have a more comple x contro l structur e
that consider s multipl e possibl e word interpretation s so that understandin g can be achieve d even
if some individua l word s are not recognize d correctly .
We will shortl y defin e a mode l that answer s question s 1 and 2, but first we will explai n a
little abou t how the speec h signa l is represented .
1 It is also true that one word can be pronounce d severa l ways—yo u say tow­may­to w and I say tow­mah­tow . Thi s
make s it mor e tediou s to construc t the pronunciatio n dictionary , but it does not mak e it any harde r to look up a word .
758 Chapte r 24 . Perceptio n
Vowel s
Phon e Exampl e
[iy] bea t[ih ] bit[ey ] bet
[ae] ba t[ah ] but
[ao] bough t
[ow] boa t
[uh] boo k
[ux] beaut y
[er] Ber t
[ay] bu y
[oy] bo y
[axr] dine r
[aw] dow n
[ax] abou t
[ix] rose s
[aa] co tConsonant s B­N
Phon e Exampl e
[b] be t
[ch] Che t
' [d ] deb t
[f] fa t
[g] ge t
[hh] ha t
[hv] hig h
Uh] je t
[k] kic k
[1] le t
[el] bottl e
[m] me t
[em] botto m
[n] ne t
[en] butto n
[ng] sin g
[eng] Washingto nConsonant s P­Z
Phon e Exampl e
[p] pe t
[r] ra t
[s] se t
[sh] sho e
[t] te n
[th] thic k
[dh] tha t
[dx] butte r
[v] ve t
[w] we t
[wh] whic h
[y] ye t
[z] zo o
[zh] measur e
[­] (silence)
Figur e 24.32 Th e DARP A phoneti c alphabet , listin g all the phone s used in English . Ther e are
severa l alternativ e notations , includin g an Internationa l Phoneti c Alphabe t (IPA) , whic h contain s
the phone s in all know n languages .
SAMPLING  RATE
QUANTIZATIO N
FACTO R
FRAME S
FEATURE SSigna l processin g
Soun d is an analo g energ y source . Whe n a soun d wav e strike s a microphone , it is converte d to
an electrica l current , whic h can be passe d to an analog­to­digita l converte r to yield a strea m of
bits representin g the sound . We have two choice s in decidin g how man y bits to keep . First , the
samplin g rate is the frequenc y with whic h we look at the signal . For speech , a samplin g rate
betwee n 8 and 16 KHz (i.e. , 8 to 16,00 0 time s per second ) is typical . Telephone s delive r only
abou t 3 KHz . Second , the quantizatio n facto r determine s the precisio n to whic h the energ y at
each samplin g poin t is recorded . Speec h recognizer s typicall y keep 8 to 12 bits. Tha t mean s
that a low­en d system , samplin g at 8 KHz with 8­bi t quantization , woul d requir e nearl y half a
megabyt e per minut e of speech . This is a lot of informatio n to manipulate , and worse , it leave s
us very far from our goal of discoverin g the phone s that mak e up the signal .
The first step in comin g up with a bette r representatio n for the signa l is to grou p the sample s
togethe r into large r block s calle d frames . Thi s make s it possibl e to analyz e the whol e fram e for
the appearanc e of speec h phenomen a such as a rise or drop in frequency , or a sudde n onse t or
cessatio n of energy . A fram e lengt h of abou t 10 msec s (i.e. , 80 sample s at 8 KHz ) seem s to be
long enoug h so that most such phenomen a can be detecte d and that few short­duratio n phenomen a
will be missed . Withi n each frame , we represen t wha t is happenin g with a vecto r of features . For
example , we migh t wan t to characteriz e the amoun t of energ y at each of severa l frequenc y ranges .
Sectio n 24.7 . Speec h Recognitio n 759
Other importan t feature s includ e overall  energ y in a frame , and the differenc e from the previou s
frame . Pickin g out feature s from a speec h signa l is like listenin g to an orchestr a and sayin g "here
the Frenc h horn s are playin g loud and the violin s are playin g softly. " Breakin g the soun d dow n
into component s like this is muc h mor e usefu l than leavin g it as a singl e undifferentiate d soun d
source . Figur e 24.3 3 show s frame s with a vecto r of three features . Not e that the frame s overlap ;
this prevent s us from losin g informatio n if an importan t acousti c even t just happen s to fall on a
fram e boundary .
Analo g acousti c signal :
Sampled , quantize d
digita l signal :
Frame s with features :
Frame s with vecto r | ­
quantizatio n values : ~1\J
1 1 11 1 1 I I III\
1
I 1 1
| |
Figur e 24.3 3 Translatin g the acousti c signa l into a sequenc e of vecto r quantizatio n values .
(Don' t try to figur e out the numbers ; they were assigne d arbitrarily. )
VECTO R
QUANTIZATIO N The fina l step in man y speec h signa l processin g system s is vecto r quantization . If there
are n feature s in a frame , we can thin k of this as an n­dimensiona l spac e containin g man y
points . Vecto r quantizatio n divide s this ^­dimensiona l spac e into , say, 256 region s labelle d Cl
throug h C256 . Eac h fram e can then be represente d with a singl e labe l rathe r than a vecto r of n
numbers . So we end up with just one byte per frame , whic h is abou t a 100­fol d improvemen t
over the origina l half megabyt e per minute . Of course , som e informatio n is lost in goin g from a
featur e vecto r to a labe l that summarize s a whol e neighborhoo d aroun d the vector , but there are
automate d method s for choosin g an optima l quantizatio n of the featur e vecto r spac e so that little
or no inaccurac y is introduce d (Jelinek , 1990) .
There are two point s to this whol e exercise . First , we end up with a representatio n of the
speec h signa l that is compact . But mor e importantly , we have a representatio n that is likel y to
encod e feature s of the signa l that will be usefu l for wor d recognition . A give n speec h soun d
can be pronounce d so man y ways : lou d or soft , fast or slow , high­pitche d or low, agains t a
backgroun d of silenc e or noise , and by any of million s of differen t speaker s each with differen t
accent s and voca l tracts . Signa l processin g hope s to captur e enoug h of the importan t feature s so
that the commonalitie s that defin e the soun d can be picke d out from this backdro p of variation .
(The dual problem , speake r identification , require s one to focu s on the variatio n instea d of the
commonalitie s in orde r to decid e who is speaking .
760 Chapte r 24 . Perceptio n
LANGUAG E MODE L
ACOUSTI C MODE LDefinin g the overal l speec h recognitio n mode l
Speec h recognitio n is the diagnosti c task of recoverin g the word s that produc e a give n acousti c
signal . It is a classi c exampl e of reasonin g with uncertainty . We are uncertai n abou t how well
the microphone s (and digitizatio n hardware ) have capture d the actua l sounds , we are uncertai n
abou t whic h phone s woul d give rise to the signal , and we are uncertai n abou t whic h word s woul d
give rise to the phones . As is ofte n the case , the diagnosti c task can best be approache d with a
causa l model—th e word s caus e the signal . We can brea k this into component s with Bayes ' rule :
P(words)P(signal\words)P(words\signal)  = ————————­——— —P(signal)
Given a signal,  our task is to find the sequenc e of words  that maximize s P(words\signal).  Of the
three component s on the right­han d side, P(signal)  is a normalizin g constan t that we can ignore .
P(words)  is know n as the languag e model . It is wha t tells us, whe n we are not sure if we heard
"bad boy" or "pad boy" that the forme r is mor e likely . Finally , P(signal\words)  is the acousti c
model . It is wha t tells us that "cat" is very likel y to be pronounce d [kcet].
BIGRA MThe languag e model : P( words )
In Jake  the Money  and Run,  a bank telle r interpret s Wood y Alien' s sloppil y writte n hold­u p note
as sayin g "I have a gub." A bette r languag e mode l woul d have enable d the teller to determin e that
the strin g "I have a gun" has a muc h highe r prio r probabilit y of bein g on a hold­u p note . Tha t
make s "gun " a bette r interpretatio n even if P(signal\gub)  is a littl e highe r than P(signal\guri).
The languag e mode l shoul d also tell us that "I have a gun" is a muc h mor e probabl e utteranc e
than "gun a have I."
At first glance , the languag e mode l task seem s daunting . We have to assig n a probabilit y
to each of the (possibl y infinite ) numbe r of strings . Context­fre e grammar s are no help for this
task, but probabilisti c context­fre e grammar s (PCFGs ) are promising . Unfortunately , as we saw
in Chapte r 22, PCFG s aren' t very good at representin g contextua l effects . In this section , we
approac h the proble m usin g the standar d strateg y of definin g the probabilit y of a comple x even t
as a produc t of probabilitie s of simple r events . Usin g the notatio n w\ • • • wn to denot e a strin g of
n word s and w, to denot e the rth word of the string , we can writ e an expressio n for the probabilit y
of a strin g as follows:2
P(W\  • • ­W n) ­ P(w t)P(w 2 W\)P(W3\W]W 2) ' ' ' P(w n\W\ • • ­W n_i)
Most of thes e term s are quit e comple x and difficul t to estimat e or compute , and they have
no obviou s relatio n to CFG s or PCFGs . Fortunately , we can approximat e this formul a with
somethin g simple r and still captur e a larg e part of the languag e model . One simple , popular ,
and effectiv e approac h is the bigra m model . Thi s mode l approximate s P(Wi\w\  • • ­w,_i ) with
P(w,­|w, _ ,). In othe r words , it says that the probabilit y of any give n word is determine d solel y by
the previou s word in the string . The probabilit y of a complet e strin g is give n by
P(\Vl  • • •  W,,)  = P(Wi)P(W2\W])P(w 3\W 2) ' ' ' P(W,,  W n­l) = n"
2 Actually , it woul d be bette r if all the probabilitie s were conditione d on the situation . Few speec h recognizer s do this,
however , becaus e it is difficul t to formaliz e wha t count s as a situation .
Sectio n 24.7 . Speech  Recognitio n 761
Word
THE
ON
OF
TO
IS
A
THA T
WE
LINE
VISIO NUnigra m
coun t
367
69
281
212
175
153
124
105
17
13Previou s word s
OF
179
0
0
0
0
36
0
0
1
3IN
143
0
0'
0
0
36
3
0
0
0IS
44
1
2
19
0
33
18
0
0
0ON
44
0
0
0
0
23
0
1
0
1TO
65
0
1
0
0
21
1
0
1
0FRO M
35
0
0
0
0
14
0
0
0
1THA T
30
0
3
0
13
3
0
12
0
0WITH
17
0
0
0
0
15
0
0
0
0LINE
0
0
4
0
1
0
0
0
0
0VISIO N
0
0
0
1
3
0
0
0
0
0
Figur e 24.3 4 A  partia l table of unigra m and bigra m count s for the word s in this chapter . The
word "the" appear s 367 time s in all (out of 1761 3 total words) , the bigra m "of the" appeare d 179
times (or abou t 1%) , and the bigra m "in the" appeare d 143 times . It turn s out thes e are the only
two bigram s that occu r more than 100 times .
TRIGRA MA big advantag e of the bigra m mode l is that it is easy to train the mode l by countin g the
numbe r of time s each word pair occur s in a representativ e corpu s of string s and usin g the count s to
estimat e the probabilities . For example , if "a" appear s 10,00 0 time s in the trainin g corpu s and it is
followe d by "gun " 37 times , then P(gMn/|a/ _ i) = 37/10,000 , wher e by P we mea n the estimate d
probability . Afte r such trainin g one woul d expec t "I have " and "a gun" to have relativel y high
estimate d probabilities , whil e "I has" and "an gun" woul d have low probabilities . One proble m
is that the trainin g corpu s woul d probabl y not contai n "gub " at all and more importantly , it woul d
be missin g man y vali d Englis h word s as well , so thes e word s woul d be assigne d an estimate d
probabilit y of zero . Therefore , it is customar y to set asid e a smal l portio n of the probabilit y
distributio n for word s that do not appea r in the trainin g corpus . Figur e 24.3 4 show s som e bigra m
count s derive d from the word s in this chapter .
It is possibl e to go to a trigra m mode l that provide s value s for />(w;|w,­_iw/_2) . Thi s is a
more powerfu l languag e model , capabl e of determinin g that "ate a banana " is mor e likel y than
"ate a bandana. " Th e proble m is that ther e are so man y mor e parameter s in trigra m model s
that it is hard to get enoug h trainin g data to com e up with accurat e probabilit y estimates . A
good compromis e is to use a mode l that consist s of a weighte d sum of the trigram , bigram , and
unigra m (i.e. , wor d frequency ) models . The mode l is define d by the followin g formul a (wit h
c\ +c 2 +c 3 = 1):
• W,,)  = C\  P(\Vi) C3P(Wi  W,_ 1 W,­_ 2
Bigra m or trigra m model s are not as sophisticate d as PCFGs , but they accoun t for loca l
context­sensitiv e effect s better , and manag e to captur e som e loca l syntax . For example , the fact
that the word pairs "I has" and "ma n have " get low score s is reflectiv e of subject­ver b agreement .
The proble m is that thes e relationship s can only be detecte d locally : "the man have " gets a low
score , but "the man over ther e have " is not penalized .
762 Chapte r 24 . Perceptio n
MARKO V MODE L
COARTICULATIO N
HIDDE N MARKO V
MODE LThe acousti c model : P(signallwords )
The acousti c mode l is responsibl e for sayin g wha t sound s will be produce d whe n a give n strin g of
word s is uttered . We divid e the mode l into two parts . First , we show how each word is describe d
as a sequenc e of phones , and then we show how each phon e relate s to the vecto r quantizatio n
value s extracte d from the acousti c signal .
Some word s have very simpl e pronunciatio n models . The wor d "cat, " for example , is
alway s pronounce d with the three phone s fk a? t]. Ther e are, however , two source s of phoneti c
variation . First , differen t dialect s have differen t pronunciations . The top of Figur e 24.3 5 give s
an exampl e of this : for "tomato, " you say [tow mey tow ] and I say [tow maa tow] . Th e
alternativ e pronunciation s are specifie d as a Marko v model . In general , a Marko v mode l is a
way of describin g a proces s that goes throug h a serie s of states . The mode l describe s all the
possibl e path s throug h the state spac e and assign s a probabilit y to each one. The probabilit y of
transitionin g from the curren t state to anothe r one depend s only on the curren t state , not on any
prior part of the path . (Thi s is the Marko v propert y mentione d in Chapte r 17.)
The top of Figur e 24.3 5 is a Marko v mode l with seve n state s (circles) , each correspondin g
to the productio n of a phone . The arrow s denot e allowabl e transition s betwee n states , and each
transitio n has a probabilit y associate d with it.3 Ther e are only two possibl e path s throug h the
model , one correspondin g to the phon e sequenc e [t ow m ey t ow] and the othe r to [t ow m aa
t ow] . The probabilit y of a path is the produc t of the probabilitie s on the arcs that mak e up the
path. In this case , mos t of the arc probabilitie s are 1 and we have
P([towmeytow]  ("tomato" ) = P([r«w»iaa?ow ] ("tomato" ) = 0.5
The secon d sourc e of phoneti c variatio n is coarticulation . Remembe r that speec h sound s are
produce d by movin g the tongu e and jaw and forcin g air throug h the voca l tract . Whe n the speake r
is talkin g slowl y and deliberately , there is time to plac e the tongu e in just the righ t spot befor e
producin g a phone . Bu t whe n the speake r is talkin g quickl y (or sometime s even at a norma l
pace) , the movement s slur together . For example , the [t] phon e is produce d with the tongu e at
the top of the mouth , wherea s the [ow ] has the tongu e near the bottom . Whe n spoke n quickly ,
the tongu e often goes to an intermediat e position , and we get [tah ] rathe r than [tow] . The botto m
half of Figur e 24.3 5 give s a mor e complicate d pronunciatio n mode l for "tomato " that take s this
coarticulatio n effec t into account . In this mode l ther e are four distinc t path s and we have
P([towmeytow]\"tomato")  = P([towmaa  tow]  ("tomato" ) = 0.1
P([tahmeytow]\"tomato")  = P([tahmaatow]\  "tomato" ) = 0.4
Simila r model s woul d be constructe d for ever y wor d we wan t to be able to recognize . Now if the
speec h signa l were a list of phones , then we woul d be done with the acousti c model . We coul d
take a give n inpu t signa l (e.g. , [towmeytow] ) and comput e P(signal\words)  for variou s wor d
string s (e.g. , "tomato, " "toe may tow, " and so on). We coul d then combin e thes e with P(words)
value s take n from the languag e mode l to arriv e at the words  that maximiz e P(words\signal).
Unfortunately , signa l processin g does not give us a strin g of phones . So all we can do
so far is maximiz e P(words\phones).  Figur e 24.3 6 show s how we can comput e P(signal\phone)
using a mode l calle d a hidde n Marko v mode l or HMM . The mode l is for a particula r phone ,
3 Arc s with probabilit y I are unlabelle d in Figur e 24.35 . The 0.5 number s are estimate s base d on the two authors '
preferre d pronunciations .
Sectio n 24.7 . Speec h Recognitio n 763
Word mode l with dialec t variation :
Word  mode l with coarticulatio n and dialec t variations :
Figur e 24.3 5 Tw o pronunciatio n model s of the wor d "tomato. " The top one account s for
dialec t differences . The botto m one does that and also account s for a coarticulatio n effect .
[m], but all phone s will have model s with simila r topology . A hidde n Marko v mode l is just like
a regula r Marko v mode l in that it describe s a proces s that goes throug h a sequenc e of states . The
differenc e is that in a regula r Marko v model , the outpu t is a sequenc e of state names , and becaus e
each state has a uniqu e name , the outpu t uniquel y determine s the path throug h the model . In a
hidde n Marko v model , each state has a probabilit y distributio n of possibl e outputs , and the same
outpu t can appea r in mor e than one state.4 HMM s are calle d hidde n model s becaus e the true
state of the mode l is hidde n from the observer . In general , whe n you see that an HM M output s
some symbol , you can't be sure wha t state the symbo l came from .
Suppos e our speec h signa l is processe d to yield the sequenc e of vecto r quantizatio n value s
[C1 ,C4,C6] . From the HM M in Figur e 24.36 , we can comput e the probabilit y that this sequenc e
was generate d by the phon e [m] as follows . First , we note that there is only one path throug h the
mode l that coul d possibl y generat e this sequence : the path from Onse t to Mid to End , wher e the
outpu t label s from the three state s are C1, C4, and C6, respectively . By lookin g at the probabilitie s
on the transitio n arcs , we see that the probabilit y of this path is 0.7 x 0.1 x 0.6 (thes e are the
value s on the three horizonta l arrow s in the middl e of the Figur e 24.36) . Next , we look at the
outpu t probabilitie s for thes e state s to see that the probabilit y of [C 1 ,C4,C6 ] give n this path is
0.5 x 0.7 x 0.5 (thes e are the value s for P(Cl\Onset),  P(C4\Mid)  and P(C6\End),  respectively) .
So the probabilit y of [Cl ,C4,C6 ] give n the [m] mode l is
/'([Cl,C4,C6]|[m] ) = (0.7 x 0.1 x 0.6) x (0.5 x 0.7 x 0.5) = 0.0073 5
4 Not e that this mean s that the "tomato " model s in Figur e 24.3 5 are actuall y hidde n Marko v models , becaus e the sam e
outpu t (e.g. , [t]) appear s on more  than one state .
764 Chapte r 24 . Perceptio n
Phon e HMM for [m]:
0.3 0. 9 0. 4
Outpu t probabilitie s for the phon e HMM :
Onset : Mid : End :
C1:0. 5 C3:0. 2 C4:0. 1
C2:0. 2 C4:0. 7 C6:0. 5
C3:0. 3 C5:0. 1 C7:0. 4
Figur e 24.3 6 A n HMM for the phon e [m]. Eac h state has severa l possibl e outputs , each with
its own probability .
We coul d repea t the calculatio n for all the othe r phon e model s to see whic h one is the mos t
probabl e sourc e of the speec h signal .
Actually , mos t phone s have a duratio n of 50­10 0 milliseconds , or 5­10 frame s at 10
msec/frame . So the [C1,C4,C6 J sequenc e is unusuall y quick . Suppos e we have a more typica l
speake r who generate s the sequenc e [C1,C1,C4,C4,C6,C6 J whil e producin g the phone . It turn s
out there are two path s throug h the mode l that generat e this sequence . In one of them both C4s
come from the Mid state (not e the arcs that loop back) , and in the othe r the secon d C4 come s
from the End state . We calculat e the probabilit y that this sequenc e cam e from the [m] mode l
in the sam e way : take the sum over all possibl e path s of the probabilit y of the path time s the
probabilit y that the path generate s the sequence .
P([Cl,C\,C4,  C4, C6, C6]jLm] ) =
(0.3 x 0.7 x 0.9 x 0.1 x 0.4 x 0.6) x (0.5 x 0.5 x 0.7 x 0.7 x 0.5 x 0.5) +
(0.3 x 0.7 x 0.1 x 0.4 x 0.4 x 0.6) x (0.5 x 0.5 x 0.7 x 0.1 x 0.5 x 0.5)
= 0.000147 7
We see that the loop s in the phon e mode l allow the mode l to represen t both fast and slow speech ,
a very importan t sourc e of variation . Th e multipl e vecto r quantizatio n value s on each state
represen t othe r source s of variation . Altogether , this make s for a fairl y powerfu l model . The
hard part is gettin g good probabilit y value s for all the parameters . Fortunately , there are way s of
acquirin g thes e number s from data , as we shal l see.
Puttin g the model s togethe r
We have describe d thre e models . The languag e bigra m mode l give s us P(wordi\wordi­\}.  The
word pronunciatio n HM M give s us P(phones\word).  The phon e HM M give s us P(signal\phone).
If we wan t to comput e P(words\signal),  we will need to combin e thes e model s in som e way . One
Sectio n 24.7 . Speec h Recognitio n 76 5
approac h is to combin e them all into one big HMM . The bigra m mode l can be though t of as an
HMM in whic h ever y state correspond s to a wor d and ever y wor d has a transitio n arc to ever y
other word . Now replac e each word­stat e with the appropriat e wor d model , yieldin g a bigge r
mode l in whic h each state correspond s to a phone . Finally , replac e each phone­stat e with the
appropriat e phon e model , yieldin g an even bigge r mode l in whic h each state correspond s to a
distributio n of vecto r quantizatio n values .
Some speec h recognitio n system s complicat e the pictur e by dealin g with coarticulatio n
effect s at eithe r the word/wor d or phone/phon e level . For example , we coul d use one phon e
mode l for [ow] whe n it follow s a [t] and a differen t mode l for [ow] whe n it follow s a [g]. Ther e
are man y trade­off s to be made— a mor e comple x mode l can handl e subtl e effects , but it will
be harde r to train . Regardles s of the details , we end up with one big HM M that can be used to
comput e P(words\signal).
The searc h algorith m
From a theoretica l poin t of view , we hav e just wha t we aske d for: a mode l that compute s
P(words\signal).  All we have to do is enumerat e all the possibl e word strings , and we can assig n
a probabilit y to each one. Practically , of course , this is infeasible , becaus e ther e are too man y
candidat e word strings . Fortunately , there is a bette r way .
VITERBIALGORITH M Th e Viterb i algorith m take s an HM M mode l and an outpu t sequence , [C\,  €2, • • •, Cn],
and return s the mos t probabl e path throug h the HM M that output s the sequence . It also return s
the probabilit y for the path . Thin k of it as an iterativ e algorith m that first find s all the path s that
outpu t the first symbol , C\. Then , for each of thos e path s it find s the mos t probabl e path that
output s the rest of the sequence , give n that we have chose n a particula r path for C\. So far this
doesn' t soun d very promising . If the lengt h of the sequenc e is n and there are M differen t state s
in the model , then this algorith m woul d seem to be at least O(M").
The key poin t of the Viterb i algorith m is to use the Marko v propert y to mak e it mor e
efficient . The Marko v propert y says that the mos t probabl e path for the rest of any sequenc e can
depen d only on the state in whic h it starts , not on anythin g else abou t the path that got there . Tha t
mean s we need not look at all possibl e path s that lead to a certai n state ; for each state , we only
need to keep track of the most  probable  path that ends in that state . Thus , the Viterb i algorith m
is an instanc e of dynami c programming .
Figur e 24.3 7 show s the algorith m workin g on the HM M from Figur e 24.3 6 and the outpu t
sequenc e [Cl ,C3,C4,C61 . Each colum n represent s one iteratio n of the algorithm . In the leftmos t
column , we see that there is only one way to generat e the sequenc e [Cl] , with the path [Onset] .
The oval labelle d "Onse t 0.5" mean s that the path ends in the Onse t state and has probabilit y
0.5. The arc leadin g into the oval has the labe l "1.0 ; 0.5," whic h mean s that the probabilit y of
makin g this transitio n is 1.0, and the probabilit y of outputtin g a Cl, give n that the transitio n is
made , is 0.5. In the secon d column , we conside r all the possibl e continuation s of the path s in
the first colum n that coul d lead to the outpu t [Cl ,C3] . Ther e are two such paths , one endin g in
the Onse t state and one in the Mid state . In the third column , it gets mor e interesting . Ther e are
two path s that lead to the Mid state , one from Onse t and the othe r from Mid . The bold arro w
indicate s that the path from Mid is mor e probabl e (it has probabilit y 0.0441) , so that is the only
766 Chapte r 24 . Perceptio n
one we have to remember . The path [Onset,Onset,Mid ] has a lowe r probability , 0.022 , so it is
discarded . We continu e in this fashio n unti l we reac h the FINA L state , with probabilit y 0.0013 .
By tracin g backward s and followin g the bold arrow s wheneve r there is a choice , we see that the
most probabl e path is [Onset,Onset,Mid,End,Final] . The Viterb i algorith m is O(bMn),  wher e b
is the branchin g facto r (the numbe r of arcs out of any state) . If the mode l is fully connected , then
b­M,  and the algorith m is O(M2ri), whic h is still quit e an improvemen t over O(M").
[C1,C3 ] [C1,C3,C4 ] ,C3,C4,C6 ]
;' Onse t '• •  Onse t '•'•, o.o ?.;—~­\  o.o ;
Mid
0.01.0; 0.5 / onse t \ 0.3; 0.3 / onse t \ 0­3; o.p / Onset' .
Figur e 24.37 A  diagra m of the Viterb i algorith m computin g the mos t probabl e path (and its
probability ) for the outpu t [C1 ,C3,C4,C6 ] on the HM M from Figur e 24.36 .
Trainin g the mode l
The HMM approac h is used in speec h recognitio n for two reasons . First , it is a reasonabl y good
performanc e element—w e saw that the Viterb i algorith m is linea r in the lengt h of the input . Mor e
importantly , HMM s can be learne d directl y from a trainin g set of [signal,words ] pairs . Thi s is
importan t becaus e it is far too difficul t to determin e all the parameter s by hand . Ther e are othe r
approache s that mak e bette r performanc e element s than HMMs , but they requir e the trainin g data
to be labelle d on a phone­by­phon e basis rathe r than a sentence­by­sentenc e basis , and that too
is a difficul t task . The standar d algorith m for trainin g an HM M is calle d the Baum­Welc h or
forward­backwar d algorithm . Rabine r (1990 ) give s a tutoria l on this and othe r HMM algorithms .
Sectio n 24.8 . Summar y 76 7
The best curren t speec h recognitio n system s recogniz e from abou t 80% to 98% of the word s
correctly , dependin g on the qualit y of the signal , the allowabl e language , the lengt h of each input ,
and the variatio n in speakers . Speec h recognitio n is easy whe n ther e is a good microphone , a
small vocabulary , a stron g languag e mode l that predict s wha t word s can com e next , a limi t of
one­wor d utterance s (or a requiremen t for pause s betwee n words) , and whe n the syste m can be
traine d specificall y for a singl e speaker . The system s are not as accurat e over phon e lines , whe n
there is nois e in the room , whe n there is a large vocabular y with no restrictions , whe n the word s
of an utteranc e run together , and whe n the speake r is new to the system .
24.8 SUMMAR Y
Althoug h perceptio n appear s to be an effortles s activit y for humans , it require s a significan t
amoun t of sophisticate d computation . Thi s chapte r studie s visio n as the prim e exampl e of
perceptua l informatio n processing . The goal of visio n is to extrac t informatio n neede d for tasks
such as manipulation , navigation , and objec t recognition . We also looke d at speec h recognition .
• The proces s of imag e formatio n is well­understoo d in its geometri c and physica l aspects .
Give n a descriptio n of a 3­D scene , we can easil y produc e a pictur e of it from some arbitrar y
camer a positio n (the graphic s problem) . Invertin g the proces s by goin g from an imag e to
a descriptio n of the scen e is difficult .
• To extrac t the visua l informatio n necessar y for the task s of manipulation , navigation ,
and recognition , intermediat e representation s have to be constructed . Image­processin g
algorithm s extrac t primitiv e element s from the image , such as edge s and regions .
• In the image , ther e exis t multipl e cues that enabl e one to obtai n 3­D informatio n abou t
the scene . Thes e includ e motion , stereopsis , texture , shading , and contou r analysis . Each
of these cues relie s on backgroun d assumption s abou t physica l scene s in orde r to provid e
unambiguou s interpretations .
• Objec t recognitio n in its full generalit y is a very hard problem . We discusse d two relativel y
simpl e techniques—alignmen t and indexin g usin g geometri c invariants—tha t provid e ro­
bust recognitio n in restricte d contexts .
• Speec h recognitio n is a proble m in diagnosis . It can be solve d with a languag e mode l and
an acousti c model . Curren t emphasi s is on system s that do wel l both as a performanc e
elemen t and a learnin g element .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
Systemati c attempt s to understan d huma n visio n can be trace d back to ancien t times . Eucli d
(ca. 300 B.C. ) wrot e abou t natura l perspective , the mappin g that associate s with each poin t P in
the three­dimensiona l worl d the directio n of the ray OP joinin g the cente r of projectio n O to the
768 Chapte r 24 . Perceptio n
point P. He was well awar e of the notio n of motio n parallax . The mathematica l understandin g
of perspectiv e projection , this time in the contex t of projectio n onto plana r surfaces , had its
next significan t advanc e in the fifteent h centur y in Renaissanc e Italy . Brunellesch i (1413 ) is
usuall y credite d with creatin g the first painting s base d on geometricall y correc t projectio n of the
three­dimensiona l scene . In 1435 , Albert i codifie d the rules and inspire d generation s of artist s
whos e artisti c achievement s amaz e us to this day (Kemp , 1990) . Particularl y notabl e in their
developmen t of the scienc e of perspective , as it was calle d in thos e days , were Leonard o Da Vinc i
and Albrech t Diirer . Leonardo' s late fifteent h centur y description s of the interpla y of ligh t and
shade (chiaroscuro) , umbr a and penumbr a region s of shadows , and aeria l perspectiv e are still
worth readin g in translatio n (Kemp , 1989) .
Althoug h perspectiv e was know n to the Greeks , they were curiousl y confuse d by the role
of the eyes in vision . Aristotl e though t of the eyes as device s emittin g rays , rathe r in the manne r
of moder n laser rang e finders . This mistake n view was laid to rest by the work of Arab scientists ,
such as Alhaze n in the tent h century . The developmen t of variou s kind s of camera s followed .
These consiste d of room s (camera  is Lati n for chamber ) wher e ligh t woul d be let in throug h a
smal l hole in one wall to cast an imag e of the scen e outsid e on the opposit e wall . Of course , in all
these cameras , the imag e was inverted , whic h cause d no end of confusion . If the eye was to be
though t of as such an imagin g device , how do we see righ t side up? Thi s exercise d the greates t
brain s of the era (includin g Leonardo) . It took the wor k of Keple r and Descarte s to settl e the
question . Descarte s place d an eye from whic h the opaqu e cuticl e had been remove d in a hole in a
windo w shutter . This resulte d in an inverte d imag e bein g forme d on a piece of pape r laid out on
the retina . Whil e the retina l imag e is indee d inverted , this need not caus e a proble m if the brain
interpret s the imag e the righ t way . In moder n jargon , one just has to acces s the data structur e
appropriately .
The next majo r advance s in the understandin g of visio n took plac e in the nineteent h
century . The work of Helmholt z and Wundt , describe d in Chapte r 1, establishe d psychophysica l
experimentatio n as a rigorou s scientifi c discipline . Throug h the wor k of Young , Maxwell , and
Helmholtz , a trichromati c theor y of colo r visio n was established . Tha t human s can see dept h
if the image s presente d to the left and righ t eyes are slightl y differen t was demonstrate d by
Wheatstone' s (1838 ) inventio n of the stereoscope . The devic e immediatel y becam e very popula r
in parlor s and salon s throughou t Europe . Th e essentia l concep t of binocula r stereopsis , that
two image s of a scen e take n from slightl y differen t viewpoint s carr y informatio n sufficien t to
obtai n a 3­D reconstructio n of the scene , was exploite d in the field of photogrammetry . Key
mathematica l result s were obtained—Krupp a (1913 ) prove d that give n two view s of five distinc t
points , one coul d reconstruc t the rotatio n and translatio n betwee n the two camer a position s as
well as the dept h of the scen e (up to a scale factor) . Althoug h the geometr y of stereopsi s had
been understoo d for a long time , the correspondenc e proble m in photogrammetr y used to be
solve d by human s tryin g to matc h up correspondin g points . The amazin g abilit y of human s
in solvin g the correspondenc e proble m was illustrate d by Julesz' s inventio n of the rando m dot
stereogra m (Julesz , 1971) . Bot h in compute r visio n and in photogrammetry , muc h effor t was
devote d to solvin g this proble m in the 1970 s and 1980s .
The secon d half of the nineteent h centur y was a majo r foundationa l perio d for the psy­
chophysica l stud y of huma n vision . In the firs t half of the twentiet h centur y the mos t signif ­
icant researc h result s in visio n were obtaine d by the Gestal t schoo l of psycholog y led by Max
Sectio n 24.8 . Summar y 76 9
Wertheimer . Wit h the sloga n "The whol e is greate r than the sum of the parts, " they laid primar y
emphasi s on groupin g processes , both of contour s and regions . Constructin g computationa l
model s of these processe s remain s a difficul t proble m to this day.
The perio d afte r Worl d War 2 was marke d by renewe d activity . Mos t significan t was
the wor k of J. J. Gibso n (1950 ; 1979) , who pointe d out the importanc e of optica l flow as
well as textur e gradient s in the estimatio n of environmenta l variable s such as surfac e slan t and
tilt. He reemphasize d the importanc e of the stimulu s and how rich it was . Gibson , Olum ,
and Rosenblat t (1955 ) pointe d out that the optical  flow field containe d enoug h informatio n to
determin e the egomotio n of the observe r relativ e to the environment . In the computationa l visio n
community , work in this area and the (mathematicall y equivalent ) area of structur e from motio n
develope d mainl y in the 1980s , followin g the semina l work s of Koenderinkan d van Doom (1975) ,
Ullma n (1979) , and Longuet­Higgin s (1981) . Faugera s (1993 ) present s a comprehensiv e accoun t
of our understandin g in this area . In the 1990s , with the increas e in compute r spee d and storage ,
the importanc e of motio n sequenc e analysi s from digita l vide o is growin g rapidly .
In computationa l vision , majo r earl y work s in shap e from textur e are due to Bajsc y and
Lieberman n (1976 ) and Steven s (1981) . Wherea s this work was for plana r surfaces , a compre ­
hensiv e analysi s for curve d surface s is due to Cardin g (1992 ) and Mali k and Rosenholt z (1994) .
In the computationa l visio n community , shap e from shadin g was first studie d by Berthol d
Horn (1970) . Hor n and Brook s (1989 ) presen t an extensiv e surve y of the main paper s in the
area. Thi s framewor k mad e a numbe r of simplifyin g assumptions , the mos t critica l of whic h
was ignorin g the effec t of mutua l illumination . The importanc e of mutua l illuminatio n has been
well­appreciate d in the compute r graphic s community , wher e ray tracin g and radiosit y have been
develope d precisel y to take mutua l illuminatio n into account . A theoretica l and empirica l critiqu e
may be foun d in Forsyt h and Zisserma n (1991) .
In the area of shap e from contour , after the key initia l contribution s of Huffma n (1971 ) and
Clowe s (1971) , Mackwort h (1973 ) and Sugihar a (1984 ) complete d the analysi s for polyhedra l
objects . Mali k (1987 ) develope d a labelin g schem e for piecewis e smoot h curve d objects . Un ­
derstandin g the visua l event s in the projectio n of smoot h curve d object s require s an interpla y of
differentia l geometr y and singularit y theory . The best stud y is Koenderink' s (1990 ) Solid  Shape.
In the area of three­dimensiona l objec t recognition , the semina l work was Roberts' s (1963 )
thesis at MIT . It is often considere d to be the first PhD thesi s in compute r visio n and introduce d
severa l key idea s includin g edge detectio n and model­base d matching . The idea of alignment ,
first introduce d by Roberts , resurface d in the 1980 s in the work of Low e (1987 ) and Huttenloche r
and Ullma n (1990) . Generalize d cylinder s were introduce d by Binfor d in 1971 , and were used
extensivel y by Brook s in the ACRONY M syste m (Brooks , 1981) . Geometrica l invariant s were
studie d extensivel y in the late nineteent h centur y by Englis h and Germa n mathematicians . Thei r
use in objec t recognitio n is surveye d by Mund y and Zisserma n (1992) , Excellen t result s have
been achieve d even in cluttere d scene s (Rothwel l et al, 1993) .
A wor d abou t the researc h methodolog y used in compute r vision . The early developmen t
of the subject , like that of rest of AI, was mostl y throug h Ph.D . these s that consiste d largel y of
description s of implemente d systems . Th e wor k lacke d significan t contac t with the literatur e
on huma n visio n and photogrammetry , in whic h man y of the sam e problem s had been studied .
Davi d Mar r playe d a majo r role in connectin g compute r visio n to the traditiona l areas of biologica l
vision—psychophysic s and neurobiology . His mai n work , Vision  (Marr , 1982) , was publishe d
770________________________ _ Chapte r 24 . Perceptio n
posthumously . Tha t book convey s the excitemen t of workin g in visio n bette r than any writte n
since , despit e the fact that man y of the specifi c hypothese s and model s propose d by Mar r have
not stoo d the test of time .
Unde r Marr' s influence , reconstructio n of the three­dimensiona l scen e from variou s cues
becam e the dominan t ideolog y of the day. Thi s of cours e prove d to be a difficul t problem , and
it was inevitabl e that peopl e woul d questio n whethe r it was reall y necessary . The old idea s of
Gibson—activ e visio n and affordances—cam e back . The mos t solid proo f that reconstructio n
was not necessar y for man y (or most ) task s cam e from the wor k of Dickmann s in Germany ,
who demonstrate d robus t drivin g usin g a contro l syste m perspectiv e (Dickmann s and Zapp ,
1987) . As a genera l philosophy , activ e visio n was advocate d by Ruzen a Bajcs y (1988 ) and John
Aloimono s (1988) . A numbe r of paper s are collecte d in a specia l issu e of CVGIP  (Aloimonos ,
1992) . In the 1990s , the dominan t perspectiv e is that of visio n as a set of processe s aime d at
extractin g informatio n for manipulation , navigation , and recognition .
Eye, Brain  and Vision  by Davi d Hube l (1988 ) and Perception  by Irvin Roc k (1984 ) pro­
vide excellen t introduction s to the field of biologica l vision . A Guided  Tour  of Computer
Vision  (Nalwa , 1993 ) is a good genera l introductio n to compute r vision ; Robot  Vision  (Horn ,
1986 ) and Three­Dimensional  Computer  Vision  (Faugeras , 1993 ) cove r mor e advance d topics .
Two of the mai n journal s for compute r visio n are the IEE E Transactions  on Pattern  Analysis
and Machine  Intelligence  and the International  Journal  of Computer  Vision.  Compute r visio n
conference s includ e ICC V (Internationa l Conferenc e on Compute r Vision) , CVP R (Compute r
Visio n and Patter n Recognition) , and ECC V (Europea n Conferenc e on Compute r Vision) .
The hidde n Marko v mode l was firs t used to mode l languag e by Marko v himsel f in a
letter­sequenc e analysi s of the text of Eugene  Onegin  (Markov , 1913) . Earl y developmen t of
algorithm s for inferrin g Marko v model s from data was carrie d by Baum and Petri e (1966) . Thes e
were applie d to speec h by Bake r (1975 ) and Jeline k (1976) . In 1971 , the Defens e Advance d
Researc h Project s Agenc y (DARPA ) of the Unite d State s Departmen t of Defense , in cooperatio n
with a numbe r of researc h centers , set out a five­yea r plan for speec h recognitio n research . The
two mos t significan t system s to emerg e from this massiv e effor t were HEARSAY­I I (Erma n et al.,
1980 ) and HARP Y (Lowerr e and Reddy , 1980) . HARP Y was the only syste m that clearl y met the
rigorou s specification s of the five­yea r plan . It used a highl y compile d networ k representatio n for
all possibl e meaningfu l sequence s of speec h elements . HEARSAY­II , however , has had a greate r
ARCHITECTUR E influenc e on othe r researc h becaus e of its use of the blackboar d architecture . HEARSAY­I I was
designe d as an exper t syste m with a numbe r of more or less independent , modula r knowledg e
source s whic h communicate d via a commo n blackboar d from whic h they coul d writ e and read .
Becaus e this representatio n was less compile d and more modula r than HARPY'S , HEARSAY­I I was
much easie r to comprehen d and modify , but was not fast enoug h to mee t the DARP A criteria .
A good introductio n to speec h recognitio n is give n by Rabine r and Juan g (1993) . Waibe l
and Lee (1990 ) collec t importan t paper s in the area , includin g som e tutoria l ones . Lee (1989 )
describe s a complet e moder n speec h recognitio n system . The presentatio n in this chapte r drew
on the surve y by Kay , Gawron , and Norvi g (1994) , and on an unpublishe d manuscrip t by Dan
Jurafsky . Speec h recognitio n researc h is publishe d in Computer  Speech  and  Language  and the
IEEE Transactions  on Acoustics,  Speech,  and  Signal  Processing,  and at the DARP A Workshop s
on Speec h and Natura l Languag e Processing .
Sectio n 24.8 . Summar y 771
EXERCISE S
24.1 In the shado w of a tree with a dense , leaf y canopy , one sees a numbe r of ligh t spots .
Surprisingly , they all appea r to be circular . Why ? Afte r all, the gaps betwee n the leave s throug h
whic h the sun shine s throug h are not likel y to be circular .
24.2 Labe l the line drawin g in Figur e 24.38 , assumin g that the outsid e edge s have been labelle d
as occludin g and that all vertice s are trihedral . Do this by a backtrackin g algorith m that examine s
the vertice s in the orde r A, B, C, and D, pickin g at each stage a choic e consisten t with previousl y
labelle d junction s and edges . Now try the vertice s in the orde r B, D, A, and C.
Figur e 24.38 A  drawin g to be labelled , in whic h all vertice s are trihedral .
24.3 Conside r an infinitel y long cylinde r of radiu s r oriente d with its axis alon g the y­axis . The
cylinde r has a Lambertia n surfac e and is viewe d by a camer a alon g the positiv e z­axis . Wha t will
you expect.t o see in the imag e of the cylinde r if the cylinde r is illuminate d by a poin t sourc e at
infinit y locate d on the positiv e jc­axis . Explai n your answe r by drawin g the isobrightnes s contour s
in the projecte d image . Are the contour s of equa l brightnes s uniforml y spaced ?
24.4 Edge s in an imag e can correspon d to a variet y of scen e events . Conside r the photograp h
on the cove r of your book and assum e that it is a pictur e of a real 3­D scene . Identif y a set
of ten differen t brightnes s edge s in the image , and for each , decid e whethe r it correspond s to a
discontinuit y in (a) depth , (b) surfac e normal , (c) reflectance , or (d) illumination .
24.5 Sho w that convolutio n with a give n function / commute s with differentiation , that is,
(/*£)'=/*£ '
772 Chapte r 24 . Perceptio n
24.6 A stere o syste m is bein g contemplate d for terrai n mapping . It will consis t of two CCD
cameras , each havin g 512x51 2 pixel s on a 10 cm x 10 cm squar e sensor . The lense s to be used
have foca l lengt h 16 cm, and focu s is fixed at infinity . For correspondin g point s (u\, vi) in the left
imag e and (u2,  V2) in the righ t image , vi = V2 as the x­axe s in the two imag e plane s are paralle l
to the epipola r lines . The optica l axes of the two camera s are parallel . The baselin e betwee n the
camera s is 1 meter .
a. If the neares t rang e to be measure d is 16 meters , wha t is the larges t disparit y that will occu r
(in pixels) ?
b. Wha t is the rang e resolution , due to the pixe l spacing , at 16 meters ?
c. Wha t rang e correspond s to a disparit y of one pixel ?
24.7 In Figur e 24.30 , physicall y measur e the cros s ratio of the point s ABCD  as well as of the
point s A'B'C'D'.  Are they equal ?
24.8 W e wish to use the alignmen t algorith m in an industria l situatio n wher e flat part s are
movin g alon g a conveyo r belt and bein g photographe d by a camer a verticall y abov e the conveyo r
belt. The pose of the part is specifie d by three variables , one for the rotatio n and two for the 2­D
position . This simplifie s the proble m and the functio n FlND­TRANSFOR M need s only two pairs
of correspondin g imag e and mode l feature s to determin e the pose . Determin e the worst­cas e
complexit y of the alignmen t procedur e in this context .
24.9 Rea d this chapte r from the beginnin g unti l you find ten example s of homophones . Doe s
the statu s of a word as a homophon e depen d on the accen t of the speaker ?
24.10 Calculat e the mos t probabl e path throug h the HM M in Figur e 24.3 6 for the outpu t
sequenc e [C1,C2,C3,C4,C4,C6,C7] . Also give its probability .
24.11 Som e sport s announcer s hav e bee n know n to celebrat e a scor e with the draw n out
pronunciatio n [g ow ow ow ow ow ow el]. Dra w a wor d HM M for "goal " such that the mos t
probabl e path has a sequenc e of four [ow]s , but any numbe r greate r than 1 is possible .
24.12 Th e Viterb i algorith m find s the mos t probabl e sequenc e of phone s correspondin g to
the speec h signal . Unde r the assumptio n that som e word s can be pronounce d with mor e than
one sequenc e of phones , explai n why the Viterb i algorith m only compute s an approximatio n to
P(words\signal}.
25ROBOTIC S
In which  agents  are endowed  with  physical  effectors with  which to  do mischief.
25.1 INTRODUCTIO N
ROBO T
AUTONOMOU S
ROBOT SThe Robo t Institut e of Americ a define s a robo t as a programmable,  multifunction  manipulator
designed  to move  material, parts,  tools,  or specific  devices  through  variable  programmed  motions
for the performance  of a variety  of tasks.  Thi s definitio n is not very demanding ; a conveye r belt
with a two­spee d switc h woul d arguabl y satisf y it.
We will defin e robo t simpl y as an active,  artificial  agent  whose environment is  the physical
world.  The active  part rules out rocks , the artificial  part rules out animals , and the physical  part
rules out pure softwar e agent s or softbots , whos e environmen t consist s of compute r file systems ,
database s and networks . We will be concerne d primaril y with autonomou s robots , thos e that
make decision s on their own , guide d by the feedbac k they get from their physica l sensors .
Most of the desig n of an autonomou s robo t is the sam e as the desig n of any autonomou s
agent . To som e extent , we coul d take a generi c plannin g agen t (Chapte r 11) or decision­makin g
agen t (Chapte r 16), equi p it with wheels , grippers , and a camera , poin t it out the door , and wish it
good luck . Unfortunately , unles s we pointe d it at an exceptionall y benig n environment , it woul d
not fare very well . The real world , in general , is very demanding . We can see this by considerin g
the five propertie s of environment s from page 46:
• The real worl d is inaccessible . Sensor s are imperfect , and in any case can only perceiv e
stimul i that are near the agent .
• The real worl d is nondeterministic , at leas t from the robot' s poin t of view . Wheel s slip,
batterie s run down , and parts break , so you neve r know if an actio n is goin g to work . That
mean s a robo t need s to deal with uncertaint y (Par t V).
• The real worl d is nonepisodic —the effect s of an actio n chang e over time . So a robo t has
to handl e sequentia l decisio n problem s (Chapte r 17) and learnin g (Par t VI).
773
774 Chapte r 25 . Robotic s
• The real worl d is dynamic . Therefore , a robo t has to know whe n it is wort h deliberatin g
and whe n it is bette r to act immediately .
• The real worl d is continuous , in that state s and action s are draw n from a continuu m of
physica l configuration s and motions . Becaus e this make s it impossibl e to enumerat e the
set of possibl e actions , man y of the searc h and plannin g algorithm s describe d in earlie r
chapter s will need to be modified .
We coul d go on abou t the uniqu e propertie s of robotics , but instea d we will refe r you to Ex­
ercise 25.1 . It does not requir e any additiona l reading , it will give you an instan t and viscera l
appreciatio n of the problem s of robotic s (and their possibl e solutions) , and it can be amusing .
This chapte r has three main points . First , we look at the task s that robot s perfor m (Sec ­
tion 25.2 ) and the specia l effector s and sensor s they use (Sectio n 25.3) . Second , we step away
from robots  per se and look at agen t architecture s for inaccessible , nondeterministic , nonepisodic ,
dynami c domain s (Sectio n 25.4) . Third , we look at the specifi c proble m of selectin g action s in
a continuou s state spac e (Section s 25.5 and 25.6) . The algorithm s rely on a level of computa ­
tiona l geometr y that is mor e suite d for a specialize d advance d text , so we only give qualitativ e
description s of the problem s and solutions .
25.2 TASKS : WHA T ARE ROBOT S GOO D FOR ?
Whil e human s do a wide variet y of thing s usin g mor e or less the sam e body , robo t design s vary
widel y dependin g on the task for whic h they are intended . In this section , we surve y som e of the
tasks , and in the next section , we look at the availabl e parts (effector s and sensors ) that mak e up
a robot' s body .
Manufacturin g and material s handlin g
Manufacturin g is seen as the traditiona l domai n of the robot . The repetitiv e tasks on a productio n
line are natura l target s for automation , and so in 1954 Georg e Devo l patente d the programmabl e
robot arm, base d on the same technology—punc h cards—tha t was used in the Jacquar d loom 150
years earlier . By 1985 , there were abou t 180,00 0 robot s in productio n lines aroun d the world ,
with Japan , the Unite d States , and Franc e accountin g for 150,00 0 of them . The automotiv e and
microelectronic s industrie s are majo r consumer s of robots . However , mos t robot s in use toda y
are very limite d in thei r abilitie s to sens e and adapt . Autonomous  robot s are still strugglin g
for acceptance . The reason s for this are historical , cultural , and technological . Manufacturin g
existe d and burgeone d long befor e robot s appeared , and man y trick s were develope d for solving
manufacturin g problem s withou t intelligence . And it is still true that simpl e machine s are the
best solutio n for simpl e tasks .
Materia l handlin g is anothe r traditiona l domai n for robots . Materia l handlin g is the storage ,
transport , and deliver y of material , whic h can rang e in size from silico n chip s to diese l trucks . The
robot s used for materia l handlin g likewis e vary from table­to p manipulator s to gantr y cranes , and
Sectio n 25.2 . Tasks : Wha t Are Robot s Goo d For? 77 5
includ e man y type s of AGV (autonomou s guide d vehicles , typicall y golf­car t sized four­wheele d
vehicle s that are used to ferry container s aroun d a warehouse) . Handlin g odd­shape d part s is
simplifie d by placin g each part in a cradl e or pallet  that has a base of fixed shape . Bar code s
on the pallet s mak e it easy to identif y and inventor y the parts . But these technique s falte r whe n
applie d to food packagin g and handling , an area likel y to see rapid growt h in the future . The large
variet y of forms , weights , textures , and firmnesse s in familia r food s mak e this task a challeng e
for futur e research .
Robot s have recentl y been makin g their mark in the constructio n industry . Larg e prototyp e
robot s have been buil t that can mov e a one­to n objec t with an accurac y of 2.5 mm in a workspac e
of radiu s 10 m. Shee p shearin g is anothe r impressiv e application . Australi a boast s a populatio n
of 140 millio n sheep , and severa l industria l and academi c developmen t group s have deploye d
automate d shee p shearers . Becaus e shee p com e in differen t sizes , som e kind of sensin g (visio n
or mechanica l probing ) is neede d to get an idea of the righ t overal l shearin g pattern . Tactil e
feedbac k is used to follo w the contour s of the anima l withou t injurin g it.
Gofe r robot s
MOBOT S Mobil e robot s (mobots ) are also becomin g widel y useful . Tw o primar y application s are as
courier s in buildings , especiall y hospitals , and as securit y guards . One compan y has sold over
3000 "mailmobiles. " Thes e robot s respon d to request s from a compute r termina l and carr y
document s or supplie s to a destinatio n somewher e else in the building . The robo t negotiate s
hallway s and elevators , and avoid s collisio n with othe r obstacle s such as human s and furniture .
The robo t is appropriat e for this task becaus e it has high availabilit y (it can work 24 hour s a day) ,
high reliabilit y so that supplie s are not lost or misplaced , and its progres s can be monitore d to
allow preparatio n for its arrival , or detectio n of failure .
We have seen (pag e 586) that autonomou s vehicle s are alread y cruisin g our highways , and
AUV s (autonomou s underwate r vehicles ) are cruisin g the seas . It is far less expensiv e to send a
robot to the botto m of the ocea n than a manne d submarine . Furthermore , a robo t can stay dow n
for month s at a time , makin g scientifi c observations , reportin g on enem y submarin e traffic , or
fixin g problem s in transoceani c cables .
Hazardou s environment s
Mobil e robot s are an importan t technolog y for reducin g risk to huma n life in hazardou s environ ­
ments . Durin g the cleanu p of the Chernoby l disaster , severa l Lunokho d luna r explore r robot s
were converte d to remote­controlle d cleanin g vehicles . In Japa n and France , robot s are used
for routin e maintenanc e of nuclea r plants . Crisi s managemen t is now bein g take n seriousl y as
a technologica l challeng e in the Unite d States . Robot s can reduc e risk to human s in unstabl e
building s after earthquakes , near fire or toxic fumes , and in radioactiv e environments . The y can
also be used for routin e task s in dangerou s situation s includin g toxi c wast e cleanup , deep sea
recovery , exploration , mining , and manipulatio n of biologicall y hazardou s materials . A huma n
operato r is often availabl e to guid e the robot , but it also need s autonom y to recogniz e and respon d
to hazard s to its own and others ' well­being .
776_____________________________________________ _ Chapte r 25 . Robotic s
Autonom y is also essentia l in remot e environment s (suc h as the surfac e of Mars ) wher e
communicatio n delay s are too long to allow huma n guidance . A robo t for hazardou s environment s
shoul d be able to negotiat e very roug h groun d withou t fallin g or tippin g over.1 It shoul d have
sensin g modalitie s that wor k with no ligh t present . It shoul d abov e all avoi d doin g harm to
human s who may be presen t but unconsciou s or disable d in its environment . Achievin g this level
of autonom y alon g with reliabilit y is a challeng e for robotics . But the retur n is very great , and
this area of researc h is boun d to continu e its growth .
Telepresenc e and virtua l realit y
As computer s sprea d from scientific/busines s application s to a consume r technology , telepresenc e
and virtua l realit y continu e to captivat e the publi c imagination . The idea of stayin g in one's hom e
and bein g able to sens e exoti c environments , eithe r real (telepresence ) or imaginar y (virtua l
reality ) is indee d compelling , and is a drivin g forc e behin d som e majo r move s in the compute r
and entertainmen t industries . Amon g the possibl e environment s to whic h one migh t connect ,
there are man y importan t applications . Today , New York City polic e use teleoperate d robot s to
answe r man y of the city' s estimate d 9000 yearl y bom b scares . Architect s and customer s can
walk throug h or fly over a buildin g befor e it is built.2 Oceanographer s can collec t sample s with
a deep­se a submersibl e off the Pacifi c coas t withou t leavin g their offices . Medica l specialist s
can examin e patient s hundred s of mile s away . Surgeon s can practic e new technique s on virtua l
organ models . Or they can operat e with miniatur e implement s throug h a cathete r inserte d in a
tiny incision.3
Much of the researc h in telepresenc e and virtua l realit y fall in the robotic s category .
Robotic s researcher s have for man y year s been studying , designing , and improvin g anthropo ­
morphi c hands . The y hav e also buil t exoskeleton s and glove s that provid e both contro l and
sensor y feedbac k for thos e hands . Tactil e sensin g researc h open s the possibilit y of a true remot e
sense of touch . Realisti c virtua l environment s requir e objec t model s with a full set of physica l
attributes . Simulatio n require s algorithm s that accuratel y facto r in thos e attributes , includin g
inertia , friction , elasticity , plasticity , color , texture , and sound . Ther e remain s muc h work to do
to full y exploi t the possibilitie s of remot e and virtua l presence .
Augmentatio n of huma n abilitie s
A precurso r to telepresenc e involve s puttin g a huma n insid e a large , stron g robot . In 1969 ,
Genera l Electri c buil t the Quadrupeda l Walkin g Machine , an 11­foot , 3000­poun d robo t with a
contro l harnes s in whic h a huma n operato r was strapped . Projec t leade r Ralp h Moshe r remarked
that the man­machin e relationshi p "is so clos e that the experience d operato r begin s to feel as if
those mechanica l legs are his own . You imagin e that you are actuall y crawlin g alon g the groun d
1 Th e robo t DANTE­I I cause d embarrassmen t to its sponsor s whe n it tippe d over whil e explorin g a volcan o in July , 1994 .
2 Th e cove r of this book show s a tiny mode l of Soda Hall , the compute r scienc e buildin g at Berkeley . The mode l was
used for walkthrough s prior to actua l construction .
3 Th e first medica l telepresenc e syste m was deploye d in 1971 , unde r NAS A sponsorship . A mobil e van parke d at the
Papag o India n Reservatio n near Tucson , Arizona , allowe d patient s to be diagnose d remotely .
Sectio n 25.3 . Parts : Wha t Are Robot s Mad e Of? 777
on all fours—bu t with incredibl e strength. " A futuristi c full­bod y exoskeleto n of this kind was
worn by Riple y (Sigourne y Weaver ) in the fina l confrontatio n of the movi e "Aliens. "
No less fascinatin g is the attemp t to duplicat e lost huma n effectors . Whe n an arm or
leg is amputated , the muscle s in the stum p still respon d to signal s from the brain , generatin g
myoelectri c currents . Prostheti c li,mb s can pick up thes e signal s and amplif y them to flex joint s
and mov e artificia l fingers . Som e prosthetic s even have electrocutaneou s feedbac k that give s a
sense of touch . Ther e has also been wor k in givin g human s artificia l sensors . A Japanes e MIT I
project , for example , buil t a prototyp e robo t guid e dog for the blind . It uses ultrasoun d to mak e
sure that its maste r stays in a safe area as they walk along together . Artificia l retina s and cochleas ,
mostl y base d on analo g VLSI , are the subjec t of intensiv e researc h at present .
25.3 PARTS : WHA T ARE ROBOT S MAD E OF?
LINK S
JOINT S
END EFFECTOR SRobot s are distinguishe d from each othe r by the effector s and sensor s with whic h they are
equipped . For example , a mobil e robo t require s som e kind of legs or wheels , and a teleoperate d
robot need s a camera . We will assum e that a robo t has som e sort of rigid body , with rigid link s
that can mov e about . Link s mee t each othe r at joints , whic h allow motion . For example , on a
huma n the uppe r arm and forear m are links , and the shoulde r and elbo w are joints . The palm
is a link , and finger s and thum b have thre e links . Wrist s and finge r joint s are joints . Robot s
need not be so anthropomorphic ; a whee l is a perfectl y good link . Attache d to the fina l link s
of the robo t are end effectors , whic h the robo t uses to interac t with the world . End effector s
may be suctio n cups , squeez e grippers , screwdrivers , weldin g guns , or pain t sprayers , to nam e a
few. Som e robot s have specia l connector s on the last link that allow them to quickl y remov e one
end effecto r and attac h another . The well­equippe d robo t also has one or mor e sensors , perhap s
includin g cameras , infrare d sensors , radar , sonar , and accelerometers .
EFFECTO R
ACTUATO R
DEGRE E OF
FREEDO M
LOCOMOTIO N
MANIPULATIO NEffectors : Tool s for actio n
An effecto r is any devic e that affect s the environment , unde r the contro l of the robot . To have
an impac t on the physica l world , an effecto r mus t be equippe d with an actuato r that convert s
softwar e command s into physica l motion . The actuator s themselve s are typicall y electri c motor s
or hydrauli c or pneumati c cylinders . For simplicity , we will assum e that each actuato r determine s
a singl e motio n or degre e of freedom . For example , an automati c phonograp h turntabl e has three
degree s of freedom . It can spin the turntable , it can raise and lowe r the stylu s arm, and it can
move the arm laterall y to the first track . A side effec t of the motio n of the turntabl e is that one or
more viny l recording s rotat e as well . This motion , assumin g the stylu s has been lowere d into a
recordin g groove , lead s to a usefu l and musica l product .
Effector s are used in two mai n ways : to chang e the positio n of the robo t withi n its
environmen t (locomotion) , and to mov e othe r object s in the environmen t (manipulation) . A
third use, to chang e the shap e or othe r physica l propertie s of objects , is mor e in the realm of
mechanica l engineerin g than robotics , so we will not cove r it.
778 Chapte r 25 . Robotic s
Locomotio n
STATICALL Y STABL E
DYNAMICALL Y
STABL E
NONHOLONOMI C
HOLONOMI CThe vast majorit y of land animal s use legs for locomotion . Legge d locomotio n turn s out to be
very difficul t for robots , and is used only in specia l circumstances . The mos t obviou s applicatio n
is motio n in roug h terrai n with large obstacles . The Amble r robo t (Simmon s et al, 1992) , for
example , is a six­legge d robot , abou t 30 feet tall, capabl e of negotiatin g obstacle s mor e than 6
feet in diameter . The Ambler , unlik e most animals , is a staticall y stabl e walker . Tha t is, it can
pause at any stag e durin g its gait withou t tumblin g over . Staticall y stabl e walkin g is very slow
and energy­inefficient , and the ques t for faster , mor e efficien t legge d machine s has led to a serie s
of dynamicall y stabl e hoppin g robot s (Raibert , 1986) , whic h woul d crash if force d to pause , but
do well as long as they keep moving . Thes e robot s use rhythmi c motio n of four , two, or even a
singl e leg to to contro l the locomotio n of the body in three dimensions . The y do not have enoug h
legs in contac t with the groun d to be stabl e statically , and will fall if their hoppin g motio n stops .
They are dynamicall y stabl e becaus e correction s to leg motio n keep the body uprigh t whe n it is
bumpe d or whe n the groun d is uneven . The contro l of legge d machine s is too comple x a subjec t
to discus s here , excep t to remar k on its difficult y and to marve l at recen t successes—fo r example ,
the one­legge d robo t show n in Figur e 25.1 can "run " in any direction , hop over smal l obstacles ,
and even somersault .
Despit e the anthropomorphi c attraction s of legge d locomotion , whee l or tread locomotio n
is still the mos t practica l for mos t environments . Wheel s and tread s are simple r to build , are
more efficien t than legs, and provid e stati c support . The y are also easie r to control , althoug h they
are not withou t subtl e problem s of their own . Conside r the car­lik e robo t of Figur e 25.2 . We
know from experienc e that withou t obstructions , we can driv e a car to any position , and leav e
it pointin g in any directio n that we choose . Thus , the car has thre e degree s of freedom , two for
its x­y position , and one for its direction . But there are only two actuators , namely , drivin g and
steering . And for smal l motions , the car seem s to have only two degree s of freedom , becaus e we
can mov e it in the directio n it points , or rotat e it slightly , but we canno t mov e it sideways .
It is importan t here to draw the distinctio n betwee n wha t the actuator s actuall y do, namely ,
turnin g or steerin g the wheels , and wha t thes e motion s do to the environment . In this case , the
side effec t of the whee l motio n is to mov e the car to any poin t in a three­dimensiona l space .
Becaus e the numbe r of controllable  degree s of freedo m is only two, whic h is less than the total
degree s of freedo m (three) , this is a nonholonomi c robot . In general , a nonholonomi c robo t has
fewe r controllabl e degree s of freedo m than total degree s of freedom . As a rule, the large r the gap
betwee n controllabl e and total degree s of freedom , the harde r it is to contro l the robot . A car with
a traile r has four total degree s of freedo m but only two controllabl e ones , and take s considerabl e
skill to driv e in reverse . If the numbe r of total and controllabl e degree s freedo m of the syste m is
the same , the robo t is holonomic .
It is possibl e to buil d truly holonomi c mobil e robots , but at the cost of high mechanica l
complexity . It is necessar y to desig n the wheel s or tread s so that motio n in the drivin g directio n is
controlled , but sideway s motio n is free. Holonomi c drive s usuall y replac e the tire or tread with a
series of roller s line d up with the driv e direction . Althoug h these design s mak e life easie r for the
contro l architect , the commo n design s of nonholonomi c mobil e robot s (Figure s 25.2 and 25.3 )
are not that difficul t to control , and thei r mechanica l simplicit y make s them the best choic e in
most situations . The mai n distinctio n to be mad e betwee n differen t design s is whethe r the robo t
Sectio n 25.3 . Parts : Wha t Are Robot s Mad e Of?779
Figur e 25.1 Raibert' s dynamicall y stabl e hoppin g robo t in motion . (© 1986 . MI T Leg
Laboratory . All right s reserved. )
Figur e 25.2 Motio n of a four­wheele d vehicl e with front­whee l steering .
780 Chapte r 25 . Robotic sI
passiv e caste r
independentl y
controlle d wheel s
Figur e 25.3 Th e whee l arrangemen t on the Hilar e mobil e robot , viewe d from the bottom .
has a minimu m turnin g radiu s or can turn on the spot . If the latte r is true , then it is alway s possibl e
to mov e from configuratio n A to B by turnin g on the spot towar d B, movin g to B in a straigh t
line, and rotatin g on the spot to the orientatio n of B. If there is a turning­radiu s constraint , or if
one woul d like to mov e withou t slowin g dow n too muc h (whic h generate s the same constraint) ,
a specia l path planne r is needed . It can be show n that the shortes t path in such case s consist s of
straight­lin e segment s joinin g segment s of circle s with the minimu m radius .
Sometime s the robot' s desig n make s the contro l of locomotio n very difficult . Reversin g a
wheele d vehicl e with severa l trailer s is beyon d the capabilit y of mos t humans ; henc e som e large
fire engine s are fitte d with a secon d steerin g whee l for the back wheel s to mak e it easier . Fire
engine s have only a singl e trailer , but two­traile r example s can be seen at any moder n airport .
Whe n leavin g a gate , mos t aircraf t are drive n by a nose­whee l tender . The nose­whee l tende r
(the car) drive s a long link attache d to the nose whee l (firs t trailer) . The aircraf t itsel f form s the
secon d trailer . Thi s combinatio n mus t be backe d out of the gate each time an aircraf t departs ,
and the only contro l is the steerin g of the tender . Fortunately , the muc h greate r lengt h of the
aircraf t make s it insensitiv e to smal l motion s of the tende r and link , and the contro l proble m is
tractabl e for an experience d driver . Recen t advance s in contro l theor y have resulte d in algorithm s
for automaticall y steerin g vehicle s with any numbe r of trailer s of any size.
MANIPULATOR SManipulatio n
We retur n now to manipulators , effector s that mov e object s in the environment . The ancestor s of
robot manipulator s were teleoperate d mechanism s that allowe d human s to manipulat e hazardou s
Sectio n 25.3 . Parts : Wha t Are Robot s Mad e Of? 781
KINEMATIC S
ROTAR Y
PRISMATI Cmaterials , and that mimicke d the geometr y of a huma n arm . Earl y robot s as a rule followe d this
precedent , and have anthropomorphi c kinematics . Broadl y defined , kinematic s is the stud y of
the correspondenc e betwee n the actuato r motion s in a mechanism , and the resultin g motio n of
its variou s parts .
Most manipulator s allow for eithe r rotar y motio n (rotatio n aroun d a fixed hub) or prismati c
motio n (linea r movement , as with a pisto n insid e a cylinder) . Figur e 25.4 show s the Stanfor d
Manipulator , used in severa l earl y experiment s in robotics . A nearl y anthropomorphi c desig n is
the Unimatio n PUM A show n in Figur e 25.5 . This desig n has six rotar y joint s arrange d sequentially .
The shorthan d descriptio n of its kinemati c configuratio n is "RRRRRR, " ("6R " for short ) listin g
joint type s from base to tip. A free body in spac e has six degree s of freedo m (thre e for x­y­z
position , thre e for orientation) , so six is the minimu m numbe r of joint s a robo t require s in orde r
to be able to get the last link into an arbitrar y positio n and orientation .
Figur e 25.4 Th e Stanfor d Manipulator , an early robo t arm with five rotar y (R) and one prismati c
(P) joints , for a total of six degree s of freedom .
In case this total of six degree s of freedo m for a free body is not intuitivel y obvious , imagin e
the body is a tenni s ball. The cente r of the ball can be describe d with three positio n coordinates .
Now suppos e that the ball is restin g on a table with its cente r fixed . You can still rotat e the ball
withou t movin g its center . Pain t a dot anywher e on the surfac e of the ball, you can rotat e the ball
so that the dot touche s the table­top . This takes two degree s of freedom , becaus e the dot can be
specifie d with latitud e and longitude . Wit h the cente r fixed and the dot touchin g the table­top ,
you can still rotat e the ball abou t a vertica l axis . Thi s is the third and last degre e of rotationa l
freedom . (In an airplan e or boat , the three type s of rotatio n are calle d pitch , yaw , and roll. )
At the end of the manipulato r is the robot' s end effector , whic h interact s directl y with
object s in the world . It may be a screwdrive r or othe r tool , a weldin g gun , pain t sprayer , or
a gripper . Gripper s vary enormousl y in complexity . Two ­ and three­fingere d gripper s perfor m
most task s in manufacturing . The mechanica l simplicit y of these gripper s make s them reliabl e
and easy to control , both importan t attribute s for manufacturing .
At the othe r end of the complexit y spectru m are anthropomorphi c hands . The Utah­MI T
hand show n in Figur e 25.6 faithfull y replicate s mos t of the kinematics  of a huma n hand , less one
finger . A huma n hand has a very large numbe r of degree s of freedo m (see Exercis e 25.4) .
782 Chapte r 25 . Robotic s
Figur e 25.5 A  PUM A robot , showin g the six rotar y joints .
Ultimately , the effector s are drive n by electrica l (or other ) signals . Som e effector s only
accep t on/of f signals , som e accep t scala r value s (e.g. , turn righ t 3°), and som e robo t developmen t
environment s provid e highe r level subroutin e librarie s or have complet e language s for specifyin g
action s that can be turne d into primitiv e signals .
Sensors : Tool s for perceptio n
Chapte r 24 covere d the genera l principle s of perception , usin g visio n as the mai n example . In
this section , we describ e the othe r kind s of sensor s that provid e percept s for a robot .
Proprioceptio n
Like humans , robot s have a proprioceptive4 sens e that tells them wher e their joint s are. Encoder s
fitted to the joint s provid e very accurat e data abou t join t angl e or extension . If the outpu t of the
encode r is fed back to the contro l mechanis m durin g motion , the robo t can have muc h greate r
positionin g accurac y than humans . For a manipulator , this typicall y translate s to aroun d a few
mils (thousandth s of an inch ) of accurac y in its end­effecto r position . In contrast , human s can
manag e only a centimete r or two. To test this for yourself , plac e your finge r at one end of a ruler
(or some othe r objec t whos e lengt h is familiar) . The n with your eyes close d try to touc h the othe r
4 Th e wor d proprioceptiv e is derive d from the sam e sourc e as proprietary , and thus mean s "perceptio n of privatel y
owne d (i.e., internal ) stimuli. "
Sectio n 25.3 . Parts : Wha t Are Robot s Mad e Of? 783
Figur e 25.6 Th e Utah­MI T Hand .
REPEATABILIT Y
ODOMETR Yend. You shoul d do bette r after a few tries , whic h show s that your positionin g repeatabilit y is
bette r than your accuracy . The same is usuall y true for robots .
Mobot s can measur e thei r chang e in positio n usin g odometry , base d on sensor s that
measur e whee l rotatio n (or, in the case of steppe r motor s that rotat e a fixe d angl e per step , the
numbe r of steps) . Unfortunately , becaus e of slippag e as the robo t moves , the positio n erro r
from whee l motio n deteriorate s as the robo t moves , and may be severa l percen t of the distanc e
travelled . Orientatio n can be measure d more reliably , usin g a magneti c compas s or a gyroscop e
system . Accelerometer s can measur e the chang e in velocity .
Force sensin g
Even thoug h robot s can sens e and contro l the position s of their joint s muc h mor e accuratel y
than humans , ther e are still man y task s that canno t be carrie d out usin g only positio n sensing .
Consider , for example , the task of scrapin g pain t off a windowpan e usin g a razo r blade . To get
all the pain t require s positionin g accurac y of abou t a micro n in the directio n perpendicula r to
the glass . An error of a millimete r woul d caus e the robo t to eithe r miss the pain t altogethe r or
break the glass . Obviously , human s are not doin g this usin g positio n contro l alone . Thi s and
784 Chapte r 25 . Robotic s
FORC E SENSO R
COMPLIAN T
MOTION Smany othe r tasks involvin g contact , such as writing , openin g doors , and assemblin g automobiles ,
requir e accurat e contro l of forces.  Forc e can be regulate d to som e exten t by controllin g electri c
moto r current , but accurat e contro l require s a forc e sensor . Thes e sensor s are usuall y place d
betwee n the manipulato r and end effecto r and can sens e force s and torque s in six directions .
Using force control , a robo t can mov e alon g a surfac e whil e maintainin g contac t with a fixed
pressure . Suc h motion s are calle d complian t motions , and are extremel y importan t in man y
roboti c applications .
Tactil e sensin g
Pickin g up a pape r coffe e cup or manipulatin g a tiny scre w require s mor e than proprioception .
The force applie d to the cup mus t be just enoug h to stop it from slipping , but not enoug h to crush
it. Manipulatin g the screw require s informatio n abou t exactl y wher e it lies agains t the finger s that
TACTIL E SENSIN G i t contacts . In both cases , tactil e sensin g (or touc h sensing ) can provid e the neede d information .
Tactil e sensin g is the roboti c versio n of the huma n sens e of touch . A robot' s tactil e senso r uses an
elasti c materia l and a sensin g schem e that measure s the distortio n of the materia l unde r contact .
The senso r may give data at an array of point s on the elasti c surface , producin g the analogu e of a
camer a image , but of deformatio n rathe r than light intensity . By understandin g the physic s of the
deformatio n process , it is possibl e to deriv e algorithm s that are analogou s to visio n algorithms ,
and can comput e positio n informatio n for the object s that the sensor  touches . Tactil e sensor s
can also sens e vibration , whic h help s to detec t the impendin g escap e of the coffe e cup from the
holder' s grasp . Huma n being s use this schem e with a very fast serv o loop5 to detec t slip and
contro l the graspin g force to near the minimu m neede d to preven t slip.
Sona r
Sona r is SOun d NAvigatio n and Ranging . Sona r provide s usefu l informatio n abou t object s very
close to the robo t and is often used for fast emergenc y collisio n avoidance . It is sometime s used
to map the robot' s environmen t over a large r area. In the latte r case, an array of a doze n or more
sonar sensor s is fitte d aroun d the perimete r of the robot , each pointin g in a differen t direction .
Each senso r ideall y measure s the distanc e to the neares t obstacl e in the directio n it is pointing .
Sona r work s by measurin g the time of fligh t for a soun d puls e generate d by the senso r to
reach an objec t and be reflecte d back . The puls e or "chirp " is typicall y abou t 50 kHz . Thi s is
more than twic e the uppe r limi t for human s of 20 kHz . Soun d at that frequenc y has a wavelengt h
of abou t 7 mm. The spee d of soun d is abou t 330 m/second , so the round­tri p time dela y for an
objec t 1 m away is abou t 6 x 10~3 seconds . Sona r has been very effectiv e for obstacl e avoidanc e
and trackin g a nearb y target , such as anothe r mobil e robot . But althoug h it shoul d be possibl e
to measur e the time dela y very accurately , sona r has rarel y been able to produc e reliabl e and
precis e data for mapping . The first proble m is beam width . Rathe r than a narro w beam of sound ,
a typica l senso r produce s a conica l beam with 10° or more of spread . The secon d proble m come s
from the relativel y long (7 mm ) wavelengt h of the sona r sound . Object s that are very smoot h
5 A servomechanis m is a devic e for controllin g a large amoun t of powe r with a smal l amoun t of power . A serv o loop
uses feedbac k to regulat e the power .
Sectio n 25.3 . Parts : Wha t Are Robot s Mad e Of? 785
relativ e to this wavelengt h look shiny or "specular " to the sensor . Such objects  reflec t soun d like
a perfec t mirror . Soun d will only be receive d back from patche s of surfac e that are at right angle s
to the beam . Object s with flat surface s and shar p edge s reflec t very little soun d in most directions .
(The sam e observatio n is used to desig n radar­eludin g stealt h aircraf t and ships. ) Afte r bein g
reflecte d from the surface , the soun d may yet strik e a roug h surfac e and be reflecte d back to the
sensor . The time dela y will correspon d not to a physica l object , but to a "ghost, " whic h may
mysteriousl y disappea r whe n the robo t moves .
As discusse d in Chapte r 17, nois y sensor s can be handle d by first constructin g a probabilisti c
mode l of the sensor , and then usin g Bayesia n updatin g to integrat e the informatio n obtaine d over
time as the robo t move s around . Eventually , reasonabl y accurat e map s can be built up, and ghos t
image s can be eliminated .
DOMAI N
CONSTRAINT S
STRUCTURE D LIGH T
SENSOR S
LASE R RANG E
FINDER S
CROSS­BEA M
SENSO R
PARALLEL­BEA M
SENSO RCamer a data
Huma n and anima l visio n system s remai n the envy of all machine­visio n researchers . Chapte r 24
provide s an introductio n to the state of the art in machin e vision , whic h is still some way from
handlin g comple x outdoo r scene s and genera l objec t recognition . Fortunately , for a robot' s
purposes , somethin g simple r than a genera l visio n syste m will usuall y suffice . If the set of tasks
the robo t need s to perfor m is limited , then visio n need only suppl y the informatio n relevan t to
those tasks . Special­purpos e robot s can also take advantag e of so­calle d domai n constraint s
that can be assume d to appl y in restricte d environments . For example , in a buildin g (as oppose d
to a forest) , flat surface s can be assume d to be vertica l or horizontal , and object s are supporte d
on a flat groun d plane .
In som e cases , one can also modif y the environmen t itself to mak e the robot' s task easier .
One simpl e way of doin g this, widel y used in warehousin g tasks , is to put bar­cod e sticker s in
variou s location s that the robo t can read and use to get an exac t positio n fix. Slightl y mor e
drasti c is the use of structure d light sensors , whic h projec t their own ligh t sourc e onto object s
to simplif y the proble m of shap e determination . Imagin e a vertica l light strip e cast as show n in
Figur e 25.7 . Whe n this strip e cuts an object , it produce s a contou r whos e 3­D shap e is easil y
inferre d by triangulatio n from any vantag e poin t not in the plan e of the stripe . A camer a place d
in the same  horizontal  plan e as the sourc e need s only to locat e the strip e withi n each horizonta l
scan line. This is a simpl e image­processin g task and easil y done in hardware .
By movin g the stripe , or by usin g severa l raster s of stripe s at differen t spacings , it is
possibl e to produc e a very dens e three­dimensiona l map of the objec t in a shor t spac e of time .
A numbe r of device s are availabl e now that includ e a laser source , strip e control , camera , and
all the imag e processin g neede d to comput e a map of distance s to point s in the image . From the
user's poin t of view , thes e laser rang e finder s reall y are dept h sensors , providin g a dept h imag e
that update s rapidly , perhap s severa l time s a second .
For model­base d recognition , some very simpl e light­bea m sensor s have been used recently .
These sensor s provid e a smal l numbe r of very accurat e measurement s of objec t geometry . Whe n
model s are known , these measurement s suffic e to comput e the object' s identit y and position . In
Figur e 25.8, two example s are shown , a cross­bea m senso r and a parallel­bea m sensor .
786 Chapte r 25 . Robotic s
Imag e
Objec t
StripeRang e
Figur e 25.7 Sensin g the shap e of an objec t usin g a vertica l light­strip e system .
//i\\­ <m^  ^
TOP VIEWFRON T VIEW
Emitte r
Receive r
Path of Objec tMirro r
\
(a)| Emitte r
I] Receive r
Objec t bein g
scanne d
Figur e 25.8 (a ) A cross­bea m sensor , (b) A parallel­bea m sensor .
25.4 ARCHITECTURE S
In this section , we step back from the nuts and bolts of robot s and look at the overal l contro l
mechanism . The  architecture  of a robot  defines  how the  job of generating  actions  from  per­
cepts  is organized.  We will largel y be concerne d with autonomou s mobil e robot s in dynami c
environments , for whic h the need for a sophisticate d contro l architectur e is clear .
The desig n of robo t architecture s is essentiall y the sam e agen t desig n proble m that we
discusse d in Chapte r 2. We saw in the introductio n to the curren t chapte r that the environmen t for
mobil e robot s is towar d the difficul t end as environment s go. Furthermore , the perceptua l inpu t
availabl e to a robo t is ofte n voluminous ; nevertheless , the robo t need s to reac t quickl y in som e
situations . In the followin g subsections , we briefl y describ e a variet y of architectures , rangin g
Sectio n 25.4 . Architecture s 78 7
from fully deliberativ e to full y reflex . Ther e is no accepte d theor y of architectur e desig n that
can be used to prov e that one desig n is bette r than another . Man y theoretician s derid e the entir e
proble m as "jus t a bunc h of boxe s and arrows. " Nonetheless , man y superficiall y differen t design s
seem to have incorporate d the same set of feature s for dealin g with the real world .
Classica l architectur e
By the late 1960s , primitiv e but serviceabl e tool s for intelligen t robot s were available . Thes e
include d visio n system s that coul d locat e simpl e polyhedra l objects ; two­dimensiona l path ­
plannin g algorithms ; and resolutio n theore m prover s that coul d construc t simple , symboli c plan s
using situatio n calculus . Fro m thes e tools , togethe r with a collectio n of wheels , motor s and
sensors , emerge d Shakey , the forerunne r of man y intelligen t robo t projects .
The first versio n of Shakey , appearin g in 1969 , demonstrate d the importanc e of experi ­
menta l researc h in bringin g to ligh t unsuspecte d difficulties . The researcher s foun d that general ­
purpos e resolutio n theorem­prover s were too inefficien t to find nontrivia l plans , that integratin g
geometri c and symboli c representation s of the worl d was extremel y difficult , and that plan s don' t
work . Thi s last discover y cam e abou t becaus e Shake y was designe d to execut e plan s withou t
monitorin g thei r succes s or failure . Becaus e of whee l slippage , measuremen t error s and so on,
almos t all plan s of any lengt h faile d at som e poin t durin g execution .
The secon d versio n of Shake y incorporate d severa l improvements . First , mos t of the
detaile d work of findin g path s and movin g object s was move d from the genera l problem­solvin g
level dow n into special­purpos e program s calle d intermediate­leve l action s (ILAs) . Thes e
action s in fact consiste d of comple x routine s of low­leve l action s (LLAs ) for controllin g the
physica l robot , and include d som e erro r detectio n and recover y capabilities . For example , one
ILA calle d NAvT o coul d mov e the robo t from one plac e to anothe r withi n a room by callin g
the A* algorith m to plan a path and then callin g LLA s to execut e the path , doin g som e path
correction s alon g the way. The LLA s were also responsibl e for updatin g the interna l mode l of the
world state , whic h was store d in first­orde r logic . Motio n error s were explicitl y modelled , so that
as the robo t moved , its uncertaint y abou t its locatio n increased . Onc e the uncertaint y exceede d
a threshol d for safe navigation , the LLA woul d call on the visio n subsyste m to provid e a new
positio n fix. The key contributio n of the ILA/LL A system , then , was to provid e a relativel y clean
and reliabl e set of action s for the plannin g system .
The plannin g syste m used the STRIP S algorithm , essentiall y a theorem­prove r speciall y
designe d for efficien t generatio n of actio n sequences . STRIP S also introduce d the idea of compilin g
MACRO­OPERATOR S th e result s of plannin g into generalize d macro­operators , so that futur e problem­solvin g coul d
be mor e efficien t (see Sectio n 21.2) . Th e entir e syste m was controlle d by PLANEX , whic h
accepte d goal s from the user , calle d STRIP S to generat e plans , then execute d them by callin g the
specifie d ILAs . The executio n mechanis m was a simpl e versio n of the method s describe d in
Chapte r 12. PLANE X kept track of the curren t worl d state , comparin g it to the precondition s of
each subsequenc e in the origina l plan . Afte r each actio n completed , PLANE X woul d execut e the
shortes t plan subsequenc e that led to a goal and whos e precondition s were satisfied . In this way ,
action s that faile d woul d be retried , and fortunat e accident s woul d lead to reduce d effort . If no
subsequenc e was applicable , PLANE X coul d call STRIP S to mak e a new plan .
788___________________________________________ _ Chapte r 25 . Robotic s
The basic element s of Shakey' s design—specialize d component s for low­leve l contro l and
geometri c reasoning , a centralize d worl d mode l for planning , compilatio n to increas e speed , and
executio n monitorin g to handl e unexpecte d problems—ar e repeate d in man y moder n systems .
Improvement s in the symboli c plannin g and executio n monitorin g component s are discusse d in
depth in Part IV. Compilation , or explanation­base d learning , has been used extensivel y in two
robot architectures : Robo­SOA R (Lair d et al., 1991 ) and THE O (Mitchell , 1990) . Computatio n
times for simpl e task s can be reduce d from severa l minute s to less than a secon d in som e cases .
In both of these architectures , compilatio n is invoke d wheneve r a proble m is solve d for whic h
no ready­mad e solutio n was available . In this way, the robo t graduall y become s competen t and
efficien t in routin e tasks , whil e still bein g able to fall back on general­purpos e reasonin g whe n
faced with unexpecte d circumstances .
Much of the researc h in robotic s since Shake y was switche d off has been at the level of
ILAs and LLAs . Shake y was able to mov e on a flat floor , and coul d push large object s aroun d
with difficulty . Moder n robot s can twir l pencil s in thei r fingers , scre w in screw s and perfor m
high­precisio n surger y with greate r accurac y than huma n experts . Section s 25.5 and 25.6 describ e
some of the theoretica l advance s in geometri c reasonin g and senso r integratio n that have mad e
such achievement s possible . Hilar e II (Giral t et al., 1991 ) is probabl y the mos t comprehensiv e
moder n exampl e of a robo t architectur e in the classica l traditio n that make s use of advance d
geometri c methods .
Situate d automat a
Since the mid­1980s , a significan t minorit y of Al and robotic s researcher s have begu n to questio n
the "classical " view of intelligen t agen t desig n base d on representatio n and manipulatio n of
•,:iilrB explici t knowledge . In robotics , the principal drawback  of the classical  view  is that  explicit
'"''85' reasoning  about  the effects  of low­level  actions  is too expensive  to generate  real­time  behavior.
We have seen that compilatio n can alleviat e this to som e extent . Researcher s workin g on situate d
SITUATE D AUTOMAT A automat a have take n this idea one step further , by eliminatin g explici t deliberatio n from their
robot design s altogether .
A situate d automato n is essentiall y a finite­stat e machin e whos e input s are provide d by
sensor s connecte d to the environment , and whos e output s are connecte d to effectors . Situate d
automat a provid e a very efficien t implementatio n of refle x agent s with state . Ther e have been
two main strand s in the developmen t of such robots . The first approac h involve s generatin g the
automato n by an offlin e compilatio n process , startin g with an explici t representation . The secon d
approac h involve s a manua l desig n proces s base d on a decompositio n accordin g to the variou s
behavior s that the robo t need s to exhibit .
The compilatio n approach , pioneere d by Stan Rosenschein , (Rosenschein , 1985 ; Kaelblin g
and Rosenschein , 1990 ) distinguishe s betwee n the use of explici t knowledg e representatio n by
huma n designer s (wha t he call s the "gran d strategy" ) and the use of explici t knowledg e withi n
the agen t architectur e (the "gran d tactic") . Throug h a carefu l logica l analysi s of the relationshi p
betwee n perception , action , and knowledge , Rosenschei n was able to desig n a compile r that
generate s finit e state machine s whos e interna l state s can be proved  to correspon d to certai n
logica l proposition s abou t the environment , provide d that the initia l state and the correc t law s
Sectio n 25.4 . Architecture s 789
Senso r
inputState
registe r
Action s
Figur e 25.9 Th e basi c structur e of Rosenschein' s situate d automato n design . Th e shade d
regio n on the left represent s a Boolea n circui t for updatin g the curren t state registers . The shade d
regio n to the righ t represent s a circui t for selectin g action s base d on the curren t state .
BEHAVIOR­BASE D
ROBOTIC Sof "physics " are give n to the compiler . Thus , the robo t "knows " a set of proposition s abou t the
environment , even thoug h it has no explici t representatio n of the proposition .
Rosenschein' s basi c desig n is show n in Figur e 25.9 . It relie s on a theore m to the effec t
that any finite­stat e machin e can be implemente d as a state registe r togethe r with a feedforwar d
circui t that update s the state base d on the senso r input s and the curren t state , and anothe r circui t
that calculate s the outpu t give n the state register . Becaus e all the computatio n is carrie d out by
fixed­depth , feedforwar d circuits , the executio n time for each decisio n cycl e is vanishingl y small .
Flakey , a robo t base d on situate d automat a theory , was able to navigat e the halls of SRI, run
errands , and even ask questions , all withou t the benefi t of explici t representations .
Rodne y Brook s (1986 ) has advocate d an approac h to robo t desig n that he calls behavior ­
based robotics . The idea is that the overal l agen t desig n can be decomposed , not into functiona l
component s such as perception , learning , and planning , but into behavior s such as obstacl e
avoidance,  wall­following , and exploration . Each behaviora l modul e accesse s the senso r input s
independentl y to extrac t just the informatio n it needs , and send s its own signal s to the effectors .
Behavior s are arrange d into a prioritize d hierarch y in whic h highe r level behavior s can acces s
the interna l state of lowe r level behavior s and can modif y or overrid e their outputs . Figur e 25.1 0
show s a hierarch y of behavior s propose d for a mobil e robo t by Brook s (1986) .
The mai n aim of behavior­base d robotic s is to eliminat e the relianc e on a centralized ,
complet e representatio n of the worl d state , whic h seem s to be the mos t expensiv e aspec t of the
classica l architecture . Interna l state is neede d only to keep track of those aspect s of the worl d
state that are inaccessibl e to the sensor s and are require d for actio n selectio n in each behavior .
For task s in whic h the appropriat e actio n is largel y determine d by the senso r inputs , the sloga n
"The worl d is its own model " is quite appropriate . In som e cases , only a few bits of interna l state
are neede d even for quite comple x tasks such as collectin g empt y soft drink cans . Simila r result s
have been foun d in Rosenschein' s designs , in whic h the state registe r can be surprisingl y small .
Behavior­base d robotic s has been quit e successfu l in demonstratin g that man y basi c com ­
petence s in the physica l worl d can be achieve d usin g simple , inexpensiv e mechanisms . At
790 Chapte r 25 . Robotic s
Sensor sreaso n abou t behavio r of object s
plan change s to the worl d
identif y object s
monito r change s
build map s
explor e
avoid object s
Figur e 25.1 0 Desig n for a behavior­base d mobil e robot , showin g the layere d decomposition .
present , however , it is difficul t to see how the desig n methodolog y can scale up to more comple x
tasks . Eve n for the task "Fin d an empt y cup and brin g it back to the startin g location, " the
robot mus t form an interna l representatio n that correspond s to a map in the classica l architecture .
Furthermore , the behavior­base d approac h require s the desig n of a new controlle r for each task ,
wherea s classica l robot s are taskable : they can be assigne d a goal and can carry out a plan to
achiev e it. In man y ways , behavior­base d design s resembl e the ILA and LLA level s in Shakey ,
whic h are "nonclassical " component s of an overal l classica l architecture .
Rosenschein' s situate d automat a fall somewher e in between . The proces s of generatin g
the automato n can be completel y automated , so that if the the compile r is include d in the
robot' s software , the robo t can be give n a task and can compil e its own situate d automaton . The
automato n can then be execute d in software . Comparin g this approac h with the explanation­base d
learnin g metho d used in SOA R and THEO , we see that the situate d automato n is compile d prior to
executio n and mus t handl e all possibl e situation s that can arise , wherea s the explanation­base d
learnin g approac h generate s efficien t rules for handlin g the type s of situation s that actuall y arise
durin g execution . In simpl e domains , the situated­automato n approac h is feasibl e and probabl y
more efficient , wherea s in very comple x domains , especiall y thos e with recursiv e structure , the
complet e automato n become s too large or even infinit e in size.
25.5 CONFIGURATIO N SPACES : A FRAMEWOR K FOR ANALYSI S
Recal l from Chapte r 3 that the main elemen t in analyzin g a proble m is the state space6, whic h
describe s all the possibl e configuration s of the environment . In robotics , the environmen t include s
the body of the robo t itself . The main distinctio n betwee n the problem s discusse d in Chapte r 3 and
those in robotic s is that robotic s usuall y involve s continuous  state spaces . Bot h the configuratio n
of the robot' s bod y and the location s of object s in physica l spac e are define d by real­value d
6 Ther e is a sourc e of confusio n here becaus e the term state spac e is used in robotic s and contro l literatur e to includ e
the robot' s state parameter s and certai n of their derivatives . Enoug h derivative s are include d that the robot' s motio n can
be predicte d from its dynamic s and contro l equations . For simplicity , we assum e here that derivative s of state parameter s
are excluded .
Sectio n 25.5 . Configuratio n Spaces : A Framewor k for Analysi s 791
CONFIGURATIO N
SPAC Ecoordinates . It is therefor e impossibl e to appl y standar d searc h algorithm s in any straightforwar d
way becaus e the number s of state s and action s are infinite . Much  of the work in  robot  planning
has dealt  with  ways  to tame  these  continuous  state  spaces.
Suppos e that we have a robo t with k degree s of freedom . The n the state or configuratio n
of the robo t can be describe d with k real value s q\,...,q k. For the PUM A robot , this woul d be a
list of six join t angle s 9\,...,  6*5. The k value s can be considere d as a poin t p in a ^­dimensiona l
space calle d the configuratio n spac e of the robot . We use C to refer to the configuratio n spac e
of the robo t itself . This descriptio n is convenien t becaus e it lets us describ e the comple x three ­
dimensiona l shap e of the robo t with a singl e fc­dimensiona l point . In othe r words , from the six
joint angles , we can determin e the exac t shap e of the entir e PUM A robot . This work s becaus e the
individua l robo t links are rigid—the y rotat e whe n the joint angle s change , but they do not chang e
size or shape .
Configuratio n spac e can be used to determin e if there is a path by whic h a robo t can mov e
from one positio n to another . Conside r the proble m of gettin g the PUM A hand to replac e a spar k
plug in a car. We have to find a path throug h three­dimensiona l physica l spac e that will lead
the hand to the righ t spot , withou t bumpin g into anything . But that is not enough—w e need to
make sure that the othe r links of the robo t do not bum p into anythin g either . This is a difficul t
proble m to visualiz e in physica l space , but it is easy to visualiz e in configuratio n space . We start
by considerin g the point s in C for whic h any part of the robo t bump s into something . This set of
point s is calle d the configuratio n spac e obstacle , or O. The set differenc e C ­ O is calle d free
space , or f, and is the set of configuration s in whic h the robo t can mov e safely . (Not e that we
also have to worr y abou t the robo t bumpin g into itself , or over­twistin g powe r and data cable s
that pass throug h its joints . For these reasons , there are uppe r and lowe r limit s on the joint angle s
of most robots , and correspondin g bound s on the size of configuratio n space. )
Assum e we have an initia l poin t c\ and a destinatio n poin t ci in configuratio n space . The
robot can safel y mov e betwee n the correspondin g point s in physica l spac e if and only j/ther e is a
continuou s path betwee n c\ and C2 that lies entirel y in T. This idea is illustrate d in Figur e 25.11 .
A two­lin k robo t is show n at the left of the figure , and its two­dimensiona l configuratio n spac e
at the right . The configuratio n obstacl e is the shade d region , and configuration s c\ and ci are
show n in both domains , with a safe path draw n in C betwee n them . Eve n in this simpl e example ,
the advantag e of searchin g for a safe path in C is clear .
We human s have it muc h easie r than the PUMA , becaus e our shoulde r joint has three degree s
of freedom , whil e the PUMA' S only has two. Tha t give s the huma n arm seve n degree s of freedom
overall , one more than is neede d to reach any point . This extr a or redundan t degre e of freedom
allow s the arm to clea r obstacle s whil e keepin g the hand in one spot , a wonderfu l facilit y that
helpe d our ancestor s clim b aroun d in trees , and help s us chang e spar k plugs .
We shoul d emphasiz e that configuratio n spac e and free spac e are mathematica l tool s
for designin g and analyzin g motion­plannin g algorithms . The algorithm s themselve s need not
explicitl y construc t or searc h a configuratio n space ; nonetheless , any motion­plannin g metho d or
contro l mechanis m can be interprete d as seekin g a safe path in f. Configuration­spac e analysi s
can be used to establis h whethe r or not a give n mechanis m is complet e and correct , and can be
used to analyz e the complexit y of the underlyin g problems .
792 Chapte r 25 . Robotic s
(a) (b)
Figur e 25.11 (a ) A workspac e with a rotar y two­lin k arm. The goal is to mov e from configu ­
ration c\ to configuratio n C2. (b) The correspondin g configuratio n space , showin g the free spac e
and a path that achieve s the goal .
GENERALIZE D
CONFIGURATIO N
SPAC E
ASSEMBL Y
PLANNIN GGeneralize d configuratio n spac e
The term generalize d configuratio n spac e has been applie d to system s wher e the state of
other object s is include d as part of the configuration . The othe r object s may be movable , and
their shape s may vary . Shap e variatio n occur s in object s such as scissor s or stapler s that have
mechanica l joints , and in deformabl e object s like strin g and paper . Generalize d configuratio n
space s are especiall y usefu l in understandin g tasks such as assembl y plannin g in whic h the robo t
must mov e a set of object s into som e desire d arrangement .
Let £ denot e the spac e of all possibl e configuration s of all possibl e object s in the world ,
other than the robot . If a give n configuratio n can be define d by a finit e set of parameter s
a\,...,a m, then £ will be an m­dimensiona l space . This will not work for object s like strin g and
paper , whos e shape s are not describabl e with finitel y man y parameters . Thes e object s represen t
a considerabl e challeng e for geometri c modellin g at this stage . But we can use £ as a conceptua l
tool even in those cases .
Now conside r W = C x £. W is the spac e of all possibl e configuration s of the world , both
robot and obstacles . In the two­lin k robo t C describe d earlier , there was no variatio n in the objec t
shapes , so £ was a singl e poin t and W and C were equivalent .
If all of the object s in the environmen t are robot s unde r centra l control , then the situatio n
is reall y the sam e as before . In this setting , it is best to cluste r all the degree s of freedo m of the
robot s togethe r to creat e a singl e super­robot . The generalize d configuratio n spac e is the spac e of
all joint angle s for all robots . Illega l configuration s now includ e those wher e two robot s overla p
Sectio n 25.5 . Configuratio n Spaces : A Framewor k for Analysi s 793
TRANSI T PATH S
TRANSFE R PATH S
FOLIATIO Nin space . The plannin g proble m again reduce s to findin g a safe path for a poin t in the generalize d
free spac e T.
If the othe r object s are not robot s but are nonetheles s movable , the proble m is very muc h
harder . Unfortunately , this is also the mos t interestin g case , becaus e usuall y we woul d like a
robot to mov e inanimat e thing s around . We can still construc t a generalize d configuratio n space ,
and eliminat e configuration s wtier e the robo t overlap s an object , or two object s overlap . Bu t
even if we find a path betwee n two configuration s c\ and c^_ in generalize d F, we may not be
able to execut e it. The proble m is that an unrestricte d path in W will usuall y describ e motio n of
object s throug h midai r withou t any help from the robot . We can partiall y addres s this proble m
by restrictin g W to compris e configuration s wher e object s are eithe r held by the robo t or are
supporte d by anothe r objec t agains t gravity .
But lega l path s in this W can still describ e object  motion s withou t robo t aid, such as
spontaneou s slidin g on a tabletop , whic h shoul d be illegal . Ther e is no easy solutio n to this
problem . We simpl y canno t think of W as a benig n sea in whic h we can navigat e freely . Instead ,
we mus t follo w certai n shippin g lanes , betwee n whic h we transfe r only at specia l configurations .
Althoug h generalize d W has man y degree s of freedom , (it + m degree s usin g the notatio n above) ,
only k of thes e are actuall y controllable . Mos t of the time , we are changin g only the robo t
configuratio n q\,...  ,qt, and the objec t configuration s a\,...  ,am stay fixed . Sometime s though ,
if the robo t is graspin g an object , we can chang e both the robo t and the object s configuration .
We are still movin g in a fc­dimensiona l subse t of W, but no longe r one involvin g only the q
(robot ) coordinates . So we are in a (k + m)­dimensiona l sea with steerag e only alon g two type s of
fc­dimensiona l lanes . Motion s wher e the robo t move s freel y are calle d transi t paths , and thos e
wher e the robo t move s an objec t are calle d transfe r paths .
The navigabl e W in this case is calle d a foliation , suggestin g an abundanc e of smal l
randoml y oriente d sheets , but it is reall y more like a book . Each page of the book is a slightl y
differen t free spac e for the robot , define d by slightl y differen t position s for the movabl e objects .
Look again at Figur e 25.12 . On the left half of the figure , one of the object s has an arrow attached ,
indicatin g possibl e motion . The variabl e b is a coordinat e measurin g the object' s displacement .
The W for the syste m is now three­dimensional , and b is the new coordinate . For b fixed , a slice
throug h the >V describe s the allowabl e motion s of the robo t with the obstacle s fixed , and look s
like the C in the righ t half of the figure . Imagin e constructin g the C for man y differen t value s
of b, printin g the result s on separat e pages , sortin g them by b coordinate , and assemblin g the
pages into a book.  Transi t motio n is possibl e withi n any page of the book , on whic h the object
position s are fixed , but not betwee n pages.  Transfe r motions , in whic h an objec t is movin g (or
being moved) , form a kind of spine for the book , and allow motio n betwee n pages .
The difficult y of plannin g for movabl e object s is real: the tower s of Hano i problem7 is a
specia l case of it. The numbe r of motion s neede d to mov e n Hano i disk s is exponentia l in n,
implyin g that plannin g for movabl e object s require s time that grow s at least exponentiall y with
the dimensio n of W. At this time , ther e are no prove d uppe r bound s for genera l plannin g with
n movabl e objects . Robo t planner s usuall y mak e some stron g assumption s to avoi d tacklin g this
proble m head­on . One can do any of the following :
7 Th e tower s of Hano i proble m is to mov e a set of n disk s of differen t sizes from one peg to another , usin g a third peg
for temporar y storage . Disk s are move d one at a time , and a large r disk canno t rest on a smalle r one.
794 Chapte r 25 . Robotic s
Figur e 25.12 A  two­lin k manipulato r in a workspac e containin g a movabl e obstacle .
1. Partitio n >V into finitel y man y states— a form of abstractio n (see Sectio n 12.2) . Th e
plannin g proble m then reduce s to a logica l plannin g proble m of the kind addresse d in
Part IV. For example , the block s worl d involve s a partitio n of bloc k configuration s into
those in whic h one bloc k is "on another, " and thos e in whic h it is "on the table. " The exac t
location s of the block s are ignored . The partitionin g mus t be done carefully , becaus e it
involve s a considerabl e loss of generality . In the block s worl d case , one can no longe r
construc t plan s involvin g block s that are leaning , or in whic h two or more block s perc h on
another , or in whic h bloc k position s are importan t (as in buildin g a wall) . Obviously , the
appropriat e partitio n depend s on the goal , but as yet no genera l metho d for constructin g
partition s has been devised .
2. Plan objec t motion s first and then plan for the robot . Thi s coul d be calle d the classica l
approac h to assembl y planning . We first restric t the objec t motion s so that all the object s
move as two rigid subgroups , one representin g the mai n assembly , and the othe r a sub­
assembl y that is bein g adde d to it. A singl e robo t then has a good chanc e to be able to
perfor m that step , and a separat e plannin g phas e generate s the robot' s motion .
Plannin g objec t motion s also can be abstracte d into discret e action s such as "scre w part
A onto part B." Man y earl y assembl y planner s used this kind of representation , and are
therefor e able to use standar d partial­orde r plannin g algorithms . The y suppor t a variet y
of constraint s on the orde r of assembl y steps , but question s of geometri c feasibilit y are
usuall y handle d by separat e geometri c algorithms , or in som e case s by the user.
Recentl y though , a very elegan t metho d was describe d that plan s a geometricall y
feasibl e assembl y sequenc e and runs in time polynomial  in n (Wilso n and Schweikard ,
1992) . Thi s schem e necessaril y does not generat e all possibl e sequences , becaus e ther e
are exponentiall y many . But a user can rejec t certai n of the subassemblie s it has chosen ,
and it will generat e anothe r feasibl e sequenc e just as fast that satisfie s the user' s strictures .
Sectio n 25.5 . Configuratio n Spaces : A Framewor k for Analysi s 79 5
3. Restric t objec t motions . Her e one choose s a parameterize d famil y of basi c motions ,
and searche s for a sequenc e of such motion s that will solv e the plannin g problem . The
best­know n exampl e of this is the "LMT " approach8 (Lozano­Pere z et al., 1984) . One
must be carefu l abou t the choic e of motions , becaus e the proble m will be intractabl e if
they are too general , but unsolvabl e if they are too restrictive . LM T suggest s the use of
complian t motions , whic h produc e straight­lin e motion s in free space , but whic h follo w the
boundarie s of configuratio n spac e whe n needed . LM T also explicitl y model s uncertaint y
in contro l and sensing . So the resul t of an LMT basic motio n is not a singl e trajectory , but
an envelop e of possibl e trajectorie s that grow s with time . LM T is describe d more fully in
Sectio n 25.6 .
Recognizabl e Sets
Configuratio n spac e is a usefu l tool for understandin g constraint s induce d by object  shape . We
have so far tacitl y assume d that robo t plannin g problem s are all of the form "How do I get from
here to there in W?" But the realit y is that mos t robot s have poor knowledg e of wher e they are
in W. The robo t has reasonabl y good knowledg e of its own state (althoug h often not accuratel y
enoug h for precisio n work) , but its knowledg e of othe r object s come s secon d hand from sensor s
like sonar , lase r rang e finders , and compute r vision . Som e object s simpl y may not be visibl e
from the robot' s curren t vantag e point , so their uncertaint y is enormous . Rathe r than a poin t in
configuratio n space , our planne r mus t start with a probabilit y cloud , or an envelop e of possibl e
RECOGNIZABL E SET configurations . We call such an envelop e a recognizabl e set.9
ABSTRAC T SENSO R I t will be usefu l to formaliz e this notion . An abstrac t senso r a is a functio n from the true
world state W to the spac e of possibl e sensor  values . A recognizabl e set is a set <r~l(s) of all
world state s in whic h the robo t woul d receiv e the sensor  readin g s.w
If the senso r is perfect , that is, if it alway s produce s the sam e senso r value s in the sam e
world state , the sets a~ '(5) form a partitio n of H'. Distinc t recognizabl e sets do not overlap ,
and their unio n is all of W. Unfortunately , becaus e of noise , and becaus e the chose n W ofte n
does not incorporat e all of the factor s that can affec t senso r readings , the valu e returne d by the
senso r may not be a uniqu e functio n of the state . To allo w for this, we can treat a as a relatio n
rathe r than a function . The relatio n is true of a state and a senso r readin g if the senso r readin g
could  possibly  be returne d in that worl d state . It still make s sens e to defin e recognizabl e sets as
cr~l(s), wher e we now take this to mea n the set of state s in whic h the senso r coul d retur n the
value s. Now it no longe r hold s that distinc t recognizabl e sets are disjoint , althoug h their unio n
is still all of W.
Recognizabl e sets simplif y the proble m of plannin g with uncertainty . A robo t will alway s
be in a recognizabl e set, becaus e it will alway s have senso r reading s available . Th e robo t
may also use memorie s of earlie r sensor  readings , but this is equivalen t to a virtua l senso r that
provide s both curren t and past readings . The (virtual ) senso r reading s determin e uniquel y whic h
8 LM T is name d after the thre e authors .
9 Recognizabl e sets are to continuou s domain s wha t multipl e state sets are to discret e problem s (see Chapte r 3).
10 Notic e the clos e analog y with the idea of possibl e world s introduce d in the contex t of moda l logi c in Chapte r 8. A
recognizabl e set is essentiall y the set of possibl e world s give n wha t the robo t know s abou t the world .
796 Chapte r 25 . Robotic s
recognizabl e set <T~I(S(\) the robo t is in. From there , the planne r can determin e wher e the robo t
migh t mov e next , give n a motio n comman d with uncertainty , and wha t the next senso r readin g
A'i coul d be. Thinkin g of recognizabl e sets as state s of the robot , the motio n comman d cause d a
nondeterministi c transitio n from the state a~ I(SQ) to one of the state s a~' (s\). Althoug h there are
infinitel y man y such state s with a continuou s sensor , it is still possibl e to represen t the possibl e
transitions , and to find a correc t plan if one exists . Unfortunately , the complexit y of doin g this is
extremel y high—i n fact, doubl y exponentia l in the numbe r of plan step s (Cann y and Reif , 1987) .
25.6 NAVIGATIO N AND MOTIO N PLANNIN G
We now turn to the questio n of how to mov e aroun d successfully . Give n our analysi s of robotic s
problem s as motio n in configuratio n spaces , we will begi n with algorithm s that handl e C directly .
Thes e algorithm s usuall y assum e that an exac t descriptio n of the spac e is available , so they canno t
be used wher e ther e is significan t senso r error and motio n error . In som e cases , no descriptio n of
the spac e is availabl e unti l the robo t actuall y start s movin g aroun d in it.
We can identif y five majo r classe s of algorithms , and arrang e them roughl y in orde r of the
amoun t of informatio n require d at plannin g time and executio n time :
<C> Cel l decompositio n method s brea k continuou s spac e into a finit e numbe r of cells , yieldin g
a discret e searc h problem .
{> Skeletonizatio n method s comput e a one­dimensiona l "skeleton " of the configuratio n
space , yieldin g an equivalen t grap h searc h problem .
<> Bounded­erro r plannin g method s assum e bound s on senso r and actuato r uncertainty , and
in some case s can comput e plan s that are guarantee d to succee d even in the face of sever e
actuato r error .
0 Landmark­base d navigatio n method s assum e that ther e are som e region s in whic h the
robot' s locatio n can be pinpointe d usin g landmarks , wherea s outsid e thos e region s it may
have only orientatio n information .
<0> Onlin e algorithm s assum e that the environmen t is completel y unknow n initially , althoug h
most assum e som e form of accurat e positio n sensor .
As always , we are intereste d in establishin g som e of of the propertie s of thes e algorithms ,
includin g their soundness , completeness , and complexity . Whe n we talk abou t the complexit y
of a plannin g method , we mus t keep in mind both offlin e costs (befor e execution ) and the onlin e
cost of execution .
Cell decompositio n
Recallin g that motio n plannin g for a robo t reduce s to navigatin g a poin t in free spac e F, the basic
DECOMPOSITIO N ide a of cel1 decompositio n is easy to state :
Sectio n 25.6 . Navigatio n and Motio n Plannin g 79 7
1. Divid e T into simple , connecte d region s calle d "cells. " This is the cell decomposition .
2. Determin e whic h cells are adjacen t to whic h others , and construc t an "adjacenc y graph. "
The vertice s of this grap h are cells , and edge s join cells that abut each other .
3. Determin e whic h cells the start and goal configuration s lie in, and searc h for a path in the
adjacenc y grap h betwee n thes e cells .
4. Fro m the sequenc e of cells foun d at the last step , comput e a path withi n each cell from a
poin t of the boundar y with the previou s cell to a boundar y poin t meetin g the next cell.
The last step presuppose s an easy metho d for navigatin g withi n cells . Th e cell s are usuall y
geometricall y "simple, " so that this step is easy . For example , one coul d use rectangula r cells ,
and then it is possibl e to join any two point s in the cell with a straight­lin e path (this propert y is
share d by all conve x cells) . The difficult y is that the cells mus t be constructe d in configuratio n
space , and T typicall y has complex , curve d boundarie s (see Figur e 25.12) .
Becaus e of the difficult y of the .F­boundary , the first approache s to cell decompositio n did
not represen t it exactly . Approximat e subdivision s were used , usin g eithe r boxe s or rectangula r
strips . A strip approximatio n to the configuratio n spac e of the 2­lin k robo t is show n in Fig­
ure 25.13 . The start and goal configuration s are visibl e as points , and the cells joinin g them from
step 3 abov e are shaded . Finally , an explici t path from start to goal is constructe d by joinin g the
midpoin t (centroid ) of each strip with the midpoint s of the boundarie s with neighborin g cells .
This approac h mus t be conservativ e if it is to produc e a collision­fre e path . The strip s
must be entirel y in free spac e or a path foun d insid e them migh t cros s a (.­­boundar y and caus e
a collision . So ther e will be som e "wasted " wedge s of free spac e at the ends of the strips . Thi s
is not usuall y a problem , but in very tigh t situations , ther e migh t be no path excep t throug h
those wedges . To find this path , thinne r strip s woul d have to be used . Thi s raise s a genera l
issue for approximat e cell decomposition : choosin g the resolutio n of the decomposition . Thi s
is sometime s done adaptivel y by the algorithm , by choosin g smalle r cells in "tight " parts of free
space . Or there migh t be a natura l "safet y clearance " belo w whic h it is inadvisabl e to move . To
be concret e abou t this, suppos e that we are able to contro l our robot' s motio n to withi n 1 cm. If
we canno t find a safe path with cells at 1 cm resolution , a path throug h cells at a fine r resolutio n
woul d take the robo t close r than 1 cm to the obstacles . Becaus e that conflict s with our desir e for
a sensibl e path , there is no poin t in performin g that more expensiv e search .
The approac h just describe d is soun d but not complete . It produce s a safe path , but may
not alway s find one if one exists . Wit h som e smal l changes , we coul d instea d have produce d a
planne r that is complet e but not sound . If we were reckles s rathe r than conservative , we coul d
have declare d all partiall y free cells as bein g free. If there were any safe path , it woul d then pass
entirel y throug h free cells , and our planne r woul d be guarantee d to find it. As you can imagin e
though , give n the unforgivin g natur e of steel and aluminu m durin g collisions , incorrec t plan s are
not very useful .
An alternativ e to approximat e algorithm s is exac t cell decomposition . An exac t cell de­
compositio n divide s free spac e into cells that exactl y fill it. Thes e cells necessaril y have comple x
shapes , becaus e som e of thei r boundarie s are also boundarie s of f. Suc h a decompositio n is
show n in Figur e 25.14 . Thi s decompositio n look s rathe r like a coars e strip decomposition , but
there are two importan t differences . The first is that the cells have curve d top and botto m ends ,
so ther e are no gaps of free spac e outsid e the decomposition . We call thes e cells cylinders . The
798 Chapte r 25 . Robotic s
Start
Figur e 25.13 A  vertica l strip cell decompositio n of the configuratio n spac e for a two­lin k
robot . The obstacle s are dark blobs , the cells are rectangles , and the solutio n is containe d withi n
the grey rectangles .
secon d differenc e is that the widt h of the cylinder s is not fixed , but determine d by the geometr y
of the environmerit . This is a key aspec t of the exac t method . To be usefu l for planning , the cell
shape s mus t be kept simple . Otherwise , we coul d declar e all of free spac e as a singl e cell. This
does not help becaus e ther e is no simpl e way to mov e aroun d in such a cell. The cells in the
cylindrica l decompositio n are easy to mov e in, becaus e their left and righ t boundarie s are straigh t
lines (althoug h sometime s of zero length) .
To construc t a cylindrica l decompositio n of a two­dimensiona l set, we first find critica l
point s of the boundary . Thes e are the point s wher e the boundar y curv e is vertical . Equivalently ,
imagin e sweepin g a vertica l line from left to right acros s Figur e 25.14 . At ever y instant , the line
can be divide d into segment s that lie in free spac e or in the obstacle . Critica l point s are exactl y
those point s wher e segment s split or join as the line moves . As the line move s withi n a cylindrica l
cell, there are no splittin g or joinin g events , and this make s it easy to plan path s withi n the cell.
For example , to mov e from the left boundar y to the righ t boundar y of a cylindrica l cell, we coul d
use this sweepin g line. A singl e segmen t of the line woul d alway s lie in this cell, and its midpoin t
woul d give us the path we want .
SKELETONIZATIO N
SKELETO NSkeletonizatio n method s
Rathe r than producin g a decompositio n into a finit e numbe r of discret e chunk s of space , skele ­
tonizatio n method s collaps e the configuratio n spac e into a one­dimensiona l subset , or skeleton .
They simplif y the task of navigatin g in a high­dimensiona l spac e by requirin g path s to lie alon g
the skeleton . The skeleto n is essentiall y a web with a finit e numbe r of vertices , and path s withi n
the skeleto n can be compute d usin g grap h searc h methods . If the start and goal point s do not
lie on the skeleton , shor t path segment s are compute d joinin g them to the neares t poin t on it.
Skeletonizatio n method s are generall y simple r than cell decomposition , becaus e they provid e a
Sectio n 25.6 . Navigatio n and Motio n Plannin g 799
VISIBILIT Y GRAP Hsweepin g line
Figur e 25.1 4 A  cylindrica l cell decompositio n of the configuratio n spac e for a two­lin k robot .
There are thre e critica l point s indicate d by dots , and nine cylinders .
"minimal " descriptio n of free space . The y avoi d an explici t descriptio n of the boundar y of free
space , and this can provid e considerabl e time savings . Ther e is only one kind of data structur e
neede d to describ e skeleto n curves , and this help s simplif y implementation .
To be complete  for motio n planning , skeletonizatio n method s mus t satisf y two properties :
1. If S is a skeleto n of free spac e F, then S shoul d have a singl e connecte d piec e withi n each
connecte d regio n of T.
2. For any point/ ? in T, it shoul d be "easy " to comput e a path from p to the skeleton .
The secon d conditio n is rathe r vague , but it will becom e cleare r from the example s of skele ­
tonizatio n method s comin g up. The conditio n is crucial , becaus e otherwis e we coul d construc t a
skeleto n that consiste d of a singl e poin t in each connecte d regio n of J­. Suc h a skeleto n woul d be
a procrastinatio n of the plannin g problem , rathe r than a solutio n of it. Ther e are man y type s of
skeletonizatio n method s in two dimensions . Thes e includ e visibilit y graphs , Vorono i diagrams ,
and roadmaps . We briefl y describ e each in turn .
The visibilit y grap h for a polygona l configuratio n spac e C consist s of edge s joinin g all
pairs of vertice s that can see each other . That is, there is an unobstructe d straigh t line joinin g those
vertices . It is show n in Figur e 25.15 . To see that the visibilit y grap h is complet e for planning ,
we need only observ e that the shortest  path betwee n two point s with polygona l obstacle s lies
entirel y on the visibilit y graph , excep t for its first and last segmen t (see Exercis e 25.7) .
800 Chapte r 25 . Robotic s
StartGoal
Figur e 25.15 Th e visibilit y grap h for a collectio n of polygons .
VORONO I DIAGRA M
ROADMAP S
SILHOUETT E
CURVE S
LINKIN G CURVE S
SILHOUETT E
METHO DThe Vorono i diagra m of a polygona l free spac e T is show n in Figur e 25.16 . You can
understan d the diagra m as follows . For each poin t in free space , comput e its distanc e to the
neares t obstacle . Plo t that distanc e on Figur e 25.1 6 as a heigh t comin g out of the page . The
heigh t of the terrai n is zero at the boundar y with the obstacle s and increase s as you mov e awa y
from them . The boundin g rectangl e of this C also count s as an obstacle . The terrai n has shar p
ridge s at point s that are equidistan t from two or mor e obstacles . The Vorono i diagra m consist s
of those shar p ridge points . Algorithm s that find path s on this skeleto n are complete , becaus e the
existenc e of a path in f implie s the existenc e of one on the Vorono i diagram . However , the path
in the Vorono i diagra m is not in genera l the shortes t path .
The most efficien t complet e metho d for motio n plannin g is base d on roadmaps . A roadma p
is a skeleto n consistin g of two type s of curves : silhouett e curve s (als o know n as "freeways" )
and linkin g curve s (also know n as "bridges") . The idea behin d roadmap s is to mak e the searc h
for a path simple r by limitin g it to trave l on a few freeway s and connectin g bridge s rathe r than
an infinit e spac e of points . Figur e 25.1 7 show s a roadma p for a three­dimensiona l torus . The
complexit y of computin g a roadma p is O(nk log n) for a robo t with k degree s of freedom . Here ,
n is the size of the C description , whic h is take n to be the numbe r of equation s in the formul a
describin g T.
There are two version s of roadmaps . Th e firs t has been calle d the silhouett e metho d
becaus e it uses curve s that defin e the visibl e silhouett e of the free­spac e boundary . The roadma p
show n in Figur e 25.1 7 is of this type . To defin e it, we first choos e two direction s in C, and thin k
of them as coordinat e axes . Cal l them X and Y. The roadma p is define d as follows :
• Silhouett e curve s are loca l extrem a in Y of slice s in X. Tha t is, take a slice throug h f
by settin g X = c for som e constan t c. The cross­sectio n of f will have severa l connecte d
pieces . Ever y piec e is eithe r unbounded , or wil l have at least  one poin t wher e the Y­
coordinat e is locall y maximized , and one wher e it is locall y minimized . Wit h som e
technica l tricks , we can mak e sure that all the piece s are bounded . If we then comput e
all the loca l maxim a and minim a of J­ in Y, we are guarantee d to get at leas t one poin t
L
Sectio n 25.6. Navigatio n and Motio n Plannin g 801
Figur e 25.1 6 Th e Vorono i diagra m for a set of polygona l obstacles .
Figur e 25.17 A  roadma p of a torus , showin g one horizonta l freewa y and four vertica l bridges .
in ever y connecte d piece of fs cross section . See Figur e 25.17 . To get silhouett e curve s
from these extrema l points , we simpl y allow c to vary , and follo w the extrema l point s in Y
as the plan e X = c move s throug h f'.
Linkin g curve s join critica l point s to silhouett e curves . Critica l point s are point s wher e
the cros s sectio n X = c  change s abruptl y as c varies . By referrin g to Figur e 25.17 , the
reade r will find two critica l point s wher e the cross­sectio n changes . If the critica l poin t
has X­coordinat e CQ, then for X < CQ, the cross­sectio n is an hourglass­shape d curve . For
X > CQ side, the hourglas s has pinche d off into two circles . We will call a slice X = c
containin g a critica l poin t a "critica l slice. " Linkin g curve s can be define d severa l ways , but
802 Chapte r 25 . Robotic s
the easies t is to defin e a linkin g curv e as the roadma p of a critica l slice . Becaus e roadmap s
are define d in term s of linkin g curves , this is a circula r definition . But becaus e the linkin g
curve is a roadma p of a slice that has one less dimension , the recursiv e constructio n is
well­define d and terminate s afte r a numbe r of steps that equal s the dimensio n of C.
One disadvantag e of this definitio n is that it give s curve s on the boundar y of free space . Thi s
is undesirabl e from both efficienc y and safet y perspectives . A slightl y bette r definitio n borrow s
some idea s from Vorono i diagrams . Instea d of usin g extremal s in Y to defin e the silhouett e
curves , it uses extremals  of distance from  obstacles  in slices  X ­ c. Thi s sound s like the sam e
definitio n as the Vorono i diagram , but there are sligh t difference s in two dimensions , and majo r
difference s in mor e than two . The definitio n of linkin g curve s also changes . The y still link
critica l point s to silhouett e curves , but this time , linkin g curve s are compute d by movin g awa y
from a critica l poin t alon g curve s that follo w the directio n of maximu m increas e of the distanc e
function . In othe r words , start from a critica l point , and hill­clim b in configuratio n spac e to a
local maximu m of the distanc e function . Tha t poin t lies on a silhouett e curv e unde r the new
definition . This kind of roadma p is exemplifie d in Figur e 25.18 .
Figur e 25.18 A  Voronoi­lik e roadma p of a polygona l environment .
FINE­MOTIO N
PLANNIN GFine­motio n plannin g
Fine­motio n plannin g (FMP ) is abou t plannin g small , precis e motion s for assembly . At the
distanc e scale s appropriat e for FMP , the environmen t is not precisel y known . Furthermore , the
robot is unabl e to measur e or contro l its positio n accurately . Dealin g with thes e uncertaintie s is
the principa l concer n of FMP . Like onlin e algorithms , fine­motio n plan s are strategie s or policie s
that mak e use of sensin g or environmen t shap e to determin e the robot' s path at run time . However ,
wherea s onlin e algorithm s assum e nothin g abou t the environment , partia l knowledg e is availabl e
to the fine­motio n planner . Thi s knowledg e include s explici t model s of the uncertaintie s in
Sectio n 25.6 . Navigatio n and Motio n Plannin g 80 3
sensing , control , and environmen t shape . A fine­motio n planne r does mos t of its wor k offline ,
generatin g a strateg y that shoul d work in all situation s consisten t with its models .
A fine­motio n plan consist s of a serie s of guarde d motions . Each guarde d motio n consist s
of (1) a motio n comman d and (2) a terminatio n condition , whic h is a predicat e on the robot' s
senso r values , and return s true to indicat e the end of the guarde d move . The motio n command s
are typicall y complian t motion s that allow the robo t to slid e if the motio n comman d woul d
cause collisio n with an obstacle . Complian t motio n require s a dynami c mode l such as a sprin g
or damper . The sprin g mode l is the simples t to understand . Rathe r than movin g the robo t itself ,
imagin e movin g one end of a sprin g with the othe r end attache d to the robot . The dampe r mode l
is simila r but instea d of a spring , a devic e calle d a dampe r is attache d to the robot . Wherea s the
sprin g give s a reactio n forc e proportiona l to relativ e displacemen t of its endpoints , the dampe r
gives a reactio n force proportiona l to relativ e velocity . Both allow the robo t to slide on a surfac e
so long as the commande d velocit y is not directl y into the surface . (In reality , there will be som e
frictio n betwee n robo t and obstacle , and the robo t will not mov e even  if the commande d velocit y
is close to right angle s with the surface. )
As an example , Figur e 25.1 9 show s a 2­D configuratio n spac e with a narro w vertica l
hole. It coul d be the configuratio n spac e for insertio n of a rectangula r peg into a hole that is
slightl y larger . The motio n command s are constan t velocities . The terminatio n condition s are
contac t with a surface . To mode l uncertaint y in control , we assum e that instea d of movin g at the
commande d velocity , the robot' s actua l motio n lies in the cone Cv abou t it. The figur e show s
what woul d happe n if we commande d a velocit y straigh t dow n from the start regio n a. Becaus e
of the uncertaint y in velocity , the robo t coul d mov e anywher e in the conica l envelope , possibl y
going into the hole , but more likel y landin g to one side of it. Becaus e the robo t woul d not then
know whic h side of the hole it was on, it woul d not know whic h way to move .
A more sensibl e strateg y is show n in Figure s 25.2 0 and 25.21 . In Figur e 25.20 , the robo t
deliberatel y move s to one side of the hole . The motio n comman d is show n in the figure , and the
terminatio n test is contac t with any surface . In Figur e 25.21 , a motio n comman d is give n that
cause s the robo t to slide alon g the surfac e and into the hole . This assume s we use a complian t
motio n command . Becaus e all possibl e velocitie s in the motio n envelop e are to the right , the
robot will slide to the right wheneve r it is in contac t with a horizonta l surface . It will slide dow n
the right­han d vertica l edge of the hole whe n it touche s it, becaus e all possibl e velocitie s are dow n
relativ e to a vertica l surface . It will keep movin g unti l it reache s the botto m of the hole , becaus e
that is its terminatio n condition . In spite of the contro l uncertainty , all possibl e trajectorie s of the
robot terminat e in contac t with the botto m of the hole . (Tha t is, unles s frictio n or irregularitie s
in the surfac e cause s the robo t to stick in one place. )
As one migh t imagine , the proble m of constructing  fine­motio n plan s is not trivial ; in fact,
it is a good deal harde r than plannin g with exac t motions . One can eithe r choos e a fixed numbe r
of discret e value s for each motio n or use the environmen t geometr y to choos e direction s that give
qualitativel y differen t behavior . A fine­motio n planne r take s as inpu t the configuration­spac e
description , the angl e of the velocit y uncertaint y cone , and a specificatio n of wha t sensin g is
possibl e for terminatio n (surfac e contac t in this case) . It shoul d produc e a multiste p conditiona l
plan or polic y that is guarantee d to succeed , if such a plan exists .
We did not includ e uncertaint y in the environmen t in our example , but there is one elegan t
way to do it. If the variatio n can be describe d in term s of parameters , thos e parameter s can
804 Chapte r 25 . Robotic s
initial
configuratio n
motio n
envelop e —=•
Figur e 25.1 9 A  2­D environment , velocit y uncertaint y cone , and envelop e of possibl e robo t
motions . The intende d velocit y is v, but with uncertaint y the actua l velocit y coul d be anywher e
in Cv, resultin g in a fina l configuratio n somewher e in the motio n envelope , whic h mean s we
wouldn' t know if we hit the hole or not.
initial
configuratio n
Figur e 25.2 0 Th e first motio n comman d and the resultin g envelop e of possibl e robo t motions .
No matte r wha t the error , we know the fina l configuratio n will be to the left of the hole .
motio n
envelop e
Figur e 25.21 Th e secon d motio n comman d and the envelop e of possibl e motions . Eve n with
error , we will eventuall y get into the hole .
L
Sectio n 25.6 . Navigatio n and Motio n Plannin g 805
be adde d as degree s of freedo m to the configuratio n space . In the last example , if the dept h
and widt h of the hole wer e uncertain , we coul d add them as two degree s of freedo m to the
configuratio n space . It is impossibl e to mov e the robo t in these direction s in C or to sens e its
positio n directly . But both thos e restriction s can be incorporate d whe n describin g this proble m
as an FMP proble m by appropriatel y specifyin g contro l and senso r uncertainties . Thi s give s a
complex , four­dimensiona l plannin g problem , but exactl y the same plannin g technique s can be
applied . Notic e that unlik e the decision­theoreti c method s in Chapte r 17, this kind of approac h
result s in plan s designe d for the wors t case outcome , rathe r than maximizin g the expecte d qualit y
of the plan . Worst­cas e plan s are only optima l in the decision­theoreti c sens e if failur e durin g
executio n is muc h wors e than any of the othe r costs involve d in execution .
Unfortunately , the complexit y of fine­motio n plannin g is extremel y high . It grow s expo ­
nentiall y not only with the dimensio n of configuratio n space , but also with the numbe r of steps
in the plan . FMP as describe d earlie r also involve s some tenuou s assumption s abou t contro l and
senso r uncertainty , uncertaint y in environmen t shape , and capabilitie s of the run­tim e system .
On the othe r hand , the FMP methodolog y can be applie d to mobil e robo t navigation , and its
assumption s seem more reasonabl e there . We will see some example s of this in the next section .
Landmark­base d navigatio n
In the last two sections , we saw motio n plannin g extende d to includ e run­tim e decision s base d
on sensing . The sensor s were assume d to be simpl e positio n or contac t sensors , and to provid e
unifor m accurac y acros s the environment . Mor e comple x sensors , such as visio n and sonar , have
very differen t attributes . It is muc h more difficul t to mode l these sensor s in a reasonabl e way, but
we will presen t a coupl e of recen t approaches .
In the first , calle d the landmar k mode l of sensing , we assum e that the environmen t contain s
LANDMARK S easil y recognizable , uniqu e landmarks . A landmar k is modele d as a poin t with a surroundin g
circula r field of influence . Withi n the field of influence , the robo t is able to kno w its positio n
exactly . If the robo t is outsid e all the field s of influence , it has no direc t positio n information .
This mode l migh t seem unrealisti c at first , but it is good mode l for landmark s such as bar codes .
Bar codes , like the ones on supermarke t items , are sometime s used as landmark s for mobil e
robot s in indoo r environments . The y are place d in strategi c location s on walls , a few feet abov e
the floor . The y have uniqu e code s enablin g each to be distinguished . As long as the robo t is
close enoug h to recogniz e a bar code , it can get a good estimat e of distanc e from that landmark .
Gettin g good angula r data is harder , but some estimat e can be computed . Beyon d the rang e of
recognitio n of the code , the robo t canno t get any informatio n from it.
The robot' s contro l is assume d to be imperfect . Whe n it is commande d to mov e in a
directio n v, we assum e the actua l motio n lies in a cone of path s centere d on v. This is the same
uncertaint y mode l that we used in the last section . It is a very reasonabl e one for mobil e robots .
If the robo t has a gyroscop e or magneti c compass , the error s in its motio n will be offset s in
directio n from the commanded  motion . This is exactl y wha t the uncertaint y cone models .
An environmen t with landmark s is show n in Figur e 25.22 . Thi s environmen t is know n at
plannin g time , but not the robot' s position . We assum e thoug h that the robo t lies somewher e insid e
a regio n (rectangula r in the figure ) that we do know . We plan a strateg y for the robo t by workin g
806____________________________________ _ Chapte r 25 . Robotic s
backward s from the goal . Figur e 25.2 2 show s a commande d velocit y v, and the backprojectio n
of the goal regio n with respec t to v. If the robo t start s anywher e in the backprojectio n and move s
with commande d velocit y v, it will definitel y reach the goal disk . Notic e that the backprojectio n
intersect s a landmar k D \. Becaus e D \ is the field of influenc e of a landmark , the robo t has perfec t
positio n sensin g insid e D\. So if it reache s any part of D\, it can mov e reliabl y to the part of D\
that intersect s the backprojectio n of the goal . From ther e it can mov e in directio n v to the goal .
This mean s that the robo t can reac h the goal from a regio n R if it can mov e reliabl y from R
straigh t to the goal or if it can mov e reliabl y from R to disk D\. Continuin g to wor k backwards ,
Figur e 25.23 show s a backprojectio n of the unio n of the goal and D\ relativ e to velocit y comman d
u. This new backprojectio n contain s the start regio n S. This give s us a guarantee d strateg y for
reachin g the goal . The executio n of this strateg y woul d be (1) mov e in directio n u to D\ and then
(2) mov e in directio n v to the goal .
As for fine­motio n planning , we mus t choos e thes e velocitie s by searchin g all possibilities .
Fortunately , in this case , we do not have an exponentia l blowu p with the numbe r of plan steps .
This is becaus e the backprojection s all have the same form , namely , they are backprojection s of
union s of disks . This give s a polynomia l boun d on the numbe r of qualitativel y differen t motio n
comman d directions . By usin g this observation , it is possibl e to plan in time that is polynomia l
in the numbe r of disks . The plan itsel f will have at mos t n step s if there are n landmarks , becaus e
no landmar k need be visite d more than once . This plannin g metho d is both soun d and complete .
Onlin e algorithm s
Most robo t application s have to deal with som e amoun t of uncertaint y abou t the environment .
Even robot s used for manufacturing , whic h may have complete  geometri c model s of robo t and
environment , have to perfor m high­precisio n task s such as assembly . Assembl y may requir e
motion s of a thousandt h of an inch or less , and at this scale , model s are far from accurate .
is^fspr * When  the environment  is poorly  known,  it is impossible  to plan  a path  for the robot  that will be
collision­free  and reach  a goal  under all circumstances.
Instead , one can try to produc e a conditiona l plan (in the languag e of Chapte r 13) or
polic y (in the languag e of Chapte r 17) that will mak e decision s at run time . In some cases , it is
possibl e to comput e such a plan with no knowledg e of the environmen t at all. Thi s avoid s the
need for an offlin e plannin g stage , and all choice s are mad e at run time . We will call such an
ONLIN E ALGORITH M algorith m an onlin e algorithm . Onlin e algorithm s need to be simple , becaus e they mus t mak e
choice s in real time . For that reason , they canno t "remember " muc h abou t their environment .
Despit e thei r simplicity , onlin e algorithm s have been foun d that are both complet e and
"efficient. " Efficienc y for onlin e algorithm s can be define d in differen t ways . Becaus e it depend s
on the environment , it must depen d on measure s of the environment' s complexity . In Figur e 25.24
we see a two­dimensiona l environmen t with start and goal s points . The environmen t is not know n
to the robo t whe n it begins , and it canno t "see" anything . It can only sens e a boundar y whe n it
runs into it. The robo t is equippe d with a positio n sensor , and it know s wher e the goal is. Her e
is one complet e onlin e strategy :
1. Let / be the straigh t line joinin g the initia l positio n of the robo t with its goal position . The
robot begin s to mov e towar d the goal alon g /.
Sectio n 25.6 . Navigatio n and Motio n Plannin g 807
Figur e 25.22 A  serie s of landmark s and the backprojectio n of G for commande d velocit y v.
Figur e 25.23 Th e backprojectio n of G and D\ for velocit y u.
808 Chapte r 25 . Robotic s
Figur e 25.24 A two­dimensiona l environment , robot , and goal .
COMPETITIV E RATI O2. If the robo t encounter s an obstacl e befor e it reache s the goal , it stop s and record s its
curren t positio n Q. The robo t then walk s aroun d the obstacl e clockwis e (the directio n is
not important ) back to Q. Durin g this walk , the robo t record s point s wher e it crosse s the
line /, and how far it has walke d to reac h them . Afte r the walk , let P0 be the closes t such
point to the goal .
3. The robo t then walk s aroun d the obstacl e from Q to P0. Becaus e it know s how far it
walke d to reac h PQ, it can decid e whethe r it will get to PO faste r by goin g clockwis e
or counterclockwise . Onc e it reache s P0, it start s movin g towar d the goal alon g /, and
continue s until it reache s anothe r obstacl e (in whic h case, it execute s step 2 again ) or until
it reache s the goal .
Onlin e algorithm s such as this are usuall y very fast in term s of computatio n time , but almos t
alway s give up any guarante e of findin g the shortes t path . Thei r efficienc y is ofte n measure d
using a competitiv e ratio . In the case of robo t motion , one typicall y uses the worst­cas e ratio
betwee n the actua l lengt h of the path found , and the shortes t path . For example , the precedin g
algorith m has a competitiv e ratio of at mos t 1 + 1.5B/\l\ , wher e B is the sum of the length s of all
the obstacl e boundarie s and \l\ is the lengt h of the line / from start to goal .
It is fairl y easy to see that this ratio can be very bad in som e cases . For example , if an
enormou s obstacl e protrude s just a teen y bit into the straight­lin e path , and the robo t happen s to
start walkin g aroun d it the wron g way , then the ratio can be unbounded . For some specia l cases ,
there are algorithm s with competitiv e ratio s bounde d by a constan t (Exercis e 25.12) , but there
are som e environment s for whic h no algorith m has a finit e competitiv e ratio . Fortunatel y for
humans , thes e do not occu r ofte n in practice .
Sectio n 25.7 . Summary____________________________________________80 9
25.7 SUMMAR Y _ _ __ _
Robotic s is a challengin g field for two reasons . First , it require s hardwar e (sensor s and effectors )
that actuall y work , a real challeng e for mechanica l engineering . Second , robot s have to work in
the physica l world , whic h is more comple x than mos t of the simulate d softwar e world s that we
have used for our example s in other chapters . Som e robot s finess e this proble m by operatin g in a
restricte d environment . But moder n autonomou s robot s with sophisticate d sensor s and effector s
provid e a challengin g testbe d for determinin g wha t it takes to build an intelligen t agent .
• In general , the physica l worl d is inaccessible , nondeterministic , nonepisodic , and dynamic .
• Robot s have made an economi c impac t in man y industries , and show promis e for explorin g
hazardou s and remot e environments .
• Robot s consis t of a body with rigid links connecte d to each othe r by joints . The movemen t
of the links is characterize d by the degree s of freedo m of the joints .
• Robot s can have sensor s of variou s types , includin g vision , forc e sensing , tactil e (touch )
sensing , sonar , and a sens e of wher e their own body parts are.
• In a dynami c world , it is importan t to be able to take actio n quickly . Robot s are designe d
with this in mind .
• The proble m of movin g a complex­shape d objec t (i.e., the robo t and anythin g it is carrying )
throug h a space with complex­shape d obstacle s is a difficul t one. The mathematica l notio n
of a configuratio n spac e provide s a framewor k for analysis .
• Cel l decompositio n and skeletonizatio n method s can be used to navigat e throug h the
configuratio n space . Bot h reduc e a high­dimensional , continuou s spac e to a discret e
graph­searc h problem .
• Som e aspect s of the world , such as the exac t locatio n of a bolt in the robot' s hand , will
alway s be unknown . Fine­motio n plannin g deal s with this uncertaint y by creatin g a
sensor­base d plan that will work regardles s of the exac t initia l conditions .
• Uncertaint y applie s to sensor s at the large scale as well . In the landmar k model , a robo t
uses certai n well­know n landmark s in the environmen t to determin e wher e it is, even in
the face of uncertainty .
• If a map of the environmen t is not available , then the robo t will have to plan its navigatio n
as it goes . Onlin e algorithm s do this. The y do not alway s choos e the shortes t route , but
we can analyz e how far off they will be.
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
The wor d robo t was popularize d by Czec h playwrigh t Kare l Cape k in his 1921 play R.U.R.
(Rossum' s Universa l Robots) . The them e of the play—th e dehumanizatio n of mankin d in a
technologica l society—appeale d to audience s in war­tor n Europ e and the Unite d States . The
810 Chapte r 25 . Robotic s
HAND­EY E
MACHINE Srobots , whic h were grow n chemicall y rathe r than constructe d mechanically , end up resentin g
their master s and decid e to take over . It appear s (Glanc , 1978 ) that it was actuall y Capek' s
brother , Josef , who first combine d the Czec h word s "robota " (obligator y work ) and "robotnik "
(serf) to yield "robot " in his 1917 shor t story Opilec.
Reichard t (1978 ) survey s the histor y and futur e of robots , groupin g them into four cat­
egories : (1) strictl y mythological , fictional , or fraudulent ; (2) working , but nonelectronic ; (3)
controlle d by very special­purpos e electroni c or electromechanica l hardware ; (4) controlle d
by general­purpos e computers . Brie f account s of early robot s of all four kind s are give n by
Raphael(1976) , McCorduc k (1979) , and Heppenheime r (1985) ; mor e detaile d treatment s are
given by Cohe n (1966 ) and Chapui s and Droz (1958) . The most famou s classica l instance s of the
first type are Talo s (supposedl y designe d and built by Hephaistos , the Gree k god of metallurgy )
and the Gole m of Prague . Perhap s the first impressiv e exampl e of the secon d type was Jacque s
Vaucanson' s mechanica l duck , unveile d in 1738 . An early instanc e of the third type is Torre s
y Quevedo' s electromechanica l automato n for playin g the ches s endgam e of king and rook vs.
king, describe d in Chapte r 5. In the 1890s , Nikol a Tesla built some experimenta l vehicle s that
were teleoperate d (or remot e controlled ) via radio . Gre y Walter' s "turtle, " buil t in 1948 , coul d
be considere d the first moder n type three robot .
In the late 1950s , Georg e Engelberge r and Georg e Devo l develope d the first usefu l industria l
robots , startin g with type three , and movin g on to type four . Engelberge r founde d Unimatio n to
marke t them , and earne d the title "fathe r of robotics. " His Robotics  in Practice  (1980 ) is a good
surve y of the early days of industria l robots . In the mid­1980s , there was a surg e of interes t in
the field , largel y funde d by automotiv e companies . Realit y did not live up to expectations , and
there was a majo r shakeou t in the robotic s industr y in the late 1980s . Perhap s in reactio n to this
shakeout , Engelberger' s Robotics  in Service  (1989 ) is muc h mor e sanguin e abou t the imminen t
practicalit y of type four robot s in industria l settings .
Type four robot s can be furthe r divide d into mobil e robot s (or mobots ) and stati c manip ­
ulators , originall y calle d hand­ey e machines . The first moder n mobil e robo t was the "Hopkin s
Beast, " built in the early 1960 s at John s Hopkin s University . It had pattern­recognitio n hardwar e
and could recogniz e the cove r plate of a standar d AC powe r outlet . It was capabl e of searchin g for
outlets , pluggin g itself in, and then rechargin g its batteries ! Still , the Beas t had a limite d reper ­
toire of skills . The first general­purpos e mobo t was "Shakey, " develope d at SRI Internationa l
from 1966 throug h 1972 (Nilsson , 1984) .
The first majo r effor t at creatin g a hand­ey e machin e was Heinric h Ernst' s MH­1 , describe d
in his MIT Ph.D . thesi s (Ernst , 1961 ) and in a retrospectiv e by Taylo r (1989) . The Machin e
Intelligenc e projec t at Edinburg h (Michie , 1972 ) also demonstrate d an impressiv e early syste m
for vision­base d assembl y calle d FREDDY .
Robotic s engage s virtuall y ever y componen t and subfiel d of AI. Som e area s of AI were
originall y drive n primaril y by the demand s of robotics , althoug h they have since becom e separat e
areas of study . The main two are compute r visio n (and othe r form s of perception ) and planning .
The Shake y robo t projec t at SRI, in particular , was semina l in the developmen t of the technique s
of planning . Ther e are also severa l proble m area s that are uniqu e to robotics . Plannin g in
continuou s state space s is usuall y restricte d to robotics . Senso r and motio n error s are take n
much more seriousl y in robotics , althoug h they are also studie d for militar y applications . Also ,
quite apar t from perceptio n and planning , it is far from trivia l simpl y to describ e the motion s
Sectio n 25.7 . Summar y 81 1
one wishe s a robo t arm (for instance ) to undertake , and then desig n physica l device s and contro l
system s to carry out the motion s described . The mechanic s and contro l of multilin k robo t arms
are amon g the mos t comple x problem s studie d in applie d mathematic s today .
Robot  Manipulators:  Mathematics,  Programming,  and  Control  (Paul , 1981 ) is a classi c
guide to the basic s of robo t arm design . Yoshikaw a (1990 ) provide s a mor e up­to­dat e text
in this area . Latomb e (1991 ) present s a good specialize d textboo k on motio n planning . The
robot motio n plannin g problem , state d in the mos t natura l way , is PSPACE­har d (Reif , 1979 )
(see page 853 for a descriptio n of PSPACE) . Cann y (1988 ) give s a book­lengt h treatmen t of the
computationa l complexit y of robo t motio n planning , dealin g with a numbe r of ways of statin g the
problem . The majo r conferenc e for robotic s is the IEEE  International  Conference  on Robotics
and Automation.  Robotic s journal s includ e IEEE  Robotics and  Automation,  the International
Journal  of Robotics  Research,  and Robotics  and Autonomous  Systems.
EXERCISE S
25.1 (Thi s exercis e was first devise d by Michae l Geneseret h and Nils Nilsson. ) Human s are so
adept at basic tasks such as pickin g up cups or stackin g block s that they often forge t wha t it was
like to try such thing s as newbor n babies . The idea of this exercis e is to mak e explici t some of
the difficultie s involved , in a very direc t manner . As you solv e these difficulties , you shoul d find
yoursel f recapitulatin g the last 20 year s of development s in robotics .
First, pick a task. The task shoul d not be too difficult ! (Fo r example , makin g a colum n
from three cerea l boxe s standin g on end take s over an hour. ) Set up the initia l environment .
Then , buil d a robo t out of four other human s as follows :
<) Brain : the job of the Brai n is to come up with a plan to achiev e the goal , and to direc t the
hand s in the executio n of the plan . The Brai n receive s inpu t from the Eyes , but cannot  see
the scene  directly.
<) Eyes : the Eyes ' job is to repor t a brief descriptio n of the scen e to the Brain . The Eyes
shoul d stan d a few feet awa y from the workin g environment , and can provid e qualitativ e
description s (such as "Ther e is a red box standin g on top of a green box, whic h is on its
side") or quantitativ e description s ("Th e gree n box is abou t two feet to the left of the blue
cylinder") . Eye s can also answe r question s from the Brai n such as, "Is there a gap betwee n
the Left Han d and the red box? " The Eyes should  not know  what  the goal is.
0 Hand s (Lef t and Right) : one perso n play s each Hand . The two Hand s stan d next to each
other ; the Left Han d uses only his or her left hand , and the Righ t Han d only his or her
right hand . The Hand s execut e only simpl e command s from the Brain—fo r example , "Lef t
Hand,  mov e two inche s forward. " The y canno t execut e command s othe r than motions ; for
example , "Pic k up the box" is not somethin g a Han d can do. To discourag e cheating , you
migh t wan t to have the hand s wea r gloves , or have them operat e tongs . As well as bein g
ignoran t of the goal , the Hand s mus t be blindfolded.  The only sensor y capabilit y they have
is the abilit y to tell whe n their path is blocke d by an immovabl e obstacl e such as a table or
the othe r Hand . In such cases , they can beep to infor m the Brai n of the difficulty .
812 Chapte r 25 . Robotic s
We have give n only the mos t basic protocol . As the task unfolds , you may find that more comple x
protocol s are neede d in orde r to mak e any progress . Wha t you learn abou t robotic s will depen d
very muc h on the task chose n and on how closel y you stick to the protocols .
25.2 Desig n and sketc h a mobil e robo t for an offic e environment . You r robo t shoul d be able
to perfor m typica l offic e task s like retrievin g files , books , journals , and mail , and photocopying .
What type s of arm/han d did you choose ? Conside r carefull y the trade­of f betwee n flexibilit y and
custo m desig n for thos e tasks . Wha t kind s of sensin g did you choose ?
25.3 Conside r the proble m of designin g a robo t that will keep an offic e buildin g tidy by
periodicall y collectin g and emptyin g bins and pickin g up empt y cans and pape r cups .
a. Describ e an appropriat e physica l desig n for the robot .
b. Defin e a suitabl e performanc e evaluatio n measure .
c. Propos e an overal l architectur e for the robo t agent . Mitchel l (1990 ) and Brook s (1989 )
describ e two differen t approache s to the problem .
25.4 Calculat e the numbe r of degree s of freedo m of your hand , with the forear m fixed .
25.5 Conside r the arm of a recor d playe r as a two­lin k robot . Let 0\ measur e the elevatio n of
the arm, and 6*2 measur e its horizonta l rotation . Sketc h the configuratio n spac e of this robo t with
6*1 and $2 axes . Includ e the configuratio n spac e obstacle s for the platte r and spindle .
25.6 Dra w the cylindrica l cell decompositio n for the environmen t in Figur e 25.25 . With n board s
in such an arrangement , how man y region s woul d there be in the cell decomposition ?
25.7 If you have not done them already , do Exercise s 4.13 to 4.15 . Also answe r the following :
a. Prov e rigorousl y that shortes t path s in two dimension s with conve x polygona l obstacle s lie
on the visibilit y graph .
b. Doe s this resul t hold in three dimensions ? If so, prov e it. If not, provid e a counterexample ,
c. Can you sugges t (and prove ) a resul t for the shortes t path s in two dimension s with curved
obstacles ?
Figur e 25.25 Ro w of board s environment .
Sectio n 25.7 . Summar y 813
Figur e 25.26 A n environmen t for onlin e navigation .
d. Can the two­dimensiona l path­plannin g proble m be solve d usin g Vorono i diagram s instea d
of visibilit y graphs ? Explai n the modification s that woul d be neede d to the overal l system .
25.8 Suppos e a robo t can see two poin t landmark s L\ and LI, and that it measure s the angl e
betwee n them as 17°. Wha t is the shap e of the recognizabl e set correspondin g to this senso r
reading ? Wha t is the shap e of the recognizabl e set if the angl e measuremen t has an uncertaint y
of ± 1 ° ? Sketc h this situation .
25.9 Fo r the environmen t in Figur e 25.26 , sketc h the path take n by the robo t in executin g
the onlin e navigatio n strategy . Thi s strateg y alway s completel y circumnavigate s any obstacl e
it encounters , whic h is ofte n unnecessary . Try to thin k of anothe r strateg y that will trave l less
distanc e aroun d som e obstacles . Mak e sure  you r strateg y will not get stuc k in cycles . You r
strateg y will probabl y have a wors e worst­cas e boun d than the one presente d in this chapter , but
shoul d be faste r in typica l cases .
25.10 Implemen t a genera l environmen t in whic h to exercis e the onlin e navigatio n algorithm ,
such that arbitrar y obstacle s can be place d in the environment . Construc t a specifi c environmen t
correspondin g to Figur e 25.26 . Then implemen t an agen t that incorporate s the onlin e algorithm ,
and show that it works .
25.11 Explai n how to approac h the proble m of onlin e navigatio n for a cylindrica l robo t of
significan t diameter . Doe s the point­robo t algorith m need to be modified ? Wha t happen s whe n
the robo t is headin g dow n a passagewa y that become s too narro w to get through ?
25.12 W e state d in our discussio n of onlin e algorithm s that some specia l classe s of environment s
are amenabl e to onlin e algorithm s with constan t competitiv e ratios . An exampl e is show n in
Figur e 25.27 , whic h show s a robo t on the bank of a river . We assum e that the robo t has an exac t
positio n sensor . The robo t is tryin g to find the bridge , whic h it know s is somewher e nearby , but is
not sure how far and in whic h direction . Unfortunately , there is a thick fog and the robo t canno t
see the bridg e unles s it stumble s righ t onto it.
a. Describ e an onlin e algorith m for the robo t that is guarantee d to find the bridg e after a finit e
search , no matte r wher e the bridg e is.
b. Calculat e the total distanc e traverse d by the robo t usin g your algorithm , and comput e its
competitiv e ratio .
814 Chapte r 25 . Robotic s
\^
Robo t * d  / _/
V
Figur e 25.27 A n onlin e motion­plannin g problem : find the bridg e in thick fog.
o0
Figur e 25.28 Landmark s L,, goal G, start regio n 5, and the motio n uncertaint y cone .
25.13 Fo r the environmen t and motio n uncertaint y cone show n in Figur e 25.28 , find a navigatio n
strateg y that is guarantee d to reach the goal G.
25.14 Her e are the three laws of robotic s from the scienc e fictio n book /, Robot  (Asimov , 1950) :
1. A robo t may not injur e a huma n bein g or throug h inactio n allow a huma n bein g to come  to
harm .
2. A robo t must obey the order s give n to it by huma n being s excep t wher e such order s woul d
conflic t with the first law.
3. A robo t mus t protec t its own existenc e as long as such protectio n does not conflic t with the
first or secon d law.
So far, only a few attempt s have been mad e to buil d such safeguard s into roboti c softwar e (Wel d
and Etzioni , 1994) .
a. How woul d you desig n a robo t to obey thes e laws ? Wha t type of architectur e woul d be
best? Wha t aspect s of the laws are hard to obey ?
b. Are the laws sensible ? Tha t is, if it were possibl e to build a sophisticate d robot , woul d you
want it to be governe d by them ? Can you thin k of a bette r set of laws ?
I
Part VIII
CONCLUSION S
In this part we summariz e wha t has com e before , and give our view s of what the
futur e of AI is likel y to hold . We also delv e into the philosophica l foundation s
of AI, whic h have been quietl y assume d in the previou s parts .
26PHILOSOPHICA L
FOUNDATION S
In which  we consider  what it means  to think  and to be conscious,  and  whether
artifacts  could  ever  do such  things.
26.1 TH E BIG QUESTION S
As we mentione d in Chapte r 1, philosopher s have been aroun d for muc h longe r than computers ,
and have been tryin g to resolv e man y of the same question s that AI and cognitiv e scienc e claim
to address : How can mind s work , how do huma n mind s work , and can nonhuman s have minds ?
For mos t of the book we have concentrate d on gettin g AI to work at all, but in this chapter , we
addres s the big questions .
Sometime s the question s have led philosopher s and AI researcher s to squar e off in heate d
debate . Mor e often , the philosopher s have debated  each other . Som e philosopher s have side d
with the computationa l approac h provide d by artificia l intelligence , partl y becaus e it has the tools
and the inclinatio n to give detailed , causa l explanation s of intelligen t behavior: '
It is rathe r as if philosopher s were to proclai m themselve s exper t explainer s of the method s
of a stag e magician , and then , whe n we ask them to explai n how the magicia n does the
sawing­the­lady­in­hal f trick , they explai n that it is reall y quite obvious : the magicia n doesn' t
really saw her in half; he simpl y make s it appea r that he does . "Bu t how does he do thatT
we ask. "No t our department, " say the philosophers . (Dennett , 1984 )
... amon g philosopher s of scienc e one find s an assumptio n that machine s can do everythin g
that peopl e can do, followe d by an attemp t to interpre t wha t this bode s for the philosoph y
of mind ; whil e amon g moralist s and theologian s one find s a last­ditc h retrenchmen t to such
highl y sophisticate d behavio r as mora l choice , love and creativ e discovery , claime d to be
beyon d the scop e of any machine . (Dreyfus , 1972 )
1 At a recen t meetin g of the America n Philosophica l Association , it was put to us by one philosopher , who may prefe r
to remai n nameless , that "Philosoph y has alread y capitulate d to AI."
817
818 Chapte r 26 . Philosophica l Foundation s
Other philosopher s have openl y deride d the effort s of AI researcher s and, by implication , their
philosophica l fello w travellers :
Artificia l intelligenc e pursued  within  the cult of computationalism  stand s not even a ghos t of
a chanc e of producin g durabl e result s ... it is time to diver t the effort s of AI researchers —
and the considerabl e monie s mad e availabl e for their support—int o avenue s othe r than the
computationa l approach . (Sayre , 1993 )
After fifty year s of effort , however , it is clea r to all but a few diehard s that this attemp t to
produc e genera l intelligenc e has failed . (Dreyfus , 1992 )
The natur e of philosoph y is such that clear disagreement s can continu e to exist unresolve d for
many years . If there were any simpl e experimen t that coul d resolv e the disagreements , the issue s
woul d not be of philosophica l interest .
We will begin in Sectio n 26.2 by lookin g at philosophica l question s concernin g whic h the
interest s of AI and philosoph y coincide—th e "how to" question s concernin g the basic capabilitie s
of perception , learning , and reasoning . This will help to clarif y the concept s used later .
In Sectio n 26.3 , we look at the questio n "Can machine s be mad e to act as if  they were
WEAKA I intelligent. " The assertio n that they can is calle d the weak AI position . Argument s agains t weak
AI mak e one of these three claims :
• Ther e are thing s that computer s canno t do, no matte r how we progra m them .
• Certai n way s of designin g intelligen t program s are boun d to fail in the long run.
• The task of constructing  the appropriat e program s is infeasible .
The argument s can be, and sometime s have been , refute d by exhibitin g a progra m with the
supposedl y unattainabl e capabilities . On the othe r hand , by makin g AI researcher s thin k more
carefull y abou t their unstate d assumptions , the argument s have contribute d to a mor e robus t
methodolog y for AI.
STRON G AI I n Sectio n 26.4 , we take on the real bone of contention : the stron g AI position , whic h
claim s that machine s that act intelligentl y canno t have real, consciou s minds . Debate s abou t
stron g AI bring up some of the most difficul t conceptua l problem s in philosophy .
Note that one coul d legitimatel y believ e in stron g AI but not wea k AI. It is perfectl y
consisten t to believ e that it is infeasibl e to build machine s that act intelligently , but to be willin g
to gran t such a machin e full consciousnes s if it coul d in fact ever be built .
We will try to presen t the argument s in their simples t forms , whil e preservin g the lunge s
and parrie s that characteriz e philosophica l debates . We do not have space to do them full justice ,
and we encourag e the reade r to consul t the origina l sources . A caution : one of the most importan t
thing s to keep in mind when readin g the contribution s to a philosophica l debat e is that what  often
appear s to be an attemp t to refut e some propositio n (such as "computer s understan d English" ) is
actuall y an attemp t to refut e a particula r justification  for the proposition , or an attemp t to show
that the propositio n is merel y possibl e rathe r than logicall y necessary . Thus , man y of the debate s
are not wha t they may seem at first , but there is still plent y to argu e about .
I
Sectio n 26.2 . Foundation s of Reasonin g and Perceptio n 819
26.2 FOUNDATION S OF REASONIN G AND PERCEPTIO N
PHYSICALIS M
MATERIALIS M
BIOLOGICA L
NATURALIS M
FUNCTIONALIS M
HOMUNCUL I
INFINIT E REGRES SAlmos t all partie s to the AI debat e share a certai n amoun t of commo n groun d regardin g the relatio n
of brain and mind . The commo n groun d goes unde r variou s names— physicalism , materialism ,
and biologica l naturalis m amon g other s —bu t it can be reduce d to the characteristicall y pithy
remar k by John Searle : "Brains  cause  minds"  Intelligenc e and menta l phenomen a are product s
of the operatio n of the physica l syste m of neuron s and their associate d cells and suppor t structures .
This doctrin e has a numbe r of corollaries . Perhap s the mos t importan t is that menta l
states—suc h as bein g in pain , knowin g that one is ridin g a horse , or believin g that Vienn a is the
capita l of Austria—ar e brain states . This does not commi t one to very much , othe r than to avoi d
speculatio n abou t nonphysica l processe s beyon d the ken of science . The next step is to abstrac t
away from specifi c brain states . One  mus t allow that differen t brain state s can correspon d to the
same menta l state , provide d they are of the same  type.  Variou s author s have variou s position s
on wha t one mean s by type  in this case . Almos t everyon e believe s that if one take s a brain
and replace s som e of the carbo n atom s by a new set of carbo n atoms,2 the menta l state will
not be affected . This is a good thing becaus e real brain s are continuall y replacin g their atom s
throug h metaboli c processes , and yet this in itself does not seem to caus e majo r menta l upheavals .
Functionalism , on the othe r hand , propose s a muc h loose r definitio n of "sam e type, " base d on
identit y of the functiona l propertie s of the component s of the brain—tha t is, their input/outpu t
specifications . In the neura l versio n of functionalism,3 wha t matter s is the input/outpu t propertie s
of the neurons , not their physica l properties . Becaus e the same input/outpu t propertie s can be
realize d by man y differen t physica l devices , includin g silico n devices , functionalis m naturall y
leads to the belie f that AI system s with the appropriat e structur e migh t have real menta l states .
Now we begi n to explai n how  it is that brain s caus e minds . Som e of the earlies t form s of
explanatio n involve d wha t are now calle d homunculi —a Latin term meanin g "miniatur e men. "
For example , whe n it was foun d that a smal l imag e of the worl d was forme d on the retin a by the
lens, som e early philosopher s propose d that visio n was achieve d by a subsyste m of the brain— a
homunculus—tha t looke d at this imag e and reporte d wha t it saw.4 Unfortunately , one then has
to explai n how it is that the homunculu s can see. An infinit e regres s begin s when one propose s
that the homunculu s sees by mean s of a smalle r homunculu s insid e his head .
Moder n proponent s of rule­base d model s of huma n behavio r have been accuse d of fallin g
into the sam e infinit e regres s trap . The objectio n goes as follows : ever y use of a logica l rule
(such as "all men are mortal" ) mus t itself be governe d by a rule (suc h as Modu s Ponens) . The
applicatio n of a rule such as Modu s Ponen s mus t in turn be governe d by anothe r rule, and so on
ad infinitum.  Therefore , intelligen t behavio r canno t be produce d by followin g rules . Now this
argumen t has a certai n superficia l plausibility , particularl y if we note that the forward­chainin g
applicatio n of inferenc e rales such as Modu s Ponen s indee d can be viewe d as itself an applicatio n
2 Perhap s even atom s of a differen t isotop e of carbon , as is sometime s done in brain­scannin g experiments .
3 Ther e are othe r version s of functionalis m that mak e the decompositio n at differen t points . In general , ther e is a wid e
range of choice s in wha t count s as a component .
4 Proponent s of this theor y were excite d by the discover y of the pinea l gland , whic h in man y mammal s look s superficiall y
like an eye. Close r examinatio n of cours e reveale d that it has no power s of vision .
820 Chapte r 26 . Philosophica l Foundation s
INTENTIONA L STATE S
INTENTIONA L
STANC Eof Modu s Ponen s at a highe r level . Clearly , however , logica l system s do manag e to work withou t
infinit e regress . This is becaus e at som e point , the rule applicatio n proces s is fixed;  for example ,
in Prolog , the proces s of left­to­right , depth­firs t chainin g applie s to all inferences . Althoug h this
proces s could  be implemente d by a meta­Prolo g interpreter , ther e is no need becaus e it is not
subjec t to alteration . This mean s that it can be implemente d directl y by a physica l process , base d
on the actua l operatio n of the laws of physics , rathe r than the applicatio n of rules representin g
those laws . The physica l implementatio n of the reasonin g syste m serve s as the base case that
terminate s the regres s befor e it become s infinite . Ther e is therefor e no in­principl e philosophica l
difficult y in seein g how a machin e can operat e as a reasonin g syste m to achiev e intelligen t
behavior . Ther e is a practica l difficulty : the syste m designe r mus t show both that the rule s
will lead to the righ t inferences , and that the reasonin g syste m (the fixed , physica l instantiation )
correctl y implement s the metaleve l reasonin g rules .
So muc h for intelligen t behavior , at least for now . Wha t abou t menta l states ? How do
we explai n thei r possessio n by humans , and do machine s have them ? We will first conside r
prepositiona l attitudes , whic h are also know n as intentiona l states . Thes e are states , such as
believing , knowing , desiring , fearing , and so on, that refer to som e aspec t of the externa l world .
For example , the belie f that Vienn a is the capita l of Austri a is a belie f about  a particula r city and
its statu s (or if you prefer , a belie f abou t a propositio n whic h is in turn abou t a city) . The questio n
is, whe n can one say that a give n entit y has such a belief ?
One approac h is to adop t wha t Danie l Dennet t (1971 ) has calle d the intentiona l stanc e
towar d the entity . On this view , ascribin g belief s or desire s to entitie s is no more than a calcula ­
tiona l devic e that allow s one to predic t the entity' s behavior . For example , a thermosta t can be
said to desir e that room temperatur e be maintaine d in a certai n range , and to believ e that the room
is currentl y too cold and that switchin g on the heat increase s the room temperatur e (McCarth y
and Hayes , 1969) . It is then reasonabl e to ascrib e intentiona l state s whe n that provide s the most
succinc t explanator y mode l for the entity' s behavior . As mentione d in Chapte r 1, this is simila r
to ascribin g pressur e to a volum e of gas. However , the thermostat' s "belief " that the room is too
cold is not identica l to the correspondin g belie f held by a person . The thermostat' s sensor s only
allow it to distinguis h three state s in the world , whic h we migh t call Hot,  OK,  and Cold.  It can
have no conceptio n of a room , nor of heat for that matter . "The room is too cold" is simpl y a way
of namin g the propositio n in whic h the thermosta t is said to believe .
The "intentiona l stance " view has the advantag e of bein g base d solel y on the entity' s
behavio r and not on any suppose d interna l structure s that migh t constitut e "beliefs. " This is also
a disadvantage , becaus e any give n behavio r can be implemente d in man y differen t ways . The
intentiona l stanc e canno t distinguis h amon g the implementations . For som e implementations ,
there are no structure s that migh t even be candidate s for representation s of the ascribe d belief s
or desires . For example , if a ches s progra m is implemente d as an enormou s looku p table , then it
does not seem reasonabl e to suppos e that the progra m actually  believe s that it is goin g to captur e
the opponent' s queen ; wherea s a huma n with identica l ches s behavio r migh t well have such a
belief . The questio n of actual  intentiona l state s is still a real one.
The intentiona l stanc e allow s us to avoi d paradoxe s and clashe s of intuitio n (such as bein g
force d to say that thermostat s have beliefs) . This make s us all feel more comfortable , but it is not
necessaril y the righ t scientifi c approach . Intuition s can change , and paradoxe s can be resolved .
The hypothesi s that the earth revolve s aroun d the sun was once calle d "the Copernica n paradox "I
Sectio n 26.2 . Foundation s of Reasonin g and Perceptio n 821
CORRESPONDENC E
THEOR Y
GROUNDIN G
CAUSA L SEMANTIC S
WIDE CONTEN T
NARRO W CONTEN T
BRAI N IN A VAT
QUALI Aprecisel y becaus e it clashe d with the intuition s afforde d by the folk cosmolog y of the time . We
may find out that our curren t folk psycholog y is so far off base that it is givin g us simila r kind s
of fault y intuitions .
The correspondenc e theor y of belie f goes somewha t furthe r towar d a realisti c account .
Accordin g to this theory , an interna l structur e in an agen t is a reasonabl e candidat e for a repre ­
sentatio n of a propositio n if the followin g condition s hold :
1. The structur e is forme d whe n sensor y evidenc e for the truth of the propositio n is obtained .
2. The structur e cease s to exis t whe n sensor y evidenc e for the proposition' s falsehoo d is
obtained .
3. The structur e play s the appropriat e causa l role in the selectio n of actions .
Thus , the interna l structur e acts as a "flag " that correlate s with the externa l proposition .
The correspondenc e theor y contain s the crucia l elemen t of grounding —the agent' s belief s
are grounde d in its sensor y experienc e of the world . Groundin g is often viewe d as essentia l to
the possessio n of intentiona l states . For example , befor e an agen t can be said to be dyin g for a
hamburger , it must have some direc t experienc e of hamburgers , or at least of relate d comestibles . It
is not enoug h that its knowledg e base contai n the sentenc e DyingFor(Me,  Hamburger).  However ,
if the sentenc e gets there "in the righ t way"—tha t is, throug h experienc e of hamburger s and so
on —the n we say it has the appropriat e causa l semantics . It has meanin g relatin g to hamburger s
becaus e of its connectio n to actua l hamburge r experiences .
There are two view s on the sens e in whic h an interna l representatio n has actua l meaning .
The first view , calle d wid e content , has it that the interna l representatio n intrinsically  refer s
to som e aspec t of the outsid e world ; that is, ther e is som e connectio n betwee n the interna l
representatio n and the externa l worl d that exist s by the natur e of the representation . For example ,
the interna l state correspondin g to the belie f that "This hamburge r is delicious " intrinsicall y refer s
to the particula r hamburge r that it is "about. " The secon d view , calle d narro w content , says that
no such connectio n exists . The "hamburgery " aspec t of the belie f is an intrinsi c aspec t of the
belief as experienced  by the agent.
We can distinguis h betwee n the two view s by considerin g one of the favorit e device s of
philosophers : the brai n in a vat. Imagine , if you will , that your brain was remove d from your
body at birth and place d in a marvellousl y engineere d vat. The vat sustain s your brain , allowin g
it to grow and develop . At the same time , electroni c signal s are fed to your brain from a compute r
simulatio n of an entirel y fictitiou s world , and moto r signal s from your brain are intercepte d and
used to modif y the simulatio n as appropriate . The brain in a vat has been wheele d out man y time s
to resolv e question s in philosophy . Its role here is to show that wide conten t is not consisten t
with physicalism . Th e brai n in the vat can be in an identica l state to the brai n of someon e
eatin g a hamburger ; yet in one case , the hamburge r exists ; in the other , it does not. Eve n in
the forme r case , then , the belie f only refer s to the actua l hamburge r in the eyes of a third party
who has independen t acces s to the internal s of the brain and to the externa l worl d containin g the
hamburger . The brai n by itsel f does not refer to the hamburger .
The narrow­conten t view goes beyon d the simpl e correspondenc e theory . The belie f that
a hamburge r is deliciou s has a certai n intrinsi c nature—ther e is somethin g that it is like to have
this belief . Now we get into the realm of qualia , or intrinsi c experience s (fro m the Latin word
meaning , roughly , "suc h things") . The correspondenc e theor y can accoun t for the verba l or
822 Chapte r 26 . Philosophica l Foundation s
discriminator y behavior s associate d with the belief s "the light is red" or "the ligh t is green, " for
example . But it canno t distinguis h betwee n the experience s of red and green—wha t it is like to
see red as oppose d to wha t it is like to see green . It does seem that there is a real questio n here , but
it is not one that seem s likel y to yield to a behaviora l analysis . If the intrinsi c experience s arisin g
from exposur e to red and gree n right s were someho w switched , it seem s reasonabl e to suppos e
that we woul d still behav e the sam e way at traffi c lights , but it also seem s reasonabl e to say
that our lives woul d be in som e way different . This fina l aspec t of intentiona l states—thei r "felt
quality " if you like—i s by far the mos t problematic . It bring s up the questio n of consciousness ,
which , as we will see, is very ticklis h indeed .
26.3 ON THE POSSIBILIT Y OF ACHIEVIN G INTELLIGEN T BEHAVIO R
One of the mos t basi c philosophica l question s for AI is "Can machine s think? " We will not
attemp t to answe r this questio n directly , becaus e it is not clearl y defined . To see why , conside r
the followin g questions :
• Can machine s fly?
• Can machine s swim ?
Most peopl e woul d agre e that the answe r to the first questio n is yes, airplane s can fly, but the
answe r to the secon d is no; boat s and submarine s do mov e throug h the water , but we do not
normall y call that swimming . However , neithe r the question s nor the answer s have any impac t
at all on the workin g lives of aeronauti c and nava l engineers . The answer s have very little to do
with the desig n or capabilitie s of airplane s and submarines , and muc h more to do with the way
we have chose n to use words . The word "swim " has com e to mea n "to mov e alon g in the wate r
by movement s of the limb s or othe r body parts, " wherea s the word "fly" has no such limitatio n
on the mean s of locomotion .
To complicat e matters , word s can be used metaphorically , so whe n we say a compute r
(or an engine , or the economy ) is runnin g well , we mean it is operatin g smoothly , not that it is
propellin g itsel f with its legs in an admirabl e fashion . Similarly , a perso n who says , "My mode m
doesn' t wor k becaus e the compute r think s it is a 2400­bau d line" is probabl y usin g "thinks "
metaphorically , and may still maintai n that computer s do not literally  think .
The practica l possibilit y of "thinkin g machines " has been with us only for abou t 40 years ,
not long enoug h for speaker s of Englis h to settl e on an agree d meanin g for the word "think. " In
the early days of the debate , som e philosopher s though t that the questio n of thinkin g machine s
could be settle d by mean s of linguisti c analysi s of the kind hinte d at earlier . If we defin e "think "
to mea n somethin g like "mak e decision s or deliberation s by mean s of an organic , natura l brain, "
then we mus t conclud e that computer s canno t think . Ultimately , the linguisti c communit y will
come to a decisio n that suits its need to communicat e clearly,5 but the decisio n will not tell us
much abou t the capabilitie s of machines .
5 Wittgenstei n said that we shoul d "loo k at the wor d 'to think ' as a tool. "
Sectio n 26.3 . O n the Possibilit y of Achievin g Intelligen t Behavio r 823
Alan Turing , in his famou s pape r "Computin g Machiner y and Intelligence " (Turing , 1950) ,
suggeste d that instea d of askin g "Can machine s think? " we shoul d instea d ask if they can pass
a behaviora l test (whic h has com e to be calle d the Turin g Test ) for intelligence . He conjecture d
that by the year 2000 , a compute r with a storag e of 109 unit s coul d be programme d well enoug h
to have a conversatio n with an interrogato r for 5 minute s and have a 30% chanc e of foolin g the
interrogato r into thinkin g it was human . Althoug h we woul d certainl y not claim that anythin g
like general , human­leve l intelligenc e will be achieve d by that time , his conjectur e may not be
that far off the truth . Turin g also examine d a wide variet y of possibl e objection s to the possibilit y
of intelligen t machines , includin g virtuall y all of those that have been raise d in the 44 year s since
his pape r appeared .
Some of the objection s can be overcom e quit e easily . For example , Lad y Ada Lovelace ,
commentin g on Babbage' s Analytica l Engine , says , "It has no pretension s to originate  anything .
It can do whatever  we know how to order it  to perform. " This objection , that computer s can only
do wha t they are told to do and are therefor e not capabl e of creativity , is commonl y encountere d
even today.  It is refute d simpl y by notin g that one of the thing s we can tell them to do is to learn
from their experience . For example , Samuel' s checker­playin g progra m performe d very poorl y
with its origina l programming . However , it was able to learn , over the cours e of a few days
of self­play , to play checker s far bette r than Samue l himsel f (see Chapte r 20). One can try to
preserv e Lady Lovelace' s objectio n by maintainin g that the program' s abilit y to learn originate d
in Samuel , and so too did its checker­playin g ability . But then one woul d also be led to say that
Samuel' s creativit y originate d in his parents , and their s originate d in their parents , and so on.
The "argumen t from disability " takes the form of a claim , usuall y unsupported , to the effec t
that "a machin e can neve r do X." As example s of X, Turin g lists the following :
Be kind , resourceful , beautiful , friendly , have initiative , have a sens e of humor , tell right from
wrong , mak e mistakes , fall in love , enjo y strawberrie s and cream , mak e someon e fall in love
with it, learn from experience , use word s properly , be the subjec t of its own thought , have as
much diversit y of behavio r as man , do somethin g reall y new .
Althoug h som e of thes e abilitie s concer n the consciousnes s of machines , whic h we discus s at
lengt h in wha t follows , man y concer n behaviora l propertie s (see Exercis e 26.1) . Turin g suggest s
that scepticis m of this natur e arise s from experienc e of machine s as device s for carryin g out
repetitiv e tasks requirin g little sensor y and no reasonin g ability . He point s to the fact that in the
late 1940s , the genera l populatio n foun d it difficul t to believ e that machine s coul d find numerica l
solution s of equation s or predic t ballisti c trajectories . Eve n today , however , man y technicall y
literat e peopl e do not believ e that machine s can learn .
The suppose d inabilit y to mak e mistake s present s an interestin g proble m whe n considerin g
the Turin g Test . Certainly , instantaneou s and correc t answer s to long divisio n problem s woul d
be a giveaway , and some attemp t to simulat e huma n fallibilit y woul d be required.6 But this is not
a mistak e in the norma l sense , becaus e the progra m is doin g exactl y wha t its designe r intended .
Somethin g mor e akin to huma n mistake s will arise whe n intractabl e problem s are involved . For
example , give n only a smal l amoun t of time to find a ches s move , the compute r mus t essentiall y
guess that its mov e is correct . Similarly , a progra m that is tryin g to induc e hypothese s from
6 In recen t Turin g Test competitions , the winnin g program s are thos e that type their answer s back slowl y and irregularly ,
with occasiona l correction s and spellin g mistakes . A Marko v mode l of typin g spee d and accurac y is sufficient .
824 Chapte r 26 . Philosophica l Foundation s
a smal l amoun t of data is boun d to mak e mistake s whe n usin g such hypothese s for prediction .
When unavoidabl y irrationa l behavio r on the part of the compute r matche s correspondin g failing s
of humans , this provide s evidenc e that simila r mechanism s are in operation . Rationa l behavior ,
on the othe r hand , provide s muc h weake r constraint s on mechanisms .
What Turin g calls the mathematica l objectio n concern s the prove n inabilit y of computer s
to answe r certai n questions . We discus s this in­principl e barrie r to intelligenc e in Sectio n 26.3 .
In­practic e objection s cente r on the so­calle d "argumen t from informality, " whic h claim s that
intelligen t behavio r canno t be capture d by forma l rules . We discus s this categor y of objection s in
Sectio n 26.3 . The final , and most interesting , objectio n claim s that even if computer s behav e as
intelligentl y as humans , they still will not be intelligent . Althoug h AI canno t do muc h more than
make machine s behav e intelligently , we still have some fun discussin g the issue in Sectio n 26.4.
The mathematica l objectio n
It is well­known , partl y throug h the wor k of Turin g himsel f (Turing , 1936 ) as well as that of
Gode l (1931) , that certai n question s canno t be answere d correctl y by any forma l system . An
HALTIN G PROBLE M exampl e is the haltin g problem : will the executio n of a progra m P eventuall y halt, or will it run
forever ? Turin g prove d that for any algorith m H that purport s to solv e haltin g problem s there
will alway s be a progra m P, such that H will not be able to answe r the haltin g proble m correctly .
Turin g therefor e acknowledge s that for any particula r machin e that is bein g subjecte d to the
Turin g Test , the interrogato r can formulat e a haltin g proble m that the machin e canno t answe r
correctly .
Philosopher s such as Luca s (1961 ) have claime d that this limitatio n make s machine s
inferio r to humans , who can alway s "step outside " the limitin g logic to see whethe r the proble m
in questio n is true or not. Luca s base s his argumen t not on the haltin g proble m but on Godel' s
incompletenes s theore m (see Chapte r 9). Briefly , for any non­trivia l forma l syste m F (a forma l
language , and a set of axiom s and inferenc e rules) , it is possibl e to construc t a so­calle d "Gode l
sentence " G(F)  with the followin g properties :
• G(F)  is a sentenc e of F, but canno t be prove d withi n F.
• If F is consistent , then G(F)  is true.
Luca s claim s that becaus e a compute r is a forma l system , in a sens e that can be mad e precise ,
then there are sentence s whos e truth it canno t establish—namely , its own Gode l sentences . A
human , on the othe r hand , can establis h the truth of these sentence s by applyin g Godel' s theore m
to the forma l syste m that describe s the compute r and its program .
Lucas' s poin t is essentiall y the same as the potentia l objectio n raise d by Turin g himself ,
and can be refute d in the sam e way . We admi t that ther e is a stron g intuitio n that huma n
mathematician s can switc h from one formalis m to anothe r until they find one that solve s the
proble m they are face d with . But there is no reaso n why we coul d not have the same intuitio n
abou t a compute r that was programme d to try thousand s of differen t formalism s (and to inven t
new formalisms ) in an attemp t to solv e the haltin g or Gode l problem . If we accep t that the brain
is a deterministi c physica l devic e operatin g accordin g to norma l physica l laws , then it is just
as muc h a forma l syste m as the compute r (althoug h a harde r one to analyze) , and thus has the
same limitation s as computers . If you believ e ther e are nondeterministi c aspect s of the worl d
Sectio n 26.3 . O n the Possibilit y of Achievin g Intelligen t Behavio r 825
that keep the brai n from bein g a forma l system , then we can buil d a compute r that incorporate s
analo g device s to achiev e the same nondeterminis m (for example , usin g a Geige r counte r to seed
a rando m numbe r generator) .
In practica l terms , the limitation s of forma l system s are not goin g to be of muc h help to the
interrogato r in the Turin g Test anyway . Firs t of all, description s of the haltin g problem s for eithe r
human s or sufficientl y intelligen t computer s are goin g to be far too larg e to talk about . Eve n if
they were smal l enough , the interrogato r does not know the detail s of the compute r it is talkin g
to, and thus has no way of posin g the righ t haltin g problem . And even if the interrogato r mad e
a luck y guess , the compute r coul d just wait a few minute s (or weeks ) and then repl y "It look s
like it doesn' t halt, but I'm not sure—it' s prett y complicated. " Presumabl y that woul d be a good
imitatio n of a typica l huma n reply .
Anothe r way of statin g the refutatio n argument , in a form appropriat e to Lucas' s version , is
that a huma n bein g canno t show the consistenc y of a forma l syste m that describe s the operatio n of
his or her brain or of a large computer , and canno t therefor e establis h the truth of the correspondin g
Gode l sentence . Therefore , human s have the same limitation s that forma l system s have .
In a more recen t reviva l of the mathematica l objection , the mathematicia n Roge r Penros e
has writte n a reasonabl y controversial  book on AI, provocatively  entitle d The  Emperor's  New
Mind,1 whic h purport s to overcom e thes e objection s to Lucas . Penros e argue s that , at leas t
when we conside r the menta l facultie s that mathematician s use to generat e new mathematica l
proposition s and thei r proofs , the claim that F is comple x canno t hold up. Thi s is becaus e
when a new resul t is found , it is usuall y a simpl e matte r for one mathematicia n to communicat e
it to another , and to provid e convincin g evidenc e by mean s of a serie s of simpl e steps.  He
begin s by assumin g that this universa l facilit y for mathematic s is algorithmic , and tries to show
a contradiction :
Now this putativ e "universal " system , or algorithm , canno t ever be know n as the one that we
mathematician s use to decid e truth . For if it were , we woul d construc t its Gode l propositio n
and know  that to be a mathematical  truth  also.  Thus , we are drive n to the conclusio n that the
algorith m that mathematician s use to decid e mathematica l truth is so complicate d or obscur e
that its very validit y can neve r be know n to us. (Penrose , 1990 , p. 654) .
And becaus e mathematica l truth s are in fact simpl e to see, at least step by step , mathematica l
"insight " canno t be algorithmic . We can paraphras e the argumen t as follows :
Mathematic s is simple , and include s the "Godelisation " process , whic h is also simple .
Henc e the "validit y of mathematics " is easil y seen .
Thus , if mathematic s were a forma l system , its Gode l sentenc e woul d easil y be seen to be
true by mathematicians .
4. Henc e mathematica l insigh t canno t be algorithmic .
In an entertainin g serie s of replie s appearin g in the journa l Behavioral  and Brain  Sciences,  a
numbe r of mathematician s includin g Georg e Boolo s (1990 ) and Marti n Davi s (1990 ) poin t out
an obviou s flaw in the first two steps . Mathematic s has a long histor y of inconsistencies , the mos t
famou s of whic h is Frege' s Basic  Laws  of Arithmetic,  show n inconsisten t by Russell' s parado x
7 Penros e (1990 ) strenuousl y denie s that the obviou s analog y to the tale of the Emperor' s New Clothe s was intende d in
any way to sugges t that AI is not all it claim s to be.
826 Chapte r 26 . Philosophica l Foundation s
in 1902 . Almos t ever y majo r figur e in the histor y of logic has at one time or anothe r published
an inconsisten t set of axioms . Eve n today , ther e is som e doub t as to whethe r the principa l
axiomatizatio n of set theor y know n as ZFC (the Zermelo­Fraenke l axiom s plus the Axio m of
Choice ) is consistent , even thoug h it is used widel y as the basis for most of mathematics . Penros e
replie s as follows :
I am not assertin g that, in any particula r case of a forma l system/7, we need necessaril y be able
to "see " that G(F)  is true , but I am assertin g that the "Godelia n insight " that enable s one to
pass from F to G(F)  is just as good a mathematica l procedur e for derivin g new truth s from old
as are any othe r procedure s in mathematics . This insigh t is not containe d withi n the rules of F
itself , however . Thus , F does not encapsulat e all the insight s availabl e to mathematicians . The
insight s that are availabl e to mathematician s are not formalizable . (Penrose , 1990 , p. 694) .
Penros e does not say why he think s the "Godelia n insight " is not formalizable , and it appear s
that in fact it has been formalized . In his Ph.D . thesis , Nataraja n Shanka r (1986 ) used the
Boyer­Moor e theore m prove r BMT P to deriv e Godel' s theore m from a set of basic axioms , in
much the sam e way that Gode l himsel f did.8 The thesi s was intende d mainl y to demonstrat e
the powe r of automati c theore m provers , in particula r the capabilit y of BMT P to develo p and
apply lemmata . Bu t it shows , as Rober t Wilensk y (1990 ) has pointe d out, that one need s to
be carefu l to distinguis h betwee n the forma l syste m that is doin g the proving—i n this case , a
compute r programme d with the BMTP—an d the forma l syste m withi n whic h the proo f is carrie d
out, namely , an axiomatizatio n of arithmeti c and of sentence s in that axiomatization . Just like a
mathematician , a good automate d theore m prove r can appl y Godelisatio n to the particula r forma l
system that it is workin g on; but it can no more establis h the consistenc y of its own progra m
and hardwar e than a mathematicia n can establis h the consistenc y of his or her own brain . We
therefor e seem to be back wher e we started , with the refutatio n of the "mathematica l objection "
that Turin g himsel f raised .
Penros e naturall y maintain s that in som e way , mathematicians ' use of insigh t remain s
nonalgorithmic . Perhap s the mos t interestin g aspec t of his book is the conclusio n he draw s
from this. Afte r providin g a valuabl e discussio n of the relevanc e of physic s to the brain , he
deduce s that nothin g in our curren t physica l understandin g of its operatio n woul d sugges t that it
has nonalgorithmi c aspects—tha t is, the simulatio n of its operatio n by a computer , as describe d
earlier , woul d in principl e be possibl e accordin g to moder n physics . Rathe r than acceptin g the
conclusio n that perhap s the brai n is, afte r all, algorithmi c in this sense , he prefers  to believ e
that moder n physic s mus t be wrong . In particular , the brai n mus t use physica l principle s not
yet discovered , but probabl y relatin g to the interactio n betwee n quantu m theor y and gravity , that
must also be nonalgorithmi c in character .
The argumen t from informalit y
One of the most influentia l and persisten t criticism s of AI as an enterpris e was raise d by Turin g as
the "argumen t from informalit y of behavior. " Essentially , this is the claim that huma n behavio r
8 Ammon' s SHUNYAT A syste m (1993 ) even  appear s to have develope d by itsel f the diagonalizatio n techniqu e used by
Oode l and develope d originall y by Cantor .
Sectio n 26.3 . O n the Possibilit y of Achievin g Intelligen t Behavio r 827
is far too comple x to be capture d by any simpl e set of rules ; and becaus e computer s can do no
more than follo w a set of rules , they canno t generat e behavio r as intelligen t as that of humans .
The principa l proponen t of this view has bee n the philosophe r Huber t Dreyfus , who
has produce d a serie s of powerfu l critique s of artificia l intelligence : What  Computers  Can't
Do (1972 ; 1979) , What  Computers.  Still  Can't  Do (1992) , and, with his brothe r Stuart , Mind
Over  Machine  (1986) . Terr y Winograd , whos e PhD thesi s (Winograd , 1972 ) on natura l languag e
understandin g is criticize d fiercel y by Dreyfu s (1979) , has also expresse d view s simila r to thos e
of Dreyfu s in his recen t work (Winogra d and Flores , 1986) .
The positio n they criticiz e cam e to be calle d "Goo d Old­Fashione d AI," or GOFAI , a term
coine d by Haugelan d (1985) . GOFA I is suppose d to claim that all intelligen t behavio r can be
capture d by a syste m that reason s logicall y from a set of facts and rules describin g the domain .
It therefor e correspond s to the simples t logica l agen t describe d in Chapte r 6. Dreyfu s claim s that
when Minsk y or Turin g claim s that man can be understoo d as a Turin g machine , they mus t
mean that a digita l compute r can reproduc e huma n behavior.. . by processing  data  represent­
ing/acts  about  the world  using  logical  operations  that can be reduce d to matching , classifyin g
and Boolea n operations . (Dreyfus , 1972 , p. 192)
It is importan t to poin t out, befor e continuin g with the argument , that AI and GOFA I are not the
same thing . In fact , it is not clea r whethe r any substantia l sectio n of the field has ever adhere d
to GOFA I in this extrem e form , althoug h the phras e "fact s and rules " did appea r in Turing' s two­
sentenc e speculatio n concernin g how system s migh t be programme d to pass the Turin g Test.9
One only has to read the precedin g chapter s to see that the scop e of AI include s far more than
logica l inference . However , we shal l see that Dreyfus ' criticism s of GOFA I mak e for interestin g
readin g and raise issue s that are of interes t for all of AI.
Whethe r or not anyon e adhere s to GOFAI , it has indee d been a workin g hypothesi s of mos t
AI researc h that the knowledge­base d approac h to intelligen t syste m desig n has a majo r role to
play.  Huber t Dreyfu s has argue d that this presumptio n is base d in a long traditio n of rationalis m
going back to Plato (see Sectio n 1.2). He cites Leibni z as providin g a clear statemen t of the goal s
of the moder n "exper t systems " industry :
The most importan t observation s and turn s of skill in all sorts of trade s and profession s are as
yet unwritte n ... We can also write up this practice , sinc e it is at botto m just anothe r theory .
Furthermore , he claim s that the presumptio n is wrong ; that competenc e can be achieve d withou t
explici t reasonin g or rule following . The Dreyfu s critiqu e therefor e is not addresse d agains t
computer s per se, but agains t one particula r way of programmin g them . It is reasonabl e to
suppose , however , that a book calle d What  First­Order Logical Rule­Based  Systems  Can't  Do
migh t have had less impact .
Dreyfus' s first targe t is the suppositio n that early successe s of GOFA I justif y optimis m that
the methodolog y will succee d in scalin g up to human­leve l intelligence . Man y AI program s
in the 1960 s and earl y 1970 s operate d in microworlds —small , circumscribe d domain s such
as the Block s World . In microworlds , the totalit y of the situatio n can be capture d by a smal l
numbe r of facts . At the time , man y AI researcher s were well awar e that succes s in microworld s
could be achieve d withou t facin g up to a majo r challeng e in the real world : the vast amoun t of
Significantly , Turin g also said that he expecte d AI to be achieve d by a learnin g machine , not a preprogramme d store .
828 Chapte r 26 . Philosophica l Foundation s
potentiall y relevan t informatio n that coul d be brough t to bear on any give n problem . As we saw
in Chapte r 22, disambiguatio n of natura l languag e seem s to requir e acces s to this backgroun d
knowledge . Dreyfu s give s as an exampl e the text fragmen t
Mary saw a dog in the window . She wante d it.
This exampl e was used originall y by Lena t (Lena t and Feigenbaum , 1991 ) to illustrat e the
commonsens e knowledg e neede d to disambiguat e the "it" in the secon d sentence . Presumably ,
"it" refer s to the dog rathe r than the window . If the secon d sentenc e had been "She smashe d
it" or "She presse d her nose up agains t it," the interpretatio n woul d be different . To generat e
these differen t interpretation s in the differen t contexts , an AI syste m woul d need a fair amoun t
of knowledg e abou t dogs , windows , and so on. The projec t of finding , encoding , and usin g such
knowledg e has been discusse d sinc e the early days of AI, and Lenat' s CYC projec t (Lena t and
Guha , 1990 ) is probabl y the mos t well­publicize d undertakin g in this area.
The positio n that Dreyfu s adopts , however , is that this genera l commonsens e knowledg e
is not explicitl y represente d or manipulate d in huma n performance . It constitute s the "holisti c
context " or "Background " withi n whic h human s operate . He give s the exampl e of appropriat e
socia l behavio r in givin g and receivin g gifts : "Normall y one simpl y respond s in the appropriat e
circumstance s by givin g an appropriat e gift." One apparentl y has "a direc t sens e of how thing s
are done and what  to expect. " The sam e claim is mad e in the contex t of ches s playing : "A
mere ches s maste r migh t need to figur e out wha t to do, but a grandmaste r just sees the boar d as
demandin g a certai n move. " Apparently , the "righ t respons e just pops into his or her head. "
Dreyfu s seem s at first to be makin g a claim that migh t appea r somewha t irrelevan t to the
weak AI program : that if human s are sometime s not consciou s of their reasonin g processes , then
on thos e occasion s no reasonin g is occurring . The obviou s AI reply woul d be to distinguis h
PHENOMENOLOG Y betwee n phenomenology —ho w things , includin g our own reasoning , appea r to our consciou s
experience—an d causation . AI is require d to find a causa l explanatio n of intelligence . One migh t
well claim that knowledg e of chess—th e legal move s and so on—i s bein g used , but perhap s not
at a consciou s level . As yet, AI does not claim to have a theor y that can distinguis h betwee n
consciou s and unconsciou s deliberations , so the phenomenologica l aspect s of decision­makin g
are unlikel y to falsif y any particula r approac h to AI.
Anothe r approac h migh t be to propos e that the grandmaster' s suppose d abilit y to see the
right mov e immediatel y derive s from a partia l situation­actio n mappin g used by a refle x agen t
with interna l state . The mappin g migh t be learne d directl y (see Chapte r 20) or perhap s compile d
from mor e explici t knowledg e (see Chapte r 21). An d as discusse d in Chapte r 2, situation ­
actio n mapping s have significan t advantages  in term s of efficiency . On the othe r hand , even
a grandmaste r sometime s need s to use his or her knowledg e of the legal move s to deal with
unfamilia r situations , to find a way out of a trap, or to ensur e that a matin g attac k is unavoidable .
Dreyfus' s positio n is actuall y more subtl e than a simpl e appea l to magica l intuition . Mind
Over  Matter  (Dreyfu s and Dreyfus , 1986 ) propose s a five­stag e proces s of acquirin g expertise ,
beginnin g with rule­base d processin g (of the sort propose d in AI) and endin g with the abilit y to
selec t correc t response s instantaneously :
We have seen that computer s do indee d reaso n thing s out rathe r like inexperience d persons ,
but only with greate r huma n experienc e come s know­how , a far superior , holistic , intuitiv e
way of approachin g problem s that canno t be imitate d by rule­followin g computers .
Sectio n 26.3 . O n the Possibilit y of Achievin g Intelligen t Behavio r 82 9
Dreyfus' s first proposa l for how this "know­how " operate s is that human s solv e problem s by
analogy , usin g a vast "cas e library " from whic h someho w the mos t relevan t precedent s are
extracted . He propose d "som e sort of holographi c memory " as a potentia l mechanism . He later
suggest s neura l network s as a possibl e implementatio n for the final "know­how " phase .
Now he reache s wha t is perhap s the inevitabl e destinatio n of the weak­A I critic : he ends up
effectivel y as an AI researcher , becaus e he canno t avoi d the questio n "If AI mechanism s canno t
work , what  mechanis m do you propos e instea d for huma n performance? " His answer , that human s
use some sort of learnin g method , is not new to AI. Sinc e the very early experiment s of Samue l
and Friedberg , researcher s have propose d using machin e learnin g as a metho d of achievin g highe r
levels of performanc e and avoidin g the difficultie s of hand coding . The questio n is, wha t is the
targe t representatio n for the learnin g process ? Dreyfu s choose s neura l network s becaus e they can
achiev e intelligence , to som e degree , without  explicit  representation  of symbolic  knowledge.10
He claims , however , that there is no reaso n to suppos e that real intelligenc e can be achieve d
withou t a brain­size d network ; nor woul d we understan d the result s of trainin g such a network .
Dreyfus' s natura l pessimis m also leads him to make two usefu l observation s on the difficult y
of a naiv e schem e to construc t intelligenc e by trainin g a large networ k with appropriat e examples :
1. Goo d generalizatio n from example s canno t be achieve d withou t a good deal of backgroun d
knowledge ; as yet, no one has any idea how to incorporat e backgroun d knowledg e into the
neura l networ k learnin g process .
2. Neura l networ k learnin g is a form of supervise d learnin g (see Chapte r 18), requirin g the
prior identificatio n of relevan t input s and correc t outputs . Therefore , it canno t operat e
autonomousl y withou t the help of a huma n trainer .
The first objectio n was also raise d in Chapte r 18, and in Chapte r 21, we saw severa l way s
in whic h backgroun d knowledg e indee d can improv e a system' s abilit y to generalize . Thos e
techniques , however , relie d on the availabilit y of knowledg e in explici t form , somethin g that
Dreyfu s denie s strenuously . In our view , this is a good reaso n for a seriou s redesig n of curren t
model s of neura l processin g so that they can take advantag e of previousl y learne d knowledge .
There has been som e progres s in this direction . Th e secon d objectio n lead s directl y to the
need for reinforcemen t learnin g (Chapte r 20), in whic h the learnin g syste m receive s occasiona l
positiv e or negativ e rewards , rathe r than bein g told the correc t actio n in ever y instance . Give n
enoug h experience , a reinforcemen t learnin g agen t can induc e a utilit y functio n on situations , or
alternativel y a mappin g from situation­actio n pairs to expecte d values . For example , by winnin g
and losin g game s a chess­playin g agen t can graduall y learn whic h sorts of position s are promisin g
or dangerous . Reinforcemen t learnin g is currentl y very popula r in neura l networ k systems .
Dreyfu s correctl y point s out that the majo r proble m associate d with reinforcemen t learnin g
is how to generaliz e from particula r situation s to more genera l classe s of situations—th e genera l
proble m of inductiv e learning . On e can take hear t from the observatio n that reinforcemen t
learnin g reduce s to ordinar y inductiv e learning , for whic h we alread y have some well­develope d
techniques . Ther e are, of course,  problem s yet to be solve d with inductiv e learning , includin g
proble m 1, mentione d earlier , concernin g how to use backgroun d knowledg e to improv e learning .
Dreyfu s also bring s up the proble m of learnin g in the contex t of a large numbe r of potentiall y
10 In fact, man y neura l networ k researcher s are prou d that their network s seem to learn distinct , higher­leve l "features "
of the inpu t spac e and to combin e them in approximatel y logica l ways .
830 Chapte r 26 . Philosophica l Foundation s
relevan t features . One possibl e solutio n is to stick to a smal l finit e set of features , and add new
ones when needed . But accordin g to him, "Ther e is no know n way of addin g new feature s shoul d
the curren t set prov e inadequat e to accoun t for the learne d facts. " As we saw in Chapte r 21, there
are well­principle d way s to generat e new feature s for inductiv e learning .
Anothe r difficul t proble m in reinforcemen t learnin g arise s whe n the availabl e perceptua l
input s do not completel y characteriz e the situation . In such cases , the agen t mus t develo p
additiona l interna l state variables , in term s of whic h outpu t mapping s can be learned . Dreyfu s
claim s that "Sinc e no one know s how to incorporat e interna l state s appropriately , a breakthroug h
will be necessary. " Again , this is a trick y problem , but one on whic h som e progres s has been
made (see Chapte r 20).
The fina l proble m to whic h Dreyfu s refer s in What  Computers  Still  Can't  Do is that of
controllin g the acquisitio n of sensor y data . He note s that the brain is able to direc t its sensor s to
seek relevan t informatio n and to proces s it to extrac t aspect s relevan t to the curren t situation . He
says (pag e xliv) , "Currently , no detail s of this mechanis m are understoo d or even hypothesize d
in a way that coul d guid e AI research. " Yet the field of activ e vision , underpinne d by the theor y
of informatio n valu e (Chapte r 16), is concerne d with exactl y this problem , and alread y robot s
incorporat e the theoretica l result s obtained .
Dreyfu s seem s willin g to gran t that succes s in overcomin g these obstacle s woul d constitut e
the kind of real progres s in AI that he believe s to be impossible . In our view , the fact that AI has
manage d to reduc e the proble m of producin g human­leve l intelligenc e to a set of relativel y well­
define d technica l problem s seem s to be progres s in itself . Furthermore , these are problem s that
are clearl y solubl e in principle , and for whic h partia l solution s are alread y emerging . In summary ,
we have seen that the life of a weak­A I critic is not an easy one. Claim s that "X is impossibl e for
computers " (e.g. , X migh t be beatin g a ches s master ) tend to be overtake n by actua l events . The y
also open the critic to the requiremen t of suggestin g a mechanis m by whic h human s do X; this
force s them , essentially , to becom e AI researchers . On the othe r hand , perceptiv e criticis m from
outsid e the field can be very useful . Man y of the issue s Dreyfu s has focuse d on—backgroun d
commonsens e knowledge , the qualificatio n problem , uncertainty , learning , compile d form s of
decisio n making , the importanc e of considerin g situate d agent s rathe r thaii disembodie d inferenc e
engines—ar e now widel y accepte d as importan t aspect s of intelligen t agen t design .
26.4 INTENTIONALIT Y AND CONSCIOUSNES S
Many critic s have objecte d to the Turin g Test, statin g that it is not enoug h to see how a machin e
acts; we also need to know what interna l "mental " state s it has. This is a valid and usefu l criticism ;
certainl y in tryin g to understan d any compute r progra m or mechanica l devic e it is helpfu l to know
abou t its interna l working s as well as its externa l behavior . Again , the objectio n was foresee n by
Turing . He cites a speec h by a Professo r Jefferson :
Not unti l a machin e coul d writ e a sonne t or compos e a concert o becaus e of thought s and
emotion s felt, and not by the chanc e fall of symbols , coul d we agre e that machin e equal s
brain—tha t is, not only writ e it but know that it had writte n it.
Sectio n 26.4 . Intentionalit y and Consciousnes s 83 1
I jii|f * Jefferson' s key poin t is consciousness : the machine  has to be aware  of its own mental  state  and
' actions.  Other s focu s on intentionality , that is, the "aboutness " (or lack thereof ) of the machine' s
purporte d beliefs , desires , intentions , and so on.
Turing' s respons e to the objectio n is interesting . He coul d have presente d reasons  why
machine s can in fact be conscious . Bu t instea d he maintain s that the questio n is just as ill­
define d as askin g "can machine s think, " and in any case , why shoul d we insis t on a highe r
standar d for machine s than we do for humans ? Afte r all, in ordinar y life we neve r have any
evidenc e abou t the interna l menta l state s of othe r humans , so we canno t know that anyon e else
is conscious . Nevertheless , "instea d of arguin g continuall y over this point , it is usua l to have the
polite conventio n that everyon e thinks, " as Turin g puts it.
Turin g argue s that Jefferso n woul d be willin g to exten d the polit e conventio n to machine s
if only he had experienc e with ones that act intelligently , as in the followin g dialog , whic h has
becom e such a part of AI's oral traditio n that we simpl y have to includ e it:
HUMAN : In the first line of your sonne t whic h reads 'shal l I compar e thee to a summer' s day, '
woul d not a 'sprin g day' do as well or better ?
MACHINE : It wouldn' t scan.
HUMAN : How abou t 'a winter' s day' . Tha t woul d scan all right .
MACHINE : Yes, but nobod y want s to be compare d to a winter' s day.
HUMAN : Woul d you say Mr. Pickwic k reminde d you of Christmas ?
MACHINE : In a way .
HUMAN : Yet Christma s is a winter' s day, and I do not think Mr. Pickwic k woul d mind the
comparison .
MACHINE : I don' t thin k you'r e serious . By a winter' s day one mean s a typica l winter' s day,
rathe r than a specia l one like Christmas .
Jefferson' s objectio n is still an importan t one, becaus e it point s out the difficult y of establishin g
any objectiv e test for consciousness , wher e by "objective " we mean a test that can be carrie d out
with consisten t result s by any sufficientl y competen t third party . Turin g also concede s that the
questio n of consciousnes s is not easily  dismissed : "I do not wish to give the impressio n that I
think there is no myster y abou t consciousnes s ... But I do not thin k these mysterie s necessaril y
need to be solve d befor e we can answe r the questio n with whic h we are concerne d in this paper, "
namely , "Can machine s think? "
Althoug h many , includin g Jefferson , have claime d that thinkin g necessaril y involve s con­
sciousness , the issue is mos t commonl y associate d with the work of the philosophe r John Searle .
We will now discus s two though t experiment s that, Searl e claims , refut e the thesi s of stron g AI.
The Chines e Room
We begi n with the Chines e Room argumen t (Searle , 1980) . The idea is to describ e a hypothetica l
syste m that is clearl y runnin g a progra m and passe s the Turin g Test , but that equall y clearl y
(accordin g to Searle ) does not understan d anythin g of its input s and outputs . The conclusio n will
be that runnin g the appropriat e progra m (i.e., havin g the righ t outputs ) is not a sufficient  conditio n
for bein g a mind .
The syste m consist s of a human , who understand s only English , equippe d with a rule
book , writte n in English , and variou s stack s of paper , som e blank , som e with indecipherabl e
832 ___________________________Chapte r 26 . Philosophica l Foundation s
inscriptions . (The huma n therefor e play s the role of the CPU , the rule book is the program , and
the stack s of pape r are the storag e device. ) The syste m is insid e a room with a smal l openin g
to the outside . Throug h the openin g appea r slips of pape r with indecipherabl e symbols . The
huma n find s matchin g symbol s in the rule book , and follow s the instructions . The instruction s
may includ e writin g symbol s on new slips of paper , findin g symbol s in the stacks , rearrangin g the
stacks , and so on. Eventually , the instruction s will caus e one or more symbol s to be transcribe d
onto a piece of pape r that is hande d throug h the openin g to the outsid e world .
So far, so good . Bu t from the outside , we see a syste m that is takin g inpu t in the form
of Chines e sentence s and generatin g answer s in Chines e that are as obviousl y "intelligent " as
those in the conversatio n imagine d by Turing.11 Searl e then argue s as follows : the perso n in
the room does not understan d Chines e (given) ; the rule book and the stack s of paper , bein g
just piece s of paper , do not understan d Chinese ; therefor e there is no understandin g of Chines e
I ifts goin g on. Hence,  According  to Searle,  running  the right  program  does  not necessarily  generate
'"­*=•• understanding.
Like Turing , Searl e considere d and attempte d to rebuf f a numbe r of replies  to his argument .
First, we will conside r the so­calle d Robo t Repl y (due to Jerry Fodo r (1980 ) amon g others) , whic h
turns out to be a red herring , althoug h an interestin g one. The Robo t Repl y is that althoug h the
symbol s manipulate d by the Chines e Roo m may not have real meanin g to the room itself (e.g.,
nothin g in the room has any experienc e of acupuncture , with respec t to whic h the symbo l for it
migh t have any meaning) , a fully equippe d robo t woul d not be subjec t to the same limitations .
Its interna l symbol s woul d have meanin g to it by virtu e of its direc t experienc e of the world .
Searle' s repl y is to put the Chines e Roo m insid e the robot' s "head" : the sensor s are redesigne d
to generat e Chines e symbol s instea d of stream s of bits, and the effector s redesigne d to accep t
Chines e symbol s as contro l inputs . The n we are back wher e we started . The Robo t Repl y is a
red herrin g becaus e the causa l semantic s of the symbol s is not the real issue . Eve n the origina l
Chines e Room need s some causa l semantics , in orde r to be able to answe r question s such as "How
many question s have I aske d so far? " Conversely , the output s of huma n sensors , for example ,
along the opti c nerv e or the auditor y nerve , migh t as well be in Chines e (see the earlie r discussio n
of wide and narro w content) . Not even Searl e woul d argu e that connectin g artificia l sensor s to
these nerve s woul d remov e consciousnes s from the brain involved.12
Severa l commentators , includin g Joh n McCarth y and Rober t Wilensky , propos e wha t
Searl e calls the System s Reply . This gets to the point . The objectio n is that althoug h one can ask
if the huma n in the room understand s Chinese , this is analogou s to askin g if the CPU can take
cube roots . In both cases , the answe r is no, and in both cases , accordin g to the System s Reply ,
the entir e syste m does  have the capacit y in question . Certainly , if one asks the Chines e Roo m
whethe r it understand s Chinese , the answe r woul d be affirmativ e (in fluen t Chinese) . Searle' s
respons e is to reiterat e the poin t that the understandin g is not in the human , and canno t be in
the paper , so there canno t be any understanding . He furthe r suggest s that one could imagin e the
huma n memorizin g the rule book and the content s of all the stack s of paper , so that there woul d
1' The fact that the stack s of pape r migh t well be large r than the entir e planet , and the generatio n of answer s woul d take
million s of years , has no bearin g on the logical  structur e of the argument . One aim of philosophica l trainin g is to develo p
a finel y hone d sens e of whic h objection s are german e and whic h are not.
12 Thi s is a good thing , becaus e artificia l inne r ears are at the prototyp e stage  (Watson , 1991) , and artificia l retinas , with
associate d imag e processing , are rapidl y becomin g feasibl e (Campbel l et al., 1991) .
Sectio n 26.4 . Intentionalit y and Consciousnes s 833
EMERGEN T
PROPERT Ybe nothin g to have understandin g except  the human ; and again , whe n one asks the huma n (in
English) , the reply will be in the negative .
Now we are dow n to the real issues . The shift from pape r to memorizatio n is a red herring ,
becaus e both form s are simpl y physica l instantiation s of a runnin g program . The real claim made
by Searl e has the followin g form :
1. Certai n kind s of object s are incapabl e of consciou s understandin g (of Chinese) .
2. The human , paper , and rule book are object s of this kind .
3. If each of a set of object s is incapabl e of consciou s understanding , then any syste m
constructe d from the objects  is incapabl e of consciou s understandin g
4. Therefor e there is no consciou s understandin g in the Chines e room .
Whil e the first two steps are on firm ground,13 the third is not. Searl e just assume s it is true
withou t givin g any suppor t for it. But notic e that if you do believ e it, and if you believ e that
human s are compose d of molecules , then eithe r you mus t believ e that human s are incapabl e of
consciou s understanding , or you mus t believ e that individua l molecule s are capable .
It is importan t to see that the rebutta l of Searle' s argumen t lies in rejectin g the third step,
and not in makin g any claim s abou t the room . You can believ e that the room is not consciou s
(or you can be undecide d abou t the room' s consciousness ) and still legitimatel y rejec t Searle' s
argumen t as invalid .
Searle' s (1992 ) more recen t position , describe d in his book The Rediscovery  of the Mind,
is that consciousnes s is an emergen t propert y of appropriatel y arrange d system s of neuron s
in the sam e way that solidit y is an emergen t propert y of appropriatel y arrange d collection s of
molecules , none of whic h are solid by themselves .
Now mos t supporter s of stron g AI woul d also say that consciousnes s is an emergen t
propert y of system s of neuron s (or electroni c components , or whatever) . The questio n is, whic h
propertie s of neuron s are essential  to consciousness , and whic h are merel y incidental ? In a
solid, wha t count s are the force s that molecule s exert on each other , and the way in whic h thos e
force s chang e with distance . The solid woul d still be solid if we replace d each molecul e with
a tiny compute r connecte d to electromagneti c forc e field generators . As yet, we do not know
whic h propertie s of neuron s are important—th e functiona l propertie s associate d with informatio n
processin g or the intrinsi c propertie s of the biologica l molecules . The Chines e Room argumen t
therefor e can be reduce d to the empirica l claim that the only  physical  medium  that can support
consciousness  is the neural medium.  The only empirica l evidenc e for this empirica l claim is that
other medi a do not resembl e neuron s in their intrinsi c physica l properties . Searl e (1992 ) admit s
that it is possible  that othe r media , includin g silicon , migh t suppor t consciousness , but he woul d
claim that in such cases , the syste m woul d be consciou s by virtue  of the physical  properties  of
the medium  and not by virtu e of the progra m it was running .
To reiterate , the aim of the Chines e Roo m argumen t is to refut e stron g AI—th e claim that
runnin g the right sort of progra m necessaril y generate s consciousness . It does this by exhibitin g
an apparentl y intelligen t syste m runnin g the righ t sort of progra m that is, accordin g to Searle ,
demonstrably  unconscious . He tried to demonstrat e this with the argumen t that unconsciou s parts
13 Searl e neve r explicitl y says wha t kind s of object s are incapabl e of consciousness . Book s and paper s for sure , but he
want s us to generaliz e this to computer s but not brain s withou t sayin g exactl y wha t the generalizatio n is.
834 __ _ Chapte r 26 . Philosophica l Foundation s
canno t lead to a consciou s whole , an argumen t that we have tried to show is invalid . Havin g
failed to prove  that the room is unconscious , Searl e then appeal s to intuition : just look at the
room ; what' s there to be conscious ? Whil e this approac h gain s some supporters , intuition s can
be misleading . It is by no mean s intuitiv e that a hun k of brain can suppor t consciousnes s whil e
an equall y larg e hunk of liver cannot . Furthermore , whe n Searl e admit s that material s othe r than
neuron s coul d in principl e suppor t consciousness , he weaken s his argumen t even further , for two
reasons : first , one has only Searle' s intuition s (or one' s own ) to say that the Chines e room is not
conscious , and second , even if we decid e the room is not conscious , that tells us nothin g abou t
whethe r a progra m runnin g on som e othe r physica l medi a migh t be conscious .
Searl e describe s his positio n as "biologica l naturalism. " The physica l natur e of the syste m
is important , and not its computationa l description . He even goes so far as to allow the logica l
possibilit y that the brain is actuall y implementin g an AI progra m of the traditiona l sort. But even
if that progra m turne d out to be an exac t copy of some existin g AI program , movin g it to a differen t
machin e woul d destro y consciousness . The distinctio n betwee n the intrinsi c propertie s (thos e
inheren t in its specifi c physica l makeup ) and functiona l propertie s (the input/outpu t specification )
of a neuro n is thus crucial .
One way to get at the distinctio n betwee n intrinsi c and functiona l propertie s is to look at
other artifacts . In 1848 , artificia l urea was synthesize d for the first time , by Wohler . Thi s was
importan t becaus e it prove d that organi c and inorgani c chemistr y coul d be united , a questio n that
had been hotl y debated . Onc e the synthesi s was accomplished , chemist s agree d that artificia l
urea was urea , becaus e it had all the righ t physica l properties . Similarly , artificia l sweetener s
are undeniabl y sweeteners , and artificia l inseminatio n (the othe r AI) is undeniabl y insemination .
On the othe r hand , artificia l flower s are not flowers , and Danie l Dennet t point s out that artificia l
Chatea u Latou r wine woul d not be Chatea u Latou r wine , even if it was chemicall y indistinguish ­
able, simpl y becaus e it was not mad e in the righ t plac e in the righ t way . Similarly , an artificia l
Picass o paintin g is not a Picass o painting , no matte r wha t it look s like.
Searl e is intereste d in the notio n of simulations  as wel l as in artifacts . He claim s that
AI program s can at best be simulation s of intelligence , and such simulation s impl y no intrinsi c
properties . The followin g quot e is representative :
No one suppose s that a compute r simulatio n of a storm will leav e us all wet.. . Why on earth
woul d anyon e in his righ t mind suppos e a compute r simulatio n of menta l processe s actuall y
had menta l processes ? (Searle , 1980 , pp. 37­38 )
Whil e it is easy to agre e that compute r simulation s of storm s do not mak e us wet, it is not
clear how to carry this analog y over to compute r simulation s of menta l processes . Afte r all, a
Hollywoo d simulatio n of a storm usin g sprinkler s and win d machine s does  mak e the actor s wet.
A compute r simulatio n of multiplicatio n does  resul t in a product—i n fact a compute r simulatio n
of multiplicatio n is multiplication . Searl e achieve s his rhetorica l effec t by choosin g example s
carefully . It woul d not have been as convincin g to say "Wh y on earth woul d anyon e in his right
mind suppos e that a compute r simulatio n of a vide o gam e actuall y is a game?, " but it woul d have
the sam e logica l forc e as his origina l quote .
To help decid e whethe r intelligenc e is mor e like Chatea u Latou r and Picass o or more like
urea and multiplication , we turn to anothe r though t experiment .
Sectio n 26.4 . Intentionalit y and Consciousnes s 83 5
The Brai n Prosthesi s Experimen t
The Brai n Prosthesi s Experimen t was touche d on by Searl e (1980) , but is mos t commonl y
associate d with the work of Hans Morave c (1988) . It goes like this. Suppos e we have develope d
neurophysiolog y to the poin t wher e the input/outpu t behavio r and connectivit y of all the neuron s
in the brai n are perfectl y understood . Furthermore , suppos e that we can buil d microscopi c
electroni c device s that mimi c this behavio r and can be smoothl y interface d to neura l tissue .
Lastly , suppos e that som e miraculou s surgica l techniqu e can replac e individua l neuron s with
the correspondin g electroni c device s withou t interruptin g the operatio n of the brain as a whole .
The experimen t consist s of graduall y replacin g all the neuron s with electroni c devices , and then
reversin g the proces s to retur n the subjec t to his or her norma l biologica l state .
We are concerne d with both the externa l behavio r and the interna l experienc e of the subject ,
durin g and after the operation . By the definitio n of the experiment , the subject' s externa l behavio r
must remai n unchange d compare d to wha t woul d be observe d if the operatio n were not carrie d
out.14 Now althoug h the presenc e or absenc e of consciousnes s canno t be easil y ascertaine d by
a third party , the subjec t of the experimen t ough t at least to be able to recor d any change s in
his or her own consciou s experience . Apparently , there is a direc t clash of intuition s as to wha t
woul d happen . Moravec , a robotic s researcher , is convince d his consciousnes s woul d remai n
unaffected . He adopt s the functionalis t viewpoint , accordin g to whic h the input/outpu t behavio r
of neuron s is their only significan t property . Searle , on the othe r hand , is equall y convince d his
consciousnes s woul d vanish :
You find , to your total amazement , that you are indee d losin g contro l of your externa l behavior .
You find, for example , that when doctor s test your vision , you hear them say "We are holdin g
up a red objec t in fron t of you; pleas e tell us wha t you see." You wan t to cry out "I can' t see
anything . I'm goin g totall y blind. " But you hear your voice sayin g in a way that is completel y
out of your control , "I see a red objec t in fron t of me." ... [Y]ou r consciou s experienc e slowl y
shrink s to nothing , whil e your externall y observabl e behavio r remain s the same . (Searle , 1992 )
But one can do more than argu e from intuition . First , note that in orde r for the externa l behavio r
to remai n the same whil e the subjec t graduall y become s unconscious , it mus t be the case that the
subject' s volitio n is remove d instantaneousl y and totally , otherwis e the shrinkin g of awarenes s
woul d be reflecte d in external  behavior—"Help , I'm shrinking! " or word s to that effect . Thi s
instantaneou s remova l of volitio n as a resul t of gradua l neuron­at­a­tim e replacemen t seem s an
unlikel y claim to have to make .
Second , conside r wha t happen s if we do ask the subjec t question s concernin g his or her
consciou s experienc e durin g the perio d whe n no real neuron s remain . By the condition s of the
experiment , we will get response s such as "I feel fine . I mus t say I'm a bit surprise d becaus e I
believe d the Chines e Roo m argument. " Or we migh t poke the subjec t with a pointe d stick , and
observ e the response , "Ouch , that hurt. " Now , in the norma l cours e of affairs , the scepti c can
dismis s such output s from AI program s as mere contrivances . Certainly , it is easy enoug h to use
a rule such as "If senso r 12 read s 'High ' then prin t 'Ouch. ' " But the poin t here is that becaus e we
have replicate d the functiona l propertie s of a norma l huma n brain , we assum e that the electroni c
brain contain s no such contrivances . The n we mus t have an explanatio n of the manifestation s of
14 One can imagin e usin g an identica l "control " subjec t who is give n a placeb o operation , so that the two behavior s can
be compared .
836 Chapte r 26 . Philosophica l Foundation s
"consciousness " produce d by the electroni c brai n that appeal s only to the functiona l propertie s
of the neurons . And  this explanation  must also  apply to  the real  brain,  which  has the same
functional  properties.  Ther e are, it seems , only two possibl e conclusions :
1. The causa l mechanism s involve d in consciousnes s that generat e thes e kind s of output s in
norma l brain s are still operatin g in the electroni c version , whic h is therefor e "conscious. "
2. The consciou s menta l event s in the norma l brain have no causa l connectio n to the subject' s
behavior , and are missin g from the electroni c brain .
Althoug h we canno t rule out the secon d possibility , it reduce s consciousnes s to what philosopher s
EPIPHENOMENA L cal l an epiphenomena l role—somethin g that happen s but casts no shadow , as it were , on the
observabl e world . Furthermore , if consciousnes s is indee d epiphenomenal , then the brain mus t
contai n a second , unconsciou s mechanis m that is responsibl e for the "Ouch. "
Third , conside r the situatio n after the operatio n has been reverse d and the subjec t has a
norma l brain . Onc e again , the subject' s external  behavio r mus t be as if the operatio n had not
occurred . In particular , we shoul d be able to ask, "Wha t was it like durin g the operation ? Do
you remembe r the pointe d stick? " The subjec t mus t have accurat e memorie s of the actua l natur e
of his or her consciou s experiences , includin g the qualia , despit e the fact that accordin g to Searl e
there were no such experiences .
Searl e migh t reply that we have not specifie d the experimenta l condition s properly . If the
real neuron s are, say, put into suspende d animatio n betwee n the time they are extracte d and the
time they are replace d in the brain , then of cours e they will not "remember " the experience s
durin g the operation . To deal with this, we simpl y need to mak e sure that the neurons ' state is
updated , by som e means , to reflec t the interna l state of the artificia l neuron s they are replacing .
If the suppose d "nonfunctional " aspect s of the real neuron s then resul t in functionall y differen t
behavio r from that observe d with artificia l neuron s still in place , then we have a simpl e reductio
ad absurdum,  becaus e that woul d mean that the artificia l neuron s are not functionall y equivalen t
to the real neurons . (Exercis e 26.4 addresse s a rebutta l to this argument. )
Patrici a Churchlan d (1986 ) point s out that the functionalis t argument s that operat e at the
level of the neuro n can also operat e at the level of any large r functiona l unit— a clum p of neurons ,
a menta l module , a lobe,  a hemisphere , or the whol e brain . Tha t mean s that if you accep t that the
brain prosthesi s experimen t show s that the replacemen t brain is conscious , then you shoul d also
believ e that consciousnes s is maintaine d whe n the entir e brain is replace d by a circui t that map s
from input s to output s via a huge looku p table . This is disconcertin g to man y peopl e (includin g
Turin g himself ) who have the intuitio n that looku p table s are not conscious .
Discussio n
We have seen that the subjec t of consciousnes s is problematic . Simpl e intuition s seem to lead
to conflictin g answer s if we propos e differen t experimenta l situations . Bu t wha t is clea r is
that if there is an empirica l questio n concernin g the presenc e or absenc e of consciousnes s in
appropriatel y programme d computers , then like any empirica l questio n it can only be settle d by
experiment . Unfortunately , it is not clear wha t sort of experimen t coul d settl e the question , nor
what  sort of scientifi c theor y coul d explai n the results . Scientifi c theorie s are designe d to accoun t
for objectiv e phenomena ; in fact Poppe r (1962 ) has characterize d all physica l laws as theorie s
Sectio n 26.5 . Summar y 837
that, ultimately , allow one to conclud e the existenc e of particle s in certai n space­tim e locations .
How woul d we view a theor y that allowe d one to infer pains , for example , from premise s of the
ordinar y physica l kind ? It seem s that it woul d be at best a description  ("Whe n a voltag e is applie d
to such­and­suc h neuron , the subjec t will feel pain" ) rathe r than an explanation.  An explanator y
theor y should , amon g othe r things , be able to explai n why it is pain  that the subjec t experience s
when a give n neuro n is stimulated , rathe r than , say, the smel l of baco n sandwiches .
It is also hard to imagin e how a bette r understandin g of neurophysiolog y coul d help .
Suppose , for example , that (1) we coul d train a subjec t to recor d all his or her consciou s
thought s withou t interruptin g them too much ; (2) neuroscientist s discovere d a syste m of neuron s
whos e activit y pattern s coul d be decode d and understood;15 and (3) the decode d activit y pattern s
corresponde d exactl y to the recorde d consciou s thoughts . Althoug h we migh t claim to have
locate d the "seat of consciousness, " it seem s we woul d still be no bette r off in our understandin g
of why these pattern s of activit y actuall y constitut e consciousness .
The proble m seem s to be that consciousness , as we currentl y (fail to) understan d it, is not
understandabl e by the norma l mean s availabl e to science .
No one can rule out a majo r intellectua l revolutio n that woul d give us a new—an d at presen t
unimaginable—concep t of reduction , accordin g to whic h consciousnes s woul d be reducible .
(Searle , 1992 , p. 124) .
If consciousnes s is indee d irreducible , that woul d sugges t that there can be no explanation s in
nonsubjectiv e term s of why red is the sort of sensatio n it is and not some othe r sort, or why pain
is like pain and not like the smel l of baco n sandwiches .
One fina l (but not necessaril y conclusive ) argumen t can be mad e concernin g the evolu ­
tionar y origi n of consciousness . Bot h side s of the debat e agre e that simpl e animal s containin g
only a few neuron s do not posses s consciousness . In such animals , neuron s fulfil l a purel y
functiona l role by allowin g simpl e adaptiv e and discriminativ e behaviors . Yet the basic desig n
and constructio n of neuron s in primitiv e animal s is almos t identica l to the desig n of neuron s in
highe r animals . Give n this observation , the proponen t of consciousnes s as an intrinsi c neura l
phenomeno n mus t argu e that neurons , whic h evidentl y evolve d for purel y functiona l purposes ,
just happe n by chanc e to have exactl y the propertie s require d to generat e consciousness.  The
functionalist , on the othe r hand , can argu e that consciousnes s necessaril y emerge s whe n system s
reach the kind of functiona l complexit y neede d to sustai n comple x behavior .
26.5 SUMMAR Y
We have presente d som e of the main philosophica l issue s in AI. Thes e were divide d into ques ­
tions concernin g its technica l feasibilit y (wea k AI), and question s concernin g its relevanc e and
explanator y powe r with respec t to the min d (stron g AI). We concluded , althoug h by no mean s
conclusively , that the argument s agains t weak AI are needlessl y pessimisti c and have ofte n mis­
characterize d the conten t of AI theories . Argument s agains t stron g AI are inconclusive ; althoug h
15 It is not clear if this reall y make s sense , but the idea is to gran t neuroscienc e as muc h succes s as we can imagine .
838 Chapte r 26 . Philosophica l Foundation s
they fail to prov e its impossibility , it is equall y difficul t to prov e its correctness . Fortunately , few
mainstrea m AI researchers , if any, believ e that anythin g significan t hinge s on the outcom e of the
debat e give n the field' s presen t stage of development . Eve n Searl e himsel f recommend s that his
argument s not stand in the way of continue d researc h on AI as traditionall y conceived .
Like geneti c engineerin g and nuclea r power , artificia l intelligenc e has its fair share of critic s
concerne d abou t its possibl e misuses . We will discus s thes e concern s in the next chapter . But
unlik e othe r fields , artificia l intelligenc e has generate d a thrivin g industr y devote d to provin g its
impossibility . This secon d batc h of critic s has raise d som e importan t issue s concernin g the basic
aims, claims , and assumption s of the field . Ever y AI scientis t and practitione r shoul d be awar e of
these issues , becaus e they directl y affec t the socia l milie u in whic h AI researc h is carrie d out and
in whic h AI technique s are applied . Althoug h genera l societa l awarenes s shoul d be part of the
educatio n of every scientist , it is especiall y importan t for AI becaus e the natur e of the field seem s
to arous e scepticis m and even hostilit y in a significan t portio n of the population . Ther e persist s
an incredibl e degre e of misunderstandin g of the basic claim s and conten t of the field , and, like it
or not, these misunderstanding s have a significan t effec t on the field itself .
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
The natur e of the min d has been a standar d topi c of philosophica l theorizin g from ancien t time s
to the present . In the Phaedo,  Plat o specificall y considere d and rejecte d the idea that the min d
could be an "attunement " or patter n of organizatio n of the part s of the body , a viewpoin t that
approximate s the functionalis t viewpoin t in moder n philosoph y of mind . He decide d instea d
that the min d had to be an immortal , immateria l soul , separabl e from the body and differen t in
substance—th e viewpoin t of dualism . Aristotl e distinguishe d a variet y of soul s (Gree k ^v\'i])
in livin g things , som e of which , at least , he describe d in a functionalis t manner . (Se e Nuss ­
baum (1978 ) for mor e on Aristotle' s functionalism. ) Aristotl e also conceive d of deliberatio n
abou t wha t actio n to take in a way reminiscen t of the moder n situate d agen t approac h (the last
part of this extrac t also appear s on the cove r of the book) :
But how does it happe n that thinkin g is sometime s accompanie d by actio n and sometime s not,
sometime s by motion , and sometime s not? It look s as if almos t the sam e thin g happen s as in
the case of reasonin g and makin g inference s abou t unchangin g objects . But in that case the
end is a speculativ e propositio n ... wherea s here the the conclusio n whic h result s from the
two premise s is an action . ... I need covering ; a cloa k is a covering . I need a cloak . Wha t I
need, I have to make ; I need a cloak . I have to mak e a cloak . And the conclusion , the "I have
to mak e a cloak, " is an action . (Nussbaum , 1978 , p. 40)
Descarte s is notoriou s for his dualisti c view of the huma n mind , but ironicall y his historica l influ ­
ence was towar d mechanis m and physicalism . He explicitl y conceive d of animal s as automata ,
and his spirite d defens e of this viewpoin t actuall y had the effec t of makin g it easie r to conceiv e of
human s as automat a as well , even thoug h he himsel f did not take this step . The book L'Homme
Machine  or Man  a Machine  (La Mettrie , 1748 ) did explicitl y argu e that human s are automata .
Sectio n 26.5 . Summar y 83 9
Twentieth­centur y analyti c philosoph y has typicall y accepte d physicalis m (ofte n in the
form of the brain­stat e "identit y theory " (Place , 1956 ; Armstrong , 1968) , whic h assert s that
menta l state s are identica l with brain states) , but has been muc h more divide d on functionalism ,
the machin e analog y for the huma n mind , and the questio n of whethe r machine s can literall y
think . A numbe r of early philosophica l response s to Turing' s (1950 ) "Computin g Machiner y
and Intelligence, " for example , Scrive n (1953) , attempte d to deny that it was even meaningful  to
say that machine s coul d think , on the groun d that such an assertio n violate d the meaning s of the
relevan t terms . Scriven , at least, had retracte d this view by 1963 ; see his addendu m to a reprin t of
his articl e (Anderson , 1964) . In general , later philosophica l response s to AI have at least grante d
the meaningfulnes s of the question , althoug h some migh t answe r it vehementl y in the negative .
Followin g the classificatio n used by Bloc k (1980) , we can distinguis h varietie s of func ­
tionalism . Functional  specification  theory  (Lewis , 1966 ; Lewis , 1980 ) is a varian t of brain­stat e
identit y theor y that select s the brain state s that are to be identifie d with menta l state s on the basis
of their functiona l role. Functional  state  identity  theory  (Putnam , 1960 ; Putnam , 1967 ) is more
closel y base d on a machin e analogy . It identifie s menta l state s not with physical  brain state s but
with abstrac t computationa l state s of the brain conceive d expressl y as a computin g device . Thes e
abstrac t state s are suppose d to be independen t of the specifi c physica l compositio n of the brain ,
leadin g som e to charg e that functiona l state identit y theor y is a form of dualism !
Both the brain­stat e identit y theor y and the variou s form s of functionalis m have com e
under attac k from author s who claim that they do not accoun t for the qualia  or "wha t it's
like" aspec t of menta l state s (Nagel , 1974) . Searl e has focuse d instea d on the allege d inabilit y of
functionalis m to accoun t for intentionalit y (Searle , 1980 ; Searle , 1984 ; Searle , 1992) . Churchlan d
and Churchlan d (1982 ) rebu t both these type s of criticism .
Functionalis m is the philosoph y of mind most naturall y suggeste d by AI, and critique s of
functionalis m often take the form of critique s of AI (as in the case of Searle) . Othe r critic s of AI,
most notabl y Dreyfus , have focuse d specificall y on the assumption s and researc h method s of AI
itself , rathe r than its genera l philosophica l implications . Even philosopher s who are functionalist s
are not alway s sanguin e abou t the prospect s of AI as a practica l enterpris e (Fodor , 1983) . Despit e
Searle' s "strong'V'weak " terminology , it is possibl e for a philosophe r to believ e that huma n
intellectua l capabilitie s coul d in principl e be duplicate d by duplicatin g their functiona l structur e
alone (and thus to suppor t "stron g AI") whil e also believin g that as a practica l matte r neithe r GOFA I
nor any othe r huma n endeavo r is likel y to discove r that functiona l structur e in the foreseeabl e
futur e (and thus to oppos e "wea k AI") . In fact, this seem s to be a relativel y commo n viewpoin t
amon g philosophers .
Not all philosopher s are critica l of GOFAI , however ; som e are, in fact, arden t advocate s
and even practitioners . Zeno n Pylyshy n (1984 ) has argue d that cognitio n can best be understoo d
throug h a computationa l model , not only in principl e but also as a way of conductin g researc h
at present , and has specificall y rebutte d Dreyfus' s criticism s of the computationa l mode l of
huma n cognitio n (Pylyshyn , 1974) . Gilber t Harma n (1983) , in analyzin g belie f revision , make s
connection s with AI researc h on truth maintenanc e systems . Michae l Bratma n has applie d
his "belief­desire­intention " mode l of huma n psycholog y (Bratman , 1987 ) to AI researc h on
plannin g (Bratman , 1992) . At the extrem e end of stron g AI, Aaro n Sloma n (1978 , p. xiii) has
even describe d as "racialist " Josep h Weizenbaum' s view (Weizenbaum , 1976 ) that hypothetica l
intelligen t machine s shoul d not be regarde d as persons .
840 Chapte r 26 . Philosophica l Foundation s
ELIMINATIV E
MATERIALIS M Eliminativ e materialis m (Rorty , 1965 ; Churchland , 1979 ) differ s from all othe r prominen t
theorie s in the philosoph y of mind , in that it does not attemp t to give an accoun t of why our
"folk psychology " or commonsens e idea s abou t the min d are true, but instea d reject s them as
false and attempt s to replac e them with a purel y scientifi c theor y of the mind . In principle ,
this scientifi c theor y coul d be give n by classica l AI, but in practice , eliminativ e materialist s
tend to lean on neuroscienc e and neura l networ k researc h instea d (Churchland , 1986) , on the
ground s that classica l AI, especiall y "knowledg e representation " researc h of the kind describe d
in Chapte r 8, tend s to rely on the truth of folk psychology . Althoug h the "intentiona l stance "
viewpoin t (Dennett , 1971 ) coul d be interprete d as functionalist , it shoul d probabl y instea d be
regarde d as a form of eliminativ e materialism , in that takin g the "intentiona l stance " is not
suppose d to reflec t any objectiv e propert y of the agen t towar d who m the stanc e is taken . It shoul d
also be note d that it is possibl e to be an eliminativ e materialis t abou t som e aspect s of mentalit y
while analyzin g other s in some othe r way. For instance , Dennet t (1978a ) is muc h more strongl y
eliminativis t abou t quali a than abou t intentionality .
The Encyclopedia  of Philosophy  (1967 ) is an impressivel y authoritativ e source . Genera l
collection s of article s on philosoph y of mind , includin g functionalis m and othe r viewpoint s
relate d to AI, are Materialism  and the Mind­Body  Problem  (Rosenthal , 1971 ) and Readings
in the Philosophy  of Psychology,  volum e 1 (Block , 1980) . Bir o and Shaha n (1982 ) presen t
a collectio n devote d to the pros and cons of functionalism . Anthologie s of article s dealin g
more specificall y with the relatio n betwee n philosoph y and AI includ e Minds and  Machines
(Anderson , 1964) , Philosophical  Perspectives  in Artificial  Intelligence  (Ringle , 1979) , Mind
Design  (Haugeland , 1981) , and The Philosophy of Artificial  Intelligence  (EoA&n,  1990) . Ther e are
severa l genera l introduction s to the philosophica l "AI question " (Boden , 1977 ; Haugeland , 1985 ;
Copeland , 1993) . The  Behavioral and  Brain  Sciences,  abbreviate d BBS,  is a majo r journa l
devote d to philosophica l question s (and high­level , abstrac t scientifi c questions ) abou t AI and
neuroscience . A BBS  articl e include s occasionall y amusin g peer commentar y from a larg e
numbe r of critic s and a rebutta l by the autho r of the main article .
EXERCISE S
26.1 Go throug h Turing' s list of allege d "disabilities " of machines , identifyin g whic h have
been show n to be achievable , whic h are achievabl e in principl e by a program , and whic h are still
problemati c becaus e they requir e consciou s menta l states .
26.2 Doe s a refutatio n of the Chines e Roo m argumen t necessaril y prov e that appropriatel y
programme d computer s are conscious ?
26.3 Suppos e we ask the Chines e Room to prov e that John Searl e is not a consciou s being . Afte r
a while , it come s up with a learne d pape r that look s remarkabl y like Searle' s paper , but switche s
"computer " and "human " throughout , alon g with all the correspondin g terms . The claim woul d
be that if Searle' s argumen t is a refutatio n of the possibilit y of consciou s machines , then the
Chines e Room' s argumen t is a refutatio n of the possibilit y of consciou s humans . Then , provide d
Sectio n 26.5 . Summar y 841
we agre e that human s are conscious , this refute s Searle' s argumen t by reductio  ad absurdum.  Is
this a soun d argument ? Wha t migh t Searle' s respons e be?
26.4 In the Brai n Prosthesi s argument , it is importan t to be able to restor e the subject' s brain
to normal , such that its externa l behavio r is as it woul d have been if the operatio n had not taken
place . Can the scepti c reasonabl y objec t that this woul d requir e updatin g those neurophysiologica l
propertie s of the neuron s relatin g to consciou s experience , as distinc t from thos e involve d in the
functiona l behavio r of the neurons ?
26.5 Fin d and analyze  an accoun t in the popula r medi a of one or more of the argument s to the
effec t that AI is impossible .
26.6 Unde r the correspondenc e theory , wha t kind s of proposition s can be represente d by a
logica l agent ? A refle x (condition­action ) agent ?
26.7 Attemp t to writ e definition s of the term s "intelligence, " "thinking, " and "consciousness. "
Sugges t some possibl e objection s to your definitions .
r\ r­i AI : PRESEN T AND
^ / FUTUR E
In which  we take stock of  where  we are and where  we are going,  this being  a good
thing  to do before continuing.
27.1 HAV E WE SUCCEEDE D YET ?
In Part I, we propose d a unifie d view of AI as intelligen t agen t design . We showe d that the
desig n proble m depend s on the percept s and action s availabl e to the agent , the goal s that the
agent' s behavio r shoul d satisfy , and the natur e of the environment . A variet y of differen t agen t
design s are possible , rangin g from refle x agent s to fully deliberative , knowledge­base d agents .
Moreover , the component s of these design s can have a numbe r of differen t instantiations—fo r
example , logical,  probabilistic , or "neural. " The intervenin g chapter s presente d the principle s by
whic h these component s operate .
In area s such as gam e playing , logica l inferenc e and theore m proving , planning , and
medica l diagnosis , we have seen system s base d on rigorou s theoretica l principle s that can perfor m
as well as, or bette r than , huma n experts . In othe r areas , such as learning , vision , robotics , and
natura l languag e understanding , rapid improvement s in performanc e are occurrin g throug h the
applicatio n of better analytica l method s and improvement s in our understandin g of the underlyin g
problems . Continue d researc h will bear fruit in the form of bette r capabilitie s in all of these areas .
Enthralle d by the technica l details , however , one can sometime s lose sigh t of the big
picture . We need an antidot e for this tendency . We will consider  therefore  whether  we have  the
tools  with  which  to build  a complete,  general­purpose  intelligent  agent.  Thi s will also help to
revea l a numbe r of gaps in our curren t understanding .
Let us begi n with the questio n of the agen t architecture . We discusse d som e genera l prin ­
ciple s and type s in Chapte r 2, and som e specifi c architecture s in Chapte r 25. One key aspec t of
a genera l architectur e is the abilit y to incorporat e a variet y of type s of decisio n making , rangin g
from knowledge­base d deliberatio n to refle x responses . Refle x response s are neede d for situa ­
tions in whic h time is of the essence , wherea s knowledge­base d deliberatio n allows  the agen t to
842
Sectio n 27.1 . Hav e We Succeede d Yet? 843
REAL­TIM E Altake into accoun t mor e information , to plan ahead , to handl e situation s in whic h no immediat e
respons e is available , and to produc e bette r response s tailore d specificall y for the curren t situa ­
tion. Compilatio n processe s such as explanation­base d learnin g (Chapte r 21) continuall y conver t
declarativ e informatio n at the deliberativ e leve l into mor e efficien t representations , eventuall y
reachin g the refle x leve l (Figur e 27.1) . Architecture s such as SOA R (Lair d et al, 1987 ) and
THE O (Mitchell , 1990 ) have exactl y this structure . Ever y time they solv e a proble m by explici t
deliberation , they save awa y a generalize d versio n of the solutio n for use by the refle x component .
Knowledge­base d
deliberatio n
Figur e 27.1 Compilatio n serve s to conver t deliberativ e decisio n makin g into mor e efficient ,
reflexiv e mechanisms .
As we saw in Part V, uncertaint y is an inevitabl e proble m in the real world . We also
saw how uncertai n knowledg e is one possibl e respons e to the fact that exactl y correc t rule s in
realisti c environment s are usuall y very complex , and hence  unusable . Unfortunately , there are
clear gaps in our understandin g of how to incorporat e uncertai n reasonin g into general­purpos e
agent architectures . First , very little work has been done on compilatio n of uncertai n reasonin g
and decisio n making . Second , we need mor e expressiv e language s for uncertai n knowledge —
a first­orde r probabilisti c logic . Third , we need muc h bette r mechanism s for plannin g unde r
uncertainty . Curren t algorithms , such as polic y iteratio n (Chapte r 17), are reall y more analogou s
to the simpl e searc h algorithm s of Part II than to the powerfu l plannin g method s of Part IV. The
latter method s incorporat e partia l ordering , goal directedness , and abstractio n in orde r to handl e
very comple x domains . Al is only  just beginning  to come  to grips  with  the problem  of integrating
techniques  from  the logical  and probabilistic  worlds  (Hanks  et al., 1994).
Agent s in real environment s also need way s to contro l their own deliberations . They mus t
be able to cease deliberatio n when actio n is demanded , and they mus t be able to use the availabl e
time for deliberatio n to execut e the mos t profitabl e computations . For example , a taxi­drivin g
agen t that sees an acciden t ahea d shoul d decid e eithe r to brak e or to take avoidin g actio n in a
split secon d rathe r than in half an hour . It shoul d also spen d that split secon d thinkin g abou t the
most importan t questions , such as whethe r the lane s to the left and righ t are clea r and whethe r
there is a larg e truc k clos e behind , rathe r than worryin g abou t wea r and tear on the tires or
wher e to pick up the next passenger . Thes e issue s are usuall y studie d unde r the headin g of real­
time Al. As Al system s mov e into mor e comple x domains , all problem s will becom e real­tim e
becaus e the agen t will neve r have long enoug h to solv e the decisio n proble m exactl y (see also
Sectio n 27.2) . This issue cam e up in our discussio n of game­playin g system s in Chapte r 5, wher e
844 Chapte r 27 . AI : Presen t and Futur e
ANYTIM E
ALGORITHM S
DECISION ­
THEORETI C
METAREASONIN Gwe describe d alpha­bet a prunin g to eliminat e irrelevan t deliberation s and dept h limit s to ensur e
timel y play . Clearly , ther e is a pressin g need for method s that wor k in mor e genera l decision ­
makin g situations . Two promisin g technique s have emerge d in recen t years . The first involve s
the use of anytim e algorithm s (Dea n and Boddy , 1988) . An anytim e algorith m is an algorith m
whos e outpu t qualit y improve s graduall y over time , so that it has a reasonabl e decisio n read y
wheneve r it is interrupted . Such algorithm s are controlle d by a metaleve l decisio n procedur e that
decide s whethe r furthe r computatio n is worthwhile . Iterativ e deepenin g searc h in gam e playin g
provide s a simpl e exampl e of an anytim e algorithm . Mor e comple x systems , compose d of man y
such algorithm s workin g together , can also be constructe d (Zilberstein , 1993) .
The secon d techniqu e is decision­theoreti c metareasonin g (Russel l and Wefald , 1991) .
This metho d applie s the theor y of informatio n valu e (Chapte r 16) to selec t computations . The
value of a computatio n depend s on both its cost (in term s of delayin g action ) and its benefit s
(in term s of improve d decisio n quality) . Metareasonin g technique s can be used to desig n bette r
searc h algorithms , and automaticall y guarante e that the algorithm s have the anytim e property .
Metareasonin g is expensive , of course , and compilatio n method s can be applie d to generat e more
efficien t implementations . The applicatio n of anytim e and metareasonin g method s to genera l
decision­makin g architecture s has not yet been investigate d in any depth .
The final architectura l componen t for a genera l intelligen t agen t is the learnin g mechanism ,
or mechanisms . As the architectur e become s mor e complicated , the numbe r of niche s for
learnin g increases . As we saw in Part VI, however , the sam e method s for inductiv e learning ,
reinforcemen t learning , and compilatio n can be used for all of the learnin g task s in an agent .
The learnin g method s do depen d on the representation s chosen , of course . Method s for attribute ­
based logica l and neura l representation s are well understood , and method s for first­orde r logica l
representation s and probabilisti c representation s are catchin g up fast. As new representations ,
such as first­orde r probabilisti c logics , are developed , new learnin g algorithm s will have to be
develope d to accompan y them . We also will need to find a way to integrat e inductiv e method s into
the agen t architectur e in the same way that compilatio n method s are integrate d into architecture s
such as SOA R and THEO .
An agen t architectur e is an empt y shell withou t knowledg e to fill it. Som e have propose d
that the necessar y knowledg e can be acquire d throug h a trainin g process , startin g virtuall y
from scratch . To avoi d recapitulatin g the entir e intellectua l histor y of the huma n race , the
trainin g proces s also migh t includ e direc t instructio n and knowledg e acquisitio n from source s
such as encyclopedia s and televisio n programs . Althoug h such method s migh t avoi d the need
for knowledg e representatio n and ontologica l engineerin g work , they currentl y seem impractical .
For the foreseeabl e future , usefu l knowledge­base d system s will requir e som e work by huma n
knowledg e engineers . Robus t natura l languag e systems , for example , may requir e very broa d
knowledg e bases . Furthe r wor k on general­purpos e ontologies , as sketche d in Chapte r 8, is
clearl y needed . The CYC projec t (Lena t and Guha , 1990 ) is a brave  effor t in this direction , but
many open problem s remai n and we have little experienc e in using  large knowledg e bases .
To sum up: by lookin g at curren t system s and how they woul d need to be extended , we
can identif y a variet y of researc h question s whos e answer s woul d take us furthe r towar d general ­
purpos e intelligen t systems . This incrementa l approac h is useful , provide d we are fairl y confiden t
that we have a reasonabl e startin g point . In the next section , we look at the AI proble m from first
principle s to see whethe r this is in fact the case .
Sectio n 27.2 . Wha t Exactl y Are We Tryin g to Do? 845
27.2 WHA T EXACTL Y ARE WE TRYIN G TO Do?
PERFEC T
RATIONALIT Y
CALCULATIV E
RATIONALIT Y
BOUNDE D
OPTIMALIT YEven befor e the beginnin g of artificia l intelligence , philosophers , contro l theorists , and economist s
sough t a satisfactor y definitio n of rationa l action . Thi s is neede d to underpi n theorie s of ethics ,
inductiv e learning , reasoning , optima l control , decisio n making , and economi c modelling . It has
also been  our goal in  this book  to show  how to design  agents  that act rationally.  Initially , we
presente d no forma l definitio n of rationa l action . Late r chapter s presente d logica l and decision ­
theoreti c definition s togethe r with specifi c design s for achievin g them . The role of such definition s
in AI is clear : if we defin e som e desirabl e propert y P, then we ough t in principl e to be able to
desig n a syste m that provabl y possesse s propert y P. Theor y meet s practic e whe n our system s
exhibi t P in reality . Furthermore , that they exhibi t P in realit y shoul d be somethin g that we
actuall y care about . In a sense , the choic e of wha t P to stud y determine s the natur e of the field .
Therefore , we need to look carefull y at wha t exactl y we are tryin g to do.
There are a numbe r of possibl e choice s for P. Here are three :
<0> Perfec t rationality : the classica l notio n of rationalit y in decisio n theory . A perfectl y
rationa l agen t acts at every instan t in such a way as to maximiz e its expecte d utility , given
the informatio n it has acquire d from the environment . In Chapte r 1, we warne d that
achievin g perfec t rationality—alway s doin g the righ t thing—i s just not possibl e in com ­
plicate d environments . The computationa l demand s are just too high . However , for most
of the book , we will adop t the workin g hypothesi s that understandin g perfec t decisio n
makin g is a good plac e to start .
Becaus e perfec t rationa l agent s do not exist for nontrivia l environments , perfec t rationalit y
is not a suitabl e candidat e for P.
0 Calculativ e rationality : the notio n of rationalit y that we have used implicitl y in designin g
logica l and decision­theoreti c agents . A calculativel y rationa l agen t eventually  return s
what would  have  been  the rationa l choic e at the beginnin g of its deliberation . This is an
interestin g propert y for a syste m to exhibi t becaus e it constitute s an "in­principle " capacit y
to do the righ t thing . Calculativ e rationalit y is sometime s of limite d value , becaus e
the actua l behavio r exhibite d by such system s can be rathe r irrational . For example , a
calculativel y rationa l ches s progra m may choos e the right move , but may take 1050 time s
too long to do so. In practice , AI syste m designer s are force d to compromis e on decisio n
qualit y to obtai n reasonabl e overal l performance , yet the theoretica l basis of calculativ e
rationalit y does not provid e for such compromises .
<C> Bounde d optimalit y (BO) : a bounde d optima l agen t behave s as well as possibl e given  its
computational  resources.  Tha t is, the expecte d utilit y of the agen t progra m for a bounde d
optima l agen t is at least as high as the expecte d utilit y of any othe r agen t progra m runnin g
on the same machine .
Of these three possibilities , choosin g P to be "bounde d optimality " seem s to offe r the best hope
for a stron g theoretica l foundatio n for AI. Clearly , a BO agen t is of real, practica l interes t becaus e
its behavio r is the best that can be obtained . Equall y clearly , BO program s exis t for any give n
task, machine , and environment . Obviously , finding  the BO progra m is the trick ; AI as the stud y
846_______ _ _ _ Chapte r 27 . AI : Presen t and Futur e
of bounde d optimalit y is feasibl e in principle , but no one said it was easy ! One obviou s difficult y
is that the designe r may not have a probabilit y distributio n over the kind s of environment s in
whic h the agen t is expecte d to operate , and so may not be able to ascertai n the bounde d optimalit y
of a give n design . In such cases , however , a suitabl y designe d learnin g agen t shoul d be able
to adap t to the initiall y unknow n environment ; analytica l result s on the bounde d optimalit y of
learnin g agent s can be obtaine d usin g computationa l learnin g theor y (Chapte r 18). One crucia l
open questio n is to wha t exten t bounde d optima l system s can be composed  from bounde d optima l
subsystems , thereb y providin g a hierarchica l desig n methodology .
Althoug h bounde d optimalit y is a very genera l specification , it is no more genera l than the
definitio n of calculativ e rationalit y on whic h muc h of past AI work has been based . The importan t
thing is that by includin g resourc e constraint s from the beginning , question s of efficienc y and
decisio n qualit y can be handle d withi n the theor y rathe r than by ad hoc syste m design . The
two approache s only coincid e if BO agent s look something  like calculativel y rationa l agent s
with variou s efficienc y improvement s adde d on. Rea l environment s are/a r more comple x than
anythin g that can be handle d by a pure calculativel y rationa l agent , however , so this woul d quite
a radica l assumption .
As yet, little is know n abou t bounde d optimality . It is possibl e to construc t bounde d optima l
program s for very simpl e machine s and for somewha t restricte d kind s of environment s (Etzioni ,
1989; Russel l and Subramanian , 1993) , but as yet we have no idea wha t BO program s are like
for large , general­purpos e computer s in comple x environments . If there is to be a constructiv e
theor y of bounde d optimality , we have to hope that the desig n of bounde d optima l program s
does not depen d too strongl y on the detail s of the compute r bein g used . It woul d mak e scientifi c
researc h very difficul t if addin g a few kilobyte s of memor y to a machin e with 100 megabyte s
made a significan t differenc e to the desig n of the BO progra m and to its performanc e in the
environment . One way to mak e sure this canno t happe n is to be slightl y mor e relaxe d abou t
the criteri a for bounde d optimality . By analog y with asymptoti c complexit y (Appendi x A), we
BOUNDE D ca n defin e asymptoti c bounde d optimalit y (ABO ) as follow s (Russel l and Subramanian , 1993) .
Suppos e a progra m P is bounde d optima l for a machin e M in a clas s of environment s E (the
complexit y of environment s in E is unbounded) . The n progra m P' is ABO for M in E if it
can outperfor m P by runnin g on a machin e kM that is k time s faste r (or larger) . Unles s k were
enormous , we woul d be happ y with a progra m that was ABO for a nontrivia l environmen t on a
nontrivia l architecture . Ther e woul d be little poin t in puttin g enormou s effor t into findin g BO
rathe r than ABO programs , becaus e the size and spee d of availabl e machine s tend s to increas e
by a constan t facto r in a fixed amoun t of time anyway .
We can hazar d a gues s that BO or ABO program s for powerfu l computer s in comple x
environment s will not necessaril y have a simple , elegan t structure . We have alread y seen that
general­purpos e intelligenc e require s som e refle x capabilit y and som e deliberativ e capability ,
a variet y of form s of knowledg e and decisio n making , learnin g and compilatio n mechanism s
for all of thos e forms , method s for controllin g reasoning , and a large store of domain­specifi c
knowledge . A bounde d optima l agen t mus t adap t to the environmen t in whic h it find s itself , so
that eventuall y its interna l organizatio n may reflec t optimization s that are specifi c to the particula r
environment . This is only to be expected , and is simila r to the way in whic h racin g cars restricte d
by weigh t and horsepowe r hav e evolve d into extremel y comple x designs . We suspec t that a
scienc e of artificia l intelligenc e base d on bounde d optimalit y will involv e a good deal of stud y of
Sectio n 27.2 . Wha t Exactl y Are We Tryin g to Do? 847
ACT UTILITARIANIS M
RULE
UTILITARIANIS M
GAM E THEOR Y
PRISONER' S
DILEMM Athe processe s that allow an agen t progra m to converg e to bounde d optimality , and perhap s less
concentratio n on the detail s of the mess y program s that result .
In summary , the concep t of bounde d optimalit y is propose d as a forma l task for artificia l
intelligenc e researc h that is both well­define d and feasible . Bounde d optimalit y specifie s optima l
programs  rathe r than optima l actions.  Action s are, after all, generate d by programs , and it is over
program s that designer s have control .
This mov e from prescribin g action s to prescribin g program s is not uniqu e to AI. Philosoph y
has also seen a gradua l evolutio n in the definitio n of rationality . Ther e has been a shif t from
consideratio n of act utilitarianism —the rationalit y of individua l acts—t o rule utilitarianism ,
or the rationalit y of genera l policie s for acting . A philosophica l proposa l generall y consisten t
with the notio n of bounde d optimalit y can be foun d in the "Mora l First Aid Manual " (Dennett ,
1986) . Dennet t explicitl y discusse s the idea of reachin g equilibriu m withi n the spac e of feasibl e
configuration s of decisio n procedures . He uses as an exampl e the Ph.D . admission s procedur e
of a philosoph y department . He concludes , as do we, that the best configuratio n may be neithe r
elegan t nor illuminating . The existenc e of such a configuratio n and the proces s of reachin g it are
the main point s of interest .
Anothe r area to underg o the sam e transitio n is gam e theory , a branc h of economic s
initiate d in the sam e book— Theor y of Games  and  Economic Behavior  (Vo n Neuman n and
Morgenstern , 1944)—tha t bega n the widesprea d stud y of decisio n theory . Gam e theor y studie s
decisio n problem s in whic h the utilit y of a give n actio n depend s not only on chanc e event s in
the environmen t but also on the action s of othe r agents . The standar d scenari o involve s a set of
agents  who mak e their decision s simultaneously , withou t knowledg e of the decision s of the othe r
agents . The Prisoner' s Dilemm a is a famou s example , in whic h each of two crim e suspect s
can "collaborate " (refus e to implicat e his or her partner ) or "defect " (spil l the bean s in retur n
for a free pardon) . If the suspect s collaborate , they will only be convicte d of a mino r offense ,
a one­yea r sentence . If they both defect , both receiv e a four­yea r sentence . If one defect s and
the othe r does not, the defector  goes free wherea s the othe r receive s the maximu m sentenc e of
ten years . Considere d from the poin t of view of eithe r playe r separately , the best plan is to
defect , becaus e this give s bette r result s whateve r the othe r agen t does . Unfortunatel y for the
suspect s (but not for the police) , this result s in both suspect s spillin g the bean s and receivin g
a four­yea r sentence , wherea s if they had collaborated , they woul d both have receive d lighte r
sentences . Eve n mor e disturbin g is the fact that defectio n also occur s whe n the gam e has a
finite numbe r of rounds . (Thi s can easil y be prove d by workin g backwards  from the last round. )
Recently , however , ther e has been a shif t from consideratio n of optima l decision s in game s to
a consideratio n of optima l decision­makin g programs . Thi s lead s to differen t result s becaus e
it limit s the abilit y of each agen t to do unlimite d simulatio n of the other , who is also doin g
unlimite d simulatio n of the first , and so on. Eve n the requiremen t of computabilit y make s a
significan t differenc e (Megidd o and Wigderson , 1986) . Bound s on the complexit y of player s
have also becom e a topic of intens e interest . Neyman' s theore m (Neyman , 1985) , recentl y prove d
by Papadimitrio u and Yannakaki s (1994) , show s that a collaborativ e equilibriu m exist s if each
agent is a finit e automato n with a numbe r of state s that is less than exponentia l in the numbe r of
rounds . This is essentiall y a bounde d optimalit y result , wher e the boun d is on spac e rathe r than
speed of computation . Again , the bounde d optima l progra m for the automato n is rathe r messy ,
but its existenc e and propertie s are wha t counts .
848 Chapte r 27 . AI : Presen t and Futur e
27.3 WHA T IF WE Do SUCCEED?______ _ ___ _
In Davi d Lodge' s Small World,  ,a nove l abou t the academi c worl d of literar y criticism , the
protagonis t cause s consternatio n by askin g a pane l of eminen t but contradictor y literar y theorist s
the followin g question : "What  if you were  right?"  Non e of the theorist s seem s to have considere d
this questio n before , perhap s becaus e debatin g unfalsifiabl e theorie s is an end in itself . Simila r
confusio n can sometime s be evoke d by askin g AI researchers , "Wha t if you succeed? " AI is
fascinating , and intelligen t computer s are clearl y mor e usefu l than unintelligen t computers , so
why worry ?
To the exten t that AI has alread y succeede d in findin g uses withi n society , we are now
facin g some of the real issues . In the litigiou s atmospher e that prevail s in the Unite d States , it
is hardl y surprisin g that lega l liabilit y need s to be discussed . Whe n a physicia n relie s on the
judgmen t of a medica l exper t syste m for a diagnosis , who is at faul t if the diagnosi s is wrong ?
Fortunately , due in part to the growin g influenc e of decision­theoreti c method s in medicine , it is
now accepte d that negligenc e canno t be show n if the physicia n perform s medica l procedure s that
have high expected  utility , even if the actual  utilit y is catastrophic . The questio n shoul d therefor e
be, "Wh o is at fault if the diagnosi s is unreasonable? " So far, court s have held that medica l exper t
system s play the same role as medica l textbook s and referenc e books ; physician s are responsibl e
for understandin g the reasonin g behin d any decisio n and for usin g their own judgmen t in decidin g
whethe r or not to accep t the system' s recommendations . In designin g medica l exper t system s
as agents , therefore , the action s shoul d not be though t of as directl y affectin g the patien t but
as influencin g the physician' s behavior . If exper t system s becom e reliabl y more accurat e than
huma n diagnosticians , doctor s may be legall y liabl e if they fail to use the recommendation s of
an exper t system .
Simila r issue s are beginnin g to arise regardin g the use of intelligen t agent s on the "informa ­
tion highway. " Som e progres s has been mad e in incorporatin g constraint s into intelligen t agent s
so that they canno t damag e the files of othe r users (Wel d and Etzioni , 1994) . Also problemati c
is the fact that networ k service s alread y involv e monetar y transactions . If those monetar y trans ­
action s are mad e "on one' s behalf " by an intelligen t agent , is one liabl e for the debt s incurred ?
Woul d it be possibl e for an intelligen t agen t to have asset s itsel f and to perfor m electroni c trade s
on its own behalf ? Coul d it own stock s and bond s in the same way that corporation s own stock s
and bonds ? So far, thes e question s do not seem to be well understood . To our knowledge , no
progra m has been grante d legal statu s as an individua l for the purpose s of financia l transactions ;
at present , it seem s unreasonabl e to do so. Program s are also not considere d to be "drivers " for
the purpose s of enforcin g traffi c regulation s on real highways . In Californi a law, at least , ther e
do not seem to be any lega l sanction s to preven t an automate d vehicl e from exceedin g the spee d
limits , althoug h the designe r of the vehicle' s contro l mechanis m woul d be liabl e in the case of
accident . As with huma n reproductiv e technology , the law has yet to catc h up with the new
developments . Thes e topics , amon g others , are covere d in journal s such as AI and Society,  Law,
Computers  and  Artificial  Intelligence,  and Artificial Intelligence  and  Law.
Lookin g furthe r into the future , one can anticipat e question s that have been the subjec t of
innumerabl e work s of scienc e fiction , mos t notabl y thos e of Asimo v (1942) . If we gran t that
Sectio n 27.3 . Wha t If We Do Succeed ? 849
machine s will achiev e high level s of intelligen t behavio r and will communicat e with human s as
apparen t equals , then these question s are unavoidable . Shoul d (or will) intelligen t machine s have
rights ? How shoul d intelligen t machine s interac t with humans ? Wha t migh t happe n if intelligen t
machine s decid e to work agains t the best interest s of human  beings ? Wha t if they succeed ?
In Computer  Power and Human  Reason,  Josep h Weizenbau m (the autho r of the ELIZ A
program ) has argue d that the effec t of intelligen t machine s on huma n societ y will be such that
continue d work on artificia l intelligenc e is perhap s unethical . One of Weizenbaum' s principa l
argument s is that AI researc h make s possibl e the idea that human s are automata—a n idea that
result s in a loss of autonom y or even of humanity . (W e note that the idea has been aroun d
much longe r than AI. See L'Homme  Machine  (La Mettrie , 1748). ) On e can perhap s grou p
such concern s with the genera l concer n that any technolog y can be misuse d to the detrimen t of
humanity . Argument s over the desirabilit y of a give n technolog y mus t weig h the benefit s and
risks, and put the onus on researcher s to ensur e that polic y maker s and the publi c have the best
possibl e informatio n with whic h to reach a decision . On the other hand , AI raise s deepe r question s
than, say, nuclea r weapon s technology . No one, to our knowledge , has suggeste d that reducin g
the plane t to a cinde r is bette r than preservin g huma n civilization . Futurist s such as Edwar d
Fredki n and Han s Morave c have , however , suggeste d that once the huma n race has fulfille d its
destin y in bringin g into existenc e entitie s of highe r (and perhap s unlimited ) intelligence , its own
preservatio n may seem less important . Somethin g to thin k about , anyway .
Lookin g on the brigh t side, succes s in AI woul d provid e grea t opportunitie s for improvin g
the materia l circumstance s of huma n life. Whethe r it woul d improv e the qualit y of life is an open
question . Will intelligen t automatio n give peopl e more fulfillin g work and more relaxin g leisur e
time? Or will the pressure s of competin g in a nanosecond­pace d worl d lead to mor e stress ?
Will childre n gain from instan t acces s to intelligen t tutors , multimedi a onlin e encyclopedias , and
globa l communication , or will they play ever more realisti c war games?  Wil l intelligen t machine s
exten d the powe r of the individual , or of centralize d government s and corporations ? Scienc e
fictio n author s seem to favo r dystopia n future s over Utopia n ones , probabl y becaus e they mak e
for more interestin g plots . In reality , however , the trend s seem not to be too terribl y negative .
ACOMPLEXIT Y ANALYSI S
AND O() NOTATIO N
Compute r scientist s are ofte n face d with the task of comparin g two algorithm s to see whic h runs
BENCHMARKIN G faste r or takes less memory . Ther e are two approache s to this task. The first is benchmarking —
runnin g the two algorithm s on a compute r and measurin g whic h is faste r (or whic h uses less
memory) . Ultimately , this is wha t reall y matters , but a benchmar k can be unsatisfactor y becaus e
it is so specific : it measure s the performanc e of a particula r progra m writte n in a particula r
languag e runnin g on a particula r compute r with a particula r compile r and particula r inpu t data .
From the singl e resul t that the benchmar k provides , it can be difficul t to predic t how well
the algorith m woul d do on a differen t compiler , computer , or data set. A usefu l varian t of
benchmarkin g is to coun t the numbe r of operation s performe d of a particula r kind : for example ,
in testin g a numerica l sortin g algorith m we migh t coun t the numbe r of "greater­than " tests .
A. 1 ASYMPTOTI C ANALYSI S
ANALYSI S OF
ALGORITHM SThe secon d approac h relie s on a mathematica l analysi s of algorithms , independen t of the
particula r implementatio n and input . We will discus s the approac h with the followin g example ,
a progra m to comput e the sum of a sequenc e of numbers :
functio n S\JMMAi:iON(sequence)  return s a numbe r
sum — 0
for i — 1 to LENGTH(sequence)
.turn — sum  + sequence[i]
end
retur n sum
The first step in the analysi s is to abstrac t over the input , to find some paramete r or parameter s that
characteriz e the size of the input . In this example , the inpu t can be characterize d by the lengt h
of the sequence , whic h we will call n. The secon d step is to abstrac t over the implementation , to
find som e measur e that reflect s the runnin g time of the algorithm , but is not tied to a particula r
852 Appendi x A. Complexit y analysi s and O() notatio n
compile r or computer . For the SUMMATIO N program , this coul d be just the numbe r of lines of
code executed . Or it coul d be mor e detailed , measurin g the numbe r of additions , assignments ,
array references , and branche s execute d by the algorithm . Eithe r way give s us a characterizatio n
of the total numbe r of steps take n by the algorithm , as a functio n of the size of the input . We will
call this T(n).  Wit h the simple r measure , we have T(n) =  2n + 2 for our example .
If all program s were as simpl e as SUMMATION , analysi s of algorithm s woul d be a trivia l
field . But two problem s mak e it mor e complicated . First , it is rare to find a paramete r like n that
completel y characterize s the numbe r of steps take n by an algorithm . Instead , the best we can
usuall y do is comput e the wors t case Twors,(ri) or the averag e case Tmg(ri). Computin g an averag e
mean s that the analys t mus t assum e som e distributio n of inputs .
The secon d proble m is that algorithm s tend to resis t exac t analysis . In that case , it is
necessar y to fall back on an approximation . We say that the SUMMATIO N algorith m is O(ri),
meanin g that its measur e is at most a constan t time s n, with the possibl e exceptio n of a few smal l
value s of n. Mor e formally ,
T(n) is O(f(n))  if T(n) < kf(n)  for som e k, for all n > n0
ANALYSIS ™ Th e OQ notatio n give s us wha t is calle d an asymptoti c analysis . We can say withou t questio n
that as n asymptoticall y approache s infinity , an O(ri)  algorith m is bette r than an O(n2) algorithm .
A singl e benchmar k figur e coul d not substantiat e such a claim .
The OQ notatio n abstract s over constan t factors , whic h make s it easie r to use than the T()
notation , but less precise . For example , an O(n2) algorith m will alway s be wors e than an O(ri)  in
the long run, but if the two algorithm s are T(n2 +1) and T( 100« +1000) , then the O(n2) algorith m
is actuall y bette r for n < 110.
Despit e this drawback , asymptoti c analysi s is the mos t widel y used tool for analyzin g
algorithms . It is precisel y becaus e the analysi s abstract s both over the exac t numbe r of operation s
(by ignorin g the constan t factor , k) and the exac t conten t of the inpu t (by only considerin g its size,
n) that the analysi s become s mathematicall y feasible . The OQ notatio n is a good compromis e
betwee n precisio n and ease of analysis .
A.2 INHERENTL Y HAR D PROBLEM S
Analysi s of algorithm s and the OQ notatio n allow us to talk abou t the efficienc y of a particula r
algorithm . However , they have nothin g to say abou t whethe r or not ther e coul d be a bette r
ANALYS^SITY algorith m for the proble m at hand . The field of complexit y analysi s analyze s problem s rathe r
than algorithms . The first gros s divisio n is betwee n problem s that can be solve d in polynomia l
time and thos e that canno t be solve d in polynomia l time , no matte r wha t algorith m is used . The
class of polynomia l problem s is calle d P. Thes e are sometime s calle d "easy " problems , becaus e
the class contain s thos e problem s with runnin g time s like O(logn ) and O(n).  But it also contain s
those with O(nloo°), so the nam e "easy " shoul d not be take n too literally .
Anothe r importan t clas s of problem s is NP, the clas s of nondeterministi c polynomia l
problems . A proble m is in this class if there is som e algorith m that can gues s a solutio n and then
verif y whethe r or not the gues s is correc t in polynomia l time . The idea is that if you eithe r have
Sectio n A.2. Inherentl y Hard Problem s 853
NP­COMPLET Ean exponentiall y large numbe r of processor s so that you can try all the guesse s at once , or you
are very luck y and alway s gues s righ t the first time , then the NP problem s becom e P problems .
One of the big open question s in compute r scienc e is whethe r the class NP is equivalen t to
the class P whe n one does not have the luxur y of an infinit e numbe r of processor s or omniscien t
guessing . Mos t compute r scientist s ^re convince d that P ^ NP, that NP problem s are inherentl y
hard and only have exponentia l time algorithms . But this has neve r been proven .
Thos e who are intereste d in decidin g if P = NP  look at a subclas s of NP calle d the NP­
complet e problems . The wor d complete  is used here in the sens e of "mos t extreme, " and thus
refer s to the hardes t problem s in the class NP. It has been prove n that eithe r all the NP­complet e
problem s are in P or none of them is. This make s the class theoreticall y interesting , but the class
is also of practica l interes t becaus e man y importan t problem s are know n to be NP­complete .
An exampl e is the satisfiabilit y problem : give n a logica l expressio n (see Chapte r 6), is there an
assignmen t of truth value s to the variable s of the expressio n that mak e it true?
Also studie d is the class of PSPAC E problems , thos e that requir e a polynomia l amoun t of
space , even on a nondeterministi c machine . It is generall y believe d that PSPACE­hard  problem s
are wors e than NP­complete , althoug h it coul d turn out that NP = PSPACE , just as it coul d turn
out that P = NP.
BIBLIOGRAPHICA L AND HISTORICA L NOTE S
The O() notatio n so widel y used in compute r scienc e toda y was first introduce d in the contex t of
numbe r theor y by the Germa n mathematicia n P. G. H. Bachman n (1894) . The concep t of NP­
completenes s was invente d by Cook (1971) , and the moder n metho d for establishin g a reductio n
from one proble m to anothe r is due to Karp (1972) . Coo k and Karp have both won the Turin g
award , the highes t hono r in compute r science , for their work .
Classi c work s on the analysi s and desig n of algorithm s includ e thos e by Knut h (1973 ) and
Aho, Hopcroft , and Ullma n (1974) ; more recen t contribution s are by Tarja n (1983 ) and Cormen ,
Leiserson , and Rives t (1990) . Thes e book s plac e an emphasi s on designin g and analyzin g
algorithm s to solv e tractabl e problems . For the theor y of NP­completenes s and othe r form s of
intractability , the best introductio n is by Gare y and Johnso n (1979) . In additio n to the underlyin g
theory , Gare y and Johnso n provid e example s that conve y very forcefull y why compute r scientist s
are unanimou s in drawin g the line betwee n tractabl e and intractabl e problem s at the borde r
betwee n polynomia l and exponentia l time complexity . The y also provid e a voluminou s catalo g
of problem s that are know n to be NP­complet e or otherwis e intractable .
BNOTE S ON LANGUAGE S
AND ALGORITHM S
B. 1 DEFININ G LANGUAGE S WITH BACKUS­NAU R FORM (BNF )
BACKUS­NAU R
FORM
BNF
TERMINA L SYMBOL S
NONTERMINA L
SYMBOL S
STAR T SYMBO LIn this book , we defin e severa l languages , includin g the language s of prepositiona l logic (pag e
166), first­orde r logi c (pag e 187) , and a subse t of Englis h (pag e 670) . A forma l languag e is
define d as a set of string s wher e each strin g is a sequenc e of symbols . All the language s we are
intereste d in consis t of an infinit e set of strings , so we need a concis e way to characteriz e the
set. We do that with a grammar . We have chose n to write our grammar s in a formalis m calle d
Backus­Nau r form , or BNF . Ther e are four component s to a BNF grammar :
• A set of termina l symbols . Thes e are the symbol s or word s that mak e up the string s of
the language . They coul d be letter s (A, B, C,... ) or word s (a, aardvark , abacus,... ) for
English) . For the languag e of arithmetic , the set of symbol s is
{0,1,2,3,4,5,6,7,8,9, +, ­, ­r, x, (,)}
• A set of nontermina l symbol s that categoriz e subphrase s of the language . For example ,
the nontermina l symbo l NounPhrase  in Englis h denote s an infinit e set of string s includin g
"you" and "the big slobber y dog. "
• A start symbol , whic h is the nontermina l symbo l that denote s the complet e string s of the
language . In English , this is Sentence;  for arithmetic , it migh t be Exp.
• A set of rewrit e rule s or production s of the form LHS  —> RHS,  wher e LHS  is a nonter ­
minal , and RHS  is a sequenc e of zero or more symbol s (eithe r termina l or nonterminal) .
A rewrit e rule of the form
Digit  — 7
mean s that anytim e we see the strin g consistin g of the lone symbo l 7, we can categoriz e it as a
Digit.  A rule of the form
Sentence  —> NounPhrase  VerbPhrase
mean s that wheneve r we have two string s categorize d as a NounPhrase  and a VerbPhrase,  we can
appen d them togethe r and categoriz e the resul t as a Sentence.  As an abbreviation , the symbo l |
Sectio n B.2. Describin g Algorithm s with Pseudo­Cod e 85 5
can be used to separat e alternativ e right­han d sides . Here is a BNF gramma r for simpl e arithmeti c
expressions :
Exp  — > Exp  Operator  Exp
I (Exp )
| Number  ­
Number  — > Digit
Number  Digit
Digit  ­>0|1|2|3|4|5|6|7|8| 9
Operator  —> + \ — \ ­J­ | X
We cove r language s and grammar s in more detai l in Chapte r 22. Be awar e that othe r book s use
slightl y differen t notation s for BNF ; for example , you migh t see (Digit}  instea d of Digit  for a
nonterminal ; 'word ' instead  of word for a terminal ; or : : = instea d of —> in a rule.
B.2 DESCRIBIN G ALGORITHM S WITH PSEUDO­COD E
In this book , we defin e over 100 algorithms . Rathe r than pickin g a programmin g languag e (and
riskin g that reader s who are unfamilia r with the languag e will be lost) , we have chose n to describ e
the algorithm s in pseudo­code . Mos t of it shoul d be familia r to user s of language s like Pascal ,
C, or Commo n Lisp , but in som e place s we use mathematica l formula s or ordinar y Englis h to
describ e parts that woul d otherwis e be mor e cumbersome . Ther e are a few idiosyncrasie s that
shoul d be remarke d on.
Nondeterminis m
It is the natur e of AI that we are often face d with makin g a decisio n befor e we know enoug h to
make the right choice . So our algorithm s have to mak e a choice , but keep track of the alternative s
in case the choic e does not succeed . Th e cleares t way to describ e such algorithm s withou t
boggin g them dow n with bookkeepin g detail s is with the primitive s choos e and fail.
The idea is that whe n we call choose(a,£,c) , the algorith m will retur n eithe r a, b, or c as
the valu e of the choos e expression . But it will also save the othe r two on an agend a of pendin g
choices . The algorith m continues ; if it terminate s normall y then all is done , and we forge t abou t
the agenda . But if a fail statemen t is encountered , then a pendin g choic e is take n off the agenda ,
and contro l is resume d at the  point  in the algorithm  where  that  choice  was  saved.  Algorithm s
A?GORirHMSNISTI C tha t mak e use of choos e are calle d nondeterministi c algorithms .
You can thin k of a nondeterministi c algorith m as a searc h throug h the spac e of possibl e
choices . As such , any of the searc h algorithm s from Chapter s 3 or 4 can be used . The beaut y
of the nondeterministi c algorith m is that the searc h strateg y can be specifie d separatel y from the
main algorithm .
856 Appendi x B. Note s on Language s and Algorithm s
ORACL E Anothe r way to think of nondeterministi c algorithm s is to imagin e an oracl e that magicall y
advise s the algorith m to mak e the correc t choic e at every choic e point . If such an oracl e coul d
be found , the algorith m woul d be deterministic ; it woul d neve r need to backtrack . You can think
of the agend a as a slowe r mean s of simulatin g the advic e of the oracle .
Nondeterministi c algorithm s are often cleare r than deterministi c versions , but unfortunatel y
only a few programmin g language s suppor t nondeterminis m directly—Prolo g and ICO N are
probabl y the best known , choos e and fail can be implemented  in SCHEM E in abou t 15 lines of
code, usin g the functio n call­with­current­continuation .
Here is an exampl e of a nondeterministi c function : the functio n call lNTEGER(^tor? ) return s
an intege r greate r than or equa l to start,  chose n nondeterministically .
functio n iNTEGER(ifart ) return s an intege r
retur n choose(start,  lNTEGER(start  + 1))
It is importan t to understan d wha t the code fragmen t PRINT(!NTEGE R (0)); fail will do. First , it
will print some integer , althoug h we canno t say whic h one withou t knowin g what contro l strateg y
is used by choose . The n fail will retur n contro l to one of the choic e point s in one of the recursiv e
calls to INTEGER . Eventually , a differen t intege r will be chose n and printed , and then fail will be
execute d again . The resul t is an infinit e loop , with a singl e intege r printe d each time .
We sometime s use the term pick to mea n a choic e that is not a backtrackin g point . For
example , an algorith m to sum the element s of a set is to initializ e the total to 0, pick an elemen t
from the set and add it to the total , and continu e until there are no more element s to pick . Ther e
is no need to backtrack , becaus e any orde r of pickin g will get the job done .
Stati c variable s
We use the keywor d stati c to say that a variable  is give n an initia l valu e the first time a functio n
is calle d and retain s that valu e (or the valu e give n to it by a subsequen t assignmen t statement )
on all subsequen t calls to the function . Thus , stati c variable s are like globa l variable s in that
they outliv e a singl e call to their function , but they are only accessibl e withi n the function . The
agent program s in the book use stati c variable s for "memory. " Program s with stati c variable s
can be implemente d as "objects " in object­oriente d language s such as C++ and Smalltalk . In
functiona l languages , they can be implemente d easily  by executin g lambda­expression s withi n
an environmen t in whic h the require d variable s are defined .
Function s as value s
Function s and procedure s have capitalize d name s and variable s have lowe r case italic names .
So mos t of the time , a functio n call look s like SOME­FuNCTlON(van'a&/<?) . However , we allow
the valu e of a variabl e to be a function ; for example , if the valu e of variable  is the squar e root
function , then variable!*))  return s 3.
Sectio n B .3. Th e Cod e Repositor y 857
B. 3 TH E COD E REPOSITOR Y
The pseudo­cod e in the book is mean t to be easy to read and understand , but it is not easy to
run on a computer . To fix this problem , we have provide d a repositor y of workin g code . Mos t
of the algorithm s in the book are implemented , as well as a set of basic tools not covere d in the
book . Currently , the algorithm s are all writte n in Lisp , althoug h we may add othe r language s in
the future . If you are readin g this book as part of a course , your instructo r will probabl y retriev e
the code for you. If not, send an electroni c mail messag e to
aima-request@cs.berkeley.ed u
with the word "help " in the subjec t line or in the body . You will receiv e a retur n messag e with
instruction s on how to get the code . (We don' t print the instruction s here becaus e they are subjec t
to frequen t change. ) You can also orde r the code on a flopp y disk by writin g to Prentice­Hal l
Inc., Englewoo d Cliffs , NJ, 07632 .
B.4 COMMENT S
If you have any comment s on the book , any typo s you have noticed , or any suggestion s on how
it can be improved , we woul d like to hear from you. Pleas e send a messag e to
aima­bug@cs.berkeley.ed u
or writ e to us in care of Prentice­Hall .
Bibliograph y
Aarup , M., Arentoft , M. M., Parrod , Y., Stader ,
J., and Stokes,  I. (1994) . OPTIMUM­AIV :
A knowledge­base d plannin g and schedulin g sys­
tem for spacecraf t AIV . In Fox , M. and Zweben ,
M., editors , Knowledge  Based  Scheduling.  Morga n
Kaufmann , San Mateo , California .
Abu­Mostafa , Y. S. and Psaltis , D. (1987) . Optica l
neura l computers . Scientific  American,  256:88­95 .
Acharya , A., Tambe , M., and Gupta , A. (1992) .
Implementatio n of productio n system s on message ­
passin g computers . IEEE  Transactions  on Parallel
and Distributed Systems,  3(4):477­487 .
Adelson­Velsky , G. M., Arlazarov , V. L., Bitman ,
A. R., Zhivotovsky , A. A., and Uskov , A. V. (1970) .
Programmin g a compute r to play chess . Russian
Mathematical  Surveys,  25:221­262 .
Adelson­Velsky , G. M., Arlazarov , V. L., and Don ­
skoy, M. V. (1975) . Som e method s of controllin g
the tree searc h in ches s programs . Artificial  Intelli­
gence,6(4):36\­37l.
Agmon , S. (1954) . The relaxatio n metho d for lin­
ear inequalities . Canadian  Journal  of Mathematics,
6(3):382­392 .
Agre , P. E. and Chapman , D. (1987) . Pengi : an
implementatio n of a theor y of activity . In Proceed­
ings of the Tenth  International  Joint  Conference  on
Artificial  Intelligence  (IJCAI­87),  page s 268­272 ,
Milan , Italy . Morga n Kaufmann .
Aho, A. V., Hopcroft , J. E., and Ullman , J. D.
(1974) . The  Design  and Analysis  of Computer  Algo­
rithms.  Addison­Wesley , Reading , Massachusetts .
Ait­Kaci , H. (1991) . Warren's  Abstract  Machine:
A Tutorial  Reconstruction.  MIT Press , Cambridge ,
Massachusetts .
Ait­Kaci , H. and Nasr , R. (1986) . LOGIN : a logic
programmin g languag e with built­i n inheritance .
Journal  of Logic  Programming,  3(3): 185­215 .
Ait­Kaci , H. and Podelski , A. (1993) . Toward s a
meanin g of LIFE . Journal  of Logic  Programming,
16(3­4) : 195­234 .
Allais , M. (1953) . Le comportmen t de Fhomm e
rationne l devan t la risque : critiqu e des postulat set axiome s de 1'ecol e Americaine . Econometrica,
21:503­546 .
Alien , J. F. (1983) . Maintainin g knowledg e abou t
tempora l intervals . Communications  of the Associ­
ation  for Computing Machinery,  26(11):832­843 .
Alien , J. F. (1984) . Toward s a genera l theor y of ac­
tion and time . Artificial  Intelligence,  23:123­154 .
Alien , J. F. (1991) . Tim e and time again : the man y
ways to represen t time . International  Journal  of
Intelligent  Systems,  6:341­355 .
Alien , J. F. (1995) . Natural  Language  Understand­
ing. Benjamin/Cummings , Redwoo d City , Califor ­
nia.
Alien , J. F, Hendler , J., andTate , A., editor s (1990) .
Readings  in Planning.  Morga n Kaufmann , San Ma­
teo, California .
Almuallim , H. and Dietterich , T. G. (1991) . Learn ­
ing with man y irrelevan t features . In Proceedings
of the Ninth  National  Conference  on Artificial Intel­
ligence  (AAAI­91),  volum e 2, page s 547­552 , Ana ­
heim , California . AAA I Press .
Aloimonos , J., Weiss , I., and Bandyopadhyay , A.
(1988) . Activ e vision . International  Journal of
Computer  Vision,  1:333­356 .
Aloimonos , Y. (1992) . Specia l issue on purposive ,
qualitative , activ e vision . CVGIP:  Image Under­
standing,  56(1) .
Alshawi , H., edito r (1992) . The  Core  Language
Engine.  MIT Press , Cambridge , Massachusetts .
Alspector , J., Alien , R. B., Hu, V., and Satya ­
narayana , S. (1987) . Stochasti c learnin g network s
and their electroni c implementation . In Anderson ,
D. Z., editor , Neural  Information  Processing  Sys­
tems,  Denver  1987,  page s 9­21 , Denver , Colorado .
America n Institut e of Physics .
Alterman , R. (1988) . Adaptiv e planning . Cognitive
Science,  12:393^22 .
Amarel , S. (1968) . On representation s of problem s
of reasonin g abou t actions . In Michie , D., editor ,
Machine  Intelligence 3,  volum e 3, page s 131­171 .
Elsevier/North­Holland , Amsterdam , London , New
York .
859
860 Bibliograph y
Ambros­Ingerson , J. and Steel , S. (1988) . Integrat ­
ing planning , executio n and monitoring . In Pro­
ceedings  of the Seventh  National  Conference  on
Artificial  Intelligence  (AAAI­88),  page s 735­740 ,
St. Paul , Minnesota . Morga n Kaufmann .
Amit , D., Gutfreund , H., and Sompolinsky , H.
(1985) . Spin­glas s model s of neura l networks . Phys­
ical Review,  A 32:1007­1018 .
Ammon , K. (1993) . An automati c proo f of Godel' s
incompletenes s theorem . Artificial  Intelligence,
61(2):291­306 .
Andersen , S. K., Olesen , K. G., Jensen , F. V., and
Jensen , F. (1989) . HUGIN— a shel l for buildin g
Bayesia n belie f universe s forexper t systems . In Pro­
ceedings  of the Eleventh International  Joint  Con­
ference  on Artificial Intelligence  (IJCAI­89),  vol ­
ume 2, page s 1080­1085 , Detroit , Michigan . Mor ­
gan Kaufmann .
Anderson , A. R., edito r (1964) . Minds  and  Ma­
chines.  Prentice­Hall , Englewoo d Cliffs , New Jer­
sey.
Anderson , J. (1980) . Cognitive  Psychology  and its
Implications.  W. H. Freeman , New York .
Anderson , J. A. and Rosenfeld , E., editor s
(1988) . Neurocomputing:  Foundations  of Re search.
MIT Press , Cambridge , Massachusetts .
Anderson , J. R. (1983) . The  Architecture  of Cog­
nition.  Harvar d Universit y Press , Cambridge , Mas ­
sachusetts .
Armstrong , D. M. (1968) . A Materialist  Theory  of
the Mind.  Routledg e and Kega n Paul , London .
Arnauld , A. (1662) . La logique,  ou I'artdepenser.
Chez Charle s Savreux , au pied de la Tour de Nostr e
Dame , Paris.Usuall y referre d to as the Port­Royal
Logic;  translate d into Englis h as Arnaul d (1964) .
Arnauld , A. (1964) . The  Art of  Thinking.  Bobbs ­
Merrill , Indianapolis , Indiana.Translatio n of Ar­
nauld (1662) , usuall y referre d to as the Port­Royal
Logic.
Ashby , W. R. (1952) . Design  for a Brain.  Wiley ,
New York .
Asimov , I. (1942) . Runaround . Astounding  Science
Fiction.
Asimov , I. (1950) . /, Robot.  Doubleday , Garde n
City, New York .Astrom , K. J. (1965) . Optima l contro l of Marko v
decisio n processe s with incomplet e state estimation .
/ Math.  Anal. Applic.,  10:174­205 .
Austin , J. L. (1962) . How  To Do Things  with
Words.  Harvar d Universit y Press , Cambridge , Mas ­
sachusetts .
Bacchus , F. (1990) . Representing  and Reasoning
with Probabilistic  Knowledge.  MIT Press , Cam ­
bridge , Massachusetts .
Bacchus , F., Grove , A., Halpern , J. Y, and Koller ,
D. (1992) . From statistic s to beliefs . In Proceed­
ings of the Tenth  National  Conference  on Artificial
Intelligence  (AAAI­92),  page s 602­608 , San Jose ,
California . AAA I Press .
Bach , E. (1986) . The algebr a of events . Linguistics
and Philosophy,  9:5­16 .
Bachmann,P . G. H. (1894) . Die anafytische  Zahlen­
theorie.  B. G. Teubner , Leipzig .
Bain , M. and Muggleton , S. H. (1991) . Non ­
monotoni c learning . In Hayes , J. E., Michie , D.,
and Tyugu , E., editors , Machine  Intelligence  12,
pages 105­119 . Oxfor d Universit y Press , Oxford .
Bajcsy , R. (1988) . Activ e perception . Proceedings
of the IEEE,  76(8):996­1005 .
Bajcsy , R. and Lieberman , L. (1976) . Textur e gra­
dient as a dept h cue. Computer  Graphics  and Image
Processing,  5(l):52­67 .
Baker , C. L. (1989) . English  Syntax.  MIT Press ,
Cambridge , Massachusetts .
Baker , J. (1975) . The Drago n system—a n overview .
IEEE  Transactions  on Acoustics,  Speech,  and  Sig­
nal Processing,  23.
Ballard , B. W. (1983) . The *­minima x searc h pro­
cedur e for trees containin g chanc e nodes . Artificial
Intelligence,  21(3):327­350 .
Bar­Hillel , Y. (1954) . Indexica l expressions . Mind,
63:359­379 .
Bar­Hillel , Y. (1960) . The presen t statu s of auto ­
matic translatio n of languages . In Alt, F. L., edi­
tor, Advances  in Computers.  Academi c Press , New
York .
Bar­Shalom , Y. andFortmann , T. E. (1988) . Track­
ing and Data Association.  Academi c Press , New
York .
Bibliograph y 861
Barr, A., Cohen , P. R., and Feigenbaum , E. A.,
editor s (1989) . The  Handbook  of Artificial  Intelli­
gence,  volum e 4. Addison­Wesley , Reading , Mas ­
sachusetts .
Barr, A. and Feigenbaum , E. A., editor s (1981) .
The Handbook  of Artificial  Intelligence,  volum e 1.
HeurisTec h Press and Willia m Kaufmann , Stanford ,
Californi a and Los Altos , California.Firs t of four
volumes ; othe r volume s publishe d separatel y (Bar r
and Feigenbaum , 1982 ; Cohe n and Feigenbaum ,
1982; Barr et al, 1989) .
Barr, A. and Feigenbaum , E. A., editor s (1982).
The Handbook  of Artificial  Intelligence,  volum e 2.
HeurisTec h Press and Willia m Kaufmann , Stanford ,
Californi a and Los Altos , California .
Barrett , A., Golden , K., Penberthy , J. S., and Weld ,
D. S. (1993) . UCPO P user' s manua l (versio n 2.0).
Technica l Repor t 93­09­06 , Departmen t of Com ­
puter Scienc e and Engineering , Universit y of Wash ­
ington .
Barrett , R., Ramsay , A., and Sloman , A. (1985) .
POP­11:  A Practical  Language  for Artificial Intel­
ligence.  Ellis Horwood , Chichester , England .
Barstow , D. R. (1979) . Knowledge­Based  Program
Construction.  Elsevier/North­Holland , Amsterdam ,
London , New York .
Barto , A. G., Bradtke , S. J., and Singh , S. P. (1991) .
Real­tim e learnin g and contro l usin g asynchronou s
dynami c programming . Technica l Repor t TR­91 ­
57, Universit y of Massachusett s Compute r Scienc e
Department , Amherst , Massachusetts .
Barto , A. G., Sutton , R. S., and Brouwer , P. S.
(1981) . Associativ e searc h network : a reinforce ­
ment learnin g associativ e memory . Biological  Cy­
bernetics,  40(3):201­211 .
Barwise , J. (1993) . Everyda y reasonin g and log­
ical inference . Behavioral  and  Brain  Sciences,
16(2):337­338 .
Barwise , J. and Etchemendy , J. (1993) . The  Lan­
guage  of First­Order  Logic:  Including  the Macin­
tosh Program  Tarski's World  4.0. Cente r for the
Study of Languag e and Informatio n (CSLI) , Stan ­
ford, California , third revise d and expande d edition .
Baum , L. E. and Petrie , T. (1966) . Statistica l in­
ferenc e for probabilistic  function s of finit e stat e
Marko v chains . Annals  of Mathematical  Statistics,
41.Bayes , T. (1763) . An essay toward s solvin g a prob ­
lem in the doctrin e of chances.  Philosophical  Trans­
actions  of the Royal Society  of London,  53:370^ t 18.
Beal, D. F. (1980) . An analysi s of minimax . In
Clarke , M. R. B., editor , Advances  in Computer
Chess  2, page s 103­109 . Edinburg h Universit y
Press , Edinburgh , Scotland .
Beck , H. W., Gala , S. K., and Navathe , S. B. (1989) .
Classificatio n as a quer y processin g techniqu e in the
CANDID E semanti c data model . In Proceedings
Fifth  International  Conference  on Data Engineer­
ing, page s 572­581 , Los Angeles , California . IEEE
Compute r Societ y Press .
Belhumeur , P. N. (1993) . A binocula r stere o algo­
rithm for reconstructin g sloping , creased , and bro­
ken surface s in the presenc e of half­occlusion . In
Proceedings  of the 4th International  Conference  on
Computer  Vision,  Berlin . IEEE Compute r Societ y
Press .
Bell, C. and Tate , A. (1985) . Usin g tempora l con­
straint s to restric t searc h in a planner . In Proceedings
of the Third  Alvey  IKBS  SIC  Workshop,  Sunning ­
dale, Oxfordshire .
Bell, J. L. and Machover , M. (1977) . A Course  in
Mathematical  Logic.  Elsevier/North­Holland , Am­
sterdam , London , New York .
Bellman , R. E. (1957) . Dynamic  Programming.
Princeto n Universit y Press , Princeton , New Jersey .
Bellman , R. E. (1978) . An Introduction  to Artifi­
cial Intelligence:  Can  Computers  Think?  Boy d &
Frase r Publishin g Company , San Francisco .
Bellman , R. E. and Dreyfus , S. E. (1962) . Applied
Dynamic  Programming.  Princeto n Universit y Press ,
Princeton , New Jersey .
Berlekamp , E. R., Conway , J. H., and Guy , R. K.
(1982) . Winning  Ways,  For  Your  Mathematical
Plays.  Academi c Press , New York .
Berliner , H. J. (1977) . BKG— A progra m that
plays backgammon . Technica l report , Compute r
Scienc e Department , Carnegie­Mello n University ,
Pittsburgh , Pennsylvania .
Berliner , H. J. (1979) . TheB * tree searc h algorithm :
A best­firs t proo f procedure . Artificial  Intelligence,
12(1):23­40 .
Berliner , H. J. (1980a) . Backgammo n compute r pro­
gram beat s worl d champion . Artificial  Intelligence,
14:205­220 .
862 Bibliograph y
Berliner , H. J. (1980b) . Compute r backgammon .
Scientific  American,  249(6):64­72 .
Berliner , H. J. (1989) . Hitec h chess : From maste r
to senio r maste r with no hardwar e change . In MIV­
89: Proceedings  of the International  Workshop  on
Industrial  Applications  of Machine  Intelligence and
Vision  (Seiken  Symposium),  page s 12­21 .
Berliner , H. J. and Ebeling , C. (1989) . Patter n
knowledg e and search : The SUPRE M architecture .
Artificial  Intelligence,  38(2) : 161­198 .
Berliner , H. J. and Goetsch , G. (1984) . A quan ­
titativ e stud y of searc h method s and the effec t of
constrain t satisfaction . Technica l Repor t CMU­CS ­
84­187 , Compute r Scienc e Department , Carnegie ­
Mello n University , Pittsburgh , Pennsylvania .
Bernoulli , D. (1738) . Specime n theoria e nova e de
mensur a sortis . Proceedings  of the St. Petersburg
Imperial  Academy  of Sciences,  5.Translate d into
Englis h as Bernoull i (1954) .
Bernoulli , D. (1954) . Expositio n of a new theor y of
the measuremen t of risk. Econometrica,  22:123­
136. Translatio n of Bernoull i (1738 ) by Louis e
Sommer .
Bernstein , A. and Roberts , M. (1958) . Compute r
vs. ches s player . Scientific  American,  198(6):96 ­
105.
Bernstein , A., Roberts , M., Arbuckle , T, and Bel­
sky, M. S. (1958) . A ches s playin g progra m for
the IBM 704. In Proceedings  of the 1958  Western
Joint  Computer  Conference,  page s 157­159 , Los
Angeles .
Berry , D. A. andFristedt , B. (1985) . Bandit  Prob­
lems:  Sequential  Allocation  of Experiments.  Chap ­
man and Hall , London .
Bertsekas , D. P. (1987) . Dynamic  Programming:
Deterministic  and Stochastic  Models.  Prentice­Hall ,
Englewoo d Cliffs , New Jersey .
Beth, E. W. (1955) . Semanti c entailmen t and for­
mal derivability . Mededelingen  van de Koninklijke
Nederlandse  Akademie  van  Wetenschappen,  Afdel­
ing Letterkunde,  N.R.,  18(13):309­342 .
Bibel , W. (1981) . On matrice s with connections .
Journal  of the Association  for Computing  Machin­
ery, 28(4):633­645 .
Bibel , W. (1986) . A deductiv e solutio n for plan
generation . New  Generation  Computing,  4(2) : 115­
132.Birnbaum , L. and Selfridge , M. (1981) . Conceptua l
analysi s of natura l language . In Schank , R. and Ries ­
beck, C., editors , Inside  Computer  Understanding.
Lawrenc e Erlbaum .
Biro, J. I. and Shahan,R . W., editor s (1982) . Mind,
Brain  and  Function:  Essays  in the Philosophy  of
Mind.  Universit y of Oklahom a Press , Norman , Ok­
lahoma .
Birtwistle , G., Dahl , O.­J. , Myrhaug , B., and Ny­
gaard , K. (1973) . Simula  Begin.  Studentliteratu r
(Lund ) and Auerbach , New York .
Bitner , J. R. andReingold , E. M. (1975) . Backtrac k
programmin g techniques . Communications  of the
Association  for Computing Machinery,  18(11):651 ­
656.
Black , E., Jelinek , P., Lafferty , J., Magerman , D.,
Mercer , R., and Roukos , S. (1992) . Toward s history ­
based grammars : usin g riche r model s for proba ­
bilisti c parsing . In Marcus , M., editor , Fifth  DARPA
Workshop  on Speech  and Natural  Language,  Arde n
Conferenc e Center , Harriman , New York .
Block , N., edito r (1980) . Readings  in Philosophy
of Psychology,volume  1. Harvar d Universit y Press ,
Cambridge , Massachusetts .
Bloom , P. (1994) . Language  Acquisition:  Core
Readings.  MIT Press .
Blum , A. L. and Rivest , R. L. (1992) . Trainin g
a 3­nod e neura l networ k is NP­complete . Neural
Networks,  5(l):l  17­127 .
Blumer , A., Ehrenfeucht , A., Haussler , D., and War­
muth , M. K. (1989) . Learnabilit y and the Vapnik ­
Chervonenki s dimerision . Journal  of the Associa­
tion for Computing  Machinery,  36(4):929­965 .
Blumer , A., Ehrenfeucht , A., Haussler,D. , andWar ­
muth , M. K. (1990) . Occam' s razor . In Shavlik , J. W.
and Dietterich , T. G., editors , Readings  in Machine
Learning,  page s 201­204 . Morga n Kaufmann .
Board , R. and Pitt, L. (1992) . On the necessit y of
Occa m algorithms . Theoretical  Computer  Science,
100(1):157­184 .
Bobrbw , D. G. (1967) . Natura l languag e inpu t
for a compute r proble m solvin g system . In Min ­
sky, M. L., editor , Semantic  Information  Process­
ing, page s 133­215 . MIT Press , Cambridge , Mas ­
sachusetts .
Bibliograph y 863
Bobrow , D. G. and Raphael , B. (1974) . New pro­
grammin g language s for artificia l intelligenc e re­
search . Computing  Surveys,  6(3):153­174 .
Boden , M. A. (1977) . Artificial  Intelligence  and
Natural  Man.  Basi c Books , New York .
Boden , M. A., edito r (1990) . The Philosophy  of Arti­
ficial  Intelligence.  Oxfor d Universit y Press , Oxford .
Boole , G. (1847) . The  Mathematical  Analysis  of
Logic:  Being  an Essay  towards  a Calculus  of
Deductive  Reasoning.  Macmillan , Barclay , and
Macmillan , Cambridge .
Boolos , G. S. (1990) . On "seeing " the truth of
the Gode l sentence . Behavioral  and  Brain  Sci­
ences,  13(4):655­656.Pee r commentar y on Pen ­
rose (1990) .
Boolos , G. S. and Jeffrey , R. C. (1989) . Computabil­
ity and Logic.  Cambridg e Universit y Press , Cam ­
bridge , third edition .
Borgida , A., Brachman , R. J., McGuinness , D. L.,
and Alperi n Resnick , L. (1989) . CLASSIC : a struc ­
tural data mode l for objects . SIGMOD  Record,
18(2):58­67 .
Boyer , R. S. (1971) . Locking:  A Restriction  of Res­
olution.  PhD thesis , Universit y of Texas , Austin ,
Texas .
Boyer , R. S. and Moore , J. S. (1972) . The sharin g of
structur e in theorem­provin g programs . In Meltzer ,
B. and Michie , D., editors , Machine  Intelligence  7,
page s 101­116 . Edinburg h Universit y Press , Edin ­
burgh , Scotland .
Boyer , R. S. and Moore , J. S. (1979) . A Computa­
tional  Logic.  Academi c Press , New York .
Boyer , R. S. andMoore , J. S. (1984) . Proo f checkin g
the RS A publi c key encryptio n algorithm . American
Mathematical  Monthly,  91 (3): 181­189 .
Brachman , R. J. (1979) . On the epistemologica l sta­
tus of semanti c networks . In Findler , N. V., editor ,
Associative  Networks:  Representation  and Use  of
Knowledge  by Computers,  page s 3­50 . Academi c
Press , New York .
Brachman , R. J., Fikes , R. E., and Levesque , H. J.
(1983) . Krypton : A functiona l approac h to knowl ­
edge representation . Computer,  16(10):67­73 .
Brachman , R. J. and Levesque , H. J., editor s (1985) .
Readings  in Knowledge  Representation.  Morga n
Kaufmann , San Mateo , California .Bransford , J. and Johnson , M. K. (1973) . Consid ­
eratio n of som e problem s in comprehension . In
Chase , W. G., editor , Visual  Information  Process­
ing. Academi c Press , New York .
Bratko , I. (1986) . Prolog  Programming  for Artifi­
cial Intelligence.  Addison­Wesley , Reading , Mas ­
sachusetts , first edition .
Bratko , I. (1990) . Prolog  Programming  for Artifi­
cial Intelligence.  Addison­Wesley , Reading , Mas ­
sachusetts , secon d edition .
Bratman , M. E. (1987) . Intention, Plans,  and Prac­
tical Reason.  Harvar d Universit y Press , Cambridge ,
Massachusetts .
Bratman , M. E. (1992) . Plannin g and the stabilit y
of intention . Minds  and Machines,  2(1):1­16 .
Brelaz , D. (1979) . New method s to colo r the ver­
tices of agraph . Communications  of the Association
for Computing  Machinery,  22(4):251­256 .
Bresnan , J. (1982) . The  Mental Representation of
Grammatical  Relations.  MI T Press , Cambridge ,
Massachusetts .
Briggs , R. (1985) . Knowledg e representatio n in
Sanskri t and artificia l intelligence . AI Magazine,
6(l):32­39 .
Brooks , R. A. (1981) . Symboli c reasonin g amon g
3­D model s and 2­D images . Artificial  Intelligence,
17:285­348 .
Brooks , R. A. (1986) . A robus t layere d contro l sys­
tem for a mobil e robot . IEEE  Journal of  Robotics
and Automation,  2:14­23 .
Brooks , R. A. (1989) . Engineerin g approac h to
buildin g complete , intelligen t beings . Proceedings
of the SPIE—The  International  Society  for Optical
Engineering,  1002:618­625 .
Brudno , A. L. (1963) . Bound s and valuation s for
shortenin g the scannin g of variations . Problems  of
Cybernetics,  10:225­241 .
Bryson , A. E. and Ho, Y.­C . (1969) . Applied  Opti­
mal Control.  Blaisdell , New York .
Buchanan,B . G. andMitchell , T. M. (1978) . Model ­
directe d learnin g of productio n rules . In Waterman ,
D. A. and Hayes­Roth , E, editors , Pattern­Directed
Inference  Systems,  page s 297­312 . Academi c Press ,
New York .
864 Bibliograph y
Buchanan , B. G., Mitchell , T. M., Smith , R. G.,
and Johnson , C. R. (1978) . Model s of learnin g sys­
tems . In Encyclopedia  of Computer  Science  and
Technology,  volum e 11. Dekker .
Buchanan , B. G. and Shortliffe , E. H., editor s
(1984) . Rule­Based  Expert  Systems:  The  MYCIN
Experiments  of the Stanford  Heuristic  Programming
Project.  Addison­Wesley , Reading , Massachusetts .
Buchanan , B. G., Sutherland , G. L., and Feigen ­
baum , E. A. (1969) . Heuristi c DENDRAL : a pro­
gram for generatin g explanator y hypothese s in or­
ganic chemistry . In Meltzer , B., Michie , D., and
Swann , M., editors , Machine  Intelligence  4, page s
209­254 . Edinburg h Universit y Press , Edinburgh ,
Scotland .
Buchler , J., edito r (1955) . Philosophical  Writings
ofPeirce.  Dover , New York .
Bundy , A. (1983) . The  Computer  Modelling  of
Mathematical  Reasoning.  Academi c Press , New
York .
Bunt , H. C. (1985) . The forma l representatio n of
(quasi­ ) continuou s concepts . In Hobbs , J. R. and
Moore , R. C., editors , Formal  Theories  of the Com­
monsense  World,  chapte r 2, page s 37­70 . Ablex ,
Norwood , New Jersey .
Burstall , R. M. (1974) . Progra m provin g as hand
simulatio n with a littl e induction . In Information
Processing  '74,  page s 308­312 . Elsevier/North ­
Holland , Amsterdam , London , New York .
Burstall , R. M. and Darlington , J. (1977) . A
transformatio n syste m for developin g recursiv e pro­
grams . Journal  of the Association  for Computing
Machinery,24(l):44­61.
Bylander , T. (1992) . Complexit y result s for seria l
decomposability . In Proceedings  of the Tenth  Na­
tional  Conference  on Artificial  Intelligence  (AAAI­
92), page s 729­734 , San Jose , California . AAA I
Press .
Caianello , E. R. (1961) . Outlin e of a theor y of
though t and thinkin g machines . Journal  of Theoret­
ical Biology,  1:204­235 .
Campbell , P. K., Johnes , K. E., Huber , R. J., Horch ,
K. W., and Normann , R. A. (1991) . A silicon ­
based , 3­dimensiona l neura l interface : manufac ­
turin g processe s for an intracortica l electrod e ar­
ray. IEEE  Transactions  on Biomedical  Engineering,
38(8):758­768 .Canny , J. (1986) . A computationa l approac h to edge
detection . IEEE  Transactions  on Pattern  Analysis
and Machine  Intelligence  (PAMI),  8:679­698 .
Canny , J. and Reif , J. (1987) . Ne w lowe r boun d
technique s for robo t motio n plannin g problems . In
IEEE  FOCS,  page s 39­48 .
Canny , J. F. (1988) . The Complexity of Robot Motion
Planning.  MIT Press , Cambridge , Massachusetts .
Carbonell , J. R. and Collins , A. M. (1973) . Natura l
semantic s in artificia l intelligence . In Proceedings
of the Third  International Joint  Conference  on Arti­
ficial  Intelligence  (IJCAI­73),  Stanford , California .
IJCAII .
Carnap , R. (1948) . On the applicatio n of inductiv e
logic . Philosophy  and PhenomenologicalResearch,
8:133­148 .
Carnap , R. (1950) . Logical  Foundations  of Proba­
bility.  Universit y of Chicag o Press , Chicago , Illi­
nois.
Cassandra , A. R., Kaelbling , L. P., and Littman ,
M. L. (1994) . Actin g optimall y in partiall y ob­
servabl e stochasti c domains . In Proceedings  of the
Twelfth  National  Conference  on Artificial Intelli­
gence  (AAAI­94),  page s 1023­1028 , Seattle , Wash ­
ington . AAA I Press .
Chakrabarti , P. P., Ghose , S., Acharya , A., and
de Sarkar , S. C. (1989) . Heuristi c searc h in restricte d
memory . Artificial  Intelligence,  41 (2): 197­122 .
Chang , C.­L . and Lee, R. C.­T . (1973) . Symbolic
Logic  andMechanicalTheoremProving.  Academi c
Press , New York .
Chapman , D. (1987) . Plannin g for conjunctiv e
goals . Artificial  Intelligence,  32(3):333­377 .
Chapuis , A. and Droz , E. (1958) . Automata:  A
Historical  and  Technological  Study.  Edition s du
Griffon , Neufchatel , Switzerland .
Charniak , E. (1972) . Toward  a Model  of Children's
Story  Comprehension.  PhD thesis , Massachusett s
Institut e of Technology .
Charniak , E. (1993) . Statistical  Language  Learning.
MIT Press .
Charniak , E. and Goldman , R. P. (1992) . A Bayesia n
mode l of plan recognition . Artificial  Intelligence,
64(l):53­79 .
Bibliograph y 865
Charniak , E. and McDermott , D. (1985) . Intro­
duction  to Artificial Intelligence.  Addison­Wesley ,
Reading , Massachusetts .
Charniak , E., Riesbeck , C., McDermott , D., and
Meehan , J. (1987) . Artificial  Intelligence  Program­
ming.  Lawrenc e Erlbau m Associates , Potomac ,
Maryland , secon d edition .
Cheeseman , P. (1985) . In defens e of probability . In
Proceedings  of the Ninth  International  Joint  Con­
ference  on Artificial  Intelligence  (IJCAI­85),  page s
1002­1009 , Los Angeles , California . Morga n Kauf ­
mann .
Cheeseman , P. (1988) . An inquir y into compute r un­
derstanding . Computational  Intelligence,  4(1):58 ­
66.
Cheeseman , P., Self , M., Kelly , J., and Stutz , J.
(1988) . Bayesia n classification . In Proceedings  of
the Seventh  National  Conference  on Artificial In­
telligence  (AAAI­88),  volum e 2, page s 607­611 ,
St. Paul , Minnesota . Morga n Kaufmann .
Chellas , B. F. (1980) . Modal  Logic:  An Introduc­
tion. Cambridg e Universit y Press , Cambridge .
Cherniak , C. (1986) . Minimal  Rationality.
MIT Press , Cambridge , Massachusetts .
Chierchia , G. and McConnell­Ginet , S. (1990) .
Meaning  and Grammar.  MIT Press .
Chitrao , M. and Grishman , R. (1990) . Statistica l
parsin g of messages . In Proceedings  of DARPA
Speech  and Natural  Language Processing.  Morga n
Kaufman : New York .
Chomsky , N. (1956) . Thre e model s for the descrip ­
tion of language . IRE  Transactions  on Information
Theory,  2(3): 113­124 .
Chomsky , N. (1957) . Syntactic  Structures.  Mouton ,
The Hagu e and Paris .
Chomsky , N. (1965) . Aspects  of the Theory  of Syn­
tax. MIT Press , Cambridge , Massachusetts .
Chomsky , N. (1980) . Rule s and representations .
The Behavioral  and Brain  Sciences,  3:1­61 .
Chung , K. L. (1979) . Elementary  Probability  The­
ory with  Stochastic  Processes.  Springer­Verlag ,
Berlin , third edition .
Church , A. (1936) . A  note on the Entschei ­
dungsproblem . Journal of Symbolic Logic,  1:40­4 1
and 101­102 .Church , A. (1941) . The  Calculi  of Lambda­
Conversion.  Princeto n Universit y Press , Princeton ,
New Jersey .
Church , K. (1988) . A stochasti c parts progra m and
noun phras e parse r for unrestricte d texts . In Pro­
ceedings  of the Second  Conference  on Applied Nat­
ural Language  Processing,  Austin , Texas .
Church , K. and Patil , R. (1982) . Copin g with syn­
tactic ambiguit y or how to put the bloc k in the box
on the table . American  Journal  of Computational
Linguistics,  8(3­4) : 139­149 .
Churchland , P. M. (1979) . Scientific  Realism  and
the Plasticity of Mind.  Cambridg e Universit y Press ,
Cambridge .
Churchland , P. M. and Churchland , P. S. (1982) .
Functionalism , qualia , and intentionality . In Biro ,
J. I. and Shahan , R. W., editors , Mind,  Brain  and
Function:  Essays  in the Philosophy of  Mind,  page s
121­145 . Universit y of Oklahom a Press , Norman ,
Oklahoma .
Churchland , P. S. (1986) . Neurophilosophy:
Toward  a Unified  Science  of the Mind­Brain.
MIT Press , Cambridge , Massachusetts .
Clark , K. L. (1978) . Negatio n as failure . In Gallaire ,
H. and Minker , J., editors , Logic  and Data  Bases,
pages 293­322 . Plenum , New York .
Clark , K. L. and Gregory , S. (1986) . PARLOG : par­
allel programmin g in logic . ACM  Transactions  on
Programming  Languages,  8:1­49 .
Clark , R. (1992) . The selectio n of syntacti c knowl ­
edge . Language  Acquisition,  2(2):83­149 .
Clarke , M. R. B., edito r (1977) . Advances  in Com­
puter  Chess  I. Edinburg h Universit y Press , Edin ­
burgh , Scotland .
Clocksin , W. F. and Mellish , C. S. (1987) . Pro­
gramming  in Prolog.  Springer­Verlag , Berlin , third
revise d and extende d edition .
Clowes , M. B. (1971) . On seein g things . Artificial
Intelligence,  2(1):79­116 .
Cobham , A. (1964) . The intrinsi c computationa l
difficult y of functions . In Bar­Hillel , Y., editor ,
Proceedings  of the 1964 International  Congress  for
Logic,  Methodology,  and  Philosophy of  Science,
pages 24—30 . Elsevier/North­Holland .
Cohen , J. (1966) . Human  Robots in  Myth  and  Sci­
ence.  Alie n and Unwin , London .
866 Bibliograph y
Cohen , J. (1988) . A view of the origin s and de­
velopmen t of PROLOG . Communications  of the
Association/or  Computing  Machinery,  31:26­36 .
Cohen , P., Morgan , J., and Pollack , M. (1990) . In­
tentions  in Communication.  MIT Press .
Cohen , P. and Perrauit , C. R. (1979) . Element s of a
plan­base d theor y of speec h acts. Cognitive Science,
3(3): 177­212 .
Cohen , P. R. and Feigenbaum , E. A., editor s (1982) .
The Handbook of  Artificial  Intelligence,  volum e 3:
HeurisTec h Press and Willia m Kaufmann , Stanford ,
Californi a and Los Altos , California .
Colmerauer , A. (1975) . Le s grammaire s
de metamorphose . Technica l report , Group e
d'lntelligenc e Artificielle , Universit e de Marseille ­
Luminy.Translate d int o Englis h as Colmer ­
auer (1978) .
Colmerauer , A. (1978) . Metamorphosi s gram ­
mars . In Bole , L., editor , Natural  Language
Communication  with  Computers.  Springer­Verlag ,
Berlin.Englis h translatio n of Colmeraue r (1975) .
Colmerauer , A. (1985) . Prolo g in 10 figures . Com­
munications  of the Association  for Computing  Ma­
chinery,  28(12) : 1296­1310 .
Colmerauer , A. (1990) . Prolo g III as it actuall y is.
In Warren , D. H. D. and Szeredi , P., editors , Logic
Programming:  Proceedings  of the Seventh Interna­
tional  Conference, page 766, Jerusalem . MIT Press .
Colmerauer , A., Kanoui , H., Pasero , R., and
Roussel , P. (1973) . U n system e de commu ­
nicatio n homme­machin e en Franjais . Rap ­
port, Group e d'lntelligenc e Artificielle , Universit e
d'Aix­Marseill e II.
Colomb , R. M. (1991) . Enhancin g unificatio n in
PROLO G throug h claus e indexing . Journal of Logic
Programming,  10(1):23^4 .
Condon , E. U., Tawney , G. L., and Derr , W. A.
(1940) . Machin e to play gam e of Nim . U.S . Paten t
2,215,544 , Unite d State s Paten t Office , Washington ,
D.C.
Condon , J. H. and Thompson , K. (1982) . Belle ches s
hardware . In Clarke , M. R. B., editor , Advances  in
Computer  Chess 3,  page s 45­54 . Pergamon , New
York .
Cook , S. A. (1971) . The complexit y of theorem ­
provin g procedures . In Proceedings  of the 3rd An­
nual ACM  Symposium  on Theory  of Computing,
page s 151­158 , New York .Cooper , G. and Herskovits , E. (1992) . A Bayesia n
metho d for the inductio n of probabilisti c network s
from data . Machine  Learning,  9:309­347 .
Cooper , G. F. (1990) . The computationa l complex ­
ity of probabilisti c inferenc e usin g Bayesia n belie f
networks . Artificial  Intelligence,  42:393—405 .
Copeland , J. (1993) . Artificial  Intelligence:
A Philosophical  Introduction.  Blackwell , Oxford .
Cormen , T. H., Leiserson , C. E., and Rivest , R. R.
(1990) . Introduction  to Algorithms.  MI T Press ,
Cambridge , Massachusetts .
Covington , M. A. (1994) . Natural  Language  Pro­
cessing  for Prolog Programmers.  Prentice­Hall , En­
glewoo d Cliffs , New Jersey .
Cowan , J. D. and Sharp , D. H. (1988a) . Neura l nets.
Quarterly  Reviews of Biophysics,  21 :365 ­ 427.
Cowan , J. D. and Sharp , D. H. (1988b) . Neura l nets
and artificia l intelligence . Daedalus,  117:85­121 .
Cox, R. T. (1946) . Probability , frequency , and rea­
sonabl e expectation . American  Journal  of Physics,
Cragg , B. G. and Temperley , H. N. V. (1954) . The
organizatio n of neurones : A cooperativ e analogy .
EEG  and Clinical  Neurophysiology,  6:85­92 .
Cragg , B. G. and Temperley , H. N. V. (1955) . Mem ­
ory: Th e analog y with ferromagneti c hysteresis .
firain,78(II):304­316 .
Craik , K. J. W. (1943) . The  Nature of  Explanation.
Cambridg e Universit y Press , Cambridge .
Crevier , D. (1993) . AI: The Tumultuous  History  of
the Search  for Artificial  Intelligence.  Basi c Books ,
New York .
Crockett , L. (1994) . The  Turing  Testandthe  Frame
Problem:  AI's  Mistaken  Understanding  of Intelli­
gence.  Ablex , Norwood , New Jersey .
Cullingford , R. E. (1981) . Integratin g knowledg e
source s for compute r 'understanding ' tasks . IEEE
Transactions  on Systems,  Man  and  Cybernetics,
SMC­11 .
Currie , K. W. and Tate , A. (1991) . O­Plan : the
Open Plannin g Architecture . Artificial  Intelligence,
52(l):49­86 .
Bibliograph y 867
Curry , H. B. and Feys , R. (1958) . Combinatory
Logic,  volum e 1. Elsevier/North­Holland , Amster ­
dam, London , New York .
Cybenko , G. (1988) . Continuou s value d neura l net­
work s with two hidde n layer s are sufficient . Techni ­
cal report , Departmen t of Compute r Science , Tuft s
University , Medford , Massachusetts .
Cybenko , G. (1989) . Approximatio n by superpo ­
sition s of a sigmoida l function . Mathematics  of
Controls,  Signals,  and Systems,  2:303­314 .
Dagum , P. and Luby , M. (1993) . Approximatin g
probabilisti c inferenc e in Bayesia n belie f network s
is NP­hard . Artificial  Intelligence,  60(1) : 141­153 .
Dahl , O.­J. , Myrhaug , B., and Nygaard , K. (1970) .
(Simul a 67) commo n base language . Technica l Re­
port N. S­22 , Nors k Regnesentra l (Norwegia n Com ­
putin g Center) , Oslo .
Dantzig , G. B. (1960) . On the significanc e of solv­
ing linea r programmin g problem s with some intege r
variables . Econometrica,  28:30­44 .
Darwiche , A. Y. and Ginsberg , M. L. (1992) . A
symboli c generalizatio n of probabilit y theory . In
Proceedings  of the Tenth  National  Conference  on
Artificial  Intelligence  (AAAI­92),  page s 622­627 ,
San Jose , California . AAA I Press .
Davidson , D. (1980) . Essays  on Actions  and Events.
Oxfor d Universit y Press , Oxford .
Davies , T. (1985) . Analogy . Informa l Not e IN­
CSLI­85­4 , Cente r for the Stud y of Languag e and
Informatio n (CSLI) , Stanford , California .
Davies , T. R. and Russell , S. J. (1987) . A logi­
cal approac h to reasonin g by analogy . In Proceed­
ings of the Tenth  InternationalJoint Conference  on
Artificial  Intelligence  (IJCAI­87),  volum e 1, page s
264­270 , Milan , Italy . Morga n Kaufmann .
Davis , E. (1986) . Representing  and Acquiring  Ge­
ographic  Knowledge.  Pitma n and Morga n Kauf ­
mann , Londo n and San Mateo , California .
Davis , E. (1990) . Representations  of Commonsense
Knowledge.  Morga n Kaufmann , San Mateo , Cali ­
fornia .
Davis , M. (1957) . A compute r progra m for Pres ­
burger' s algorithm . In Robinson , A., editor , Prov­
ing Theorems,  (as Done  by Man,  Logician,  or Ma­
chine),  page s 215­233 , Cornel l University , Ithaca ,New York . Communication s Researc h Division , In­
stitut e for Defens e Analysis.Summarie s of Talk s
Presente d at the 1957 Summe r Institut e for Sym ­
bolic Logic . Secon d edition ; publicatio n date is
1960.
Davis , M. and Putnam , H. (1960) . A computin g
procedur e for quantificatio n theory . Journal  of the
Association  for Computing Machinery,  7(3):201 ­
215.
Davis , R. (1980) . Meta­rules : reasonin g abou t con­
trol. Artificial  Intelligence,  15(3):179­222 .
Davis , R. and Lenat , D. B. (1982) . Knowledge­
Based  Systems  in Artificial  Intelligence.  McGraw ­
Hill, New York .
Dayan , P. (1992) . Th e convergenc e of TDA for
genera l A. Machine  Learning,  8(3^1):341­362 .
de Dombal , F. T, Leaper , D. J., Horrocks , J. C.,
and Staniland , J. R. (1974) . Huma n and computer ­
aided diagnosi s of abdomina l pain : Furthe r repor t
with emphasi s on performanc e of clinicians . British
Medical  Journal,  1:376­380 .
de Dombal , F. T, Staniland , J. R., and Clamp , S. E.
(1981) . Geographica l variatio n in diseas e presenta ­
tion. Medical  Decision  Making,  1:59­69 .
de Finetti , B. (1937a) . Foresight : Its logica l laws ,
its subjectiv e sources . In Kyburg , H. E. and Smok ­
ier, H. E., editors , Studies  in Subjective  Probability,
pages 55­118 . Krieger , New York .
de Finetti , B. (1937b) . Le prevision : ses lois
logiques , ses source s subjectives . Ann.  Inst.
Poincare,  7:l­68.Translate d into Englis h as De­
Finetti(1937a) .
de Groot , A. D. (1946) . Het  Denken  van den
Schaker.  Elsevier/North­Holland , Amsterdam ,
London , New York.Translate d as DeGroo t (1978) .
de Groot , A. D. (1978) . Thought  and Choice in
Chess.  Mouton , The Hagu e and Paris , secon d edi­
tion.
de Kleer , J. (1975) . Qualitativ e and quantitativ e
knowledg e in classica l mechanics . Technica l Repor t
AI­TR­352 , MIT Artificia l Intelligenc e Laboratory .
de Kleer , J. (1986) . An assumption­base d TMS .
Artificial  Intelligence,  28(2) : 127­162 .
de Kleer , J. (1986a) . Extendin g the ATMS . Artificial
Intelligence,  28(2) : 163­196 .
de Kleer , J. (1986b) . Proble m solvin g with the
ATMS . Artificial  Intelligence,  28(2) : 197­224 .
Bibliograph y
de Kleer , J. and Brown , J. S. (1985) . A qualitativ e
physic s base d on confluences . In Hobbs , J. R. and
Moore , R. C., editors , Formal  Theories  of the Com­
monsense  World,  chapte r 4, page s 109­183 . Ablex ,
Norwood , New Jersey .
de Kleer , J., Doyle , J., Steele , G. L.,'an d Sussman ,
G. J. (1977) . AMORD : explici t contro l of reason ­
ing. SIGPLAN  Notices,  12(8) : 116­125 .
De Morgan , A. (1864) . On the syllogis m IV and
on the logic of relations . Cambridge  Philosophical
Transactions,  x:331­358 .
De Raedt , L. (1992) . Interactive  Theory  Revision:
An Inductive  Logic  Programming  Approach.  Aca ­
demi c Press , New York .
Dean , T. and Boddy , M. (1988) . An analysi s of
time­dependen t planning . In Proceedings  of the Sev­
enth National  Conference  on Artificial Intelligence
(AAAI­88),  page s 49­54 , St. Paul , Minnesota . Mor ­
gan Kaufmann .
Dean , T., Firby , J., and Miller , D. (1990) . Hier ­
archica l plannin g involvin g deadlines , trave l time ,
and resources . Computational  Intelligence,  6(1) .
Dean , T., Kaelbling , L. P., Kirman , J., and Nichol ­
son, A. (1993) . Plannin g with deadline s in stochas ­
tic domains . In Proceedings  of the Eleventh  Na­
tional  Conference  on Artificial Intelligence  (AAAI­
93), page s 574­579 , Washington , D.C. AAA I Press .
Dean , T. and Kanazawa , K. (1989) . A mode l for
reasonin g abou t persistenc e and causation . Compu­
tational  Intelligence,  5(3): 142­150 .
Dean , T. L. and Wellman , M. P. (1991) . Planning
and Control.  Morga n Kaufmann , San Mateo , Cali ­
fornia .
Debreu , G. (1960) . Topologica l method s in cardi ­
nal utilit y theory . In Arrow , K. J., Karlin , S., and
Suppes , P., editors , Mathematical  Methods  in the
Social  Sciences,  1959.  Stanfor d Universit y Press ,
Stanford , California .
Dechter , R. and Pearl , J. (1985) . Generalize d best­
first searc h strategie s and the optimalit y of A*. Jour­
nal of the Association  for Computing  Machinery,
32(3):505­536 .
DeGroot , M. H. (1970) . Optimal  Statistical  Deci­
sions.  McGraw­Hill , New York .
DeGroot , M. H. (1989) . Probability  and Statistics.
Addison­Wesley , Reading , Massachusetts , secon d
edition.Reprinte d with corrections .DeJong , G. (1981) . Generalization s base d on expla ­
nations . In Proceedings  of the Seventh  International
Joint  Conference  on Artificial  Intelligence  (IJCAI­
81), page s 67­69 , Vancouver , Britis h Columbia .
Morga n Kaufmann .
DeJong , G. and Mooney , R. (1986) . Explanation ­
based learning : An alternativ e view . Machine
Learning,  1:145­176 .
Dempster , A., Laird , N., and Rubin , D. (1977) .
Maximu m likelihoo d from incomplet e data via the
EM algorithm . Journal  of the Royal  Statistical  So­
ciety,  39 (Serie s B): 1­38.
Dempster , A. P. (1968) . A  generalizatio n of
Bayesia n inference . Journal  of the Royal  Statistical
Society,  30 (Serie s B):205­247 .
Dennett , D. C. (1969) . Content  and Consciousness.
Routledg e and Kega n Paul , London .
Dennett , D. C. (1971) . Intentiona l systems . The
Journal  of Philosophy,  68(4):87­106 .
Dennett , D. C. (1978a) . Brainstorms:  Philosoph­
ical Essays on  Mind  and  Psychology.  MI T Press ,
Cambridge , Massachusetts , first edition .
Dennett , D. C. (1978b) . Wh y you can' t mak e a
compute r that feels pain . Synthese,  38(3) .
Dennett , D. C. (1984) . Cognitiv e wheels : the fram e
proble m of AI. In Hookway , C., editor , Minds,
Machines,  and  Evolution:  Philosophical  Studies,
page s 129­151 . Cambridg e Universit y Press , Cam ­
bridge .
Dennett , D. C. (1986) . Th e mora l first aid man ­
ual. Tanne r lecture s on huma n values , Universit y of
Michigan .
Deo, N. and Pang , C. (1982) . Shortes t path al­
gorithms : Taxonom y and annotation . Technica l
Repor t CS­80­057 , Compute r Scienc e Department ,
Washingto n State University .
Descotte , Y. and Latombe , J. C. (1985) . Mak ­
ing compromise s amon g antagonis t constraint s in
a planner . Artificial  Intelligence,  27:183­217 .
Devanbu , P., Brachman , R. J., Selfridge , P. G.,
and Ballard , B. W. (1991) . LaSSIE : a knowledge ­
based softwar e informatio n system . Communica­
tions  of the Association  for Computing  Machinery,
34(5):34­49 .
Bibliograph y 869
Dickmanns , E. D. and Zapp , A. (1987) . Au ­
tonomou s high speed road vehicl e guidanc e by com­
puter vision . In Isermann , R., editor , Automatic
Control—World  Congress,  1987:  Selected  Papers
from the 10th  Triennial  World  Congress  of the In­
ternational  Federation of Automatic Control,  page s
221­226 , Munich , Germany . Pergamon .
Dietterich , T. G. (1990) . Machin e learning . Annual
Review  of Computer  Science,  4.
Dijkstra , E. W. (1959) . A note on two problem s in
connexio n with graphs . Numerische  Mathematik,
1:269­271 .
Dincbas , M. and LePape , J.­P. (1984) . Metacontro l
of logic program s in METALOG . In Proceedings
of the International  Conference  on Fifth­Generation
Computer  Systems,  Tokyo . Elsevier/North­Holland .
Dingwell , W. O. (1988) . The evolutio n of huma n
communicativ e behavior . In Newmeyer , F. J., ed­
itor, Linguistics:  The  Cambridge  Survey,  Vol.  Ill,
pages 274­313 . Cambridg e Universit y Press .
Doran , J. and Michie , D. (1966) . Experiment s with
the grap h traverse r program . Proceedings  of the
Royal  Society  of London,  294, Serie s A:235­259 .
Dowty , D., Wall , R., andPeters , S. (1991) . Introduc­
tion to Montague Semantics.  D. Reidel , Dordrecht ,
The Netherlands .
Doyle , J. (1979) . A truth maintenanc e system.  Artifi­
cial Intelligence,  12(3):231­272.Reprintedi n Web ­
ber and Nilsso n (1981) .
Doyle , J. (1980) . A mode l for deliberation , ac­
tion, and introspection . Technica l Repor t AI­
TR­581 , Artificia l Intelligenc e Laboratory , Mas ­
sachusett s Institut e of Technology , Cambridge ,
Massachusetts.Republishe d PhD dissertation .
Doyle , J. (1983) . Wha t is rationa l psychology ? To­
ward a moder n menta l philosophy . AI Magazine,
4(3):50­53 .
Doyle , J. and Patil , R. S. (1991) . Tw o these s of
knowledg e representation : languag e restrictions ,
taxonomi c classification , and the utilit y of represen ­
tation services . Artificial  Intelligence,  48(3):261 ­
297.
Drabble , B. (1990) . Missio n schedulin g for space ­
craft: Diarie s of T­SCHED . In Expert  Planning  Sys­
tems,  page s 76­81 . Institut e of Electrica l Engineers .Draper , D., Hanks , S., and Weld , D. (1994) . Prob ­
abilisti c plannin g with informatio n gatherin g and
contingen t execution . In Proceedings  2nd AIPS,
San Mateo , California . Morga n Kaufmann .
Dreyfus , H. L. (1972) . What  Computers  Can't  Do:
A Critique  of Artificial  Reason.  Harpe r and Row ,
New York , first edition .
Dreyfus , H. L. (1979) . What  Computers  Can't  Do:
The Limits  of Artificial  Intelligence.  Harpe r and
Row, New York , revise d edition .
Dreyfus , H. L. (1992) . What  Computers  Still Can't
Do: A Critique  of Artificial  Reason.  MI T Press ,
Cambridge , Massachusetts .
Dreyfus , H. L. and Dreyfus , S. E. (1986) . Mind
over Machine:  The  Power of  Human  Intuition  and
Expertise  in the Era of the Computer.  Blackwell ,
Oxford.Wit h Tom Athanasiou .
Dreyfus , S. E. (1969) . A n appraisa l of som e
shortest­path s algorithms . Operations  Research,
17:395­412 .
Dubois , D. and Prade , H. (1994) . A surve y of belie f
revisio n and updatin g rule s in variou s uncertaint y
models . International  Journal of  Intelligent  Sys­
tems,  9(l):6l­\00.
Duda , R., Gaschnig , I., and Hart , P. (1979) . Mode l
desig n in the Prospecto r consultan t syste m for min­
eral exploration . In Michie , D., editor , Expert  Sys­
tems in the Microelectronic  Age,  page s 153­167 .
Edinburg h Universit y Press , Edinburgh , Scotland .
Dyer , M . (1983) . In­Depth  Understanding.
MIT Press , Cambridge , Massachusetts .
Dzeroski , S., Muggleton , S., and Russell , S. I.
(1992) . PAC­learnabilit y of determinat e logi c
programs . In Proceedings  of the Fifth Annual
ACM  Workshop  on Computational  Learning  Theory
(COLT­92),  Pittsburgh , Pennsylvania . ACM Press .
Barley , I. (1970) . An efficien t context­fre e parsin g
algorithm . Communications  of the Association  for
Computing  Machinery,  13(2):94­102 .
Ebeling , C. (1987) . All the Right  Moves.  MIT Press ,
Cambridge , Massachusetts .
Edmonds , J. (1962) . Cover s and packing s in a fam­
ily of sets. Bulletin  of the American  Mathematical
Society,  68:494­499 .
Edmonds , J. (1965) . Paths , trees , and flowers . Cana­
dian Journal  of Mathematics,  17:449­467 .
870 Bibliograph y
Edwards , P., edito r (1967) . The  Encyclopedia  of
Philosophy.  Macmillan , London .
Elkan , C. (1992) . Reasonin g abou t actio n in first ­
order logic . In Proceedings  of the Conference  of
the Canadian  Society  for Computational  Studies  of
Intelligence,  Vancouver , Britis h ColuYnbia .
Elkan , C. (1993) . The paradoxica l succes s of fuzz y
logic . In Proceedings  of the Eleventh  National  Con­
ference  on Artificial Intelligence  (AAAI­93),  page s
698­703 , Washington , D.C. AAA I Press .
Empson , W. (1953) . Seven Types of Ambiguity.  New
Directions .
Enderton , H. B. (1972) . A Mathematical  Introduc­
tion to Logic.  Academi c Press , New York .
Engelberger , J. F. (1980) . Robotics  in Practice.
Amacom , New York .
Engelberger , J. F. (1989) . Robotics  in Service.
MIT Press , Cambridge , Massachusetts .
Erman , L. D., Hayes­Roth , P., Lesser , V. R., and
Reddy , D. R. (1980) . Th e HEARSAY­1 1 speech ­
understandin g system : Integratin g knowledg e to re­
solve uncertainty . Computing  Surveys,  12(2):213 ­
253.Reprinte d in Webbe r and Nilsso n (1981) .
Ernst , H. A. (1961) . MH­l,  a Computer­Operated
Mechanical  Hand.  PhD thesis , Massachusett s In­
stitut e of Technology , Cambridge , Massachusetts .
Erol, K., Hendler , J., and Nau , D. S. (1994) . HTN
planning : complexit y and expressivity . In Proceed­
ings of the Twelfth  National Conference  on Artificial
Intelligence  (AAAI­94),  Seattle , Washington . AAA I
Press .
Etzioni , O. (1989) . Tractabl e decision­analyti c con­
trol. In Proc.  of 1st International  Conference  on
Knowledge  Representation  and  Reasoning,  page s
114­125 , Toronto , Ontario .
Etzioni , O., Hanks , S., Weld , D., Draper , D., Lesh ,
N., and Williamson , M. (1992) . An approac h to
plannin g with incomplet e information . In Proceed­
ings of the 3rd International  Conference  on Princi­
ples of Knowledge  Representation and  Reasoning.
Evans , T. G. (1968) . A progra m for the solutio n of
a class of geometric­analog y intelligence­tes t ques ­
tions . In Minsky , M. L., editor , Semantic  Informa­
tion Processing,  page s 271­353 . MIT Press , Cam ­
bridge , Massachusetts .Fahlman , S.E. (1974) . A plannin g syste m for robo t
constructio n tasks . Artificial  Intelligence,  5(1):1 ­
49.
Fahlman , S. E. (1979) . NETL:  A System  for
Representing  and  Using Real­World  Knowledge.
MIT Press , Cambridge , Massachusetts .
Farhat , N. H., Psaltis , D., Prata , A., and Paek , E.
(1985) . Optica l implementatio n of the Hopfiel d
model . Applied  Optics,  24:1469­1475.Reprinte d
in Anderso n and Rosenfel d (1988) .
Faugeras , O. (1993) . Three­Dimensional  Computer
Vision:  A Geometric  Viewpoint.  MIT Press , Cam ­
bridge , Massachusetts .
Feigenbaum , E. and Shrobe , H. (1993) . Th e
Japanes e nationa l fift h generatio n project : intro ­
duction , survey , and evaluation . Future  Generation
Computer  Systems,  9(2):105­117 .
Feigenbaum , E. A. (1961) . The simulatio n of ver­
bal learnin g behavior . Proceedings  of the Western
Joint  Computer  Conference,  19:121­13 1 .Reprinte d
in (Feigenbau m and Feldman , 1963 , pp. 297­309) .
Feigenbaum , E. A., Buchanan , B. G., and Leder ­
berg, J. (1971) . On generalit y and proble m solving :
A case stud y usin g the DENDRA L program . In
Meltzer , B. and Michie , D., editors , Machine  In­
telligence  6, page s 165­190 . Edinburg h Universit y
Press , Edinburgh , Scotland .
Feigenbaum , E. A. and Feldman , J., editor s (1963) .
Computers  and Thought.  McGraw­Hill , New York .
Feldman , J. A. and Sproull , R. F. (1977) . Deci ­
sion theor y and artificia l intelligenc e II: The hungr y
monkey . Technica l report , Compute r Scienc e De­
partment , Universit y of Rochester .
Feldman , J. A. and Yakimovsky , Y. (1974) . Decisio n
theor y and artificia l intelligenc e I: Semantics­base d
regio n analyzer . Artificial  Intelligence,  5(4):349 ­
371.
Fikes , R. E., Hart , P. E., and Nilsson , N. J. (1972) .
Learnin g and executin g generalize d robo t plans . Ar­
tificial  Intelligence,  3(4):251­288 .
Fikes , R. E. and Nilsson , N. J. (1971) . STRIPS : a
new approac h to the applicatio n of theore m prov ­
ing to proble m solving . Artificial  Intelligence,  2(3­
4): 189­208 .
Fikes , R. E. and Nilsson , N. J. (1993) . STRIPS , a
retrospective . Artificial  Intelligence,  59(l­2):227 ­
232.
Bibliograph y 871
Findlay , J. N. (1941) . Time : A treatmen t of som e
puzzles . Australasian  Journal  of Psychology  and
Philosophy,  19(3):216­235 .
Fischer , M. J. and Ladner , R. E. (1977) . Preposi ­
tiona l moda l logi c of programs . In Proceedings  of
the 9th ACM  Symposium  on the Theory  of Comput­
ing, page s 286­294 .
Fisher , R. A. (1922) . On the mathematica l founda ­
tions of theoretica l statistics . Philosophical  Trans­
actions  of the Royal  Society  of London,  Serie s A
222:309­368 .
Floyd , R. W. (1962a) . Algorith m 96: Ancestor .
Communications  of the Association  for Computing
Machinery,  5:344­345 .
Floyd , R. W. (1962b) . Algorith m 97: Shortes t path.
Communications  of the Association  for Computing
Machinery,  5:345 .
Fodor , J. A. (1980) . Searl e on wha t only brain s can
do. Behavioral  and Brain  Sciences,  3:43 l­432.Pee r
commentar y on Searl e (1980) .
Fodor , J. A. (1983) . The Modularity  of Mind: An Es­
say on Faculty  Psychology.  MIT Press , Cambridge ,
Massachusetts .
Forbus , K. D. (1985) . The role of qualitativ e dy­
namic s in naive physics . In Hobbs , J. R. and Moore ,
R. C., editors , Formal Theories  of the Commonsense
World,  chapte r 5, page s 185­226 . Ablex , Norwood ,
New Jersey .
Forbus , K. D. and de Kleer , J. (1993) . Building
Problem  Solvers.  MI T Press , Cambridge , Mas ­
sachusetts .
Forsyth , D. and Zisserman , A. (1991) . Reflection s
on shading . IEEE  Transactions on  Pattern  Analysis
and Machine  Intelligence  (PAMI),  13(7):671­679 .
Fox, M. S. (1990) . Constraint­guide d scheduling :
a shor t histor y of researc h at CMU . Computers  in
Industry,  14(l­3):79­88 .
Fox, M. S., Alien , B., and Strohm , G. (1981) . Job
shop scheduling : an investigatio n in constraint ­
based reasoning . In Proceedings  of the Seventh
International  Joint  Conference  on Artificial Intel­
ligence  (IJCAI­81),  Vancouver , Britis h Columbia .
Morga n Kaufmann .
Fox, M. S. and Smith , S. F. (1984) . Isis : a
knowledge­base d syste m for factor y scheduling .
Expert  Systems,  l(l):25­49 .Frean , M. (1990) . The upstar t algorithm : A metho d
for constructin g and trainin g feedforwar d neura l
networks . Neural  Computation,  2:198­209 .
Frege , G. (1879) . Begriffsschrift,  eine  der  arith­
metischen  nachgebildete  Formelsprache  des reinen
Denkens.  Halle , Berlin.Reprinte d in Englis h trans ­
lation in van Heijenoor t (1967) .
Friedberg , R., Dunham , B., and North , T. (1959) .
A learnin g machine : Par t II. IBM  Journal  of Re­
search  and Development,  3(3):282­287 .
Friedberg , R.M . (1958) . A learnin g machine : Parti .
IBM Journal,  2:2­13 .
Fu, K.­S . and Booth , T. L. (1986a) . Grammat ­
ical inference : Introductio n and survey—par t I.
IEEE  Transactions  on Pattern  Analysis  and  Ma­
chine  Intelligence,  PAMI­8(3):343­359.Reprinte d
from IEEE  Trans.  on Systems, Man,  and  Cybernet­
ics, Vol. SMC­5 , No. 1, Januar y 1975 .
Fu, K.­S . and Booth , T. L. (1986b) . Grammat ­
ical inference : Introductio n and survey—par t II.
IEEE  Transactions  on Pattern  Analysis  and  Ma­
chine  Intelligence,  PAMI­8(3):360­375.Reprinte d
from IEEE Trans . on Systems , Man , and Cybernet ­
ics, Vol. SMC­5 , No. 4, Januar y 1975 .
Fuchs , J. J., Gasquet , A., Olalainty , B., and Cur­
rie, K. W. (1990) . PlanERS­1 : An exper t plannin g
syste m for generatin g spacecraf t missio n plans . In
First  International  Conference  on Expert  Planning
Systems,  page s 70­75 , Brighton , Unite d Kingdom .
Institut e of Electrica l Engineers .
Fung , R. and Chang , K. C. (1989) . Weightin g
and integratin g evidenc e for stochasti c simulatio n
in Bayesia n networks . In Proceedings  of the Fifth
Conference  on Uncertainty  in Artificial  Intelligence
(UAI­89),  Windsor , Ontario . Morga n Kaufmann .
Furukawa , K. (1992) . Summar y of basic researc h
activitie s of the FGC S project . In Fifth  Generation
Computer  Systems  1992,  volum e 1, page s 20­32 ,
Tokyo . IOS Press .
Furuta , K., Ochiai , T, and Ono , N. (1984) . Attitud e
contro l of a triple inverte d pendulum . International
Journal  of Control,  39(6) : 1351­1365 .
Gabbay , D. M. (1991) . Abductio n in labelle d de­
ductiv e systems : A conceptua l abstract . In Kruse ,
R. andSiegel , P., editors , Symbolic  and Quantitative
Approaches  to Uncertainty:  Proceedings  of Euro­
pean  Conference  ECSQAU,  page s 3­11 . Springer ­
Verlag .
872 Bibliograph y
Gallaire , H. and Minker , J., editor s (1978) . Logic
and Databases.  Plenum , New York .
Gallier , J. H. (1986) . Logic  for Computer  Science:
Foundations  of Automatic  Theorem  Proving.  Harpe r
and Row , New York .
Gamba , A., Gamberini , L., Palmieri , G., and Sanna ,
R. (1961) . Furthe r experiment s with PAPA . Nuovo
Cimento  Supplement,  20(2):221­231 .
Carding , J. (1992) . Shap e from textur e for smoot h
curve d surface s in perspectiv e projection . Journal
of Mathematical Imaging and Vision,  2(4):327­350 .
Gardner , M. (1968) . Logic  Machines,  Diagrams
and Boolean  Algebra.  Dover , New York .
Garey , M. R. and Johnson , D. S. (1979) . Computers
and Intractability.  W. H. Freeman , New York .
Garside , R., Leech , F, and Sampson , G., editor s
(1987) . The  Computational  Analysis  of English.
Longman .
Gaschnig,J . (1979) . Performanc e measuremen t and
analysi s of certai n searc h algorithms . Technica l Re­
port CMU­CS­79­124 , Compute r Scienc e Depart ­
ment , Carnegie­Mello n University .
Gauss , K. F. (1809) . Theoria  Motus  Corporum
Coelestium  in Sectionibus  Conicis  Solem  Ambien­
tium.  Sumtibu s F. Perthe s et I. H. Besser , Hamburg .
Gazdar , G. (1989) . COMI T =>* PATR . In Wilks , Y,
editor , Theoretical  Issues  in Natural  Language  Pro­
cessing,  Potomac , Maryland . Lawrenc e Erlbau m
Associates .
Gazdar , G., Klein , E., Pullum , G., and Sag , I.
(1985) . Generalized  Phrase  Structure  Grammar.
Blackwell , Oxford .
Geffner , H. (1992) . Default  Reasoning:  Causal
and Conditional  Theories.  MIT Press , Cambridge ,
Massachusetts .
Gelb , A. (1974) . Applied  Optimal  Estimation.
MIT Press , Cambridge , Massachusetts .
Gelernter , H. (1959) . Realizatio n of a geometry ­
theore m provin g machine . In Proceedings  of an
International  Conference on  Information  Process­
ing, page s 273­282 , Paris . UNESC O House .
Gelernter , H., Hansen , J. R., and Gerberich , C. L.
(1960) . A FORTRAN­compile d list processin g lan­
guage . Journal  of the Association  for Computing
Machinery,  7(2):87­101 .Gelfond , M. and Lifschitz , V. (1988) . Compilin g cir­
cumscriptiv e theorie s into logic programs . In Rein ­
frank , M., de Kleer , J., Ginsberg , M. L., and Sande ­
wall, E., editors , Non­Monotonic  Reasoning:  2nd
International  Workshop  Proceedings,  page s 74­99 ,
Grassau , Germany . Springer­Verlag .
Gelfond , M. and Lifschitz , V. (1991) . Classi ­
cal negatio n in logi c program s and disjunctiv e
databases . New  Generation Computing,  9(3 ­
4):365­385 .
Genesereth , M. R. (1984) . The use of desig n de­
scription s in automate d diagnosis . Artificial  Intelli­
gence,  24(1­3):411^436 .
Genesereth , M. R. and Ketchpel , S. P. (1994) . Soft ­
ware agents . Communications  of the Association
for Computing  Machinery,  37(7) .
Genesereth , M. R. and Nilsson , N. J. (1987) . Log­
ical Foundations  of Artificial  Intelligence.  Morga n
Kaufmann , San Mateo , California .
Genesereth , M. R. and Smith , D. (1981) . Meta­leve l
architecture . Mem o HPP­8 1 ­6, Compute r Scienc e
Department , Stanfor d University , Stanford , Califor ­
nia.
Gentner , D. (1983) . Structur e mapping : A theo ­
retica l framewor k for analogy . Cognitive  Science,
7:155­170 .
Gentzen , G. (1934) . Untersuchunge n uber das logis ­
che Schliessen . MathematischeZeitschrift,  39:176 ­
210,405­431 .
Georgeff , M. P. and Lansky , A. L., editor s (1986) .
Reasoning  about  Actions  and Plans:  Proceedings
of the 1986  Works  hop,Timberline,  Oregon . Morga n
Kaufmann .
Gibson , J. J. (1950) . The  Perception  of the Visual
World.  Houghto n Mifflin , Boston , Massachusetts .
Gibson , J. J. (1979) . The  Ecological  Approach  to
Visual  Perception.  Houghto n Mifflin , Boston , Mas ­
sachusetts .
Gibson , J. J., Olum , P., and Rosenblatt , F. (1955) .
Paralla x and perspectiv e durin g aircraf t landings .
American  Journal  of Psychology,  68:372­385 .
Gilmore , P. C. (1960) . A proo f metho d for quantifi ­
catio n theory : Its justificatio n and realization . IBM
Journal  of Research  and Development,  4:28­35 .
Ginsberg , M. (1993) . Essentials of Artificial Intelli­
gence.  Morga n Kaufmann , San Mateo , California .
Bibliograph y 873
Ginsberg , M. L., edito r (1987) . Readings  in Non­
monotonic  Reasoning.  Morga n Kaufmann , San Ma­
teo, California .
Ginsberg , M. L. (1989) . Universa l planning : An (al­
most ) universall y bad idea. AI Magazine,  10(4):40 ­
44.
Giralt , G., Alami , R., Chatila , R., and Freedman ,
P. (1991) . Remot e operate d autonomou s robots . In
Intelligent  Robotics:  Proceedings  of the Interna­
tional  Symposium,  volum e 1571 , page s 416­427 ,
Bangalore , India . Internationa l Societ y for Optica l
Engineerin g (SPIE) .
Glanc , A. (1978) . On the etymolog y of the wor d
"robot" . SIGARTNewsletter,  67:12 .
Glover , F. (1989) . Tab u search : 1. ORSA  Journal
on Computing,  1(3) : 190­206 .
Godel , K. (1930) . Uber  die Vollstdndigkeit  des
Logikkalkiils.  PhD thesis , Universit y of Vienna .
Godel , K. (1931) . Ube r forma l unentscheidbar e
Satze der Principi a mathematic a und verwandte r
System e I. Monatshefte  fur Mathematik  und Physik,
38:173­198 .
Gold , E. M. (1967) . Languag e identificatio n in the
limit . Information  and Control,  10:447­474 .
Goldberg , D. E. (1989) . Genetic  Algorithms
in Search,  Optimization  and  Machine  Learning.
Addison­Wesley , Reading , Massachusetts .
Goldman , R. P. andCharniak , E. (1992) . Probabilis ­
tic text understanding . Statistics  and Computing,
2(2): 105­114 .
Goldszmidt , M., Morris , P., and Pearl , I. (1990) . A
maximu m entrop y approac h to nonmonotoni c rea­
soning . In Proceedings  of the Eighth  National  Con­
ference  on Artificial Intelligence  (AAAI­90),  vol ­
ume 2, page s 646­652 , Boston , Massachusetts .
MIT Press .
Good , I. J. (1950) . Contributio n to the discussio n
of Elio t Slater' s "Statistic s for the ches s compute r
and the facto r of mobility" . In Symposium  on In­
formation  Theory,  page 199 , London . Ministr y of
Supply .
Good , I. J. (1961) . A causa l calculus . British Journal
of the Philosophy of Science,  11:305­318.Reprinte d
in Goo d (1983) .
Good , I. J. (1983) . Good  Thinking:  The  Founda­
tions  of Probability  and Its Applications.  Universit y
of Minnesot a Press , Minneapolis , Minnesota .Goodman , N. (1954) . Fact,  Fiction  and  Forecast.
Universit y of Londo n Press , London , first edition .
Goodman , N. (1977) . The Structure of Appearance.
D. Reidel , Dordrecht , The Netherlands , third edi­
tion.
Gorry , G. A. (1968) . Strategie s for computer­aide d
diagnosis . Mathematical  Biosciences,  2(3^):293 ­
318.
Gorry , G. A., Kassirer , J. P., Essig , A., and Schwartz ,
W. B. (1973) . Decisio n analysi s as the basi s for
computer­aide d managemen t of acute rena l failure .
American  Journal  of Medicine,  55:473^484 .
Gould , S. J. (1994) . Thi s view of life. Natural
History,  8:10­17 .
Graham , S. L., Harrison , M. A., and Ruzzo , W. L.
(1980) . An improve d context­fre e recognizer . ACM
Transactions  on Programming  Languages  and Sys­
tems,  2(3):4  15­462 .
Grayson , C. I. (1960) . Decision s unde r uncertainty :
Drillin g decision s by oil and gas operators . Techni ­
cal report , Divisio n of Research , Harvar d Busines s
School , Boston .
Green , C. (1969a) . Applicatio n of theore m prov ­
ing to proble m solving . In Proceedings  of the First
International  Joint  Conference  on Artificial  Intel­
ligence  (IJCAI­69),  page s 219­239 , Washington ,
D.C. IJCAII .
Green , C. (1969b) . Theorem­provin g by resolu ­
tion as a basi s for question­answerin g systems . In
Meltzer , B., Michie , D., and Swann , M., editors ,
Machine  Intelligence  4, page s 183­205 . Edinburg h
Universit y Press , Edinburgh , Scotland .
Greenblatt , R. D., Eastlake , D. E., and Crocker,S . D.
(1967) . The Greenblat t ches s program . In Proceed­
ings of the Fall  Joint  Computer  Conference,  page s
801­810 .
Greiner , R. (1989) . Toward s a forma l analysi s of
EBL. In Proceedings  of the Sixth International
Machine  Learning  Workshop,  Ithaca , NY. Morga n
Kaufmann .
Grice , H. P. (1957) . Meaning . Philosophical  Re­
view,  66:377­388 .
Grimes , J. (1975) . The  Thread  of Discourse.  Moul ­
ton.
874 Bibliograph y
Grosz , B., Appelt , D., Martin , P., and Pereira , F.
(1987) . Team : An experimen t in the desig n of
transportabl e natural­languag e interfaces . Artificial
Intelligence,  32(2) : 173­244 .
Grosz , B. J. and Sidner,  C. L. (1986) . Attention ,
intentions , and the structur e of discourse . Compu­
tational  Linguistics,  12(3) : 175­204 .
Grosz , B. J., Sparc k Jones , K., and Webber , B. L.,
editor s (1986) . Readings  in Natural  Language  Pro­
cessing.  Morga n Kaufmann , San Mateo , California .
Gu,J . (1989).  Parallel  Algorithms  and Architectures
for Very  Fast Al Search.  PhD thesis , Universit y of
Utah .
Guard , J., Oglesby , R, Bennett , J., and Settle , L.
(1969) . Semi­automate d mathematics . Journal  of
the Association  for Computing Machinery,  16:49 ­
62.
Haas , A. (1986) . A syntacti c theor y of belie f and
action . Artificial  Intelligence,  28(3):245­292 .
Hacking , I. (1975) . The  Emergence  of Probability.
Cambridg e Universit y Press , Cambridge .
Hald , A. (1990) . A History  of Probability andStatis­
tics and  Their  Applications  Before  1750.  Wiley ,
New York .
Halpern , J. Y. (1987) . Usin g reasonin g abou t knowl ­
edge to analyz e distribute d systems . In Traub , J. R,
Grosz,B . J., Lampson,B . W., andNilsson , N. J., ed­
itors, Annual  review  of computer  science,  volum e 2,
page s 37­68 . Annua l Reviews , Palo Alto .
Hamming , R. W. (1991) . The  Art of Probability for
Scientists  and Engineers.  Addison­Wesley , Read ­
ing, Massachusetts .
Hammond , K. (1989) . Case­BasedPlanning:  View­
ing Planning  as a Memory  Task.  Academi c Press ,
New York .
Hanks , S., Russell , S., and Wellman , M., editor s
(1994) . Proceedings  of the AAAI  Spring  Symposium
on Decision­Theoretic  Planning,  Stanford , Califor ­
nia.
Hanski , I. and Cambefort , Y, editor s (1991) . Dung
Beetle  Ecology.  Princeto n Universit y Press , Prince ­
ton, New Jersey .
Hansson , O. and Mayer , A. (1989) . Heuristi c searc h
as evidentia l reasoning.  In Proceedings  of the Fifth
Workshop  on Uncertainty  in Artificial Intelligence,
Windsor , Ontario . Morga n Kaufmann .Haralick , R. and Elliot , G. (1980) . Increasin g tree
searc h efficienc y for constraint­satisfactio n prob ­
lems . Artificial  Intelligence,  14(3):263­313 .
Harel , D. (1984) . Dynami c logic . In Gabbay , D.
and Guenthner , R, editors , Handbook of Philosoph­
ical Logic,  volum e 2, page s 497­604 . D. Reidel ,
Dordrecht , The Netherlands .
Harkness , K. and Battell , J. S. (1947) . This mad e
chess history . Chess  Review.
Harman , G. H. (1983) . Change  in View:  Princi­
ples of Reasoning.  MIT Press , Cambridge , Mas ­
sachusetts .
Harp , S. A., Samad , T., and Guha , A. (1990) . De­
signin g application­specifi c neura l network s usin g
the geneti c algorithm . In Touretzky , D. S., editor ,
Advances  in Neural  Information  Processing  Sys­
tems II, page s 447^­54 . Morga n Kaufmann , San
Mateo , California .
Harris , L. R. (1984) . Experienc e with INTELLECT :
Artificia l intelligenc e technolog y transfer . Al Mag­
azine,  5(2, Summer):43­55 .
Hart, P. E., Nilsson , N. J., andRaphael , B. (1968) . A
forma l basis for the heuristi c determinatio n of min­
imum cost paths . IEEE  Transactions  on Systems
Science  and Cybernetics,  SSC­4(2):100­107 .
Hart, P. E., Nilsson , N. J., and Raphael , B. (1972) .
Correctio n to "A forma l basi s for the heuristi c deter ­
minatio n of minimu m cost paths" . SIGART'Newslet­
ter, 37:28­29 .
Hart, T. P. and Edwards , D. J. (1961) . The tree
prune (TP) algorithm . Artificia l Intelligenc e Projec t
Mem o 30, Massachusett s Institut e of Technology ,
Cambridge , Massachusetts .
Haugeland , J., edito r (1981) . Mind  Design.
MIT Press , Cambridge , Massachusetts .
Haugeland , J., edito r (1985) . Artificial  Intelli­
gence:  The  Very  Idea.  MI T Press , Cambridge ,
Massachusetts .
Haussler , D. (1989) . Learnin g conjunctiv e concept s
in structura l domains . Machine  Learning,  4(1):7 ­
40.
Hawkins , J. (1961) . Self­organizin g systems : A re­
view and commentary . Proceedings  of the IRE,
Bibliograph y 875
Hayes , J. E. and Levy , D. N. L. (1976) . The  World
Computer  Chess  Championship:  Stockholm  1974.
Edinburg h Universit y Press , Edinburgh , Scotland .
Hayes , P. J. (1973) . Computatio n and deduc ­
tion. In Proceedings  of the Second  Symposium
on Mathematical  Foundations  of Computer Science,
Czechoslovakia . Czechoslovakia n Academ y of Sci­
ence .
Hayes , P. J. (1978) . The naiv e physic s manifesto . In
Michie , D., editor . Expert  Systems  in the Microelec­
tronic  Age.  Edinburg h Universit y Press , Edinburgh ,
Scotland .
Hayes , P. J. (1979) . The logic of frames . In Metz ­
ing, D., editor , Frame  Conceptions  and Text  Under­
standing,  page s 46­61 . de Gruyter , Berlin .
Hayes , P. J. (1985a) . Naiv e physic s I: ontolog y for
liquids . In Hobbs , J. R. and Moore , R. C., editors ,
Formal  Theories  of the Commonsense  World,  chap ­
ter 3, page s 71­107 . Ablex , Norwood , New Jersey .
Hayes , P. J. (1985b) . The secon d naive physic s man ­
ifesto . In Hobbs , J. R. and Moore , R. C., editors ,
Formal  Theories of  the Commonsense  World,  chap ­
ter 1, page s 1­36 . Ablex , Norwood , New Jersey .
Kazan , M. (1973) . The  Classic  Italian  Cookbook.
Ballantine .
Hebb , D. O. (1949) . The  Organization  of Behavior.
Wiley , New York .
Heckerman , D. (1986) . Probabilisti c interpretatio n
for MYCIN' s certaint y factors . In Kanal , L. N. and
Lemmer , J. F., editors , Uncertainty  in Artificial In­
telligence,  page s 167­196 . Elsevier/North­Holland ,
Amsterdam , London , New York .
Heckerman , D. (1991) . Probabilistic  Similarity Net­
works.  MIT Press , Cambridge , Massachusetts .
Heckerman , D., Geiger , D., and Chickering , M.
(1994) . Learnin g Bayesia n networks : The combi ­
natio n of knowledg e and statistica l data . Technica l
Repor t MSR­TR­94­09 , Microsof t Research , Red ­
mond , Washington .
Held , M. and Karp , R. M. (1970) . The travelin g
salesma n proble m and minimu m spannin g trees .
Operations  Research,  18:1138­1162 .
Helman , D. H., edito r (1988) . Analogical  Reason­
ing: Perspectives  of Artificial  Intelligence,  Cogni­
tive Science,  and  Philosophy.  Kluwer , Dordrecht ,
The Netherlands .Hendrix , G. G. (1975) . Expandin g the utilit y of se­
manti c network s throug h partitioning . In Proceed­
ings of the Fourth International  Joint  Conferenceon
Artificial  Intelligence  (UCAI­75),  page s 115­121 ,
Tbilisi , Georgia . IJCAII .
Henrion , M. (1988) . Propagatio n of uncertaint y in
Bayesia n network s by probabilisti c logic sampling .
In Lemmer , J. F. and Kanal , L. N., editors , Uncer­
tainty  in Artificial  Intelligence  2, page s 149­163 .
Elsevier/North­Holland , Amsterdam , London , New
York .
Heppenheimer , T. A. (1985) . Man make s man . In
Minsky , M., editor , Robotics,  page s 28­69 . Double ­
day, Garde n City , New York .
Herbrand , J. (1930) . Recherches  sur la  Theorie  de
la Demonstration.  PhD thesis , Universit y of Paris .
Hertz , J., Krogh , A., and Palmer , R. G. (1991) . In­
troduction  to the  Theory  of Neural  Computation.
Addison­Wesley , Reading , Massachusetts .
Hewitt , C. (1969) . PLANNER : a languag e for prov ­
ing theorem s in robots . In Proceedings  of the First
International  Joint  Conference  on Artificial Intel­
ligence  (IJCAI­69),  page s 295­301 , Washington ,
D.C. IJCAII .
Hintikka , J. (1962) . Knowledge  and Belief.  Cornel l
Universit y Press , Ithaca , New York .
Hinton , G. E. and Anderson , J. A. (1981) . Parallel
Models  of Associative Memory.  Lawrenc e Erlbau m
Associates , Potomac , Maryland .
Hinton , G. E. and Sejnowski , T. (1983) . Optima l
perceptua l inference . In Proceedings  of the IEEE
Computer  Society  Conference  on Computer  Vision
and Pattern  Recognition,  page s 448­453 , Washing ­
ton, D.C. IEEE Compute r Societ y Press .
Hinton , G. E. and Sejnowski , T. J. (1986) . Learnin g
and relearnin g in Boltzman n machines . In Rumel ­
hart, D. E. and McClelland , J. L., editors , Parallel
Distributed  Processing,  chapte r 7, page s 282­317 .
MIT Press , Cambridge , Massachusetts .
Hirsh , H. (1987) . Explanation­base d generalizatio n
in a logic programmin g environment . In Proceed­
ings of the Tenth  International Joint  Conference  on
Artificial  Intelligence  (IJCAI­87),  Milan , Italy . Mor ­
gan Kaufmann .
Hirst , G. (1987) . Semantic  Interpretation  Against
Ambiguity.  Cambridg e Universit y Press .
876 Bibliograph y
Hobbs , J. R. (1985) . Ontologica l promiscuity . In
Proceedings,  23rd  Annual  Meeting  of the Associ­
ation  for Computational  Linguistics,  page s 61­69 ,
Chicago , Illinois .
Hobbs , J. R. (1986) . Overvie w of the TACITU S
project . Computational  Linguistics,  12(3):220­222 .
Hobbs , J. R. (1990) . Literature  and Cognition.  CSL I
Press , Stanford , California .
Hobbs , J. R., Blenko,T. , Croft , B., Hager , G., Kautz ,
H. A., Kube , P., and Shoham , Y. (1985) . Com ­
monsens e summer : Fina l report . Technica l Repor t
CSLI­85­35 , Cente r for the Stud y of Languag e and
Informatio n (CSLI) , Stanford , California .
Hobbs , J. R., Croft , W., Davies , T., Edwards , D. D.,
and Laws , K. I. (1987) . Commonsens e metaphysic s
and lexica l semantics . Computational  Linguistics,
13(3­4):241­250 .
Hobbs , J. R. and Moore , R. C., editor s (1985) . For­
mal Theories of  the Commonsense  World.  Ablex ,
Norwood , New Jersey .
Hobbs , J. R., Stickel , M., Appelt , D., and Martin , P.
(1990) . Interpretatio n as abduction . Technica l Note
499, SRI International , Menl o Park , California .
Hobbs , J. R., Stickel , M. E., Appelt , D. E., and Mar­
tin, P. (1993) . Interpretatio n as abduction . Artificial
Intelligence,  63(l­2):69­142 .
Holland , J. H. (1975) . Adaption  in Natural and
Artificial  Systems.  Universit y of Michiga n Press .
Hopfield , J. J. (1982) . Neuron s with grade d respons e
have collectiv e computationa l propertie s like thos e
of two­stat e neurons . Proceedings  of the National
Academy  of Sciences  (USA),  79:2554­2558 .
Horn , A. (1951) . On sentence s whic h are true of di­
rect union s of algebras . Journal  of Symbolic  Logic,
16:14­21 .
Horn , B. K. P. (1970) . Shap e from shading : a
metho d for obtainin g the shap e of a smoot h opaqu e
objec t from one view . Technica l Repor t 232, MIT
Artificia l Intelligenc e Laboratory , Cambridge , Mas ­
sachusetts .
Horn , B. K. P. (1986) . Robot  Vision.  MIT Press ,
Cambridge , Massachusetts .
Horn,B . K. P. andBrooks,M . J. (1989) . Shape/mm
Shading.  MIT Press , Cambridge , Massachusetts .Horvitz , E. J., Breese , J. S., andHenrion , M. (1988) .
Decisio n theor y in exper t system s and artificia l in­
telligence . International  Journal of  Approximate
Reasoning,  2:247­302 .
Horvitz , E. J. and Heckerman , D. (1986) . Th e
inconsisten t use of measure s of certaint y in arti­
ficia l intelligenc e research . In Kanal , L. N. and
Lemmer , J. P., editors , Uncertainty  in Artificial  In­
telligence,  page s 137­151 . Elsevier/North­Holland ,
Amsterdam , London , New York .
Horvitz , E. J., Heckerman , D. E., and Langlotz ,
C. P. (1986) . A framewor k for comparin g alterna ­
tive formalism s for plausibl e reasoning . In Proceed­
ings of the Fifth  National Conference  on Artificial
Intelligence  (AAAI­86),  volum e 1, page s 210­214 ,
Philadelphia , Pennsylvania . Morga n Kaufmann .
Horvitz , E. J., Suermondt , H. J., and Cooper , G. F.
(1989) . Bounde d conditioning : Flexibl e inferenc e
for decision s unde r scarc e resources . In Proceed­
ings of the Fifth  Conference  on Uncertainty  in Arti­
ficial  Intelligence  (UAI­89),  page s 182­193 , Wind ­
sor, Ontario . Morga n Kaufmann .
Howard , R. A. (1960) . Dynamic  Programming and
Markov  Processes.  MIT Press , Cambridge , Mas ­
sachusetts .
Howard , R. A. (1966) . Informatio n valu e theory .
IEEE  Transactions  on Systems  Science and  Cyber­
netics,  SSC­2:22­26 .
Howard , R. A. (1977) . Risk preference . In Howard ,
R. A. and Matheson , J. E., editors , Readings in  De­
cision  Analysis,  page s 429­465 . Decisio n Analysi s
Group , SRI International , Menl o Park , California .
Howard , R. A. (1989) . Microrisk s for medica l deci­
sion analysis . International  Journal of  Technology
Assessment  in Health  Care,  5:357­370 .
Howard , R. A. and Matheson , J. E. (1984) . Influenc e
diagrams . In Howard , R. A. and Matheson , J.E., ed­
itors, Readings  on the Principles  and Applications
of Decision  Analysis,  page s 721­762 . Strategi c De­
cision s Group , Menl o Park , California.Articl e date s
from 1981 .
Hsu, F.­H. , Anantharaman , T. S., Campbell , M. S.,
and Nowatzyk , A. (1990) . A grandmaste r ches s
machine . Scientific  American,  263(4):44­50 .
Hsu, K., Brady , D., and Psaltis , D. (1988) . Ex ­
perimenta l demonstratio n of optica l neura l comput ­
ers. In Anderson , D. Z., editor , Neural  Information
Bibliograph y 877
Processing  Systems,  Denver  1987,  page s 377­386 ,
Denver , Colorado . America n Institut e of Physics .
Huang , T., Roller , D., Malik , J., Ogasawara , G.,
Rao, B., Russell , S., and Weber , J. (1994) . Au ­
tomati c symboli c traffi c scen e analysi s usin g be­
lief networks . In Proceedings  of the Twelfth  Na­
tional  Conference  on Artificial  Intelligence  (AAAI­
94), page s 966­972 , Seattle , Washington . AAA 1
Press .
Hubel , D. H. (1988) . Eye,  Brain,  and Vision.  W. H.
Freeman , New York .
Huddleston , R. D. (1988) . English  Grammar:  An
Outline.  Cambridg e Universit y Press , Cambridge .
Huffman , D. A. (1971) . Impossibl e object s as non­
sense sentences . In Meltzer , B. and Michie , D.,
editors , Machine  Intelligence 6,  page s 295­324 . Ed­
inburg h Universit y Press , Edinburgh , Scotland .
Hughes , G. E. and Cresswell , M. J. (1968) . An
Introduction  to Modal  Logic.  Methuen , London .
Hughes , G. E. and Cres s well, M. J. (1984) . A Com­
panion  to Modal  Logic.  Methuen , Eondon .
Hume , D. (1978) . A  Treatise  of Human  Na­
ture. Oxfor d Universit y Press , Oxford , secon d edi­
tion.Edite d by E. A. Selby­Bigg e and P. H. Nidditch .
Hunt , E. B., Marin , J., and Stone , P. T. (1966) . Ex­
periments  in Induction.  Academi c Press , New York .
Hunter , G. (1971) . Metalogic:  An Introduction to
the Metatheory  of Standard  First­Order  Logic.  Uni ­
versit y of Californi a Press , Berkele y and Eos Ange ­
les.
Hunter , E. and States , D. J. (1992) . Bayesia n classi ­
ficatio n of protei n structure . IEEE  Expert,  7(4):67 ­
75.
Huttenlocher , D. P. and Ullman , S. (1990) . Recog ­
nizin g solid object s by alignmen t with an image . In­
ternational  Journal of  Computer  Vision,  5(2) : 195­
212.
Huygens , C. (1657) . Ratiocinii s in ludo aleae . In
van Schooten , R, editor , Exercitionum  Mathemati­
corum.  Elsevirii , Amsterdam .
Hwang , C. H. and Schubert , E. K. (1993) . EE :
a formal , yet natural , comprehensiv e knowledg e
representation . In Proceedings  of the Eleventh  Na­
tional  Conference  on Artificial Intelligence  (AAAI­
93), page s 676­682 , Washington , D.C. AAA I Press .Hyatt , R. M., Gower , A. E., and Nelson , H. E.
(1986) . Cray Blitz . In Beal , D. P., editor , Advances
in Computer  Chess 4,  page s 8­18 . Pergamon , New
York .
Ingerman , P. Z. (1967) . Panini­Backu s form sug­
gested . Communications  of the Association  for
Computing  Machinery,  10(3):137 .
Jackson , P. (1986) . Introduction  to Expert  Systems.
Addison­Wesley , Reading , Massachusetts .
Jacobs , P. and Rau, E. (1990) . Scisor : A syste m for
extractin g informatio n from on­lin e news . Commu­
nications  of the ACM,  33(11):88­97 .
Jaffar , J. and Eassez , J.­E. (1987) . Constrain t logic
programming . In Proceedings  of the Fourteenth
ACM  Conference on  Principles of  Programming
Languages,  Munich . Associatio n for Computin g
Machinery .
Jaffar , J., Michaylov , S., Stuckey , P. J., and Yap , R.
H. C. (1992a) . The CEP(R ) languag e and system .
ACM  Transactions on  Programming Languages and
Systems,  14(3):339­395 .
Jaffar , J., Stuckey , P. J., Michaylov , S., and Yap, R.
H. C. (1992b) . An abstrac t machin e for CEP(R) .
SIGPLAN  Notices,  27(7) : 128­139 .
Jaskowski , S. (1934) . On the rule s of supposition s
in forma l logic . Studia  Logica,  1.
Jeffrey , R. C. (1983) . The  Logic  of Decision.  Uni ­
versit y of Chicag o Press , Chicago , Illinois , secon d
edition .
Jelinek , F. (1976) . Continuou s speec h recognitio n
by statistica l methods . Proceedings  of the IEEE,
64(4):532­556 .
Jelinek , F. (1990) . Self­organizin g languag e mod ­
eling for speec h recognition . In Waibel , A. and
Eee, K.­F. , editors , Readings  in Speech  Recogni­
tion, page s 450­506 . Morga n Kaufmann .
Jensen , F. V., Eauritzen , S. E., and Olesen , K. G.
(1990) . Bayesia n updatin g in causa l probabilisti c
network s by loca l computations . Computational
Statistics  Quarterly,  5(4):269­282 .
Jerison , H. J. (1991) . Brain  Size  and the Evolution
of Mind.  America n Museu m of Natura l History ,
New York .
Jochem , T., Pomerleau , D., and Thorpe , C. (1993) .
Maniac : A nex t generatio n neurall y base d au­
tonomou s road follower . In Proceedings  of the In­
ternational  Conference  on Intelligent  Autonomous
Systems:  IAS­3.
878 Bibliograph y
Johnson , W. W. and Story , W. E. (1879) . Note s on
the "15" puzzle . American Journal of  Mathematics,
2:397­404 .
Johnson­Laird , P. N. (1988) . The  Computer  and the
Mind:  An Introduction to  Cognitive  Science.  Har ­
vard Universit y Press , Cambridge , Massachusetts .
Johnston,  M. D. and Adorf , H.­M . (1992) . Schedul ­
ing with neura l networks : the case of the Hub ­
ble spac e telescope . Computers  & Operations  Re­
search,  19(3­4):209­240 .
Jones,N . D., Gomard , C. K., andSestoft , P. (1993) .
Partial  Evaluation  and Automatic  Program  Gener­
ation.  Prentice­Hall , Englewoo d Cliffs , New Jersey .
Joshi , A. (1985) . Tree­adjoinin g grammars : How
much contex t sensitivit y is require d to provid e rea­
sonabl e structura l descriptions . In Dowty , D., Kart ­
tunen , L., and Zwicky , A., editors , Natural  Lan­
guage  Parsing.  Cambridg e Universit y Press .
Joshi , A., Webber , B., and Sag, I. (1981) . Elements
of Discourse Understanding.  Cambridg e Universit y
Press .
Judd , J. S. (1990) . Neural  Network  Design and the
Complexity  of Learning.  MI T Press , Cambridge ,
Massachusetts .
Julesz , B. (1971) . Foundations  of Cyclopean  Per­
ception.  Universit y of Chicag o Press , Chicago , Illi­
nois.
Kaelbling , L. P. (1990) . Learnin g function s in k­
DNF from reinforcement . In Machine  Learning:
Proceedings  of the Seventh International Confer­
ence,  page s 162­169 , Austin , Texas . Morga n Kauf ­
mann .
Kaelbling , L. P. and Rosenschein , S. J. (1990) . Ac­
tion and plannin g in embedde d agents . Robotics
and Autonomous  Systems,  6(I­2):35­48 .
Kahneman , D., Slovic , P., and Tversky , A., editor s
(1982) . Judgment  under  Uncertainty:  Heuristics
and Biases.  Cambridg e Universit y Press , Cam ­
bridge .
Kaindl , H. (1990) . Tree searchin g algorithms . In
Marsland , A. T. and Schaeffer , J., editors , Com­
puters,  Chess,  and  Cognition,  page s 133­158 .
Springer­Verlag , Berlin .
Kaindl , H. and Khorsand , A. (1994) . Memory ­
bounde d bidirectiona l search . In Proceedings  of theTwelfth  National  Conference  on Artificial  Intelli­
gence  (AAA1­94),  page s 1359­1364 , Seattle , Wash ­
ington . AAA I Press .
Kalman , R. E. (1960) . A new approac h to linea r
filterin g and predictio n problems . Journal  of Basic
Engineering,  page s 35^6 .
Kambhampati , S. and Nau , D. S. (1993) . On the
natur e and role of moda l trut h criteri a in plan ­
ning. Technica l Repor t ISR­TR­93­30 , Universit y
of Maryland , Institut e for System s Research .
Kanal , L. N. and Kumar , V. (1988) . Search  in Arti­
ficial  Intelligence.  Springer­Verlag , Berlin .
Kanal , L. N. and Lemmer , J. P., editor s (1986) . Un­
certainty  in Artificial Intelligence.  Elsevier/North ­
Holland , Amsterdam , London , New York .
Kandel , E. R., Schwartz , J. H., and Jessell , T. M.,
editor s (1991) . Principles  of Neural  Science.
Elsevier/North­Holland , Amsterdam , London , New
York , third edition .
Kaplan , D. and Montague , R. (1960) . A parado x
regained . Notre  Dame Journal  of Formal  Logic,
l(3):79­90.Reprinte d in Thomaso n (1974) .
Karger , D. R., Roller , D., and Phillips , S. J. (1993) .
Findin g the hidde n path : tim e bound s for all­
pairs shortes t paths . SIAM  Journal  on Computing,
22(6):1199­1217 .
Karp , R. M. (1972) . Reductibilit y amon g combi ­
natoria l problems . In Miller , R. E. and Thatcher ,
J. W., editors , Complexity  of Computer  Computa­
tions,  page s 85­103 . Plenum , New York .
Kasami , T. (1965) . An efficien t recognitio n and
synta x analysi s algorith m for context­fre e lan ­
guages . Technica l Repor t AFCRL­65­758 , Air
Force Cambridg e Researc h Laboratory , Bedford ,
Massachusetts .
Kautz , H. A. and Selman , B. (1991) . Hard problem s
for simpl e defaul t logics . Artificial  Intelligence,
49(l­3):243­279 .
Kay, M., Gawron , J. M., and Norvig , P. (1994) .
Verbmobil:  A Translation  System  for Face­To­Face
Dialog.  CSL I Press , Stanford , California .
Kearns , M. J. (1990) . The Computational  Complex­
ity of Machine  Learning.  MIT Press , Cambridge ,
Massachusetts .
Keeney , R. L. (1974) . Multiplicativ e utilit y func ­
tions . Operations  Research,  22:22­34 .
Bibliograph y 879
Keeney , R. L. and Raiffa , H. (1976) . Decisions  with
Multiple  Objectives:  Preferences  and Value  Trade­
offs. Wiley , New York .
Kemp , M., edito r (1989) . Leonardo  on Painting:
An Anthology  of Writings.  Yal e Universit y Press ,
New Haven , Connecticut .
Kemp , M. (1990) . The  Science  of Art:  Optical
Themes  in Western  Art from  Brunelleschi  to Seurat.
Yale Universit y Press , New Haven , Connecticut .
Keynes , J. M. (1921) . A Treatise  on Probability.
Macmillan , London .
Kierulf , A., Chen , K., and Nievergelt , J. (1990) .
Smar t Gam e Boar d and Go Explorer : A stud y in
softwar e and knowledg e engineering . Communica­
tions  of the Association  for Computing  Machinery,
33(2) : 152­167 .
Kietz , J.­U . and Dzeroski , S. (1994) . Inductiv e logic
programmin g and learnability . SIGART  Bulletin,
5(l):22­32 .
Kirn, J. H. (1983) . CONVINCE:  A Conversational
Inference  Consolidation  Engine.  PhD thesis , De­
partmen t of Compute r Science , Universit y of Cali­
forni a at Los Angeles .
Kirn, J. H. and Pearl , J. (1983) . A computationa l
mode l for combine d causa l and diagnosti c reason ­
ing in inferenc e systems . In Proceedings  of the
Eighth  International  Joint  Conference  on Artificial
Intelligence  (IJCAI­83),  page s 190­193 , Karlsruhe ,
Germany . Morga n Kaufmann .
Kirn, J. H. and Pearl , J. (1987) . CONVINCE : A con­
versationa l inferenc e consolidatio n engine . IEEE
Transactions  on Systems,  Man,  and  Cybernetics,
17(2):120­132 .
King , R. D., Muggleton , S., Lewis , R. A., and
Sternberg , M. J. E. (1992) . Dru g desig n by ma­
chine learning : the use of inductiv e logi c pro­
grammin g to mode l the structur e activit y relation ­
ships of trimethopri m analogue s bindin g to dihy ­
drofolat e reductase . Proceedings  of the National
Academy  of Sciences  of the United  States  of Amer­
ica, 89(23): ! 1322­11326 .
King , S., Motet , S., Thomere , J., and Arlabosse , F.
(1993) . A visua l surveillanc e syste m for inciden t
detection . In AAAI93  Workshop  on AI in Intelligent
Vehicle  Highway  Systems,  page s 30­36 , Washing ­
ton, D.C.Kirkpatrick , S., Gelatt , C. D., and Vecchi , M. P.
(1983) . Optimizatio n by simulate d annealing . Sci­
ence , 220:671­680 .
Kirkpatrick , S. and Selman , B. (1994) . Critica l
behavio r in the satisfiabilit y of rando m Boolea n ex­
pressions . Science,  264(5163):1297­1301 .
Kirousis , L. M. an d Papadimitriou , C. H.
(1988) . The complexit y of recognizin g polyhedra l
scenes . Journal  of Computer  and System Sciences,
37(1) : 14­38 .
Kister , J., Stein , P., Ulam , S., Walden , W., and Wells ,
M. (1957) . Experiment s in chess . Journalofthe  As­
sociation  for Computing Machinery,  4:174­177 .
Kjaerulff , U. (1992) . A computationa l schem e for
reasonin g in dynami c probabilisti c networks . In
Proceedings  of the Eighth  Conference  on Uncer­
tainty  in Artificial Intelligence,  page s 121­129 .
Knight , K. (1989) . Unification : A multidisciplinar y
survey . ACM  Computing  Surveys,  21(1):93­121 .
Knoblock , C. A. (1990) . Learnin g abstractio n hi­
erarchie s for proble m solving . In Proceedings  of
the Eighth  National  Conference  on Artificial In­
telligence  (AAAI­90),  volum e 2, page s 923­928 ,
Boston , Massachusetts . MIT Press .
Knuth , D. (1968) . Semantic s for context­fre e lan­
guages . Mathematical  Systems  Theory  2, page s
127­145 .
Knuth , D. E. (1973) . The  Art of  Computer  Pro­
gramming,  volum e 2: Fundamenta l Algorithms .
Addison­Wesley , Reading , Massachusetts , secon d
edition .
Knuth , D. E. and Bendix , P. B. (1970) . Simpl e word
problem s in universa l algebras . In Leech , J., edi­
tor, Computational  Problems  in Abstract  Algebra,
pages 263­267 . Pergamon , New York .
Knuth , D. E. and Moore , R. W. (1975) . An anal ­
ysis of alpha­bet a pruning . Artificial  Intelligence,
6(4):293­326 .
Koenderink , J. J. (1990) . Solid  Shape.  MIT Press ,
Cambridge , Massachusetts .
Koenderink , J. J. and van Doom , A. J. (1975) . In­
varian t propertie s of the motio n paralla x field due to
the movemen t of rigid bodie s relativ e to an observer .
OpticaActa,  22(9):773­791 .
Kohn , W. (1991) . Declarativ e contro l architecture .
Communications  of the Association  for Computing
Machinery,  34(8):65­79 .
880 Bibliograph y
Kohonen , T. (1989) . Self­Organization  and Asso­
ciative  Memory.  Springer­Verlag , Berlin , third edi­
tion.
Roller , D., Weber , J., Huang , T., Malik , J., Oga ­
sawara,G. , Rao, B., and Russell , S. (1994) . Toward s
robus t automati c traffi c scen e analysi s in real­time .
In Proceedings  of the International Conference  on
Pattern  Recognition,  Israel .
Kolmogorov , A. N. (1941) . Interpolatio n und ex­
trapolatio n von stationare n zufallige n folgen . Bul­
letin of the Academy of  Sciences  of the USSR,  Ser.
Math.5:3­14 .
Kolmogorov , A. N. (1950) . Foundations  of the
Theory  of Probability.  Chelsea , New York.Englis h
translatio n of Kolmogoro v (1950) .
Kolmogorov , A. N. (1963) . On table s of rando m
numbers . Sankhya,  the Indian  Journal  of Statistics,
Serie s A 25.
Kolmogorov , A. N. (1965) . Thre e approache s to the
quantitativ e definitio n of information . Problems  in
Information  Transmission,  l(l):l­7 .
Kolodner , J. (1983) . Reconstructiv e memory : A
compute r model . Cognitive  Science,  7:281­328 .
Kolodner , J. (1993) . Case­Based  Reasoning.  Mor ­
gan Kaufmann , San Mateo , California .
Konolige , K. (1982) . A first orde r formalizatio n
of knowledg e and actio n for a multi­agen t plannin g
system . In Hayes , J. E., Michie , D., and Pao, Y.­H. ,
editors , Machine  Intelligence  10. Elli s Horwood ,
Chichester , England .
Koopmans , T. C. (1972) . Representatio n of pref ­
erenc e ordering s ove r time . In McGuire , C. B.
and Radner , R., editors , Decision  and Organiza­
tion. Elsevier/North­Holland , Amsterdam , London ,
New York .
Korf, R. E. (1985a) . Depth­firs t iterative­deepening :
an optima l admissibl e tree search . Artificial  Intelli­
gence,21(l):91­l09.
Korf, R. E. (1985b) . Iterative­deepenin g A*: An
optima l admissibl e tree search . In Proceedings  of
the Ninth  International  Joint  Conference  on Artifi­
cial Intelligence  (IJCAI­85),  page s 1034 ^ 1036 , Los
Angeles , California . Morga n Kaufmann .
Korf, R. E. (1988) . Optima l path findin g algorithms .
In Kanal , L. N. and Kumar , V., editors , Search in
Artificial  Intelligence,  chapte r 7, page s 223­267 .
Springer­Verlag , Berlin .Korf, R. E. (1993) . Linear­spac e best­firs t search .
Artificial  Intelligence,  62(l):41­78 .
Kotok , A. (1962) . A ches s playin g progra m for the
IBM 7090 . AI Projec t Mem o 41, MIT Computatio n
Center , Cambridge , Massachusetts .
Kowalski , R. (1974) . Predicat e logi c as a pro­
grammin g language . In Proceedings  of the IFIP­74
Congress,  page s 569­574 . Elsevier/North­Holland .
Kowalski , R. (1979a) . Algorith m = logic + control .
Communications  of the Association  for Computing
Machinery,  22:424^136 .
Kowalski , R. (1979b) . Logic  for Problem  Solving.
Elsevier/North­Holland , Amsterdam , London , New
York .
Kowalski , R. (1988) . The early year s of logic pro­
gramming . Communications  of the Association  for
Computing  Machinery,  31:38­43 .
Kowalski , R. and Sergot , M. (1986) . A logic­base d
calculu s of events . New  Generation  Computing,
4(l):67­95 .
Koza , J. R. (1992) . Genetic  Programming:  On the
Programming  of Computers  by Means  of Natural
Selection.  MIT Press , Cambridge , Massachusetts .
Kripke , S. A. (1963) . Semantica l consideration s on
moda l logic . Acta  Philosophica  Fennica,  16:83­94 .
Kruppa , E. (1913) . Zur Ermittlun g eine s Objeck ­
tes aus zwei Perspektive n mil innere r Orientierung .
Sitz.­Ber.  Akad.  Wiss.,  Wien,  Math.  Naturw.,  Kl. Abt.
Ila, 122:1939­1948 .
Kuehner , D. (1971) . A note on the relatio n betwee n
resolutio n and Maslov' s invers e method . DC L
Mem o 36, Universit y of Edinburgh .
Kukich , K. (1992) . Technique s for automaticall y
correctin g word s in text. ACM  Computing Surveys,
24(4):377­439 .
Kumar , V. (1991) . A genera l heuristi c bottom­u p
procedur e for searchin g AND/O R graphs . Informa­
tion Sciences,  56(l­3):39­57 .
Kumar , V. and Kanal , L. N. (1983) . A genera l
branc h and boun d formulatio n for understandin g
and synthesizin g and/o r tree searc h procedures . Ar­
tificial  Intelligence,  21:179­198 .
Bibliograph y 881
Kumar , V. and Kanal , L. N. (1988) . The CDP : A
unifyin g formulatio n for heuristi c search , dynami c
programming , and branch­and­bound . In Kanal ,
L. N. and Kumar , V., editors , Search in  Artificial  In­
telligence,  chapte r 1, page s 1­27 . Springer­Verlag ,
Berlin .
Kumar , V., Nau , D. S., an d Kanal , L. N.
(1988) . A genera l branch­and­boun d formulatio n
for AND/O R grap h and gam e tree search . In Kanal ,
L. N. and Kumar , V., editors , Search  in Artificial
Intelligence,  chapte r 3, page s 91­130 . Springer ­
Verlag , Berlin .
Kurzweil , R. (1990) . The  Age  of Intelligent  Ma­
chines.  MIT Press , Cambridge , Massachusetts .
Kyburg , H. E. (1977) . Randomnes s and the right ref­
erenceclass . The Journal of Philosophy,  74(9):501 ­
521.
Kyburg , H. E. (1983) . The referenc e class . Philos­
ophy  of Science,  50:374­397 .
La Mettrie , J. O. d. (1748) . L'homme  machine.
E. Luzac , Leyde.Translatedint o Englis h as La Met­
trie (1912) .
La Mettrie , J. O. d. (1912) . Man  a Machine.  Ope n
Court , La Salle , Illinois.Englis h translatio n of La
Mettri e (1748) .
Ladkin , P. (1986a) . Primitive s and unit s for time
specification . In Proceedings  of the Fifth National
Conference  on Artificial  Intelligence (AAAI­86),
volum e 1, page s 354­359 , Philadelphia , Pennsyl ­
vania . Morga n Kaufmann .
Ladkin , P. (1986b) . Tim e representation : a taxon ­
omy of interva l relations . In Proceedings  of the
Fifth National  Conference  on Artificial Intelligence
(AAAI­86),  volum e 1, page s 360­366 , Philadelphia ,
Pennsylvania . Morga n Kaufmann .
Laird , J. E., Newell , A., and Rosenbloom , P. S.
(1987) . SOAR : an architectur e for genera l intelli ­
gence . Artificial  Intelligence,  33(1) : 1­64 .
Laird , J. E., Rosenbloom , P. S., and Newell , A.
(1986) . Chunkin g in Soar : the anatom y of a genera l
learnin g mechanism . Machine  Learning,  1:11­46 .
Laird , J. E., Yager , E. S., Hucka , M., and Tuck , M.
(1991) . Robo­Soar : an integratio n of externa l inter ­
action , planning , and learnin g usin g Soar . Robotics
and Autonomous  Systems,  8(1­2):113­129 .Lakoff , G. (1987) . Women,  Fire, and  Dangerous
Things:  What  Categories  Reveal  About  the Mind.
Universit y of Chicag o Press , Chicago , Illinois .
Lakoff , G. and Johnson , M. (1980) . Metaphors  We
Live By. Universit y of Chicag o Press , Chicago , Illi­
nois.
Langley , P., Simon , H. A., Bradshaw , G. L., and
Zytkow , J. M. (1987) . Scientific  Discovery:  Com­
putational  Explorations  of the Creative  Processes.
MIT Press , Cambridge , Massachusetts .
Lassez , J.­L. , Maher , M. J., and Marriott , K. (1988) .
Unificatio n revisited . In Minker , J., editor , Founda­
tions  of Deductive  Databases and  Logic  Program­
ming,  page s 587­625 . Morga n Kaufmann , San Ma­
teo, California .
Latombe , J.­C . (1991) . Robot  Motion  Planning.
Kluwer , Dordrecht , The Netherlands .
Lauritzen , S. L. (1991) . Th e EM algorith m for
graphica l associatio n model s with missin g data .
Technica l Repor t TR­91­05 , Departmen t of Statis ­
tics, Aalbor g University .
Lauritzen , S. L. and Spiegelhalter , D. J. (1988) .
Loca l computation s with probabilitie s on graphi ­
cal structure s and thei r applicatio n to exper t sys­
tems . Journal  of the Royal  Statistical  Society,
B 50(2) : 157­224 .
Lauritzen , S. L. and Wermuth , N. (1989) . Graphica l
model s for association s betwee n variables , som e of
whic h are qualitativ e and some quantitative . Annals
of Statistics,  17:31­57 .
Lawler , E. L. and Wood , D. E. (1966) . Branch­and ­
boun d methods : A survey . Operations  Research,
14(4):699­719 .
Le Cun , Y, Jacket , L. D., Boser , B., and Denker ,
J. S. (1989) . Handwritte n digit recognition : appli ­
cation s of neura l networ k chip s and automati c learn ­
ing. IEEE  Communications  Magazine,  27(11):41 ­
46.
Lee, K.­F . (1989) . Automatic  Speech  Recognition:
The Development  of the SPHINX  System.  Kluwer ,
Dordrecht , The Netherlands .
Lee, K.­F . and Mahajan , S. (1988) . A patter n clas ­
sificatio n approac h to evaluatio n functio n learning .
Artificial  Intelligence,  36(1) : 1­26 .
Lefkovitz , D. (1960) . A strategi c patter n recogni ­
tion progra m for the gam e Go. Technica l Note 60­
243, Wrigh t Air Developmen t Division , Universit y
882 Bibliograph y
of Pennsylvania , The Moor e Schoo l of Electrica l
Engineering .
Lenat , D. B. (1983) . EURISKO : a progra m that
learn s new heuristic s and domai n concepts : the na­
ture of heuristics , III: progra m desig n and results .
Artificial  Intelligence,  21(l­2):61­98 .
Lenat , D. B. and Brown , J. S. (1984) . Why AM and
EURISK O appea r to work . Artificial  Intelligence,
23(3):269­294 .
Lenat , D. B. and Feigenbaum , E. A. (1991) . On
the threshold s of knowledge . Artificial  Intelligence,
47(1­3) : 185­250 .
Lenat , D. B. and Guha , R. V. (1990) . Building
Large  Knowledge­Based  Systems:  Representation
and Inference  in the CYC  Project.  Addison­Wesley ,
Reading , Massachusetts .
Leonard , H. S. and Goodman , N. (1940) . The calcu ­
lus of individual s and its uses . Journal  of Symbolic
Logic,  5(2):45­55 .
Leonard , J. J. and Durrant­Whyte , H. F. (1992) . Di­
rected  sonar sensing  for mobile  robot  navigation.
Kluwer , Dordrecht , The Netherlands .
Lesniewski , S. (1916) . Podstaw y ogolne j teori i
mnogosci . Moscow .
Levesque , H. J. and Brachman , R. J. (1987) . Ex­
pressivenes s and tractabilit y in knowledg e represen ­
tation and reasoning . Computational  Intelligence,
3(2):78­93 .
Levy , D. N. L. (1983) . Computer  Gamesmanship:
The Complete Guide to  Creating  and Structuring
Intelligent  Games Programs.  Simo n and Schuster ,
New York .
Levy , D. N. L., edito r (1988a) . Computer  Chess
Compendium.  Springer­Verlag , Berlin .
Levy , D. N. L., edito r (1988b) . Computer  Games.
Springer­Verlag , Berlin.Tw o volumes .
Lewis , D. K. (1966) . An argumen t for the identit y
theory . The  Journal  of Philosophy,  63(l):17­25 .
Lewis , D. K. (1972) . Genera l semantics . In David ­
son, D. and Harman , G., editors , Semantics  of Nat­
ural Language,  page s 169­218 . D. Reidel , Dor ­
drecht , The Netherlands .
Lewis , D. K. (1980) . Ma d pain and Martia n pain .
In Block , N., editor , Readings in Philosophy of Psy­
chology,  volum e 1, page s 216­222 . Harvard  Uni­
versit y Press , Cambridge , Massachusetts .Li, M. and Vitanyi , P. M. B. (1993) . An  Intro­
duction  to Kolmogorov  Complexity  and Its Applica­
tions.  Springer­Verlag , Berlin .
Lifschitz , V. (1986) . On the semantic s of STRIPS .
In Georgeff , M. P. and Lansky , A. L., editors , Rea­
soning  about  Actions  and  Plans:  Proceedings  of
the 1986  Workshop,  page s 1­9, Timberline , Ore ­
gon. Morga n Kaufmann .
Lifschitz , V. (1989) . Betwee n circumscriptio n
and autoepistemi c logic . In Brachman , R. J. and
Levesque , H. J., editors , Proceedings  of the First
International  Conference  on Principles  of Knowl­
edge Representation and  Reasoning,  page s 235 ­
244, Toronto , Ontario . Morga n Kaufmann .
Lighthill , J. (1973) . Artificia l intelligence : A gen­
eral survey . In Lighthill , J., Sutherland , N. S., Need ­
ham, R. M., Longuet­Higgins , H. C., and Michie ,
D., editors,  Artificial  Intelligence:  A Paper  Sympo­
sium.  Scienc e Researc h Counci l of Grea t Britain ,
London .
Lin, S. (1965) . Compute r solution s of the travellin g
salesma n problem . Bell  Systems  Technical  Journal,
44(10):2245­2269 .
Linden , T. A. (1991) . Representin g softwar e design s
as partiall y develope d plans . In Lowry , M. R. and
McCartney , R. D., editors , Automating  Software  De­
sign,  page s 603­625 . MIT Press , Cambridge , Mas ­
sachusetts .
Lindsay , R. K. (1963) . Inferentia l memor y as the ba­
sis of machine s whic h understan d natura l language .
In Feigenbaum , E. A. andFeldman , J., editors , Com­
puters  and Thought,  page s 217­236 . McGraw­Hill ,
New York .
Lindsay , R, K., Buchanan , B. G., Feigenbaum ,
E. A., and Lederberg , J. (1980) . Applications  of
Artificial  Intelligence  for Organic  Chemistry:  The
DENDRAL  Project.  McGraw­Hill , New York .
Lloyd , J. W. (1987) . Foundations of Logic Program­
ming.  Springer­Verlag , Berlin .
Locke , W. N. and Booth , A. D. (1955) . Ma­
chine  Translation  of Languages:  Fourteen  Essays.
MIT Press , Cambridge , Massachusetts .
Longuet­Higgins , H. C. (1981) . A compute r algo ­
rithm for reconstructin g a scen e from two projec ­
tions . Nature,  293:133­135 .
Bibliograph y 883
Lovejoy , W. S. (1991) . A surve y of algorithmi c
method s for partiall y observe d Marko v decisio n
processes . Annals  of Operations  Research,  28(1 ­
4):47­66 .
Loveland , D. W. (1968) . Mechanica l theore m prov ­
ing by mode l elimination . Journal of the Association
for Computing  Machinery,  15(2):236­251 .
Loveland , D. W. (1984) . Automate d theorem ­
proving : A quarter­centur y review . Contemporary
Mathematics,  29:1­45 .
Lowe , D. G. (1987) . Three­dimensiona l objec t
recognitio n from singl e two­dimensiona l images .
Artificial  Intelligence,  31:355­395 .
Lowenheim , L. (1915) . Ube r moglichkeite n im
Relativkalkiil . Mathematische  Annalen,  76:447 ­
470. Reprinte d in Englis h translatio n in van Hei­
jenoor t (1967) .
Lowerre , B. T. and Reddy , R. (1980) . The HARP Y
speec h recognitio n system . In Lea, W. A., editor ,
Trends  in Speech  Recognition,  chapte r 15. Prentice ­
Hall, Englewoo d Cliffs , New Jersey .
Lowry , M. R. and McCartney , R. D. (1991) . Au­
tomating  Software  Design.  MIT Press , Cambridge ,
Massachusetts .
Loyd , S. (1959) . Mathematical  Puzzles  of Sam
Loyd:  Selected  and Edited  by Martin  Gardner.
Dover , New York .
Lozano­Perez , T., Mason , M., and Taylor , R. (1984) .
Automati c synthesi s of fine­motio n strategie s for
robots . International  Journal  of Robotics  Research,
3(l):3­24 .
Lucas , J. R. (1961) . Minds , machines , and Godel .
Philosophy,  36.
Luger , G. F. and Stubblefield , W. A. (1993) . Ar­
tificial  Intelligence:  Structures  and  Strategies  for
Complex  Problem Solving.  Benjamin/Cummings ,
Redwoo d City , California , secon d edition .
Mackworth , A. K. (1973) . Interpretin g picture s of
polyhedra l scenes . Artificial  Intelligence,  4:121 ­
137.
Maes , P., Darrell , T., Blumberg , B., and Pentland ,
A. (1994) . ALIVE : Artificia l Life Interactiv e Vide o
Environment . In Proceedings  of the Twelfth Na­
tional  Conference  on Artificial  Intelligence  (AAAI­
94), page 1506 , Seattle , Washington . AAA I Press .Magerman , D. (1993) . Natural  Language Parsing
as Statistical  Pattern Recognition.  PhD thesis , Stan ­
ford University .
Mahanti , A. and Daniels , C. J. (1993) . A SIM D
approac h to paralle l heuristi c search . Artificial  In­
telligence,  60(2):243­282 .
Malik , J. (1987) . Interpretin g line drawing s of
curve d objects . International  Journal  of Computer
Vision,  1(1):73­103 .
Malik , J. and Rosenholtz , R. (1994) . Recover ­
ing surfac e curvatur e and orientatio n from textur e
distortion : a leas t square s algorith m and sensi ­
tivity analysis . In Eklundh , J.­O. , editor , Pro­
ceedings  of the Third  European  Conf.  on Com­
puter  Vision,  page s 353­364 , Stockholm . Springer ­
Verlag.Publishe d as Lectur e Note s in Compute r Sci­
ence 800.
Manin , Y. I. (1977) . A Course  in Mathematical
Logic.  Springer­Verlag , Berlin .
Mann , W. C. and Thompson , S. A. (1983) . Rela ­
tiona l proposition s in discourse . Technica l Repor t
RR­83­115 , Informatio n Science s Institute .
Manna , Z. and Waldinger , R. (1971) . Towar d au­
tomati c progra m synthesis . Communications  of the
Association  for Computing Machinery,  14(3):151 ­
165.
Manna , Z. and Waldinger , R. (1985) . The  Logi­
cal Basis  for Computer  Programming:  Volume  I:
Deductive  Reasoning.  Addison­Wesley , Reading ,
Massachusetts .
Manna , Z. and Waldinger , R. (1986) . Specia l rela­
tions in automate d deduction . Journal  of the Asso­
ciation  for Computing  Machinery,  33(1) : 1­59 .
Manna , Z. and Waldinger , R. (1992) . Fundamental s
of deductiv e progra m synthesis . IEEE  Transactions
on Software  Engineering,  18(8):674­704 .
Marchand , M., Golea , M., and Rujan , P. (1990) . A
convergenc e theore m for sequentia l learnin g in two­
layer perceptrons . Europhysics  Letters,  11:487 ­
492.
Markov , A. A. (1913) . An exampl e of statistica l
investigatio n in the text of "Eugen e Onegin " illus ­
tratin g couplin g of "tests " in chains . Proceedings
of the Academy  of Sciences  of St. Petersburg,  1.
Marr , D. (1982) . Vision:  A Computational  Inves­
tigation  into  the Human  Representation  and Pro­
cessing  of Visual  Information.  W. H. Freeman , New
York .
884 Bibliograph y
Marsland , A. T. and Schaeffer , J., editor s (1990) .
Computers,  Chess,  and Cognition.  Springer­Verlag ,
Berlin .
Martelli , A. and Montanari , U. (1973) . Additiv e
AND/O R graphs . In Proceedings  of the Thirdlnter­
national  Joint  Conference  on Artificial Intelligence
(IJCAI­73),  page s 1­11 , Stanford , California . IJ­
CAII .
Martelli , A. and Montanari , U. (1976) . Unificatio n
in linea r time and space : A structure d presentation .
Interna l Repor t B 76­16 , Istitut o di Elaborazion e
della Informazione , Pisa , Italy .
Martelli , A. and Montanari , U. (1978) . Optimizin g
decisio n trees throug h heuristicall y guide d search .
Communications  of the Association  for Computing
Machinery,  21:1025­1039 .
Martin , C. D. (1993) . Th e myt h of the awesom e
thinkin g machine . Communications  of the Associa­
tion for Computing  Machinery,  36(4) : 120­133 .
Martin , J. H. (1990) . A Computational  Model  of
Metaphor  Interpretation.  Academi c Press .
Maslov , S. Y. (1964) . An invers e metho d for estab ­
lishin g deducibilit y in classica l predicat e calculus .
Doklady  Akademii  nauk  SSSR,  159:17­20 .
Maslov , S. Y. (1967) . An invers e metho d for estab ­
lishin g deducibilit y of nonprene x formula s of the
predicat e calculus . Doklady  Akademii nauk  SSSR,
172:22­25 .
Maslov , S. Y. (1971) . Relationshi p betwee n tactic s
of the invers e metho d and the resolutio n method .
Seminars  in Mathematics,  V. A. Steklov  Mathe­
matical  Institute,  Leningrad, Consultants  Bureau,
New York­London,  16:69­73 .
Mason , M. T. (1993) . Kickin g the sensin g habit .
AI Magazine,  14(l):58­59 .
Mates , B. (1953) . Stoic  Logic.  Universit y of Cali ­
forni a Press , Berkele y and Los Angeles .
Maxwell , J. and Kaplan , R. (1993) . The interfac e
betwee n phrasa l and functiona l constraints . Com­
putational  Linguistics,  19(4):571­590 .
Mays , E., Apte , C., Griesmer , J., and Kastner , J.
(1987) . Organizin g knowledg e in a comple x finan ­
cial domain . IEEE  Expert,  2(3):61­70 .
McAllester , D. and Rosenblitt , D. (1991) . Sys ­
temati c nonlinea r planning . In Proceedings  of the
Ninth  National Conference  on Artificial  Intelligence(AAAI­91),  volum e 2, page s 634­639 , Anaheim ,
California . AAA I Press .
McAllester , D. A. (1980) . An outloo k on truth main ­
tenance . AI Mem o 551, MIT AI Laboratory , Cam ­
bridge , Massachusetts .
McAllester , D. A. (1988) . Conspirac y number s for
min­ma x search . Artificial  Intelligence,  35(3):287 ­
310.
McAllester , D. A. (1989) . Ontic:  A Knowledge
Representation  System  for Mathematics.  MIT Press ,
Cambridge , Massachusetts .
McAllester , D. A. and Givan , R. (1992) . Natura l
languag e synta x and first­orde r inference . Artificial
Intelligence,  56(1):1­20 .
McCarthy , J. (1958) . Program s with commo n sense .
In Proceedings  of the Symposium  on Mechanisation
of Thought  Processes,  volum e 1, page s 77­84 , Lon ­
don. Her Majesty' s Stationer y Office .
McCarthy , J. (1963) . Situations , actions , and causa l
laws. Mem o 2, Stanfor d Universit y Artificia l Intel ­
ligenc e Project , Stanford , California.Reprinte d as
part of McCarth y (1968) .
McCarthy , J. (1968) . Program s with commo n sense .
In Minsky , M. L., editor , Semantic  Information  Pro­
cessing,  page s 403­418 . MIT Press , Cambridge ,
Massachusetts .
McCarthy , J. (1977) . Epistemologica l problem s in
artificia l intelligence . In Proceedings  of the Fifth
International  Joint  Conference  on Artificial  Intel­
ligence  (IJCAI­77),  Cambridge , Massachusetts . U­
CAII .
McCarthy , J. (1978) . Histor y of LISP . In Wex ­
elblat , R. L., editor , History  of Programming  Lan­
guages:  Proceedings  of the ACM  SIGPLAN  Con­
ference,  page s 173­197 . Academi c Press.Publishe d
in 1981 ; 1978 is date of conference .
McCarthy , J. (1980) . Circumscription : a form of
non­monotoni c reasoning . Artificial  Intelligence,
13(l­2):27­39 .
McCarthy , J. and Hayes , P. J. (1969) . Som e philo ­
sophica l problem s from the standpoin t of artificia l
intelligence . In Meltzer , B., Michie , D., and Swann ,
M., editors , Machine  Intelligence  4, page s 463­502 .
Edinburg h Universit y Press , Edinburgh , Scotland .
Bibliograph y 885
McCawley , J. D. (1993) . Everything  That  Linguists
Have  Always  Wanted  to Know  About  Logic  But
Were  Ashamed  to Ask.  Universit y of Chicag o Press ,
Chicago , Illinois , secon d edition .
McCorduck , P. (1979) . Machines  Who Think:  A
Personal  Inquiry  into  the History and  Prospects  of
Artificial  Intelligence.  W. H. Freeman , New York .
McCulloch , W. S. and Pitts , W. (1943) . A logica l
calculu s of the idea s immanen t in nervou s activity .
Bulletin  of Mathematical  Biophysics,  5:1 15­137 .
McCune , W. W. (1992) . Automate d discover y of
new axiomatization s of the left grou p and righ t
group calculi . Journal  of Automated  Reasoning,
McDermott , D. (1976) . Artificia l intelligenc e meet s
natura l stupidity . SIGART  Newsletter,  57.
McDermott , D. (1978a) . Plannin g and acting . Cog­
nitive  Science,  2(2):71­109 .
McDermott , D. (1978b) . Tarskia n semantics , or, no
notatio n withou t denotation ! Cognitive  Science,
2(3).
McDermott , D. (1987) . A critiqu e of pure reason .
Computational  Intelligence,  3(3): 1 5 1­237 . Include s
response s by a numbe r of commentator s and fina l
rebutta l by the author .
McDermott , D. (1991) . Regressio n planning . Inter­
national  Journal  of Intelligent  Systems,  6:357^­16 .
McDermott , D. and Doyle , J. (1980) . Non ­
monotoni c logi c I. Artificial  Intelligence,  13(1 ­
2):41­72 .
McDermott , J. (1982) . Rl: A rule­base d config ­
urer of compute r systems . Artificial  Intelligence,
19(l):39­88 .
Mead , C. (1989) . Analog VLSI  and Neural  Systems.
Addison­Wesley , Reading , Massachusetts .
Megiddo , N. and Wigderson , A. (] 986) . On play by
mean s of computin g machines . In Halpern , J. Y., ed­
itor, Theoretical Aspects of Reasoning about Knowl­
edge:  Proceedings  of the 1986  Conference  (TARK­
86), page s 259­274 , Monterey , California . IBM and
AAAI , Morga n Kaufmann .
Melcuk , I. A. and Polguere , A. (1988) . A forma l
lexico n in the meaning­tex t theor y (or how to do lex­
ica with words) . Computational  Linguistics,  13(3 ­
4):261­275 .Mero , L. (1984) . A heuristi c searc h algorith m
with modifiabl e estimate . Artificial  Intelligence,
23(1) : 13­27 .
Metropolis , N., Rosenbluth , A., Rosenbluth , M.,
Teller , A., and Teller , E. (1953) . Equation s of state
calculation s by fast computin g machines . Journal
of Chemical  Physics,  21:1087­1091 .
Mezard , M. and Nadal , J.­P. (1989) . Learnin g in
feedforwar d layere d networks : Th e tilin g algo ­
rithm . Journal  of Physics,  22:2191­2204 .
Michalski , R. S., Carbonell , J. G., and Mitchell ,
T. M., editor s (1983) . Machine  Learning:  An Ar­
tificial  Intelligence  Approach,  volum e 1. Morga n
Kaufmann , San Mateo , California .
Michalski , R. S., Carbonell , J. G., and Mitchell ,
T. M., editor s (1986) . Machine  Learning:  An Ar­
tificial  Intelligence  Approach,  volum e 2. Morga n
Kaufmann , San Mateo , California .
Michie , D. (1966) . Game­playin g and game ­
learnin g automata . In Fox , L., editor , Advances
in Programming  and Non­Numerical  Computation,
page s 183­200 . Pergamon , New York .
Michie , D. (1972) . Machin e intelligenc e at Edin ­
burgh . Management  Informatics,  2(1):7—12 .
Michie , D. (1982) . Th e state of the art in ma­
chine learning . In Introductory  Readings  in Expert
Systems,  page s 209­229 . Gordo n and Breach , New
York .
Michie , D. (1986) . Curren t development s in ex­
pert systems . In Proc.  2nd  Australian  Conference
on Applications  of Expert  Systems,  page s 163­182 ,
Sydney , Australia .
Michie , D. and Chambers , R. A. (1968) . BOXES :
An experimen t in adaptiv e control . In Dale , E.
and Michie , D., editors , Machine  Intelligence 2,
page s 125­133 . Elsevier/North­Holland , Amster ­
dam, London , New York .
Michie , D., Spiegelhalter , D. J., and Taylor , C. C.,
editor s (1994) . Machine  Learning, Neural  and Sta­
tistical  Classification.  Elli s Horwood , Chichester ,
England .
Miles , F. A. (1969) . Excitable  Cells.  Willia m Heine ­
mann Medica l Books , London .
Mill, J. S. (1843) . A System  of Logic,  Ratiocina­
tive and  Inductive:  Being  a Connected  View  of the
Principles  of Evidence,  and  Methods  of Scientific
Investigation.  J. W. Parker , London .
886 Bibliograph y
Mill, J. S. (1863) . Utilitarianism.  Parker , Son and
Bourn , London .
Miller , A. C, Merkhofer , M. M., Howard , R. A.,
Matheson , J. E., and Rice , T. R. (1976) . Develop ­
ment of automate d aids for decisio n analysis . Tech ­
nical report , SRI International , Menl o Park , Cali ­
fornia .
Miller , G. P., Todd , P. M., and Hegde , S. U.
(1989) . Designin g neura l network s usin g geneti c
algorithms . In Schaffer , J. D., editor , Proceedings
of the Third  International  Conference on Genetic  Al­
gorithms,  page s 379­384 , Arlington , Virginia . Mor­
gan Kaufmann .
Milne , A. A. (1926) . Winnie­the­Pooh.  Methuen ,
London.Wit h decoration s by Ernes t H. Shepard .
Minker , J., edito r (1988) . Foundations  of Deduc­
tive Databases and  Logic  Programming.  Morga n
Kaufmann , San Mateo , California .
Minsky , M. L. (1954) . Neural  Nets  and the Brain­
Model  Problem.  PhD thesis , Princeto n University .
Minsky , M. L., edito r (1968) . Semantic  Information
Processing.  MIT Press , Cambridge , Massachusetts .
Minsky,M . L. (1975) . Aframewor k for representin g
knowledge . In Winston , P. H., editor , The Psychol­
ogy of Computer  Vision,  page s 211­277 . McGraw ­
Hill, New York.Originall y appeare d as an MIT Ar­
tificia l Intelligenc e Laborator y memo ; the presen t
versio n is abridged , but is the mos t widel y cited .
Another , later abridge d versio n appeare d in Hauge ­
land(1981) .
Minsky , M. L. and Papert , S. (1969) . Percep­
trons:  An Introduction  to Computational  Geometry.
MIT Press , Cambridge , Massachusetts , first edition .
Minsky , M. L. and Papert , S. (1988) . Percep­
trons:  An Introduction to  Computational  Geometry.
MIT Press , Cambridge , Massachusetts , expande d
edition .
Minton , S. (1984) . Constraint­base d generalization :
Learnin g game­playin g plan s from singl e examples .
In Proceedings  of the National  Conference  on Artifi­
cial Intelligence  (AA4/­S4) , page s 251­254 , Austin ,
TX. Morga n Kaufmann .
Minton , S. (1988) . Quantitativ e result s concernin g
the utilit y of explanation ­ base d learning . In Pro­
ceedings  of the Seventh National Conference  on Ar­
tificial  Intelligence  (AAAI­88),  St. Paul , Minnesota .
Morga n Kaufmann .Minton , S., Johnston , M. D., Philips , A. B., and
Laird , P. (1992) . Minimizin g conflicts : a heuris ­
tic repai r metho d for constrain t satisfactio n and
schedulin g problems . Artificial  Intelligence,  58(1 ­
3):161­205 .
Mitchell , T., Keller , R., and Kedar­Cabelli , S.
(1986) . Explanation­base d generalization : A uni­
fying view . Machine  Learning,  1:47­80 .
Mitchell , T. M. (1977) . Versio n spaces : a candidat e
eliminatio n approac h to rule learning . In Proceed­
ings of the Fifth International  Joint  Conference  on
Artificial  Intelligence  (IJCA1­77),  page s 305­310 ,
Cambridge , Massachusetts . IJCAII .
Mitchell , T. M. (1980) . Th e need for biase s in
learnin g generalizations . Technica l Repor t CBM ­
TR­117 , Departmen t of Compute r Science , Rutger s
University , New Brunswick , New Jersey .
Mitchell , T. M. (1982) . Generalizatio n as search .
Artificial  Intelligence,  18(2):203­226 .
Mitchell , T. M. (1990) . Becomin g increasingl y
reactiv e (mobil e robots) . In Proceedings  of the
Eighth  National  Conference  on Artificial  Intelli­
gence  (AAAI­90),  volum e 2, page s 1051­1058 ,
Boston , Massachusetts . MIT Press .
Mitchell , T. M., Utgoff , P. E., and Banerji , R.
(1983) . Learnin g by experimentation : acquirin g
and refinin g problem­solvin g heuristics . In Michal ­
ski, R. S., Carbonell , J. G., and Mitchell , T. M., ed­
itors, Machine  Learning:  An Artificial  Intelligence
Approach,  page s 163­190 . Morga n Kaufmann , San
Mateo , California .
Montague , R. (1970) . Englis h as a forma l language .
In Linguaggi  nella  Societa  e nella  Tecnica,  page s
189­224 . Edizion i di Comunita , Milan.Reprinte d
in (Thomason , 1974 , pp. 188­221) .
Montague , R. (1973) . Th e prope r treatmen t of
quantificatio n in ordinar y English . In Hintikka , K.
J. J., Moravcsik , J. M. E., and Suppes , P., editors ,
Approaches  to Natural  Language.  D. Reidel , Dor ­
drecht , The Netherlands .
Moore , A. W. and Atkeson , C. G. (1993) . Prior ­
itized sweeping—reinforcemen t learnin g with less
data and less time . Machine  Learning,  13:103­130 .
Moore , J. and Newell , A. (1973) . Ho w can Mer ­
lin understand ? In Gregg , L., editor , Knowledge
and Cognition.  Lawrenc e Erlbau m Associates , Po­
tomac , Maryland .
Bibliograph y 887
Moore , R. C. (1980) . Reasonin g abou t knowledg e
and action . Artificia l Intelligenc e Cente r Technica l
Note 191 , SRI International , Menl o Park , Califor ­
nia.
Moore , R. C. (1985a) . A forma l theor y of knowl ­
edge and action . In Hobbs , J. R. and Moore ,
R. C., editors , Formal  Theoriesofthe  Commonsense
World,  page s 319­358 . Ablex , Norwood , New Jer­
sey.
Moore , R. C. (1985b) . Semantica l consideration s
on nonmonotoni c logic . Artificial  Intelligence,
25(l):75­94 .
Moore , R. C. (1993) . Autoepistemi c logic revisited .
Artificial  Intelligence,  59(l­2):27­30 .
Moravec , H. (1988) . Mind  Children:  The  Future  of
Rabat  and Human  Intelligence.  Harvar d Universit y
Press , Cambridge , Massachusetts .
Morgenstern , L. (1987) . Knowledg e precondition s
for action s and plans . In Proceedings  of the Tenth
International  Joint  Conference  on Artificial  Intel­
ligence  (IJCAI­87),  page s 867­874 , Milan , Italy .
Morga n Kaufmann .
Morrison , P. and Morrison , E., editor s (1961) .
Charles  Babbage  and His  Calculating  Engines:  Se­
lected  Writings  by Charles  Babbage  and  Others.
Dover , New York .
Mostow , J. and Prieditis , A. E. (1989) . Discoverin g
admissibl e heuristic s by abstractin g and optimiz ­
ing: a transformationa l approach . In Proceedings
of the Eleventh International  Joint  Conference  on
Artificial  Intelligence  (IJCAI­89),  volum e 1, page s
701­707 , Detroit , Michigan . Morga n Kaufmann .
Motzkin , T. S. and Schoenberg , I. J. (1954) . The
relaxatio n metho d for linea r inequalities . Canadian
Journal  of Mathematics,  6(3):393^t04 .
Mourelatos , A. P. D. (1978) . Events , processes , and
states . Linguistics  and Philosophy,  2:415­434 .
Moussouris , J., Holloway , J., and Greenblatt , R.
(1979) . CHEOPS : A chess­oriente d processin g sys­
tem. In Hayes , J. E., Michie , D., and Mikulich , L. I.,
editors , Machine  Intelligence  9, page s 351 ­360 . El­
lis Horwood , Chichester , England .
Muggleton , S. (1991) . Inductiv e logi c program ­
ming . New  Generation  Computing,  8:295­318 .
Muggleton , S. (1992) . Inductive  Logic  Program­
ming.  Academi c Press , New York .Muggleton , S. and Buntine , W. (1988) . Machin e
inventio n of first­orde r predicate s by invertin g res­
olution . In MLC­88.
Muggleton , S. and Cao , R (1990) . Efficien t in­
ductio n of logic programs . In Proceedings  of the
Workshop  on Algorithmic  Learning  Theory,  Tokyo .
Muggleton , S., King , R. D., and Sternberg , M. J. E.
(1992) . Protei n secondar y structur e predictio n usin g
logic­base d machin e learning . Protein  Engineering,
5(7):647­657 .
Mundy , J. and Zisserman , A., editor s (1992) . Geo­
metric  Invariance in  Computer  Vision.  MIT Press ,
Cambridge , Massachusetts .
Nagel , T. (1974) . Wha t is it like to be a bat? Philo­
sophical  Review,  83:435­450 .
Nalwa , V. S. (1993) . A Guided  Tour  of Computer
Vision.  Addison­Wesley , Reading , Massachusetts .
Nau, D. S. (1980) . Patholog y on gam e trees : A sum ­
mary of results . In Proceedings  of the First  An­
nual National  Conference  on Artificial  Intelligence
(AAAI­80),  page s 102­104 , Stanford , California .
AAAI .
Nau, D. S. (1983) . Patholog y on gam e trees revis ­
ited, and an alternativ e to minimaxing . Artificial
Intelligence,  21(1 ­2):221­244 .
Nau, D. S., Kumar , V., and Kanal , L. N. (1984) .
Genera l branc h and bound , and its relatio n to A*
and AO* . Artificial  Intelligence,  23:29­58 .
Naur , P. (1963) . Revise d repor t on the algorithmi c
languag e Algo l 60. Communications  of the Associ­
ation  for Computing Machinery,  6(1) : 1­17 .
Neal , R. M. (1991) . Connectionis t learnin g of belie f
networks . Artificial  Intelligence,  56:71­113 .
Neapolitan , R. E. (1990) . Probabilistic  Reasoning
in Expert  Systems:  Theory  and Algorithms.  Wiley ,
New York .
Netto , E. (1901) . Lehrbuch  der Combinatorik.  B.
G. Teubner , Leipzig .
Newell , A. (1982) . The knowledg e level . Artificial
Intelligence,  18(1):82­127 .
Newell , A. (1990) . Unified  Theories of  Cogni­
tion. Harvar d Universit y Press , Cambridge , Mas ­
sachusetts .
Bibliograph y
Newell , A. and Ernst , G. (1965) . The searc h for
generality . In Kalenich , W. A., editor , Information
Processing  1965:  Proceedings  of IFIP  Congress
1965,  volum e 1, page s 17­24 . Spartan .
Newell , A. and Shaw , J. C. (1957) . Programmin g
the logi c theory  machine . In Proceedings  of the
1957  Western  Joint Computer  Conference,  page s
230­240 . IRE.
Newell , A., Shaw , J. C., and Simon , H. A. (1957) .
Empirica l exploration s with the logic theor y ma­
chine . Proceedings  of the Western  Joint  Computer
Conference,  15:218­239.Reprinte d in Feigenbau m
and Feldma n (1963) .
Newell , A., Shaw , J. C., and Simon , H. A. (1958) .
Ches s playin g program s and the proble m of com ­
plexity . IBM Journal of Research and Development,
4(2):320­335.Reprinte d in Feigenbau m and Feld ­
man (1963) .
Newell , A. and Simon , H. A. (1961) . GPS , a pro­
gram that simulate s huma n thought . In Billing ,
H., editor , Lernende  Automaten,  page s 109­124 .
R. Oldenbourg , Munich , Germany.Reprinte d in
(Feigenbau m and Feldman , 1963 , pp. 279­293) .
Newell , A. and Simon , H. A. (1972) . Human  Prob­
lem Solving.  Prentice­Hall , Englewoo d Cliffs , New
Jersey .
Neyman , A. (1985) . Bounde d complexit y justi ­
fies cooperatio n in the finitel y repeate d prisoners '
dilemma . Economics  Letters,  19:227­229 .
Nicholson , A. E. and Brady , J. M. (1992) . The data
associatio n proble m whe n monitorin g robo t vehi ­
cles usin g dynami c belie f networks . In ECAI  92:
10th European Conference  on Artificial  Intelligence
Proceedings,  page s 689­693 , Vienna , Austria . Wi­
ley.
Nilsson , N. J. (1965) . Learning  Machines:  Foun­
dations  of Trainable  Pattern­Classifying Systems.
McGraw­Hill , New York .
Nilsson , N. J. (1971) . Problem­Solving  Methods  in
Artificial  Intelligence.  McGraw­Hill , New York .
Nilsson , N. J. (1980) . Principles  of Artificial  Intel­
ligence.  Morga n Kaufmann , San Mateo , California .
Nilsson , N. J. (1984) . Shake y the robot . Technica l
Note 323, SRI International , Menl o Park , Califor ­
nia.Nilsson , N. J. (1990) . The  Mathematical  Founda­
tions  of Learning  Machines.  Morga n Kaufmann ,
San Mateo , California.Introductio n by Terrenc e I.
Sejnowsk i and Halber t White .
Nitta , K., Taki.K. , and Ichiyoshi , N. (1992) . Exper ­
imenta l paralle l inferenc e software . In Fifth  Gen­
eration  Computer  Systems  1992,  volum e 1, page s
166­190 , Tokyo . IOS Press .
Norvig , P. (1988) . Multipl e simultaneou s interpre ­
tation s of ambiguou s sentences . In Proceedings  of
the 10th  Annual  Conference  of the Cognitive  Sci­
ence Society.
Norvig , P. (1992) . Paradigms  of Artificial  Intel­
ligence  Programming:  Case  Studies  in Common
Lisp.  Morga n Kaufmann , San Mateo , California .
Nowick , S. M., Dean , M. E., Dill , D. L., and
Horowitz , M. (1993) . Th e desig n of a high ­
performanc e cach e controller : a case stud y in asyn ­
chronou s synthesis . Integration:  The  VLSI  Journal,
15(3):241­262 .
Nunberg , G. (1979) . The non­uniquenes s of seman ­
tic solutions : Polysemy . Language  and Philosophy,
3(2):143­184 .
Nussbaum , M. C. (1978) . Aristotle's  De Motu Ani­
malium . Princeto n Universit y Press , Princeton , New
Jersey .
O'Keefe , R. (1990) . The Craft of Prolog.  MIT Press ,
Cambridge , Massachusetts .
Olawsky , D. and Gini,  M. (1990) . Deferre d plannin g
and senso r use. In Sycara , K. P., editor , Proceed­
ings,  DARPA  Workshop  on Innovative  Approaches
to Planning,  Scheduling,  and  Control,  San Diego ,
California . Defens e Advance d Researc h Project s
Agenc y (DARPA) , Morga n Kaufmann .
Olesen , K. G. (1993) . Causa l probabilisti c network s
with both discret e and continuou s variables . IEEE
Transactions  on Pattern  Analysis  and Machine  In­
telligence  (PAMI),  15(3):275­279 .
Oliver , R. M. and Smith , J. Q., editor s (1990) . Influ­
ence Diagrams,  Belief  Nets and Decision Analysis.
Wiley , New York .
Olson , C. F. (1994) . Tim e and spac e efficien t pose
clustering . In Proceedings  of the IEEE  Conference
on Computer  Vision and  Pattern  Recognition,  page s
251­258 , Seattle , Washington .
Ortony , A., edito r (1979) . Metaphor  and Thought.
Cambridg e Universit y Press , Cambridge .
Bibliograph y 889
Osherson,D.N.,Stob,M. , and Weinstein,S . (1986) .
Systems  That  Learn:  An  Introduction  to Learn­
ing Theory  for Cognitive  and  Computer  Scientists.
MIT Press , Cambridge , Massachusetts .
Paige , R. and Henglein , F. (1987) . Mechanica l
translatio n of set theoreti c proble m specification s
into efficien t RAM code— a case study . Journal  of
Symbolic  Computation,  4:207­232 .
Palay , A. J. (1985) . Searching  with  Probabilities.
Pitman , London .
Palmieri , G. and Sanna,R . (1960) . Automati c prob ­
abilisti c programmer/analyze r for patter n recogni ­
tion. Methodos,  12(48):331­357 .
Papadimitriou , C. H. and Yannakakis , M. (1994) .
On complexit y as bounde d rationality . In Sympo­
sium on Theory  of Computation  (STOC­94).
Parker , D. B. (1985) . Learnin g logic . Techni ­
cal Repor t TR­47 , Cente r for Computationa l Re­
searc h in Economic s and Managemen t Science ,
Massachusett s Institut e of Technology , Cambridge ,
Massachusetts .
Partridge , D. (1991) . A New  Guide  to Artificial
Intelligence.  Ablex , Norwood , New Jersey .
Paterson , M. S. and Wegman , M. N. (1978) . Lin ­
ear unification . Journal  of Computer and  System
Sciences,  16:158­167 .
Patrick , B. G., Almulla , M., and Newborn , M. M.
(1992) . An uppe r boun d on the time complexit y
of iterative­deepening­A* . Annals  of Mathematics
and Artificial  Intelligence,  5(2­4):265­278 .
Paul, R. P. (1981) . Robot  Manipulators:  Math­
ematics,  Programming,  and  Control.  MI T Press ,
Cambridge , Massachusetts .
Peano , G. (1889) . Arithmetices  principia,  nova
methodo  exposita.  Fratre s Bocca , Turin .
Pearl , J. (1982a) . Reveren d Baye s on inferenc e en­
gines : A distribute d hierarchica l approach . In Pro­
ceedings  of the National  Conference  on Artificial
Intelligence  (AAAI­82),  page s 133­136 , Pittsburgh ,
Pennsylvania . Morga n Kaufmann .
Pearl , J. (1982b) . The solutio n for the branchin g
facto r of the alpha­bet a prunin g algorith m and its
optimality . Communications  of the Association  for
Computing  Machinery,  25(8):559­564 .
Pearl , J. (1984) . Heuristics:  Intelligent  Search
Strategies  for Computer  Problem  Solving.  Addison ­
Wesley , Reading , Massachusetts .Pearl , J. (1986) . Fusion , propagation , and struc ­
turin g in belie f networks . Artificial  Intelligence,
29:241­288 .
Pearl , J. (1987) . Evidentia l reasonin g usin g stochas ­
tic simulatio n of causa l models . Artificial  Intelli­
gence,  32:247­257 .
Pearl , J. (1988) . Probabilistic  Reasoning  in Intel­
ligent  Systems:  Networks  of Plausible  Inference.
Morga n Kaufmann , San Mateo , California .
Pednault , E. P. D. (1986) . Formulatin g multiagent ,
dynamic­worl d problem s in the classica l plannin g
framework . In Georgeff , M. P. and Lansky , A. L.,
editors , Reasoning  about  Actions  and Plans:  Pro­
ceedings  of the 1986  Workshop,  page s 47­82 , Tim ­
berline , Oregon . Morga n Kaufmann .
Peirce , C. S. (1870) . Descriptio n of a notatio n for
the logi c of relatives , resultin g from an amplifica ­
tion of the conception s of Boole' s calculu s of logic .
Memoirs  of the American  academy  of arts and sci­
ences,  9:317­378 .
Peirce , C. S. (1883) . A theor y of probabl e inference .
Note B. The logi c of relatives . In Studies  in logic
by members of  the Johns  Hopkins  University,  page s
187­203 , Boston .
Peirce , C. S. (1902) . Logi c as semiotic : the the­
ory of signs . Unpublishe d manuscript ; reprinte d in
(Buchler , 1955 , pp. 98­119) .
Penberthy , J. S. and Weld , D. S. (1992) . UCPOP :
A sound , complete , partia l orde r planne r for ADL .
In Proceedings  of KR­92,  page s 103­114 .
Peng , J. and Williams , R. J. (1993) . Efficien t learn ­
ing and plannin g withi n the Dyn a framework . Adap­
tive Behavior,  2:437­454 .
Penrose , R. (1990) . The emperor' s new mind : con­
cernin g computers , minds , and the laws of physics .
Behavioral  and Brain  Sciences,  13(4):643­654 .
Peot, M. and Smith , D. (1992) . Conditiona l nonlin ­
ear planning . In Hendler , J., editor , Proceedings  of
the First  International Conference on  AI Planning
Systems,  page s 189­197 , Colleg e Park , Maryland .
Morga n Kaufmann .
Pereira , F. (1983) . Logi c for natura l languag e anal ­
ysis. Technica l Note 275, SRI International .
890 Bibliograph y
Pereira , F. C. N. and Shieber , S. M. (1987) . Pro­
log and Natural­Language  Analysis.  Cente r for the
Stud y of Languag e and Informatio n (CSLI) , Stan ­
ford, California .
Pereira , F. C. N. and Warren , D. H. D. (1980) . Def­
inite claus e grammar s for languag e analysis : a sur­
vey of the formalis m and a compariso n with aug­
mente d transitio n networks . Artificial  Intelligence,
13:231­278 .
Peterson , C., Redfield , S., Keeler , J. D., and Hart ­
man, E. (1990) . An optoelectroni c architectur e for
multilaye r learnin g in a singl e photorefractiv e crys ­
tal. Neural  Computation,  2:25­34 .
Pinker , S. (1989) . Learnability  and Cognition.  MIT
Press , Cambridge , MA.
Place , U. T. (1956) . Is consciousnes s a brai n pro­
cess? British  Journal  of Psychology,  47:44­50 .
Plotkin , G. D. (1971) . Automatic Methods of Induc­
tive Inference.  PhD thesis , Edinburg h University .
Pnueli , A. (1977) . Th e tempora l logi c of pro­
grams . In Proceedings  of the 18th  IEEE  Symposium
on the Foundations  of Computer  Science  (FOCS­
77), page s 46­57 , Providence , Rhod e Island . IEEE ,
IEEE Compute r Societ y Press .
Pohl, I. (1969) . Bi­directiona l and heuristi c searc h
in path problems . Technica l Repor t 104 , SLA C
(Stanfor d Linea r Accelerato r Center , Stanford , Cal­
ifornia .
Pohl, I. (1970) . Firs t result s on the effec t of er­
ror in heuristi c search . In Meltzer , B. and Michie ,
D., editors , Machine  Intelligence  5, page s 219­236 .
Elsevier/North­Holland , Amsterdam , London , New
York .
Pohl, I. (1971) . Bi­directiona l search . In Meltzer ,
B. and Michie , D., editors , Machine  Intelligence  6,
page s 127­140 . Edinburg h Universit y Press , Edin ­
burgh , Scotland .
Pohl, I. (1973) . The avoidanc e of (relative ) catas ­
trophe , heuristi c competence , genuin e dynami c
weightin g and computationa l issue s in heuristi c
proble m solving . In Proceedings  of the Third  Inter­
national  Joint  Conference  on Artificial Intelligence
(UCAI­73),  page s 20­23 , Stanford , California . IJ­
CAII .
Pohl , I. (1977) . Practica l and theoretica l considera ­
tions in heuristi c searc h algorithms . In Elcock , E. W.
and Michie , D., editors , Machine  Intelligence  8,
page s 55­72 . Ellis Horwood , Chichester , England .Pollard , C. and Sag , I. A. (1994) . Head­Driven
Phrase  Structure  Grammar.  Universit y of Chicag o
Press , Chicago , Illinois .
Polya , G. (1957) . How  to Solve  It: A New  Aspect
of Mathematical  Method.  Doubleday , Garde n City ,
New York , secon d edition .
Pomerleau , D. A. (1993) . Neural  Network  Per­
ception  for Mobile  Robot Guidance.  Kluwer , Dor ­
drecht , The Netherlands .
Popper , K. R. (1959) . The  Logic of  Scientific  Dis­
covery.  Basi c Books , New York .
Popper , K. R. (1962) . Conjecture  sand  Refutations:
The Growth of  Scientific  Knowledge.  Basi c Books ,
New York .
Post, E. L. (1921) . Introductio n to a genera l theor y
of elementar y propositions . American  Journal of
Mathematics,  43:163­185 .
Pradhan , M., Provan , G. M., Middleton , B., and
Henrion , M. (1994) . Knowledg e engineerin g for
large belie f networks . In Proceedings  of Uncer­
tainty  in Artificial  Intelligence,  Seattle , Washington .
Morga n Kaufmann .
Pratt, V. R. (1976) . Semantica l consideration s on
Floyd­Hoar e logic . In Proceedings  of the 17th  IEEE
Symposium  on the Foundations  of Computer Sci­
ence,  pages  109­121 .
Prawitz , D. (1960) . An improve d proo f procedure .
Theoria,  26:102­139 .
Prawitz , D. (1965) . Natural  Deduction:  A Proof
Theoretical  Study.  Almquis t and Wiksell , Stock ­
holm .
Prieditis , A. E. (1993) . Machin e discover y of ef­
fectiv e admissibl e heuristics . Machine  Learning,
12(1­3): ! 17­141 .
Prinz , D. G. (1952) . Robo t chess . Research,  5:261 ­
266.
Prior , A. N. (1967) . Past,  Present, and  Future.  Ox­
ford Universit y Press , Oxford .
Pullum , G. K. (1991) . The Great  Eskimo  Vocabulary
Hoax  (and  Other  Irreverent  Essays  on the Study  of
Language).  Universit y of Chicag o Press , Chicago ,
Illinois .
Bibliograph y 891
Purdom , P. (1983) . Searc h rearrangemen t back ­
trackin g and polynomia l averag e time . Artificial
Intelligence,  21:117­133 .
Putnam , H. (1960) . Mind s and machines . In Hook ,
S., editor , Dimensions  of Mind,  page s 138­164 .
Macmillan , London .
Putnam , H. (1963) . 'Degre e of confirmation ' and
inductiv e logic . In Schilpp , P. A., editor , The Phi­
losophy  of Rudolf  Carnap.  Ope n Court , La Salle ,
Illinois .
Putnam , H. (1967) . The natur e of menta l states . In
Capitan , W. H. and Merrill , D. D., editors , Art,
Mind,  and  Religion,  page s 37­48 . Universit y of
Pittsburg h Press , Pittsburgh , Pennsylvania.Origina l
title was "Psychologica l predicates" ; title change d
in later reprint s at the reques t of the author .
Pylyshyn , Z. W. (1974) . Minds , machine s and phe­
nomenology : Som e reflection s on Dreyfus ' "Wha t
Computer s Can' t Do" . International  Journal  of
Cognitive  Psychology,  3(l):57­77 .
Pylyshyn , Z. W. (1984) . Computation  and Cogni­
tion: Toward  a Foundation  for Cognitive  Science.
MIT Press , Cambridge , Massachusetts .
Quillian , M. R. (1961) . A desig n for an under ­
standin g machine . Pape r presente d at a colloquium :
Semanti c Problem s in Natura l Language , King' s
College , Cambridge , England .
Quillian , M. R. (1968) . Semanti c memory . In Min ­
sky, M. L., editor , Semantic  Information  Process­
ing, page s 216­270 . MIT Press , Cambridge , Mas ­
sachusetts .
Quine , W. V. (1953) . Two dogma s of empiricism . In
From  a Logical  Point  of View,  page s 20­46 . Harpe r
and Row , New York .
Quine , W. V. (1960) . Word  and Object.  MIT Press ,
Cambridge , Massachusetts .
Quine , W. V. (1982) . Methods  of Logic.  Harvar d
Universit y Press , Cambridge , Massachusetts , fourt h
edition .
Quinlan , E. and O'Brien , S. (1992) . Sublanguage :
characteristic s and selectio n guideline s for MT. In
AI and Cognitive  Science  '92:  Proceedings  of An­
nual Irish  Conference  on Artificial Intelligence and
Cognitive  Science  '92,  page s 342­345 , Limerick ,
Ireland . Springer­Verlag .Quinlan , J. R. (1979) . Discoverin g rules from large
collection s of examples : a case study . In Michie , D.,
editor , Expert  Systems  in the Microelectronic  Age.
Edinburg h Universit y Press , Edinburgh , Scotland .
Quinlan , J. R. (1986) . Inductio n of decisio n trees .
Machine  Learning,  1:81­106 .
Quinlan , J. R. (1990) . Learnin g logica l definition s
from relations . Machine  Learning,  5(3):239­266 .
Quinlan , J. R. (1993) . Combinin g instance­base d
and model­base d learning . In Proceedings  of the
Tenth  International Conference  on Machine Learn­
ing, page s 236­243 , Amherst , Massachusetts . Mor­
gan Kaufmann .
Quinlan , J. R. and Cameron­Jones , R. M. (1993) .
FOIL : a midter m report . In Brazdil , P. B., editor , Eu­
ropean  Conference  on Machine Learning  Proceed­
ings (ECML­93),  page s 3­20 , Vienna . Springer ­
Verlag .
Quirk , R., Greenbaum , S., Leech , G., and Svartvik ,
J. (1985) . A Comprehensive  Grammar of  the En­
glish Language.  Longman , New York .
Rabiner , L. R. (1990) . A tutoria l on hidde n Marko v
model s and selecte d application s in speec h recogni ­
tion. Proceedings  of'the  /£££'.Reprinte d in Waibe l
and Lee (1990) .
Rabiner , L. R. and Juang , B.­H . (1993) . Fundamen­
tals of Speech  Recognition.  Prentice­Hall .
Raibert , M. H. (1986) . LeggedRobots  That  Balance.
MIT Press , Cambridge , Massachusetts .
Ramsey , F. P. (1931) . Trut h and probability . In
Braithwaite , R. B., editor , The Foundations of Math­
ematics  and Other  Logical  Essays.  Harcour t Brac e
Jovanovich , New York .
Raphael , B. (1968) . SIR : Semanti c informatio n
retrieval . In Minsky , M. L., editor , Semantic  In­
formation  Processing,  page s 33­134 . MIT Press ,
Cambridge , Massachusetts .
Raphael , B. (1976) . The  Thinking  Computer:  Mind
Inside  Matter.  W. H. Freeman , New York .
Ratner , D. and Warmuth , M. (1986) . Findin g a
shortes t solutio n for the n x n extensio n of the 15­
puzzl e is intractable . In Proceedings  of the Fifth  Na­
tional  Conference  on Artificial  Intelligence  (AAAI­
86), volum e 1, page s 168­172 , Philadelphia , Penn ­
sylvania . Morga n Kaufmann .
Reichardt , J. (1978) . Robots:  Fact,  Fiction,  and
Prediction.  Pengui n Books , New York .
892 Bibliograph y
Reichenbach , H. (1949) . The  Theory  of Probabil­
ity: An Inquiry into  the Logical  and Mathematical
Foundations  of the Calculus of  Probability.  Univer ­
sity of Californi a Press , Berkele y and Los Angeles ,
secon d edition .
Reif, J. H. (1979) . Complexit y of the mover' s prob ­
lem and generalizations . In Proceedings  of the 20th
IEEE  Symposium  on Foundations  of Computer  Sci­
ence,  page s 421­427 , San Juan , Puert o Rico . IEEE ,
IEEE Compute r Societ y Press .
Reiter , R. (1980) . A logi c for defaul t reasoning .
Artificial  Intelligence,  13(1 ­2):81­132 .
Reiter , R. (1991) . The fram e proble m in the situa ­
tion calculus : a simpl e solutio n (sometimes ) and a
completenes s resul t for goal regression . In Lifschitz ,
V., editor , Artificial  Intelligence  and  Mathematical
Theory  of Computation:  Papers  in Honor  of John
McCarthy,  page s 359­380 . Academi c Press , New
York .
Reitman , W. and Wilcox , B. (1979) . The structur e
and performanc e of the INTERIM. 2 Go program .
In Proceedings  of the Sixth International  Joint  Con­
ference  on Artificial Intelligence  (IJCAI­79),  page s
711­719 , Tokyo . IJCAII .
Remus , H. (1962) . Simulatio n of a learnin g machin e
for playin g Go. In Proceedings  IFIP  Congress,
pages 428­432 . Elsevier/North­Holland .
Renyi , A . (1970) . Probability  Theory.
Elsevier/North­Holland , Amsterdam , London , New
York .
Rescher , N. and Urquhart , A. (1971) . Temporal
Logic.  Springer­Verlag , Berlin .
Resnik , P. (1993) . Semanti c classe s and syntacti c
ambiguity . ARP A Worksho p on Huma n Languag e
Technology.Princeton .
Rich , E. and Knight , K. (1991) . Artificial  Intelli­
gence.  McGraw­Hill , New York , secon d edition .
Rieger , C. (1976) . An organizatio n of knowledg e
for proble m solvin g and languag e comprehension .
Artificial  Intelligence,  7:89­127 .
Ringle , M. (1979) . Philosophical  Perspectives  in
Artificial  Intelligence.  Humanitie s Press , Atlanti c
Highlands , New Jersey .
Rissanen , J. (1984) . Universa l coding , information ,
prediction , and estimation . IEEE  Transactions  on
Information  Theory,  IT­30(4):629­636 .Ritchie , G. D. and Hanna , F. K. (1984) . AM: a case
study in AI methodology . Artificial  Intelligence,
23(3):249­268 .
Rivest , R. L. (1987) . Learnin g decisio n lists . Ma­
chine  Learning,  2(3):229­246 .
Roach , J. W., Sundararajan , R., and Watson , L. T.
(1990) . Replacin g unificatio n by constrain t satis ­
factio n to improv e logi c progra m expressiveness .
Journal  of Automated Reasoning,  6(1 ):51­75 .
Roberts , D. D. (1973) . The  Existential  Graphs of
Charles  S. Peirce.  Mouton , The Hagu e and Paris .
Roberts , L. G. (1963) . Machin e perceptio n of three ­
dimensiona l solids . Technica l Repor t 315, MIT Lin­
coln Laboratory.Ph.D . dissertation .
Robinson , J. A. (1965) . A machine­oriente d logic
based on the resolutio n principle . Journal  of the
Association  for Computing  Machinery,  12:23­41 .
Rock , I. (1984) . Perception.  W. H. Freeman , New
York .
Rorty , R. (1965) . Mind­bod y identity , privacy , and
categories . Review  of Metaphysics,  19(l):24­54 .
Rosenblatt , F. (1957) . The perceptron : A perceiv ­
ing and recognizin g automaton . Repor t 85­460 ­
1, Projec t PARA , Cornel l Aeronautica l Laboratory ,
Ithaca , New York .
Rosenblatt , F. (1960) . On the convergenc e of re­
inforcemen t procedure s in simpl e perceptrons . Re­
port VG­1196­G­4 , Cornel l Aeronautica l Labora ­
tory, Ithaca , New York .
Rosenblatt , F. (1962) . Principles  of Neurodynamics.
Spartan , Chicago .
Rosenschein , J. S. and Genesereth , M. R. (1987) .
Communicatio n and cooperatio n amon g logic ­
based agents . In Friesen , O. and Golshani , F,
editors , Sixth  Annual  International  Phoenix  Con­
ference  on Computers  and Communications:  1987
Conference  Proceedings,  page s 594­600 . IEE E
Compute r Societ y Press .
Rosenschein , S. J. (1985) . Forma l theorie s of knowl­
edge in AI and robotics . New  Generation  Comput­
ing, 3(4):345­357 .
Rosenthal , D. M., edito r (1971) . Materialism  and
the Mind­Body  Problem.  Prentice­Hall , Englewoo d
Cliffs , New Jersey .
Ross , S. M. (1988) . A First  Course  in Probability.
Macmillan , London , third edition .
Bibliograph y 893
Rothwell , C. A., Zisserman , A., Mundy , J. L., and
Forsyth , D. A. (1993) . Efficien t mode l librar y ac­
cess by projectivel y invarian t indexin g functions . In
Proceedings  1992  IEEE  Computer  Society  Confer­
ence on Computer  Vision  and Pattern  Recognition,
pages 109­ 1 14, Champaign , Illinois . IEEE Com ­
puter Societ y Press .
Roussel , P. (1975) . Prolog : manua l de
referenc e et d'utilization . Technica l report ,
Group e d'lntelligenc e Artificielle , Universit e
d'Aix­Marseille .
Rouveirol , C. and Puget , J.­F. (1989) . A simpl e
and genera l solutio n for invertin g resolution . In
Proceedings  of the European  Working  Session  on
Learning,  page s 201­210 , Porto , Portugal . Pitman .
Rowe , N. C. (1988) . Artificial  intelligence  through
Prolog.  Prentice­Hall , Englewoo d Cliffs , New Jer­
sey.
Rumelhart , D. E., Hinton , G. E., and Williams , R. J.
(1986) . Learnin g interna l representation s by error
propagation . In Rumelhart , D. E. and McClelland ,
J. L., editors , Parallel  Distributed  Processing,  vol­
ume 1 , chapte r 8, page s 3 1 8­362 . MIT Press , Cam ­
bridge , Massachusetts .
Rumelhart , D. E. and McClelland , J. L., editor s
(1986) . Parallel  Distributed  Processing.  MIT Press ,
Cambridge , Massachusetts.I n two volumes .
Ruspini , E. H., Lowrance , J. D., and Strat,  T. M.
(1992) . Understandin g evidentia l reasoning . In­
ternational  Journal  of Approximate  Reasoning,
Russell , J. G. B. (1990) . Is screenin g for abdomina l
aortic aneurys m worthwhile ? Clinical  Radiology,
41:182­184 .
Russell , S. J. (1985) . The complea t guid e to MRS .
Repor t STAN­CS­85­1080 , Compute r Scienc e De­
partment , Stanfor d University .
Russell , S. J. (1986a) . Preliminar y step s towar d
the automatio n of induction . In Proceedings  of
the Fifth National Conference  on Artificial Intelli­
gence  (AAAI­86),  Philadelphia , Pennsylvania . Mor ­
gan Kaufrnann .
Russell , S. J. (1986b) . A  quantitativ e analy ­
sis of analog y by similarity . In Proceedings  of
the Fifth  National Conference  on Artificial  Intelli­
gence  (AAAI­86),  Philadelphia , Pennsylvania . Mor ­
gan Kaufrnann .Russell , S. J. (1988) . Tree­structure d bias. In Pro­
ceedings  of the Seventh  National  Conference  on
Artificial  Intelligence  (AAAI­88),  volum e 2, page s
641­645 , St. Paul , Minnesota . Morga n Kaufrnann .
Russell , S. J. (1992) . Efficien t memory­bounde d
searc h methods . In ECAI92:  10th  European  Con­
ference  on Artificial Intelligence  Proceedings,  pages
1­5, Vienna , Austria . Wiley .
Russell , S. J., Binder , J., and Roller , D. (1994) .
Adaptiv e probabilisti c networks . Technica l Re­
port UCB/CSD­94­824 , Compute r Scienc e Divi ­
sion, Universit y of Californi a at Berkeley .
Russell , S. J. and Grosof , B. (1987) . A declarativ e
approac h to bias in concep t learning . In Proceed­
ings of the Sixth  National  Conference  on Artificial
Intelligence  (AAAI­87),  Seattle , Washington . Mor ­
gan Kaufrnann .
Russell , S. J. and Subramanian , D. (1993) . Prov ­
ably bounde d optima l agents . In Proceedings  of the
Thirteenth  International  Joint  Conference  on Arti­
ficial  Intelligence  (IJCAI­93),  Chambery , France .
Morga n Kaufmann .
Russell , S. J. and Wefald , E. H. (1989) . On optima l
game­tre e searc h using rationa l meta­reasoning . In
Proceedings  of the Eleventh International  Joint
Conference  on Artificial  Intelligence  (IJCAI­89),
pages 334­340 , Detroit , Michigan . Morga n Kauf ­
mann .
Russell , S. J. and Wefald , E. H. (1991) . Do the Right
Thing:  Studies  in Limited  Rationality.  MIT Press ,
Cambridge , Massachusetts .
Ryder , J. L. (1971) . Heuristi c analysi s of large trees
as generate d in the gam e of Go. Mem o AIM­155 ,
Stanfor d Artificia l Intelligenc e Project , Compute r
Scienc e Department , Stanfor d University , Stanford ,
California .
Sacerdoti , E. D. (1974) . Plannin g in a hierarch y of
abstractio n spaces . Artificial  Intelligence,  5(2) : 115­
135.
Sacerdoti , E. D. (1975) . The nonlinea r natur e of
plans . In Proceedings  of the Fourth  International
Joint  Conference  on Artificial Intelligence  (IJCAI­
75), page s 206­214 , Tbilisi , Georgia . IJCAII .
Sacerdoti , E. D. (1977) . A Structure  for Plans
and Behavior.  Elsevier/North­Holland , Amster ­
dam, London , New York .
894 Bibliograph y
Sacerdoti , E. D., Pikes , R. E.. Reboh , R., Sagalow ­
icz, D., Waldinger , R, J., and Wilber , B. M. (1976) .
QLISP— a languag e for the interactiv e developmen t
of comple x systems . In Proceedings  of the AFIPS
National  Computer  Conference,  page s 349­356 .
Sachs , J. S. (1967) . Recognitio n memor y for syn­
tactic and semanti c aspect s of connecte d discourse .
Perception  and Psychophysics,  2:437—442 .
Sacks , E. and Joskowicz , L. (1993) . Automate d
modelin g and kinemati c simulatio n of mechanisms .
Computer  Aided  Design,  25(2) : 106­118 .
Sager , N. (1981) . Natural  Language  Information
Processing:  A Computer  Grammar of  English  and
Its Applications.  Addison­Wesley , Reading , Mas ­
sachusetts .
Salton , G. (1989) . Automatic  Text  Processing.
Addison­Wesley .
Sammut , C., Hurst , S., Kedzier , D., and Michie ,
D. (1992) . Learnin g to fly. In Proceedings  of the
Ninth  International  Conference  on Machine  Learn­
ing, Aberdeen . Morga n Kaufmann .
Samuel , A. L. (1959) . Som e studie s in machin e
learnin g usin g the gam e of checkers . IBM  Journal
of Research  and Development,  3(3):210­229 .
Samuel , A. L. (1967) . Som e studie s in machin e
learnin g usin g the gam e of checker s II—Recen t
progress . IBM  Journal  of Research  and  Develop­
ment,  11(6):601­617 .
Samuelsson , C. and Rayner , M. (1991) . Quantita ­
tive evaluatio n of explanation­base d learnin g as an
optimizatio n tool for a large­scal e natura l languag e
system . In Proceedings  of the Twelfth  International
Joint  Conference  on Artificial  Intelligence  (IJCAI­
91), page s 609­615 , Sydney . Morga n Kaufmann .
Saraswat , V. A. (1993) . Concurrent  constraint  pro­
gramming.  MIT Press , Cambridge , Massachusetts .
Savage , L. J. (1954) . The  Foundations  of Statistics.
Wiley , New York .
Sayre , K. (1993) . Thre e more flaw s in the computa ­
tiona l model . Pape r presente d at the APA (Centra l
Division ) Annua l Conference , Chicago , Illinois .
Schabes , Y., Abeille , A., and Joshi , A. K. (1988) .
Parsin g strategie s with 'lexicalized ' grammars : ap­
plicatio n to tree adjoinin g grammars . In Vargha , D.,
editor , Proceedings  of the 12th  International Con­
ference  on Computational  Linguistics  (COLING),volum e 2, page s 578­583 , Budapest , Hungary . John
von Neuman n Societ y for Compute r Science .
Schaeffer , J., Culberson , J., Treloar , N., and Knight ,
B. (1992) . A worl d championshi p calibe r checker s
program . Artificial  Intelligence,  53(2­3):273­289 .
Schalkoff , R. I. (1990) . Artificial  Intelligence:  An
Engineering  Approach.  McGraw­Hill , New York .
Schank , R. C. and Abelson , R. P. (1977) . Scripts,
Plans,  Goals, and  Understanding.  Lawrenc e Erl­
baum Associates , Potomac , Maryland .
Schank , R. C. and Riesbeck , C. K. (1981) . In­
side Computer  Understanding:  Five  Programs  Plus
Miniatures.  Lawrenc e Erlbau m Associates , Po­
tomac , Maryland .
Scherl , R. B. andLevesque,H . J. (1993) . The fram e
proble m and knowledge­producin g actions . In Pro­
ceedings  of the Eleventh  National Conference  on
Artificial  Intelligence  (AAAI­93),  page s 689­695 ,
Washington , D.C. AAA I Press .
Schmolze , J. G. and Lipkis , T. A. (1983) . Classi ­
ficatio n in the KL­ON E representatio n system . In
Proceedings  of the Eighth  International  Joint  Con­
ference  on Artificial Intelligence  (IJCAI­83),  Karl ­
sruhe , Germany . Morga n Kaufmann .
Schofield , P. D. A. (1967) . Complet e solutio n of
the eigh t puzzle . In Dale , E. and Michie , D.,
editors , Machine  Intelligence  2, page s 125­133 .
Elsevier/North­Holland , Amsterdam , London , New
York .
Schonfinkel , M. (1924) . Ube r die Baustein e der
mathematische n Logik . Mathematische  Annalen,
92:305­316.Translate d into Englis h and repub ­
lished as "On the buildin g block s of mathematica l
logic " in (van Heijenoort , 1967 , pp. 355­366) .
Schoppers , M. J. (1987) . Universa l plan s for reac ­
tive robot s in unpredictabl e environments . In Pro­
ceedings  of the Tenth  International  Joint  Conference
on Artificial Intelligence  (IJCAI­87),  page s 1039 ­
1046, Milan , Italy . Morga n Kaufmann .
Schoppers , M. J. (1989) . In defens e of reactio n
plans as caches . AIMagazine,  10(4):51­60 .
Schroder , E. (1877) . Der  Operationskreis  des
Logikkalkiils.  B. G. Teubner , Leipzig .
Schwuttke , U. M. (1992) . Artificia l intelligenc e for
real­tim e monitorin g and control . In Proceedings
of the IEEE International Symposium  on Industrial
Electronics,  volum e 1, page s 290­294 , Xian , China .
Bibliograph y 895
Scriven , M. (1953) . Th e mechanica l concep t of
mind . Mind,  62:230­240 .
Searle , J. R. (1969) . Speech  Acts:  An Essay  in
the Philosophy  of Language.  Cambridg e Universit y
Press , Cambridge .
Searle , J. R. (1980) . Minds , brains , and programs .
Behavioral  and Brain  Sciences,  3:417­457 .
Searle , J. R. (1984) . Minds,  Brains  and  Sci­
ence.  Harvar d Universit y Press , Cambridge , Mas ­
sachusetts .
Searle , J. R. (1992) . The  Rediscovery  of the Mind.
MIT Press , Cambridge , Massachusetts .
Sejnowski , T. J. and Rosenberg , C. R. (1987) . Par­
allel network s that learn to pronounc e Englis h text.
Complex  Systems,  1:145­168 .
Selfridge , O. G. (1959) . Pandemonium : A paradig m
for learning . In Blake , D. V. and Uttley , A. M.,
editors , Proceedings  of the Symposium  on Mecha­
nization  of Thought  Processes,  page s 511 ­529 , Ted­
dington , Unite d Kingdom . Nationa l Physica l Labo ­
ratory , Her Majesty' s Stationer y Office .
Selfridge , O. G. and Neisser , U. (1960) . Pat ­
tern recognitio n by machine . Scientific  Ameri­
can, 203:60­68.Reprinte d in FeigenbaumandFeld ­
man (1963) .
Sells , P. (1985) . Lectures  on Contemporary  Syn­
tactic  Theories:  An Introduction to  Government­
Binding  Theory,  Generalized  Phrase  Structure
Grammar,  and Lexical­Functional  Grammar.  Cen ­
ter for the Stud y of Languag e and Informatio n
(CSLI) , Stanford , California .
Selman , B., Levesque , H., and Mitchell , D. (1992) .
A new metho d for solvin g hard satisfiabilit y prob ­
lems. In Proceedings  of the Tenth  National  Con­
ference  on Artificial  Intelligence  (AAAI­92),  page s
440^46 , San Jose , California . AAA I Press .
Selman , B. and Levesque , H. J. (1993) . The com ­
plexit y of path­base d defeasibl e inheritance . Artifi­
cial Intelligence,  62(2):303­339 .
Shachter , R. D. (1986) . Evaluatin g influenc e dia­
grams . Operations  Research,  34:871­882 .
Shachter , R. D., D'Ambrosio , B., and Del Favero ,
B. A. (1990) . Symboli c probabilisti c infer ­
ence in belie f networks . In Proceedings  of the
Eighth  National  Conference  on Artificial Intelli­
gence  (AAAI­90),  page s 126­131 , Boston , Mas ­
sachusetts . MIT Press .Shachter , R. D. and Peot , M. A. (1989) . Simula ­
tion approache s to genera l probabilisti c inferenc e
on belie f networks . In Proceedings  of the Fifth
Conference  on Uncertainty  in Artificial  Intelligence
(UAI­89),  Windsor , Ontario . Morga n Kaufmann .
Shachter , R. S. and Kenley , C. R. (1989) . Gaus ­
sian influenc e diagrams . Management  Science,
35(5):527­550 .
Shafer , G. (1976) . A Mathematical  Theory  of Evi­
dence.  Princeto n Universit y Press , Princeton , New
Jersey .
Shafer , G. and Pearl , J., editor s (1990) . Readings
in Uncertain  Reasoning.  Morga n Kaufmann , San
Mateo , California .
Shankar , N. (1986) . Proof­Checking  Metamathe­
matics.  PhD thesis , Compute r Scienc e Department ,
Universit y of Texa s at Austin .
Shannon , C. E. (1950) . Programmin g a compute r for
playin g chess . Philosophical  Magazine,  41 (4):256 ­
275.
Shannon , C. E. and Weaver , W. (1949) . The  Math­
ematical  Theory  of Communication.  Universit y of
Illinoi s Press , Urbana .
Shapiro , E. (1981) . An algorith m that infer s theo ­
ries from facts . In Proceedings  of the Seventh  Inter­
national  Joint  Conference  on Artificial Intelligence
(IJCAI­8J),  Vancouver , Britis h Columbia . Morga n
Kaufmann .
Shapiro , E. Y. (1983) . A subse t of Concurren t Pro­
log and its interpreter . ICO T Technica l Repor t TR­
003, Institut e for New Generatio n Computin g Tech ­
nology , Tokyo .
Shapiro , S. C. (1979) . The SNeP S semanti c net­
work processin g system . In Findler , N. V., edi­
tor, Associative Networks:  Representation  and Use
of Knowledge  by Computers,  page s 179­203 . Aca ­
demi c Press , New York .
Shapiro , S. C. (1992) . Encyclopedia  of Artificial
Intelligence.  Wiley , New York , secon d edition.Tw o
volumes .
Sharpies , M., Hogg , D., Hutchinson , C., Torrance ,
S., and Young , D. (1989) . Computers  and Thought:
A Practical  Introduction  to Artificial Intelligence.
MIT Press , Cambridge , Massachusetts .
896 Bibliograph y
Shavlik , J. and Dietterich , T., editor s (1990) . Read­
ings in Machine Learning.  Morga n Kaufmann , San
Mateo , California .
Shenoy , P. P. (1989) . A valuation­base d languag e
for exper t systems . International  Journal  of Ap­
proximate  Reasoning,  3(5):383­411 .
Shieber , S. M. (1986) . An  Introduction  to
Unification­Based  Approaches  to Grammar.  Cente r
for the Stud y of Languag e and Informatio n (CSLI) ,
Stanford , California .
Shirayanagi , K. (1990) . Knowledg e representatio n
and its refinemen t in Go programs . In Marsland ,
A. T. and Schaeffer , J., editors , Computers,  Chess,
and Cognition,  page s 287­300 . Springer­Verlag ,
Berlin .
Shoham , Y. (1987) . Tempora l logic s in AI: seman ­
tical and ontologica l considerations . Artificial  In­
telligence,33(1)39­104.
Shoham , Y. (1988) . Reasoning  about  Change:  Time
and Causation  from  the Standpoint  of Artificial  In­
telligence.  MIT Press , Cambridge , Massachusetts .
Shoham , Y. and McDermott , D. (1988) . Problem s in
forma l tempora l reasoning . Artificial  Intelligence,
36(1):49­61 .
Shortliffe , E. H. (1976) . Computer­Based  Medical
Consultations:  MYCIN.  Elsevier/North­Holland ,
Amsterdam , London , New York .
Shwe , M. and Cooper , G. (1991) . An empirica l anal ­
ysis of likelihood­weightin g simulatio n on a large ,
multipl y connecte d medica l belie f network . Com­
puters  and Biomedical  Research,  1991 (5):453­475 .
Siekmann , J. and Wrightson , G., editor s (1983) .
Automation  of Reasoning.  Springer­Verlag ,
Berlin.Tw o volumes .
Sietsma , J. and Dow , R. J. F. (1988) . Neura l net
pruning—wh y and how . In IEEE  International  Con­
ference  on Neural  Networks,  page s 325­333 , San
Diego . IEEE .
Siklossy , L. and Dreussi , J. (1973) . An efficien t
robo t planne r whic h generate s its own procedures .
In Proceedings  of the Third  International  Joint  Con­
ference  on Artificial Intelligence  (IJCAI­73),  page s
423­430 , Stanford , California . IJCAII .
Simmons , R., Krotkov , E., Whittaker , W., and Al­
brecht , B. (1992) . Progres s toward s roboti c explo ­
ratio n of extrem e terrain . Applied  Intelligence:  TheInternational  Journal  of Artificial  Intelligence,  Neu­
ral Networks,  and  Complex  Problem­Solving  Tech­
nologies,  2(2): 163­180 .
Simon , H. A. (1958) . Rationa l choic e and the struc ­
ture of the environment . In Models of  Bounded
Rationality,  volum e 2. MIT Press , Cambridge , Mas ­
sachusetts .
Simon , H. A. (1963) . Experiment s with a heuristi c
compiler . Journal  of the Associationfor  Computing
Machinery,  10:493­506 .
Simon , H. A. (1981) . The  Sciences  of the Artificial.
MIT Press , Cambridge , Massachusetts , secon d edi­
tion.
Simon , H. A. and Newell , A. (1958) . Heuristi c
proble m solving : The next advanc e in operation s
research . Operations  Research,  6:l­10.Basedo n a
talk give n in 1957 .
Simon , H. A. and Newell , A. (1961) . Compute r
simulatio n of huma n thinkin g and proble m solving .
Datamation,  page s 35­37 .
Siskind , J. M. (1994) . Lexica l acquisitio n in the
presenc e of nois e and homonymy . In Proceedings
ofAAAI­94.
Skinner , B. F. (1953) . Science  and Human  Behav­
ior. Macmillan , London .
Skolem , T. (1920) . Logisch­kombinatorisch e Un­
tersuchunge n iibe r die Erfiillbarkei t oder Beweis ­
barkei t mathematische r Satz e nebs t eine m Theo ­
reme iiber die dicht e Mengen . Videnskapsselskapets
skrifter,  I. Matematisk­naturvidenskabeligklasse,4.
Skolem , T. (1928) . Ube r die mathematisch e Logik .
Norsk  maternatisk  tidsskrift,  10:125­142 .
Slagle , J. R. (1963a) . A heuristi c progra m that
solve s symboli c integratio n problem s in freshma n
calculus . Journal  of the Associationfor  Comput­
ing Machinery,  10(4).Reprinte d in Feigenbau m and
Feldman(1963) .
Slagle , J. R. (1963b) . Gam e trees , m & n minimax ­
ing, and the m & n alpha­bet a procedure . Artificia l
Intelligenc e Grou p Repor t 3, Universit y of Cali ­
fornia , Lawrenc e Radiatio n Laboratory , Livermore ,
California .
Slagle , J. R. (1971) . Artificial  Intelligence:  The
Heuristic  Programming  Approach.  McGraw­Hill ,
New York .
Bibliograph y 897
Slagle , J. R. and Dixon , J. K. (1969) . Experiment s
with som e program s that searc h gam e trees . Jour­
nal of the Association  for Computing  Machinery,
16(2) : 189­207 .
Slate , D. J. and Atkin , L. R. (1977) . CHES S 4.5—
The Northwester n Universit y ches s program . In
Frey, P. W., editor , Chess  Skill  in Man and  Machine,
page s 82­118 . Springer­Verlag , Berlin .
Slater , E. (1950) . Statistic s for the ches s compute r
and the facto r of mobility . In Symposium  on Infor­
mation  Theory,  page s 150­152 , London . Ministr y
of Supply .
Slotnan , A. (1978) . The  Computer  Revolution  in
Philosophy.  Harveste r Press , Hassocks , Sussex .
Sloman , A. (1985) . POPLOG , a multi­purpos e
multi­languag e progra m developmen t environment .
In Artificial  Intelligence—Industrial  and Commer­
cial Applications.  First  International  Conference,
page s 45­63 , London . Queensdale .
Smallwood , R. D. and Sondik , E. J. (1973) . The
optima l contro l of partiall y observabl e Marko v pro­
cesse s over a finit e horizon . Operations  Research,
21:1071­1088 .
Smith , D. E. (1989) . Controllin g backwar d infer ­
ence. Artificial  Intelligence,  39(2) : 145­208 .
Smith , D. E., Genesereth , M. R., and Ginsberg ,
M. L. (1986) . Controllin g recursiv e inference . Ar­
tificial  Intelligence,  30(3):343­389 .
Smith , D. R. (1990) . KIDS : a semiautomati c pro­
gram developmen t system . IEEE  Transactions  on
Software  Engineering,  16(9):1024­1043 .
Soderland , S. and Weld , D. (1991) . Evaluatin g non­
linea r planning . Technica l Repor t TR­91­02­03 ,
Universit y of Washingto n Departmen t of Compute r
Scienc e and Engineering , Seattle , Washington .
Solomonoff , R. J. (1964) . A forma l theor y of in­
ductiv e inference . Information  and Control,  1:1­22,
224­254 .
Sondik , E. J. (1971) . The  Optimal Control  of Par­
tially  Observable  Markov  Decision  Processes.  PhD
thesis , Stanfor d University , Stanford , California .
Spiegelhalter , D., Dawid , P., Lauritzen , S., and
Cowell , R. (1993) . Bayesia n analysi s in exper t sys­
tems . Statistical  Science,  8:219­282 .
Spiegelhalter , D. J. (1986) . Probabilisti c reason ­
ing in predictiv e exper t systems . In Kanal , L. N.and Lemmer , J. E, editors , Uncertainty  in Artificial
Intelligence,  page s 47­67 . Elsevier/North­Holland ,
Amsterdam , London , New York .
Spirtes , P., Glymour , C, and Schemes , R. (1993) .
Causation,  prediction,  and search.  Springer­Verlag ,
Berlin .
Srivas , M. and Bickford , M. (1990) . Forma l veri­
ficatio n of a pipeline d microprocessor . IEEE  Soft­
ware,  7(5):52­64 .
Steele , G. (1990) . Common  LISP:  The Language.
Digita l Press , Bedford , Massachusetts , secon d edi­
tion.
Stefik , M. J. (198la) . Plannin g and meta­planning .
Artificial  Intelligence,  16:141­169 .
Stefik , M. J. (1981b) . Plannin g with constraints .
Artificial  Intelligence,  16:111­140 .
Sterling , L. and Shapiro , E. (1986) . The Art of Pro­
log. MIT Press , Cambridge , Massachusetts .
Stevens , K. A. (1981) . The informatio n conten t of
textur e gradients . Biological  Cybernetics,  42:95 ­
105.
Stickel , M. E. (1985) . Automate d deductio n by the­
ory resolution . Journal  of Automated  Reasoning,
l(4):333­355 .
Stickel , M. E. (1988) . A Prolo g Technolog y Theo ­
rem Prover : implementatio n by an extende d Prolo g
compiler . Journal of Automated Reasoning,  4:353 ­
380.
Stockman , G. (1979) . A minima x algorith m bette r
than alpha­beta ? Artificial  Intelligence,  12(2) : 179­
196.
Stolcke , A. (1993) . A n efficien t probabilisti c
context­fre e parsin g algorith m that compute s prefi x
probabilities . Repor t TR­93­065 , ICSI , Berkeley .
Stone , H. S. and Stone , J. (1986) . Efficien t searc h
techniques : an empirica l stud y of the n­queen s
problem . Technica l Repor t RC 12057 , IBM Thoma s
J. Watso n Researc h Center , Yorktow n Heights , New
York .
Stonebraker , M. (1992) . The integratio n of rule sys­
tems and databas e systems . IEEE  Transactions  on
Knowledge  and Data  Engineering,  4(5):415­423 .
Strachey , C. S. (1952) . Logica l or non­mathematica l
programmes . In Proceedings  of the Association  for
Computing  Machinery (ACM),  page s 46—49,  On­
tario, Canada .
898 Bibliograph y
Subramanian,D . (1993) . Artificia l intelligenc e and
conceptua l design . In Proceedings  of the Thir­
teenth  International  Joint  Conference  on Artificial
Intelligence  (IJCAI­93),  page s 800­809 , Chambery ,
France . Morga n Kaufmann .
Subramanian , D. and Feldman , R. (1990) . The util­
ity of EBL in recursiv e domai n theories . In Proceed­
ings of the Eighth  National  Conference  on Artificial
Intelligence  (AAAI­90),  volum e 2, page s 942­949 ,
Boston , Massachusetts . MIT Press .
Subramanian , D. and Wang , E. (1994) . Constraint ­
based kinemati c synthesis . In Proceedings  of the
International  Conference  on Qualitative  Reasoning.
AAA I Press .
Sugihara , K. (1984) . A necessar y and sufficien t
conditio n for a pictur e to represen t a polyhedra l
scene . IEEE  Transactions  on Pattern  Analysis  and
Machine  Intelligence  (PAMI),  6(5):578­586 .
Sussman , G. J. (1975) . A Computer  Model  of Skill
Acquisition.  Elsevier/North­Holland , Amsterdam ,
London , New York .
Sussman , G. J. and McDermott , D. V. (1972) . From
PLANNE R to CONNIVER— a geneti c approach .
In Proceedings  of the 1972  AFIPS  Joint  Computer
Conference,  page s 1171­1179 .
Sussman , G. J. and Winograd , T. (1970) . MICRO ­
PLANNE R Referenc e Manual . AI Mem o 203, MIT
AI Lab, Cambridge , Massachusetts .
Sutton , R. S. (1988) . Learnin g to predic t by the
method s of tempora l differences . Machine  Learn­
ing, 3:9­44 .
Swade , D. D. (1993) . Redeemin g Charle s Bab ­
bage' s mechanica l computer . Scientific  American,
268(2):86­91 .
Tadepalli , P. (1993) . Learnin g from querie s and ex­
ample s with tree­structure d bias . In Proceedings
of the Tenth  International Conference  on Machine
Learning,  Amherst , Massachusetts . Morga n Kauf ­
mann .
Tail, P. G. (1880) . Not e on the theor y of the "15
puzzle" . Proceedings  of the Royal  Society  of Edin­
burgh,  10:664­665 .
Taki, K. (1992) . Paralle l inferenc e machin e PIM .
In Fifth  Generation  Computer  Systems  1992,  vol­
ume 1, page s 50­72 , Tokyo . IOS Press .Tambe , M., Newell , A., and Rosenbloom , P. S.
(1990) . Th e proble m of expensiv e chunk s and
its solutio n by restrictin g expressiveness . Machine
Learning,  5:299­348 .
Tanimoto , S. (1990) . The Elements of Artificial In­
telligence  Using  Common  LISP.  Compute r Scienc e
Press , Rockville , Maryland .
Tarjan , R. E. (1983) . Data  Structures  and Network
Algorithms.  CBMS­NS F Regiona l Conferenc e Se­
ries in Applie d Mathematics . SIA M (Societ y for
Industria l and Applie d Mathematics , Philadelphia ,
Pennsylvania .
Tarski , A. (1935) . Die Wahrheitsbegrif f in den for­
malisierte n Sprachen . Studio  Philosophica,  1:261 ­
405.
Tarski , A. (1956) . Logic,  Semantics,  Metamathe­
matics:  Papers  from 1923 to 1938.  Oxfor d Univer ­
sity Press , Oxford .
Tash , J. K. and Russell , S. J. (1994) . Contro l strate ­
gies for a stochasti c planner . In Proceedings  of
the Twelfth  National Conference on  Artificial  In­
telligence  (AAA1­94),  Seattle , Washington . AAA I
Press .
Tate, A. (1975a) . Interactin g goal s and their use. In
Proceedings  of the Fourth  International  Joint  Con­
ference  on Artificial  Intelligence  (IJCAI­75),  page s
215­218 , Tbilisi , Georgia . IJCAII .
Tate, A. (1975b) . Using  Goal  Structure to  Direct
Search  in a Problem Solver.  PhD thesis , Universit y
of Edinburgh , Edinburgh , Scotland .
Tate, A. (1977) . Generatin g projec t networks . In
Proceedings  of the Fifth  International  Joint  Con­
ference  on Artificial  Intelligence  (IJCAI­77),  page s
888­893 , Cambridge , Massachusetts . IJCAII .
Tate, A. and Whiter , A. M. (1984) . Plannin g with
multipl e resourc e constraint s and an applicatio n to
a nava l plannin g problem . In Proceedings  of the
First  Conference  on AI Applications,  page s 410 ­
416, Denver , Colorado .
Tatman , J. A. and Shachter , R. D. (1990) . Dy ­
nami c programmin g and influenc e diagrams . IEEE
Transactions  on Systems,  Man  and  Cybernetics,
20(2):365­379 .
Taylor , R. J. (1989) . Revie w of erns t (1961) . In
Khatib , O., Craig , J. J., and Lozano­Perez , T.,
editors , The  Robotics  Review  ], page s 121­127 .
MIT Press , Cambridge , Massachusetts .
Bibliograph y 899
Tenenberg , J. (1988) . Abstractio n in planning .
Technica l Repor t TR250 , Universit y of Rochester ,
Rochester , New York .
Tennant , H. R., Ross , K. M., Saenz , R. M., and
Thompson , C. W. (1983) . Menu­base d natura l lan­
guag e understanding . In 21st  Annual  Meeting  of the
Association  for Computational  Linguistics:  Pro­
ceedings  of the Conference,  page s 151­158 , Cam ­
bridge , Massachusetts .
Tesauro , G. (1992) . Practica l issue s in tempora l dif­
ferenc e learning . Machine  Learning,  8(3­4):257 ­
277.
Tesauro , G. and Sejnowski , T. J. (1989) . A paralle l
networ k that learn s to play backgammon . Artificial
Intelligence,  39(3):357­390 .
Thomason , R. H., edito r (1974) . Formal  Philoso­
phy: Selected  Papers  of Richard  Montague.  Yal e
Universit y Press , New Haven , Connecticut .
Thorne , J., Bratley , P., and Dewar , H. (1968) . The
syntacti c analysi s of Englis h by machine . In Michie ,
D., editor , Machine  Intelligence 3,  page s 281­310 .
Elsevier/North­Holland , Amsterdam , London , New
York .
Todd , B. S., Stamper , R., and Macpherson , P.
(1993) . A probabilisti c rule­base d exper t system .
International  Journal  of Bio­Medical  Computing,
33(2):129­148 .
Tomasi , C. and Kanade , T. (1992) . Shap e and mo­
tion from imag e stream s unde r orthography : a fac­
torizatio n method . International  Journal of  Com­
puter  Vision,  9:137­154 .
Touretzky , D. S. (1986) . The  Mathematics  of In­
heritance  Systems.  Pitma n and Morga n Kaufmann ,
Londo n and San Mateo , California .
Touretzky , D. S., edito r (1989) . Advances  in Neural
Information  Processing  Systems  1. Morga n Kauf ­
mann , San Mateo , California .
Turing , A. M. (1936) . On computabl e numbers , with
an applicatio n to the Entscheidungsproblem . Pro­
ceedings  of the London  Mathematical Society,  2nd
.series , 42:230­26 5 .Correctio n publishe d in Vol. 43,
page s 544­546 .
Turing , A. M. (1950) . Computin g machiner y and
intelligence . Mind,  59:433—460 .
Turing , A. M., Strachey , C., Sates , M. A., and
Bowden , B. V. (1953) . Digita l computer s applie dto games . In Bowden , B. V., editor , Faster  Than
Thought,  page s 286­310 . Pitman , London.Turin g is
believe d to be sole autho r of the sectio n of this pape r
that deal s with chess .
Tversky , A. and Kahneman , D. (1982) . Causa l
schemat a in judgement s unde r uncertainty . In Kah ­
neman , D., Slovic , P., and Tversky , A., editors ,
Judgement  Under  Uncertainty:  Heuristics  and  Bi­
ases.  Cambridg e Universit y Press , Cambridge .
Ueda , K. (1985) . Guarde d Horn clauses . ICO T
Technica l Repor t TR­103 , Institut e for New Gener ­
ation Computin g Technology , Tokyo .
Ullman , J. D. (1989) . Principles  of Database  and
Knowledge­Base  By stems.  Compute r Scienc e Press ,
Rockville , Maryland .
Ullman , S. (1979) . The  Interpretation  of Visual  Mo­
tion. MIT Press , Cambridge , Massachusetts .
Valiant , L. (1984) . A theor y of the learnable . Com­
munications  of the Association  for Computing  Ma­
chinery,  27:1134­1142 .
van Benthem , J. (1983) . The  Logic  of Time.  D. Rei­
del, Dordrecht , The Netherlands .
van Benthem , J. (1985) . A Manual  of Intensional
Logic.  Cente r for the Stud y of Languag e and Infor ­
matio n (CSLI) , Stanford , California .
van Harmelen , F. an d Bundy , A. (1988) .
Explanation­base d generalisatio n = partia l evalua ­
tion. Artificial  Intelligence,  36(3):401­412 .
van Heijenoort , J., edito r (1967) . From  Frege  to
Godel:  A Source  Book  in Mathematical  Logic,
1879­1931.  Harvar d Universit y Press , Cambridge ,
Massachusetts .
Van Roy , P. L. (1990) . Can logi c programmin g
execut e as fast as imperativ e programming ? Re ­
port UCB/CS D 90/600 , Compute r Scienc e Divi ­
sion, Universit y of California , Berkeley.Ph.D . dis­
sertation .
VanLehn , K. (1978) . Determinin g the scop e of En­
glish quantifiers . Technica l Repor t AI­TR­483 , MIT
AI Lab.
Vapnik , V. N. and Chervonenkis , A. Y. (1971) . On
the unifor m convergenc e of relativ e frequencie s of
event s to thei r probabilities . Theory  of Probability
and Its Applications,  16:264­280 .
900 Bibliograph y
Vasconcellos , M. and Leon , M. (1985) . SPANA M
and ENGSPAN : machin e translatio n at the Pan
America n Healt h Organization . Computational  Lin­
guistics,  ll(2­3):122­136.
Veloso , M. and Carbonell , J. (1993) . Derivationa l
analog y in PRODIGY : automatin g case acquisition ,
storage , and utilization . Machine  Learning,  10:249 ­
278.
Vendler , Z. (1967) . Linguistics  and  Philosophy.
Cornel l Universit y Press , Ithaca , New York .
Vendler , Z. (1968) . Adjectives  andNominalizations.
Mouton , The Hagu e and Paris .
Vere, S. A. (1983) . Plannin g in time : window s and
duration s for activitie s and goals . IEEE  Transac­
tions  on Pattern  Analysis  and Machine  Intelligence
(PAMI),  5:246­267 .
von Mises , R. (1928) . Wahrscheinlichkeit,  Statistik
und Wahrheit.  J. Springer , Berlin.Translate d into
Englis h as von Mise s (1957) .
von Mises , R. (1957) . Probability,  Statistics,  and
Truth.  Alie n and Unwin , London .
Von Neumann , J. (1958) . The  Computer  and the
Brain.  Yal e Universit y Press , New Haven , Con ­
necticut .
Von Neumann , J. and Morgenstern , O. (1944) . The­
ory of games  and  economic  behavior.  Princeto n
Universit y Press , Princeton , New Jersey , first edi­
tion.
von Winterfeldt , D. and Edwards , W. (1986) . Deci­
sion Analysis  and Behavioral  Research.  Cambridg e
Universit y Press , Cambridge .
Voorhees , E. M. (1993) . Usin g WordNe t to dis­
ambiguat e wor d sense s for text retrieval . In Six­
teenth  Annual  International  ACM  SIGIR  Confer­
ence on Research  and Development in  Information
Retrieval,  page s 171­80 , Pittsburgh , Pennsylvania .
Associatio n for Computin g Machinery .
Waibel , A. and Lee, K.­F . (1990) . Readings  in
Speech  Recognition.  Morga n Kaufmann , San Ma­
teo, California .
Waldinger , R. (1975) . Achievin g severa l goal s si­
multaneously . In Elcock , E. W. and Michie , D.,
editors , Machine Intelligence  8, page s 94­138 . El­
lis Horwood , Chichester , England .
Waltz , D. (1975) . Understandin g line drawing s of
scene s with shadows . In Winston , P. H., editor ,The Psychology  of Computer  Vision.  McGraw­Hill ,
New York .
Wand , M. (1980) . Continuation­base d progra m
transformatio n strategies . Journal  of the ACM,
27(1):174­180 .
Wang , H. (1960) . Towar d mechanica l mathematics .
IBMJournal  of'Researchand  Development,  4:2­22 .
Wanner , E. and Gleitman , L., editor s (1982) . Lan­
guage  Acquisition:  The  State of  the Art. Cambridg e
Universit y Press .
Warren , D. H. D. (1974) . WARPLAN : a syste m
for generatin g plans . Departmen t of Computationa l
Logic Mem o 76, Universit y of Edinburgh , Edin ­
burgh , Scotland .
Warren , D. H. D. (1976) . Generatin g conditiona l
plans and programs . In Proceedings  of the AISB
Summer  Conference,  page s 344­354 .
Warren , D. H. D. (1983) . An abstrac t Prolo g in­
structio n set. Technica l Note 309, SRI International ,
Menl o Park , California .
Warren , D. H. D., Pereira , L. M., and Pereira , F.
(1977) . PROLOG : The languag e and its imple ­
mentatio n compare d with LISP . SIGPLANNotices,
12(8):109­115 .
Wasserman , P. D. and Oetzel , R. M., editor s (1990) .
NeuralSource:  The  Bibliographic  Guide to  Artifi­
cial Neural  Networks.  Van Nostran d Reinhold , New
York .
Watkins , C. J. (1989) . Models of Delayed Reinforce­
ment  Learning.  PhD thesis , Psycholog y Depart ­
ment , Cambridg e University , Cambridge , Unite d
Kingdom .
Watson , C. S. (1991) . Speech­perceptio n aids for
hearing­impaire d people : curren t statu s and neede d
research . Journal  of the Acoustical Society of Amer­
ica, 90(2):637­685 .
Webber , B. L. (1983) . So wha t can we talk abou t
now. In Brady , M. and Berwick , R., editors , Compu­
tational  Models  of Discourse.  MIT Press.Reprinte d
inGroszefa/ . (1986) .
Webber , B. L. (1988) . Tens e as discours e anaphora .
Computational  Linguistics,  14(2):61­73 .
Webber , B. L. and Nilsson , N. J. (1981) . Readings
in Artificial Intelligence.  Morga n Kaufmann , San
Mateo , California .
Bibliograph y 901
Weiss , S. M. and Kulikowski , C. A. (1991) . Com­
puter  Systems  That  Learn:  Classification  and  Pre­
diction  Methods  from  Statistics,  Neural  Nets,  Ma­
chine  Learning,  and Expert  Systems.  Morga n Kauf ­
mann , San Mateo , California .
Weizenbaum , J. (1965) . ELIZA— a compute r pro­
gram for the stud y of natura l languag e communica ­
tion betwee n man and machine . Communications  of
the Associationfor  Computing Machinery,  9( 1 ):36­
45.
Weizenbaum , J. (1976) . Computer  Power and  Hu­
man Reason.  W. H. Freeman , New York .
Weld , D. and Etzioni , O. (1994) . Th e firs t law
of robotics : A call to arms . In Proceedings  of
the Twelfth  National  Conference  on Artificial  In­
telligence  (AAAI­94),  Seattle , Washington . AAA I
Press .
Weld , D. S. (1994) . An introductio n to leas t com ­
mitmen t planning . AI Magazine.To  appear .
Weld , D. S. and de Kleer , J. (1990) . Readings
in Qualitative  Reasoning about  Physical  Systems.
Morga n Kaufmann , San Mateo , California .
Wellman , M. and Doyle , J. (1992) . Modula r utilit y
representatio n for decision­theoreti c planning . In
Proceedings,  First  International  Conference  on AI
Planning  Systems,  Colleg e Park , Maryland . Morga n
Kaufmann .
Wellman , M. P. (1985) . Reasonin g abou t preferenc e
models . Technica l Repor t MIT/LCS/TR­340 , Lab­
orator y for Compute r Science , MIT , Cambridge ,
Massachusetts.M.S . thesis .
Wellman , M. P. (1988) . Formulation  of Trade­
offs in Planning  under  Uncertainty.  Ph D thesis ,
Massachusett s Institut e of Technology , Cambridge ,
Massachusetts .
Wellman , M. P. (1990) . Fundamenta l concept s of
qualitativ e probabilisti c networks . Artificial  Intelli­
gence,  44(3):257­3Q3.
Werbos , P. (1974) . Beyond  Regression:  New  Tools
for Prediction  and  Analysis  in the Behavioral Sci­
ences.  PhD thesis , Harvar d University , Cambridge ,
Massachusetts .
Wheatstone , C. (1838) . On som e remarkable , and
hithert o unresolved , phenomen a of binocula r vision .
Philosophical  Transactions  of the Royal Society  of
London,  2:37'1­394 .Whitehead , A. N. (1911) . An Introduction  to Math­
ematics.  William s and Northgate , London .
Whitehead , A. N. and Russell , B. (1910) . Principia
Mathematica.  Cambridg e Universit y Press , Cam ­
bridge .
Whorf , B. (1956) . Language,  Thought,  andReality.
MIT Press , Cambridge , Massachusetts .
Widrow , B. (1962) . Generalizatio n and informa ­
tion storag e in network s of adalin e "neurons" . In
Yovits , M. C., Jacobi , G. T., and Goldstein , G. D.,
editors , Self­Organizing  Systems  1962,  page s 435­
461, Chicago , Illinois . Spartan .
Widrow , B. and Hoff , M. E. (1960) . Adaptiv e
switchin g circuits . In 796 0 IRE  WESCON  Con­
vention  Record,  page s 96­104 , New York .
Wiener , N. (1942) . The extrapolation , interpolation ,
and smoothin g of stationar y time series . OSR D
370, Repor t to the Service s 19, Researc h Projec t
DIC­6037 , MIT .
Wiener , N. (1948) . Cybernetics.  Wiley , New York .
Wilensky , R. (1983) . Planning  and Understanding.
Addison­Wesley .
Wilensky , R. (1990) . Computability , conscious ­
ness, and algorithms . Behavioral  and Brain  Sci­
ences,  13(4):690­69 1 .Pee r commentar y on Pen ­
rose (1990) .
Wilkins , D. E. (1980) . Usin g pattern s and plan s in
chess . Artificial  Intelligence,  14(2) : 165­203 .
Wilkins , D. E. (1986) . Hierarchica l planning : def­
initio n and implementation . In ECAI  '86:  Seventh
European  Conference  on Artificial  Intelligence,  vol­
ume 1, page s 466^178 , Brighton , Unite d Kingdom .
Wilkins , D. E. (1988) . Practical  Planning:  Extend­
ing the AI Planning Paradigm.  Morga n Kaufmann ,
San Mateo , California .
Wilkins , D. E. (1990) . Ca n AI planner s solv e
practica l problems ? Computational  Intelligence,
6(4):232­246 .
Wilks , Y. (1975) . An intelligen t analyze r and under ­
stande r of English . Communications  of the ACM,
18(5):264­274.Reprintedi n Groszefa/ . (1986) .
Wilson , R. H. and Schweikard , A. (1992) . Assem ­
bling polyhedr a with singl e translations . In IEEE
Conference  on Robotics  and  Automation,  page s
2392­2397 .
902 Bibliograph y
Winograd , S. and Cowan , J. D. (1963) . Reliable
Computation  in the  Presence  of Noise.  MIT Press ,
Cambridge , Massachusetts .
Winograd , T. (1972) . Understandin g natura l lan­
guage . Cognitive  Psychology,  3(l).Reprinte d as a
book by Academi c Press .
Winograd , T. and Flores , F. (1986) . Understanding
Computers  and Cognition.  Ablex , Norwood , New
Jersey .
Winston , P. H. (1970) . Learnin g structura l descrip ­
tions from examples . Technica l Repor t MAC­TR ­
76, Departmen t of Electrica l Engineerin g and Com ­
puter Science , Massachusett s Institut e of Technol ­
ogy, Cambridge , Massachusetts.Ph D dissertation .
Winston , P. H. (1992) . Artificial  Intelligence.
Addison­Wesley , Reading , Massachusetts , thir d
edition .
Wittgenstein , L. (1922) . Tractatus  Logico­
Philosophicus.  Routledg e and Kega n Paul , Lon ­
don, secon d edition.Reprinte d 1971 , edite d by D. F.
Pears and B. F. McGuinness . This editio n of the En­
glish translatio n also contain s Wittgenstein' s origi ­
nal Germa n text on facin g pages , as well as Bertran d
Russell' s introductio n to the 192 2 edition .
Wittgenstein , L. (1953) . Philosophical  Investiga­
tions.  Macmillan , London .
Woehr , J. (1994) . Lotf i visions , part 2. Dr. Dobbs
Journal,  217.
Wojciechowski , W. S. and Wojcik , A. S. (1983) .
Automate d desig n of multiple­value d logic circuit s
by automate d theore m provin g techniques . IEEE
Transactions  on Computers,  C­32(9):785­798 .
Wojcik , A. S. (1983) . Forma l desig n verificatio n
of digita l systems . In ACM  IEEE  20th  Design  Au­
tomation  Conference  Proceedings,  page s 228­234 ,
Miam i Beach , Florida . IEEE .
Woods , W. (1972) . Progres s in natura l languag e
understanding : An applicatio n to luna r geology . In
AFIPS  Conference  Proceedings.Vol.  42.
Woods , W. (1978) . Semantic s and quantificatio n in
natura l languag e questio n answering . In Advances
in Computers.  Academi c Press.Reprinte d in Gros z
etal. (1986) .
Woods , W. A. (1970) . Transitio n networ k gram ­
mars for natura l languag e analysis . Communica­
tions  of the Association  for Computing  Machinery,
13(10):591­606 .Woods , W. A. (1973) . Progres s in natura l languag e
understanding : An applicatio n to luna r geology . In
AFIPS  Conference  Proceedings,  volum e 42, page s
441­450 .
Woods , W. A. (1975) . What' s in a link : Foun ­
dation s for semanti c networks . In Bobrow , D. G.
and Collins , A. M., editors , Representation and  Un­
derstanding:  Studies  in Cognitive  Science,  page s
35­82 . Academi c Press , New York .
Wos, L., Carson , D., and Robinson , G. (1964) . The
unit preferenc e strateg y in theore m proving . In Pro­
ceedings  of the Fall  Joint  Computer  Conference,
page s 615­621 .
Wos, L., Carson , D., and Robinson , G. (1965) . Effi ­
cienc y and completenes s of the set­of­suppor t strat ­
egy in theore m proving . Journal  of the Association
for Computing  Machinery,  12:536­541 .
Wos, L., Overbeek , R., Lusk , E., and Boyle , J.
(1992) . Automated  Reasoning:  Introduction  and
Applications.  McGraw­Hill , New York , secon d edi­
tion.
Wos, L., Robinson , G., Carson , D., and Shalla , L.
(1967) . The concep t of demodulatio n in theore m
proving . Journal  of the Association  for Computing
Machinery,  14:698­704 .
Wos, L. and Robinson , G. A. (1968) . Paramod ­
ulatio n and set of support . In Proceedings  of
the IRIA  Symposium  on Automatic  Demonstration,
page s 276­310 . Springer­Verlag .
Wos, L. and Winker , S. (1983) . Ope n question s
solve d with the assistanc e of AURA . In Bledsoe ,
W. W. and Loveland , D. W., editors , Automated
Theorem  Proving:  After  25 Years:  Proceedings  of
the Special  Session  of the 89th  Annual Meeting  of
the American Mathematical Society,  page s 71­88 ,
Denver , Colorado . America n Mathematica l Society .
Wright , S. (1921) . Correlatio n and causation . Jour­
nal of Agricultural  Research,  20:557­585 .
Wright , S. (1934) . The metho d of path coefficients .
Annals  of Mathematical  Statistics,  5:161­215 .
Wu, D. (1993) . Estimatin g probabilit y distribution s
over hypothese s with variabl e unification . In Pro­
ceedings  of the Thirteenth  International  Joint  Con­
ference  on Artificial Intelligence  (IJCAI­93),  page s
790­795 , Chambery , France . Morga n Kaufmann .
Bibliograph y 903
Yang , Q. (1990) . Formalizin g plannin g knowledg e
for hierarchica l planning . Computational  Intelli­
gence,  6:12­24 .
Yarowsky , D. (1992) . Word­sens e disambigua ­
tion usin g statistica l model s of Roge(' s categorie s
traine d on larg e corpora . In Proceedings  of
COL1NG­92,  page s 454^60 , Nantes , France .
Yip, K. M.­K . (1991) . KAM:  A System  for In­
telligently  Guiding  Numerical  Experimentation  by
Computer.  MIT Press , Cambridge , Massachusetts .
Yoshikawa , T. (1990) . Foundations  of Robotics:
Analysis  and Control.  MIT Press , Cambridge , Mas ­
sachusetts .
Younger , D. H. (1967) . Recognitio n and parsin g of
context­fre e language s in time n3. Information  and
Control,  10(2) : 189­208 .
Zadeh , L. A. (1965) . Fuzz y sets. Information  and
Control,  8:338­353 .
Zadeh , L. A. (1978) . Fuzz y sets as a basis for a the­
ory of possibility . Fuzzy  Sets and Systems,  1:3­28 .
Zermelo , E. (1976) . An applicatio n of set theor y to
the theor y of chess­playing . Firbush  News,  6:37 ­42.Englis h translatio n of Germa n pape r given  at the
5th Internationa l Congres s of Mathematics , Cam ­
bridge , England , in 1912 .
Zilberstein , S. (1993) . Operational  Rationality
through  Compilation  of Anytime  Algorithms.  PhD
thesis , Universit y of California , Berkeley , Califor ­
nia.
Zimmermann , H.­J . (1991) . Fuzzy  Set Theory—And
Its Applications.  Kluwer , Dordrecht , The Nether ­
lands , secon d revise d edition .
Zobrist , A. L. (1970) . Feature  Extraction  and Rep­
resentation  for Pattern  Recognition  and the  Game
of Go. PhD thesis , Universit y of Wisconsin .
Zue, V., Seneff , S., Polifroni , J., Phillips , M., Pao,
C., Goddeau , D., Glass , J., and Brill , E. (1994) .
Pegasus : A spoke n languag e interfac e for on­lin e
air trave l planning . In ARPA  Workshop  on Human
Language  Technology.
Zuse , K. (1945) . Th e Plankalkul . Repor t
175, Gesellschaf t fur Mathemati k und Datenverar ­
beitung , Bonn.Technica l repor t versio n republishe d
in 1989 .
Index
Page number s in bold are definition s
of term s and algorithms ; page
number s in italics  are reference s to
the bibliography .
Symbol s
­ (in parsing) , 697
Si _i+ Sj (achieves) , 347
­< (before) , 347
", (link) , 319
{xlC}  (substitution) , 265
u (gap) , 711
a (learnin g rate) , 576
7 (discoun t factor) , 507
e­admissible , 106
e­ball , 553
i (uniquenes s operator) , 196
A (functiona l abstraction) , 195
A­expression , 195
X2 (chi squared) , 543
=>• (implies) , 167
A (and) , 166
O­ (equivalent) , 167
­. (not) , 167
V(or) , 166
i—>• (uncertai n rule) , 461
>­ (determination) , 633
V, 238
Dp (always) , 258
<>p (eventually) , 258
|= (entailment) , 158
r­ (derives) , 159
3! (exist s a unique) , 196
3 (ther e exists) , 191
V (for all), 190
n (Intersection) , 200
U (Union) , 200
G (Member),  200
C (subset) , 200
lp, A; q,B]  (lottery) , 473
~ (indifferent) , 473
>­ (preferred) , 473
MT (bes t prize) . 478
u\_ (wors t catastrophe) , 478
a, (activatio n value) , 568
A*­SEARCH , 97
A* search , 115
AAA I (America n Associatio n for
AI), 28
Aarup , M., 390, 859
ABC computer , 14Abeille , A., 687, 894
Abelson , R., 23, 894
ABO , see bounde d optimality ,
asymptoti c
ABSOLVER , 103
abstraction , 62, 409,410 , 794
abstractio n hierarchy , 380
abstractio n planning , 389
abstrac t operator , 376
abstrac t solution , 376
ABSTRIPS , 389
Abu­Mostafa,Y. , 595,859
accelerometer , 783
accessible , see environment ,
accessibl e
accusativ e case, see objectiv e case
Acharya , A., 117,316 , 859,  864
achievemen t (in planning) , 349, 357
acknowledgment , 684
acousti c model , 760, 762­76 4
in disambiguation , 682
ACRONYM , 769
ACT* , 645
ACTION , 48
action , 31, 39
conditional , 411
cooperative , 180
intelligent , 5
intermediate­leve l (ILA) , 787
low­level , 787
partiall y instantiated , 346
rational , 10,27,5 0
sensing , 392, 394,411,48 7
simultaneous , 216
action­utilit y table , 485
action­value , see agent , action­valu e
action­valu e function , 599
actio n description , 344
action model , 510
actio n monitoring , 401
action potential , 564
activatio n function , 567
activatio n level , 567
ACTIVE­ADP­AGENT , 608, 608
activ e learning , 599
activ e vision , 830
act phase , 314
actuator , 777
adaline , 19,59 4
Adams , J., 240
adaptiv e contro l theory , 601
adaptiv e dynami c programming ,
603,62 3adaptiv e network , see neura l
networ k
adaptiv e planning , 389
ADD­EDGE , 702,70 3
additivit y (of utilit y function) , 601
add list (in STRIPS) , 344
Adelson­Velsky , G., 144
adjectiv e phrase , 707
adjoi n (to a set), 199
adjunct , 671
ADL (plannin g formalism) , 390
admissibl e heuristic , see heuristic ,
admissibl e
Adorf , H.­M. , 390, 878
ADP, see dynami c programming ,
adaptiv e
ADP­UPDATE , 623
Advic e Taker , 18,22,17 9
agent,? , 26,31,4 9
action­value , 210
active , 607
architecture , 26, 842
autonomous , 153, 625
communicating , 683
decision­making , 773
decision­theoretic , 471, 508­51 3
design , 786
forward­chaining , 314
function , 500
goal­based , 43, 49, 201
greedy,  609
ideal , 49
ideal rational , 33
immortal , 507
intelligent , 27, 842, 848
knowledge­based , 13, 151, 185,
201,265,84 2
learning , 525
limite d rational , 246
logical , 213
model­based , 201
naive , 658
omniscient , 182
passive , 600
passiv e ADP , 623
passiv e learning , 623
planning , 337, 337­341,77 3
problem­solving , 55
program , 35, 37, 49
prepositiona l logic , 174
rational , 7,31 , 50, 493, 607
reflex , 40,49 , 201, 202,216 , 500,
529,587,82 8
reinforcemen t learning , 624
906 Index
replanning , 410
situate d planning , 403
software , 36
taxi­driving , 39, 507, 526, 843
tropistic , 201
uncooperative , 658
utility­based , 44, 49, 508
vacuum , 32, 51­52,5 8
wacky , 609
wumpu s world , 174­177 ,
201­203,66 2
agent­base d softwar e engineering ,
219
agent function , 411
aggregatio n (in planning) , 409
Agmon , S., 594, 859, 887
agrammatica l strings , 712
Agre,P.E. , 411,85 9
agreemen t (in a sentence) , 668
Aho, A. V., 853, 859
AI, see artificia l intelligenc e
Aiken,H. , 14
aircraf t carrie r scheduling , 371, 390
airport , drivin g to, 415
airpor t siting , 480,48 4
AISB , 28
Ait­Kaci , H., 328, 329,85 9
AI Winter , 25
al­Khowarazmi , 11
Alami , R., 788, 873
Alberti , 726, 768
Albrecht , B., 778, 896
alchemy , 4
algorithm , 11
algorithmi c complexity , see
Kolmogoro v complexit y
Alhazen , 768
Alien s (movie) , 777
ALIGN , 754,75 4
alignmen t method , 752
ALIVE , 36
ALL­RELATED­TO? , 322,32 2
ALL­RELS­lN , 321
ALL­RELS­OUT , 321, 322
Allais , M., 479, 859
Alien , B., 390,87 7
Alien , J., 28, 259, 364, 688, 859
Alien , R., 595, 859
Alien , W., 94, 760
Almana c Game , 495
Almuallim , H., 646, 859
Almulla , M., 117, 889
Aloimonos , J., 770, 859
alpha­beta , see search , alpha­bet a
alpha­bet a pruning , 130, 146, 844
Alshawi , H., 685, 686, 859
Alspector , J., 595, 859
Alterman , R., 389, 859altruism , 419
Alve y report , 24
ALVINN , 586 , 587
AM, 646,64 7
Amarel , S., 67, 86, 859
ambiguity , 658, 680­682,712­71 5
Ambros­Ingerson , J., 411 , 860
Amit , D., 595, 860
Ammon , K., 826,86 0
AMORD , 330
analogica l reasoning , 646
ANALOGY , 19, 29
analysi s of algorithms , 851
Analytica l Engine , 15, 142,82 3
Anantharaman , T. S., 144,87 6
anaphora , 679
And­Elimination , 172
And­Introduction , 172
Andersen , S., 465,86 0
Anderson , A. R., 839, 840, 860
Anderson.J. , 13,28 , 162,316,595 ,
645,860,87 5
annealing , see search , simulate d
annealing , 113
antecedent , 167
antelope , 658
anytim e algorithm , 844
aphasia , 565
APN , see probabilisti c network ,
adaptiv e
apparen t motion , 735
Appelt , D., 258,694 , 720, 874,87 6
APPEND , 395,40 2
applicabl e operator , 344
APPLY , 126
apposition , 707
approximatio n hierarchy , 380, 390
Apte , C., 325, 884
Arbuckle,T. , 862
arc consistency , 84
architecture , 35, 786
agent , 26, 842
blackboard , 770
cognitive , 316
for speec h recognition , 25
hybrid , 137
open planning , 369
parallel , 117
real­time , 329
robot , 786­79 0
rule­based , 316
subsumption , 411
types , 50
Arentoft , M., 390, 859
ARCS , 299,30 3
argumen t from disability , 823
argumen t from informality , 826Aristotle , 6, 9, 179,212,257,564 ,
838
Arlabosse , F., 27, 879
Arlazarov , V., 144,85 9
Armstrong , D., 839, 860
Arnauld , A., 10,471,493,86 0
article , 664
artificia l insemination , 834
artificia l intelligence , 3, 17
applications , 26
conferences , 28
definition , 4, 29
foundations , 615
histor y of, 8­26 , 28
industry , 24, 28
journals , 28
programmin g language , 18
as rationa l agen t design , 7
real­time . 843
societies , 28
strong , 29,81 8
subfields , 4
weak , 29, 818
artificia l sweeteners , 834
artificia l urea . 834
asbesto s removal , 480
Ashby,W . R.,594,86 0
Asimov,!. , 814,848 , 860
ASK , 152, 153,201,211,214,298 ,
299,323,325,342,629 ,
660,72 1
assertio n (logical) , 201
assignmen t (in an ATN) , 686
associativ e memory , 571
assumption , 326
Astrom , K., 520, 860
astronomer , 468
asymptoti c analysis , 852
Atanasoff , J., 14
Atkeson.C , 623,88 6
Atkin , L. R., 87, 897
ATMS , see truth maintenanc e
system , assumption­base d
ATN , see augmente d transitio n
networ k
atom , 667
atomi c event , 425
atomi c sentence , see sentence ,
atomi c
attention , 719
augmentation , 667, 669,677 , 684
augmente d transitio n network , 686
AURA , 297,313 , 330
Austin , J., 685,86 0
AUTOCLASS , 647
autoepistemi c logic , 466
auto insurance , 469
automata , 838, 849
Index 907
automate d debugging , 646
automate d reasoners , see theore m
prover s
automati c pilot , 329
automati c programming , 400,40 0
automotiv e industry , 774
autonomou s vehicle , 26, 517, 587,
775
underwate r (AUV) , 775
autonomy , 35, 39,49,77 3
avoidin g obstacles , 725
axiom , 198, 222
domai n closure , 254
effect , 205
frame , 206
independent , 198
successor­state , 206
usabl e (in OTTER) , 310
axon, 564
B
b* (branchin g factor) , 102
B* search , 143
b­tree , 704
Babbage,C. , 15,82 3
Bacchus , E, 432, 519,86 0
Bach , E., 259, 860
bachelor , 231
Bachmann,P. , 853, 860
BACK­CHAIN , 275,275 , 305,30 6
BACK­CHAIN­LIST , 275,27 5
BACK­PROP­UPDATE , 581
back­propagation , 21,24,578 ,
578­583,593,59 5
backed­u p value , 108
backgammon , 133­135,139,144 ,
615
background , 828, 830
backgroun d assumptions , 735
backgroun d knowledge , 282,62 6
backjumping , 309
backprojection , 806
backtracking , 84,77 1
chronological , 309
dependency­directed , 309
Backus­Nau r form , 186,655 , 667,
854
backwar d chaining , 272,275 , 305,
313,44 7
Bacon , R, 9
Bain , M., 466, 860
Bajcsy , R., 769, 770, 860
Baker , C. L., 28, 685, 860
Baker , J., 770, 860
Ballard , B., 144,325, 860,86 8
bandi t problem , 610
Bandyopadhyay , A., 770,85 9
Banerji , R., 552,559, 886bang­ban g control , 618
Bar­Hillel , Y., 685,720 , 860
Bar­Shalom , Y., 520,860,87 2
bar codes , 805
Ban, A., 28, 861
Barrett , A., 390, 867
Barrett , R., 330, 861
Barstow , D. R., 330,86 7
Barto , A., 623,86 7
Barwise , J., 181,213,86 7
BASE , 672
baseline , 739
Bates , M., 16, 142, 899
Battell , J. S., 141,87 4
Baum , L., 770,86 7
Baum­Welc h algorithm , 766
Bayes ' rule, 426, 426­427,431 ,
434, 436,449 , 760
Bayes , T., 426,431,86 7
Bayesia n learning , see learning ,
Bayesia n
Bayesia n network , see belie f
networ k
Bayesia n updating , 428,431,434 ,
435,51 0
Beal, D., 143, S67
Beck , H., 325,86 7
bee, 651
beer factor y scheduling , 371
beetle , see dung beetl e
behavior , see actio n
behavior­base d robotics , see
robotic s
behaviora l module , 789
behaviorism , 13, 15,5 0
Bel (belie f abou t state) , 509
^el (prediction) , 509
Belhumeur , P., 740, 867
belief , 157,243 , 820
degre e of, 424
BELIEF­NET­ASK , 446,447,45 2
BELIEF­NET­TELL , 446
belief function , 462
belief network , 25,436,464,498 ,
712,72 0
dynamic , 514,51 9
vs. neura l network , 592
sigmoid , 596
belief networ k approximation , 453
belief networ k construction , 440
belief networ k inference , 445­45 6
complexity , 453
belief networ k learning , 531
Bell, C., 390, 861
Bell, J., 213, 867
Belle (ches s computer) , 137
Bellman , R. E., 5, 86,503 , 504,520 ,
867Belsky , M. S., 862
benchmarking , 851
Bendix , P., 293,87 9
BennettJ. , 313, 330,87 4
Berlekamp , E., 143, 867
Berliner , H. J., 26,117,137 ,
143­145,867,86 2
Bernoulli , D., 494,86 2
Bernoulli,!. , 12,432,47 6
Bernstein , A., 143,86 2
Berry , C., 14
Berry , D. A., 623, 862
Bertsekas , D. P., 520,86 2
Berwick , R., 690
BEST­FIRST­SEARCH , 92,93,93,9 7
best­firs t search , 92, 115
best possibl e prize , 478
Beth, E. W., 292, 862
bettin g game , 424
Bezzel , M., 86
bias, 529
declarative , 636
Bibel , W., 293, 390,86 2
Bickford,M , 313,89 7
biconditional , 167
bidirectiona l search , 85, 89
bigra m model , 760,76 5
Binder , J., 596,89 5
bindin g list, 201,26 5
BINDINGS , 347,372 , 374
binocula r stereopsis , 737­742,750 ,
768
binomia l nomenclature , 258
biologica l naturalism , 819, 834
Birnbaum , L., 687, 862
Biro, J., 840, 862
Birtwistle , G., 331,86 2
bit (of information) , 540
Bitman , A., 144,85 9
Bitner,!. , 116,86 2
BKG , 139, 144
Black , E., 687,86 2
black­box , 625
blackboar d architecture , 770
Blenko.T. , 258,87 6
blind search , 73
Block , N., 839, 840,86 2
blockin g (in belie f networks) , 444
block s world , 19,23 , 260, 359,382 ,
383,404 , 827
Bloom , P., 687, 862
Blum , A., 595,86 2
Blumberg , B., 36,88 3
Blumer , A., 560,86 2
BMTP , 826
BNF , see Backus­Nau r form
BO, see bounde d optimalit y
Board , R., 560,86 2
908 Index
Bobrow , D., 19, 646, 862, 863
Boddy , M., 844, 868
Boden , M. A., 840, 863
body (of a clause) , 305
Boltzman n machine , 571,59 5
Boole , G., 11, 179, 212,291 , 863
Boolea n connective , see connective ,
Boolea n
Boolea n function , 570
Boolea n logic , 179
Boolos , G. S., 293, 825, 863
Booth , A. D., 720, 882
Booth , T., 687, 871
Borgida , A., 324, 332, 863
Boser.B. , 586, 595,881
BOTTOM­UP­PARSE , 666,666 , 697
boundar y set, 549
bounde d cutse t conditioning , 465
bounde d optimality , 845
asymptoti c (ABO) , 846
Bowden.B. , 16, 142,59 9
BOXES , 618
Boyer.R . S., 293,313 , 328,330 ,
826, 863
Boyer­Moor e theore m prover , 313,
330
Boyle , J., 294, 902
BP (Britis h Petroleum) , 539
Brachman , R., 261, 324. 325, 331,
332, 863,868,  882
Bradshaw , G. L., 881
Bradtke , S., 623, 861
Brady , D., 595, 877
Brady , J., 888
brain , 3, 9, 16,56 3
aphasia , 565
computationa l power , 566
cortex , 565
damage , optimal , 572
prosthesis , 835, 841
as recurren t network , 570
states , 819
super , 4, 12
in a vat, 821
vs. computer , 565­56 6
brain s caus e minds , 565, 819
branc h and bound , 116
branchin g factor , 74, 632
effective , 102, 131
Bransford , J., 722, 863
Bratko , I., 330, 644, 863
Bratley , P., 686, 899
Bratman , M. E., 839, 863
BREADTH­FIRST­SEARCH , 74
breadth­firs t search , 74, 85
Breese , J., 50, 495, 876
Brelaz , D., 116,863
Bresnan , J., 687, 863bridg e (card game) , 29
bridges , 800
Briggs , R., 256, 863
brightness , 729
Brill, E., 903
Broca , P., 565
Brooks , R., 411, 769, 789, 812, 863
Brouwer.P. , 623,861
Brown , A., 363
Brown , J. S., 260, 647, 868,882
Brudno,A. , 143
Brudno , A. L., 863
Brunelleschi , 768
Bryson , A., 21,578,595,86. ?
Buchanan , B., 22, 94,257,466 , 552,
559, 863,  864,  870,882
Buchler,J. , 864
buffalo , 690
BUILD , 260,33 0
bunch , 234
Bundy , A., 330, 331, 645, 864,  899
Bunt , H. C., 258, 864
Buntine , W., 646, 887
burgla r alarm , 437­43 8
Burstall , R., 258, 330, 864
Bylander , T., 363, 864
C (configuratio n space) , 791
C­BURIDAN , 41 1
C4.5, 559
Caianello , E., 594, 864
CALL , 307
Cambefort , Y., 50, 874
Camember t cheese , 221
Cameron­Jones , R., 644, 89J
Campbell , M. S., 144,57 6
Campbell , P. K., 832, 864
candidat e definition , 544
candidat e elimination , 549
can machine s think , 822, 831
Canny,J. , 733, 796, 811 , 864
canonica l distribution , 443
canonica l form , 270
Cantor , 826
Cao, E, 646, 887
Capek.J. , 810
Capek , K., 809
Carbonell , J., 560, 646, 864,885,
900
Cardano,G. , 12,43 1
Carnap , R., 424, 430,432, 864
Carnegi e Mello n University , 17,
138,586,70 7
Carson , D., 293, 902
cart­pol e problem , 617
case (of nouns) , 668
ease­base d planning , 389Casimir , A., 907
Cassandra , A., 520, 864
CATEGORY , 666
category , 228,229 , 317,32 3
categor y theor y (not) , 228
Caterpilla r English , 692
causa l influence , 441
causa l link , 347, 350, 368, 375
causa l network , see belie f networ k
causa l rules , 209
causa l support , 448
causa l theory , 254
causation , 169, 209, 429
caveman , 626
CCD , see charge­couple d devic e
cell decomposition , 796, 809, 812
exact , 797
cell layout , 69
Censu s Bureau , 301
cerebra l cortex , see brai n
certaint y equivalent , 478
certaint y factor , 23, 461, 466
CFG, see grammar , context­fre e
chaining , 280
chain problem , 89
Chakrabarti , P., 111,864
Chambers , R., 618, 622, 885
chanc e node (decisio n network) , 484
chanc e node (gam e tree) , 133
chanc e of winning , 127
Chang , C.­L. , 294, 864
Chang , K., 465, 871
change , 203, 234
continuous , 237
dealin g with , 177
channe l routing , 69
Chapman , D., 25, 363, 390,411 ,
859, 864
Chapuis.A. , 810,56 4
charge­couple d device , 727, 772
Charniak , E., 5, 23, 328, 331, 688,
720, 864,  865,  873
chart , 697
CHART­PARSE , 702
CHAT , 693, 694,72 0
Chatea u Latour , 834
Chatila , R., 788, 873
checkers , 18, 138, 142, 148,559 ,
823
chees e factor y (Camembert) , 221
Cheeseman , P., 25,467, 865
Chellas , B. F., 261,56 5
chemica l transmitter , 564
chemistry , 22
Chen , K., 145, 579
Cherniak , C., 50, 565
Chernobyl , 775
Chervonenkis , A., 560, 899
Index 909
chess , 15, 20, 26, 122, 137­138 ,
412,82 0
automaton , 141
program , 16
ratings , 137
CHES S 4.5, 87
X2 pruning , 543
Chickering,M. , 596,87 5
Chierchia , G., 28, 685, 865
CHILDREN , 666
chimpanzees , 651
Chines e room , 831­834,84 0
Chinook , 138, 148
Chitrao , M., 688, 865
choic e point , 306, 309
Chomsky , N., 15, 653, 656, 685,
686, 865
choose , 355, 855
CHOOSE­ATTRIBUTE , 537,540 , 541
CHOOSE­BEST­ACTION , 38
CHOOSE­BEST­CONTINUATION , 402
CHOOSE­DECOMPOSITION , 374 , 379
CHOOSE­LITERAL , 642­64 4
CHOOSE­OPERATOR , 356,356 , 357,
358, 374, 379, 382, 384,
385,38 5
Christmas , 831
chronologica l backtracking , 326
cHUGIN , 465
Chung , K. L., 433, 865
Church , A., 11, 292, 329, 865
Church , K., 688, 702, 720, 865
Churchland , P. M., 839, 840, 865
Churchland , P. S., 836, 839, 840,
865
ClGOE , 646
circui t verification , 226
circumscription , 459,46 6
city bloc k distance , 102
clarification , 680
Clark , K., 329,466 , 865
Clark , R., 687, 865
Clarke , A., 465
Clarke,M. , 144,56 5
class , see categor y
CLASSIC , 298,32 5
classificatio n (in descriptio n logic) ,
323
classificatio n (of examples) , 534
class probability , 561
clause , 182
CEINT , 646
Clinton , H., 308
CLIPS , 298
clobbering , 353
clock , 33
Clocksin , W., 330, 865
CLOSE­ENOUGH , 504close d class , 664
Clowes , M. B., 746, 769, 865
CLP, see logic programming ,
constrain t
CLP(R) , 329
clustering , 453, 465
CMU , see Carnegi e Mello n
Universit y
CNF, see conjunctiv e norma l form
CNLP , 411
coarticulation , 762, 765
Cobham.A. , 11,865 , 869
coercion , 409,410,41 1
cognitiv e architecture , 316
cognitiv e psychology , 13
cognitiv e robotics , 259
cognitiv e science , 6, 13, 17, 28
Cohen.J. , 328,810,565 , 866
Cohen , P., 720, 866
coherenc e relation , 717
coin flip, 383,461 , 462,476 , 540
collection , see categor y
collectiv e computation , see neura l
networ k
Collins , A., 646, 864
Colmerauer , A., 328 , 329, 685, 866
Colomb , R., 328, 866
Colossus , 14
command , 683
commitmen t
epistemological , 165
ontological , 165,185,417,45 9
premature , 381
commo n ground , 661
Commo n Lisp , see Lisp
commo n sense , 458
commonsens e ontology , 317
commonsens e summer , 258
communication , 161,254 , 651­68 5
competitiv e ratio , 808
compilation , 307, 379, 788
compilatio n time , 309
complemen t (of a verb) , 670, 710
COMPLETE? , 702
completenes s
of a plan , 349
of a proo f procedure , 160, 178 ,
277
of resolution , 286­29 0
of a searc h algorithm , 73, 85
theorem , 277
complet e plan , see plan , complet e
complet e proo f procedure , 277
COMPLETER , 698­701,702 , 702,
703
completer , 698
complexit y
sample , 554space , 73, 85
time, 73, 85
complexit y analysis , 74,85 2
comple x sentence , see sentence ,
comple x
comple x term , 211
complian t motion , 784, 795
COMPOSE , 273, 303
composit e decisio n process , 116
composit e object , see object ,
composite , 251
compositio n (of substitutions) , 273
compositionality , 163, 672
COMPOUND , 299
COMPOUND­ACTION , 663
COMPOUND? , 303
compounding , 703
compression , 615
computability , 11
computationa l learnin g theory , 553,
557,560,59 5
computer , 4, 14­1 5
vs. brain , 565­56 6
Computer s in Biomedicine , 23
compute r vision , see visio n
concept , see categor y
Conceptua l Graphs , 298
concerto , 830
conclusio n (of an implication) , 167
condition­actio n rule , see rule
CONDITION­PART , 395
CONDITIONAL­PLANNING­AGENT ,
395
CONDITIONAL? , 395
conditiona l effect , see effect ,
conditiona l
conditiona l independence , 429,431 ,
434,449,464 , 509
conditiona l link , 396
conditiona l planner , see planning ,
conditiona l
conditiona l probability , see
probability , conditiona l
conditiona l probabilit y table , 438
conditiona l step, 396
conditionin g (in belie f networks) ,
453
conditionin g (in planning) , 398
conditionin g case , 438
Condon,E. , 142,86 6
Condon,!. , 137,144,56 6
configuratio n space , 791,790­796 ,
809
generalized , 792
configuratio n spac e obstacle , 791
confirmatio n theory , 10,432
conflic t resolution , 298, 314 , 315
confrontation , 382
910 INDE X
conjunct , 166
conjunction , 166
conjunctio n (natura l language) , 664
conjunctiv e norma l form , 182,278 ,
282, 290,29 2
conjunctiv e queries , 309
connectionism , see neura l networ k
connectio n method , 293
connectiv e
Boolean , 165
logical , 16, 165,18 9
CONNIVER , 330
consciousness , 9, 29, 564, 823,
831­83 6
consequent , 167
consistency , 116, 277,54 5
CONSISTENT , 356,358,382,38 5
CONSISTENT­DET? , 635,63 5
consisten t decomposition , 373
consisten t plan , see plan , consisten t
conspirac y number , 143
constan t symbol , 186, 188, 211
constraint , 83
binary , 83
temporal , 370
time, 368
unary , 83
constrain t logic programming , see
logic programmin g
constrain t propagation , 84
constrain t satisfaction , 19, 65,
83­84,91,328,38 8
constrain t satisfactio n problem , 83
heuristic s for, 104
constructio n industry , 775
constructiv e induction , see
induction , constructiv e
CONTENTS , 663
contex t (in planning) , 395
context­fre e grammar , see grammar ,
context­fre e
context­sensitiv e grammar , see
grammar , context­sensitiv e
contingency , 392
contingenc y problem , see problem .
contingenc y
continuation , 307
continuit y (of preferences) , 474
continuous , see environment ,
continuou s
contour , 117, 735,745­74 9
contou r (of a state space) , 98
contradiction , 639
contrapositive , 312
contro l
lateral , 749
longitudinal , 750
contro l systems , 463contro l theory , 364, 498,510 , 539,
594,617,78 0
adaptive , 601,62 2
contro l uncertainty , 803
conventiona l signs , 651
conversio n to norma l form , 281­28 2
CONVINCE , 465
convolution , 731, 771
Conway,J. , 143, 861
Cook,S. , 12,173 , 853,86 6
Cooper , G., 465, 596, 866, 876,896
coordinat e frame , 734
Copeland,J. , 258, 840,86 6
Core Languag e Engine , 685
Cormen,T . H., 853,36 6
corpus , 704
correspondenc e theory , 821
coun t noun , 242
Covington , M. A., 688,86 6
Cowan , J., 19, 596, 866, 902
Cowell , R., 596, 897
Cox, R., 424,432 , 866
CPOP , 394, 395, 398, 399,401 ,
406,41 1
CPSC , 444, 456
CPT, see conditiona l probabilit y
table
crack , 746
Cragg,B. , 594, 866
Craik , K. J., 13, 180,86 6
Cray Blitz , 144
creativity , 15,817,82 3
Cresswell , M., 261,877
Crevier , D., 28, 866
crisis management , 775
critic (in learning) , 526,56 2
critic (in planning) , 379
criticalit y level , 380
critica l point , 798, 801
Cracker , S., 143, 873
Crockett , L., 207, 866
Croft , B., 258, 876
Croft , W., 685, 876
cross­correlation , 735, 740
cross­indexing , 302
cross­over , 620
cross­validation , 543,573,59 7
cross ratio , 754
crosswor d puzzle , 118
cryptarithmeti c problem , 65, 91
CSP, see constrain t satisfactio n
proble m
Culberson , J., 142,59 4
Cullingford , R., 23, 866
cult of computationalism , 818
cumulativ e learning , 638, 644
curiosity , 612current­best­hypothesis , 546, 559,
576
CURRENT­BEST­LEARNING , 547,
547, 548,55 5
Currie , K., 369, 390, 866, 871
Curry , H. B., 329, 867, 894
CUTOFF­TEST , 126,13 2
cutset , 455
cutse t conditioning , 454
bounded , 455
Cybenko,G. , 595,86 7
CYC , 258,317,828 , 844
D
D'Ambrosio , B., 465, 895
d­separation , 444,44 9
DAG , see directe d acycli c grap h
Dagum , P., 465,86 7
Dahl,O.­J. , 331,862 , 867
DALTON , 647
Daniels , C., 117,88 3
DANTE­II , 776
Dantzig,G. , 12,86 7
Darlington , J., 330, 864
Darrell , T., 36, 883
Dartmout h workshop , 17
Darwiche , A., 465,86 7
Darwin , C, 619
data­directe d inference , 274
data­drive n inference , 274
database , 328,693­69 4
data extraction , 696
data fusion , 512
dativ e case , 668
Davidson , D., 259,86 7
Davies , T., 633,646,685 , 867,87 6
Davies , T. R., 646
Da Vinci , L., 768
Davis , E., 182, 258,260 , 261, 825,
867
Davis , M., 286,292,86 7
Davis , R., 330, 646,86 7
Dawid,  P., 596,89 7
Dayan , P., 604,86 7
DBN , see belie f network , dynami c
DCG , see grammar , definit e claus e
DDN , see decisio n network ,
dynami c
deadlines , 368
Dean , M., 313,88 8
Dean , T., 364, 389,520 , 844, 868
Debreu , G., 483, 868
debugging , 222
Dechter , R., 99, 868
decisio n
one­shot , 472
problem , 11
rational , 416,471,49 1
Index 911
sequential , 472,487 , 498,510 ,
603
unde r uncertainty , 418
DECISION­LIST­LEARNING , 556 ,
556, 557
DECISION­THEORETIC­AGENT , 508 ,
511
DECISION­TREE­LEARNING , 535 ,
537,537,538 , 542,543,545 ,
557,561,562,635,636,63 8
decisio n analysis , 491
decisio n analyst , 491
decisio n cycle , 508
decisio n list, 555
decisio n maker , 491
decisio n network , 436,465,471 ,
484,484­486,493,49 4
dynamic , 516, 519
evaluation , 517
evaluation , 486
decisio n node , 485
decisio n theory , 12, 25,419 , 493
decisio n tree, 494, 531, 531
expressiveness , 532
learning , 530, 559
pruning , 542
declarativ e approach , 304
declarativ e bias, see bias, declarativ e
declarativism , 153,21 9
decomposabilit y (of lotteries) , 474
decomposition , 390
de Dombal , F. T, 432,433 , 867
deduction , 163
Deep  Blue , 138
deep structure , 686
Deep Thought , 138, 144
defaul t logic , see logic , defaul t
defaul t reasoning , see reasoning ,
defaul t
defaul t value , 229, 319
deferring , see planning , deferre d
de Finetti , B., 423,432,435 , 867
definit e claus e grammar , see
grammar , definit e claus e
definitio n (logical) , 198
definitions , 323
degre e of belief , 417
interval­valued , 459
degree s of freedom , 777,778 , 781,
791
de Groot , A., 142, 867
DeGroot , M. H., 433, 868
DeJong , G., 645, 868
de Kleer , J., 260, 328, 330, 332,
867, 868,  87J,  901
delete list (in STRIPS) , 344
Del Favero , B., 465, 895
deliberat e ambiguity , 682deliberation , 403, 843
demodulation , 284,284 , 293
demodulator , 310, 333
De Morgan , A., 212, 868
De Morga n rules , 193
demotion , 353
Dempster' s rule , 462
Dempster , A., 462,466 , 596, 868
Dempster­Shafe r theory , 457,459 ,
462, 466
Den (denotation) , 245
DENDRAE , 22­23,25 7
dendrite , 564
Denker , A., 26, 138
Denker , J., 585, 586,595, 621,88 1
Dennett , D. C., 50, 817, 820, 834,
840, 847, 868
Deo, N., 87, 868
DEPTH , 72,75
depth (of a node) , 72
DEPTH­FIRST­SEARCH , 78, 91
depth­firs t search , 77, 85, 305
DEPTH­LIMITED­SEARCH , 79
depth­limite d search , 78, 85
depth limit , 129
depth of field , 727
De Raedt , L., 868
derive d sentences , 160
Derr, W., 142, 866
deSarkar , S., 111,864
Descartes , R., 9, 10,768 , 838
Descotte , Y., 390, 868
descriptio n logic , see logic ,
description , 331
descriptiv e theory , 479
desire , 820
detachment , 460, 460
determination , 633,64 7
minimal , 635
determinations , 646
determiner , 708,70 8
deterministic , see environment ,
deterministi c
deterministi c node , 443
Devanbu.R , 325,868
DEVISER , 387, 389
Devol , G., 774, 810
Dewar , H., 686, 899
Dewe y Decima l system , 230
DFS­CONTOUR , 106,107 , 107
diachronic , 203
diagnosis , 416
dental , 416
medical , 209,443,461,487 , 848
diagnosti c inference , see inference ,
diagnosti c
diagnosti c rules , 209
diamete r (of a state space) , 78Dickmanns , E., 587, 750, 770, 869
dictionary , 21,251,70 4
Dietterich , T., 560, 859, 869,  896
differentiation . 629
diffusel y reflecte d light , 729
Digita l Equipmen t Corporation , 24,
316
Dijkstra , E. W., 87, 869
Dill, D., 313, 888
Dincbas , M., 330, 869
Dingwell , W. O., 653,86 9
directe d acycli c graph , 437, 465,
570
disabilities , 840
DISAMBIGUATE , 662 , 663
disambiguation , 658, 673,678 ,
680­682,685,71 2
discontinuities , 730
discontinuou s function , 571
discoun t factor , 507,62 4
discounting , 507, 519,52 0
discourse , 715
coherent , 717­72 0
understanding , 715­71 9
DISCOURSE­UNDERSTANDING , 715
discrete , see environment , discret e
discret e event , see event , discret e
discretization , 544
discriminatio n net, 559
dish, 250
disjoin t sets, 231
disjunct , 166
disjunction , 166
disjunctiv e effects , 409
disjunctiv e norma l form , 292
disparity , 737, 738
distribute d encoding , 577
divide­and­conquer , 341, 379,40 7
Dixon,J . K., 143,89 7
DNA.61 9
DO, 663
dolpHins , 651
domai n (in a CSP) , 83
domai n (in knowledg e
representation) , 197
domai n (of a rando m variable) , 420
domai n constraint , 785
dominanc e
stochastic , 481
strict , 481
dominatio n (of heuristics) , 102
Donskoy.M. , 144,85 9
Doran,J. , 86, 115, 117, 869
Double­Negatio n Elimination , 172
Dow , R. J. F, 595,89 6
Dow Jones , 696
downwar d solution , 376, 389, 391
Dowty,  D., 685,86 9
912 INDE X
Doyle , J., 50, 330, 332,459 , 494,
868, 869,885,901
Drabble , B., 390, 869
Draper , D., 411, 869,87 0
Dreussi , J., 389, 896
Dreyfus , H. L., 9, 817, 818, 827,
828, 869
Dreyfus , S. E., 86, 87, 504, 520,
827, 828 , 861,  869
drillin g rights , 487
drivin g
automated , 29, 586
droppin g conditions , 548
Droz.E. , 810, 864
DT­AGENT , 419
dualism , 9, 838
Dubois , D., 466, 869
duck , mechanical , 810
Duda , R., 23, 466, 869
dung beetle , 35,50 , 203
Dunham,B. , 21,87 7
du Pont , 24
Diirer , A., 768
Durrant­Whyte , H., 520, 882
Dyer,M. , 23,869
DYLAN , 329
dynamic , see environment , dynami c
dynamica l systems , 520
dynami c belie f network , see belie f
network , dynami c
dynami c belie f networks , 520
dynami c decisio n network , see
decisio n network , dynami c
dynami c logic , 261
dynami c programming , 87, 116 ,
503,520,603,623,697 ,
742, 765
adaptive , 603, 623
dynami c weighting , 116
Dzeroski , S., 869, 879
Dzeroski , S., 646
E(cat,  i) (event) , 236
£o (Englis h subset) , 662
£\ (Englis h subset) , 670
£2 (Englis h subset) , 680
Barley , J., 720, 869
earthquake , 437
Eastlake,D. , 143, 873
Ebeling,C , 137, 144,862,86 9
EBL, see explanation­base d learnin g
Eckert , J., 14
economics , 50,476,507 , 847
edge (in a chart) , 697
edge (in an image) , 730
edge (in a scene) , 745
edge detection , 730,73 3Edinburgh , 646, 810
Edmonds , D., 16
Edmonds.J. , 11­12,86 9
EDVAC , 14
Edwards , D., 143,685 , 874,87 6
Edwards , P., 840,87 0
Edwards , W., 493,90 0
EFFECT , 358, 381,382 , 385
effect , 344
conditional , 381, 389
disjunctive , 383
knowledge , 395
universall y quantified , 383, 389
effec t axiom , see axiom , effec t
effectiv e branchin g factor , 116
effector , 31,77 7
EFFECTS , 349,40 8
egomotion , 736
Ehrenfeucht , A., 862
8­puzzle,63 , 101, 103, 115
8­queen s problem , 64, 83, 86, 89
Einstein , A., 3
electrocutaneou s feedback , 777
electroni c circuit s domain , 223­22 6
ELEMENTS , 321
ELIZA , 20, 849
Elkan , C., 213,463,87 0
Elliot , G., 116,87 4
ELSE­PART , 395
emergen t property , 833
empiricism , 9
Empson , W., 687, 870
EMPTY? , 72
EMV , see expecte d monetar y valu e
encode d messag e model , 659,68 4
end effector , 777, 781
Enderton,H . B., 213,292,87 0
Engelberger , G., 810
Engelberger , J. F., 810, 870
English , 19,21,2 9
subset , 662
EN1AC , 14
ENQUEUE­AT­END , 74
ENQUEUE­AT­FRONT , 78
entailment , 158, 163 , 170, 277
entailmen t constraint , 626,637 , 644
Entscheidungsproblem , 11
environment , 31,40 , 790
accessible , 46, 122,50 0
artificial , 36
continuous , 46, 774
deterministic , 46
discrete , 46
dynamic , 46, 774
episodic , 46
game­playing , 147,62 4
inaccessible , 46, 500, 773
nondeterministic , 46, 773nonepisodic , 46, 773
properties , 46
propertie s of, 773
semidynamic , 46
static , 46
taxi, 39
vacuum , 65
environmen t class , 49
environmen t history , 499
environmen t simulator , 47
EPAM (Elementar y Perceive r And
Memorizer) , 559
epiphenomenalism , 836
epipola r line, 740
episodic , see environment , episodi c
epistemologica l commitment , 165
epistemologica l level , 153
epoch , 576,60 0
equalit y (logical) , 193­19 4
equalit y substitution , 291
equalit y symbol , 193
equilibrium , 604
equivalenc e (logical) , 167
Erman , L. D., 770,87 0
Ernst , G., 115,88 8
Ernst , H. A., 810,87 0
Erol, K., 389, 870
error (of a hypothesis) , 553
error recover y (in tokenization) , 704
error surface , 580
ERS­1,37 0
Eskimos , 162
Essig , A., 432, 873
estimatio n phase , 509
Etchemendy , J., 181,86 7
Etzioni , O., 411 , 814, 846, 848, 870,
907
El], see utility , expecte d
Euclid , 767
Eureka , 94
EURISKO , 647
Europea n Spac e Agency , 367
Euthyphro , 9
EVAL , 126 , 132
EVAL­FN , 93
EVAL­TRUTH , 182
evaluatio n function , 92, 115 , 123,
126­12 8
accuracy , 127
linear , 128
Evans , T. G., 19,29,87 0
event , 235
discrete , 237
liquid , 237
EVENT­VAR , 677, 680
even t calculus , 235, 234­236,259 ,
675
even t category , 262
Index 913
evidence , 417
incorporating , 428
EVIDENCE­EXCEPT , 451,452,452 ,
452
EVIDENCE? , 452
evidenc e variable , 445
evidentia l support , 448
evolution , 30, 162,61 9
machine , 21
evolutionar y programming , 620
example , 529, 534
exceptions , 229, 319
excitator y synapse , 564
exclusiv e or, 168,59 6
execution , 57
executio n monitoring , 392,401 ,
402,410,411,78 8
exhaustiv e decomposition , 231
Existentia l Elimination , 266
existentia l graph , 316, 323
Existentia l Introduction , 266
EXPAND , 72,73
EXPANSION , 385
expansio n (of states) , 70
expected  monetar y value , 476
expecte d utility , see utility , expecte d
expecte d valu e (in a gam e tree) , 133
expectimax , 144
expectima x value , 133
expectiminimax , 135
complexity , 135
EXPECTIMINIMAX­VALUE , 135
expectimi n value , 135
exper t system , 257, 491, 539,647 .
827, 848
commercial , 316
first, 22
first commercial , 24
HPP project , 22
logical , 458
medical , 26, 466
metaleve l reasoning , 330
Prolog­based , 304
with uncertainty , 25
explainin g away , 447, 461
explanation , 326,63 0
natura l language , 721
explanation­base d learning , 627,
644, 645
explici t representation , 615
exploration , 623
exploratio n function , 612.61 3
exploratio n problem , see problem ,
exploratio n
expressivenes s vs. efficiency , 531
EXTENDER , 698
extensio n (of a causa l link) , 405
extensio n (of a concept) , 545extrinsi c property , 243
eyes, 724,727 , 728, 768
f (free space) , 791
/­ COST , 107
factorin g (in resolution) , 280
Fahlman , S. E., 19, 260, 330, 331,
870
fail, 355, 855
false negative , 545
false positive , 546
famil y tree, 637
Farhat , N., 595, 870
Faugeras , O., 769, 770, 870
fault tolerance , 566
fax, 664
featur e (of a state) , 104
featur e (speech) , 758
featur e detector , 586
feed­forwar d network , see neura l
network , feed­forwar d
feedbac k loop , 461
feedforwar d circuit , 789
Feigenbaum , E. A., 22, 28, 94, 257,
329, 559,828,861,864,
870, 882
Feldman , J., 28, 494,495, 870
Feldman , R., 646, 898
Fermat,R , 12,43 1
FETCH , 299­302,33 4
field of influence , 805
Fifth Generatio n project , 24, 308,
329
figur e of speech , 714,71 5
Fikes , R., 330,332 , 363,411 , 645,
863, 870,894
filler, 710
FILTER , 311
FIND­AND­INFER , 273,273,27 3
FIND­TRANSFORM , 753,753,754 ,
772
Findlay , J., 258, 871
FINISH , 406,40 8
finite­stat e machine , 656, 788
Firby , J., 389, 868
FIRST , 273, 275, 303, 338,395,40 2
first­orde r logic , see logic , first­orde r
first­orde r probabilisti c logic , 843
Fischer , M., 259, 877
Fisher , R., 432, 871
FITNESS­FN , 620
fitnes s function , 619, 619
fixation , 739
Flakey , 789
flat tire, 393,41 2
floor­planning , 91
Flores , E, 827,90 2Floyd , R., 87,871
fluent , 241,67 9
fly eyes , 737, 749
FMP , see planning , fine­motio n
Focus , 646
focus , 727
focus of expansion , 736
Fodor , J. A., 653, 832, 839, 877
FOIL , 642,643,643,644 , 646,64 8
FOL, see logic , first­orde r
foliation , 793
folk psychology , 13, 261, 840
FOPC , see predicat e calculu s
FORBIN , 389 , 390
Forbus , K. D., 260, 328, 332,87 7
force sensor , see sensor , force
foreshortening , 743
forgotte n node , 108
forma l language , see language ,
forma l
formulate , search , execute , 57
FORMULATE­GOAL , 57
FORMULATE­PROBLEM , 57
Forsyth.D. , 769,871,893
Fortmann , T. E., 520,860,87 2
FORTRAN , 623
forward­backwar d algorithm , 766
FORWARD­CHAIN , 273,273 , 274,
294
forwar d chaining , 272, 273, 298,
313
forwar d checking , 84
Fox, M., 369,390 , 87/
Fraenkel , 826
FRAIL , 298
frame (representation) , 23
fram e (speech) , 758
fram e axiom , see axiom , fram e
fram e problem , 207,21 3
inferential , 207
representational , 207
frame s paper , 331
fram e system , 298, 316
Frean , M., 595, 871
FREDDY , 70, 810
Fredkin , E., 849
Fredki n Prize , 144
Freedman , P., 788, 873
free space , 791
freeways , 800
free will , 9
Frege.G. , 11, 179,212,291,825 ,
871
frequentism , 430
Friedberg , R., 21, 623, 829, 877
fringe , 72
Fristedt , B., 623,86 2
frontier , 72
914 Index
Fu, K, 687, 871
Fuchs , J., 390, 871
function , 185
learnin g a, 529, 558
functiona l decomposition , 249
functiona l dependency , 633, 646
factionalism , 50, 819, 835, 839
functionalist , 836
functiona l programming , 329, 329
functio n symbol , 188, 188, 211
Fung,R. , 465,871
Furukawa , K., 329, 871
Furuta , K., 618,57 7
fuzz y logic , see logic , fuzz y
fuzzy set, 466
fuzzy set theory , 463
Ga(x)  (Gaussian) , 733
g (activatio n function) , 568
G­set , 550
Gabbay,D. , 180, 871
Gabor,Z. , 495
gain ratio , 544, 562
Gala, S., 325, 861
Galileo , G., 3,526,64 1
Gallaire , H., 328,87 2
Gallier , J. H., 213,292,57 2
Gamba , A., 594, 872
Gamberini , L., 594, 572
gambling , 12,47 4
game , see also backgammon ,
bridge , checkers , chess , Go,
Othell o
perfec t information , 122
playing , 122­14 1
theory , 122,84 7
three­player , 147
zero­sum , 148
game show , 476
gap, 710
Carding , J., 743,769,57 2
Gardner , M., 292, 572
Garey , M. R., 853, 572
GARI , 390
Garside , R., 688, 572
Gaschnig , J., 23, 116, 466,569 , 572
GASOIL , 539
Gasquet , A., 390,57 7
gates , 223
gathering , 253­25 4
Gauss , C., 572
Gauss , C. E, 86
Gauss , K. F., 520
Gaussia n elimination , 506
gaussia n function , 733
Gawron , J. M., 770, 878
Gazdar , G., 686, 872Geffner , H., 466, 572
Geiger , D., 596, 575
Gelatt,C , 117,57 9
Gelb , A., 520
Gelernter , H., 18,329,57 2
Gelfond , M., 466, 572
gene, 619
GENERAL­SEARCH , 71,73,74,77 ,
78,92,93 , 115
Genera l Electric , 776
generalization , 546, 547,58 4
generalizatio n hierarchy , 552
generalize d cylinder , 752,76 9
genera l ontology , 226­24 7
Genera l Proble m Solver , 6, 363
genera l search , 85
GENERATE­DESCRIPTION , 662,66 3
generatio n (of language) , see natura l
language , generatio n
generatio n (of states) , 70
generativ e capacity , 656,67 1
Genesereth , M. R., 153,180 , 213,
219,286,293,295,309,313 ,
330,363,811,572,592,59 7
GENETIC­ALGORITHM , 620
geneti c algorithm , 21,572 , 595,
611,619­621,62 3
geneti c engineering , 838
Gentner , D., 646,57 2
Gentzen,G. , 179,291,57 2
Geometr y Theore m Prover , 18
Georgeff , M. P., 364, 572
Gerberich , C., 329,57 2
Gestal t school , 768
GHC, 329
Ghose , S., 117,56 4
Gibson , J., 769,57 2
Gift of the Magi , 378
Gilmore , P., 292, 572
Gini, M., 411.55 5
Ginsberg , M. L., 29, 295,411,465 ,
466, 572 , 873,  897
Ginsberg , M.L. , 567
Giralt , G., 788,57 3
gist, 162
Givan , R., 884
Glanc , A., 810, 873
Glass , J., 903
GLAUBER , 647
Gleitman , L., 687,90 0
GLOBAL­TRAIL­POINTER , 307
Glover , E, 117,57 3
Glymour , C., 596,59 7
Go, 122 , 139
goal, 42, 55, 56, 211
concept , see goal predicat e
formulation , 56
maintenance , 400predicate , 531
representation , 339
test, 60, 85
goal (inferential) , 201
GOAL­TEST , 60,73 , 107, 110
goal predicate , 531
God, existenc e of, 432
Goddeau , D., 26, 903
Godel , K., 11, 265, 277, 292, 824,
573
Goetsch,G. , 117,56 2
GOFAI , see good old­fashione d AI
gold, 153
Gold , E. M., 559, 687,57 3
Goldbach' s conjecture , 647
Goldberg , D. E., 619,57 3
Golden , K., 390,56 7
Goldman , R., 688,720 , 864, 873
Goldszmidt , M., 467,57 3
Golea , M., 595, 553
GOLEM , 641,64 6
GOLUX , 330
Gomard , C. K., 645,57 5
Good , I. J., 142,465,57 3
good and evil, 471
Goodman , N., 258, 645,573 , 552
good old­fashione d AI, 827, 839
good style , 217
gorillas , 653
Gorry , G., 432, 573
Gould , S. J., 458, 573
Gower , A., 144,57 7
GPS, 6, 10,1 7
GPSG , 686
gracefu l degradation , 518,56 6
gradien t descent , 111, 577, 580,61 6
Graham , S. L., 720,57 3
grammar , 854
attribute , 685
categorial , 687
context­free , 656, 684,68 5
probabilistic , 683, 687
context­sensitive , 656
definit e clause , 697
definit e claus e (DCG) , 667,68 5
dependency , 687
formal , 662
generalize d phras e structur e
(GPSG) , 686
head­drive n phras e structur e
(HPSG) , 686
lexical­functiona l (LFG) , 687
multistratal , 686
phras e structure , 684
recursivel y enumerable , 656
regular , 656
semantic , 687
transformational , 686,68 6
t
Index 915
tree­adjoinin g (TAG) , 687
grammatica l formalisms , 656.68 6
Gran d Canyon , 682
Gran d Prix , 141
granularity , 380
grasping , 725
gravity , 826
Grayson , C. J., 477, 873
greedy , see search , greed y
GREEDY­SEARCH , 93
Green , C., 19,212,313,330,363 ,
873
Greenbaum , S., 685. 891
Greenblatt , R., 143, 144,873,  887
Gregory , S., 329, 865
Greiner , R., 646, 873
Grice , H. P., 685, 873
Griesmer , J., 325, 884
Grimes , J., 720, 873
Grishman , R., 688, 865
Grosof , B., 646, 893
Grosz , B., 688, 694, 719, 720, 874
groun d clause , 289
grounding , 821
groun d literal , 300
groun d resolutio n theorem , 289
groun d term , 190
Grove , A., 432, 519, 860
GSAT , 114 , 116, 182, 183,183 . 184
Gu, J., 116,87 4
Guard,:. , 313,330 , 874
Guha , A., 595, 874
Guha , R., 242 , 258, 828, 844, 582
Gupta , A., 316,85 9
Gutfreund , H., 595, 860
Guy, R., 143, 867
H
WMA P (MA P hypothesis) , 588
#ML (ML hypothesis) , 589
Haas , A., 260, 874
HACKER , 330,36 3
Hacking , I., 433,87 4
Hager , G., 258,87 6
HAL computer , 465
Raid , A., 433, 874
Halpern , J., 260,432 , 519,860 , 874
haltin g problem , 277, 824, 824
hamburger , 821
Hamming , R. W., 433, 874
Hammond , K., 389, 874
ham sandwich , 715
hand­ey e machine , 810
Hanks , S., 411, 843,869 , 870,87 4
Hanna , F. K., 647, 892
Hansen , J., 329,87 2
Hanski , I., 50, 874
Hansson,O. , 116 , 874happ y graph , 538
Haralick , R., 116,87 4
HareLD. , 261,87 4
Harkness , K., 141,87 4
Harman , G. H., 839,87 4
Harp , S. A., 595,87 4
HARPY , 770
Harris , L. R., 720, 874
Harrison , M. A., 720, 873
Hart, P., 23, 115,116,411,466.645 ,
869, 870, 874
Hart,T. , 143,87 4
Hartman , E., 595
Harvard , 14, 479
hash table , 300,70 4
Haugeland , J., 5, 28, 827, 840, 874
Haussler , D., 560, 646, 862,87 4
Hawkins , J., 594,87 4
Hayes.J . E., 144,87 5
Hayes,P . J., 50, 213, 258­260,330 ,
331,820,875,88 4
Hayes­Roth , E, 770,87 0
Hazan , M., 250, 875
HD­POP , 374, 374, 389
head (of a clause) , 305
hearer , 652
HEARSAY­!! , 770
Heath Robinson , 14
Hebb,D . 0., 16, 19,594,622,87 5
Heckerman , D., 25, 27,457 , 461,
462,465,466,495,596 ,
875, 876
Hegde,S.U. , 595,88 6
Held.M. , 116,87 5
Helman , D. H., 720,87 5
Helmholtz , H., 12
Hendler , J., 364, 389, 859,87 0
Hendrix , G., 323,87 5
Henglein , E, 330,88 9
Henrion , M., 50, 444,456 , 465,495 ,
875, 876,89 0
Henry , O., 378
Hephaistos , 810
Heppenheimer,T. , 810 , 875
Herbrand' s theorem , 287
Herbrand , J., 287, 292, 328, 875
Herbran d base , 287
Herbran d universe , 286, 292, 293
hermeneutic , 94
Herskovits , E., 596, 866
Hertz , J., 595, 875
heuristic , 94, 115, 118
admissible , 97
composite , 104
function , 93, 101
least­constraining­value , 105, 116
Manhattan , 102, 106,11 8
min­conflicts , 114monotonic , 97
most­constrained­variable , 105.
116
most­constraining­variable , 105
most­constrainin g conjunct , 309
search , see search , heuristi c
straight­line , 93
Heuristi c Programmin g Project , 22,
94
heuristi c repair , 114
Hewitt , C., 330,87 5
hidde n Marko v model , 25,712 , 762,
770
hidde n unit , 571
hierarchica l abstraction , 654
hierarchica l decomposition , see
planning , hierarchica l
hierarchica l task network , see
planning , hierarchica l
higher­orde r logic , see logic ,
higher­orde r
Hilar e II, 788
Hilbert , D., 11
HILL­CLIMBING , 112
hill­climbing , 111, 119 , 572 , 590
random­restart , 112
Hintikka , J., 260, 875
Hinton , G., 24, 595, 875, 893
Hirsh , H., 645, 875
Hirst , G., 688,87 5
Hitachi , 369
HITECH , 26,137 , 144
HMM , see hidde n Marko v mode l
Ho, Y., 21,578,595,86 3
Hobbs , J. R., 258, 261, 685, 687,
718,720,87 6
Hoff, M., 19,594 , 601,622 , 901
Hogg , D., 29, 895
holisti c context , 828
Holland,; . H., 623,87 6
Holloway , J., 144,88 7
holonomic , see robot , holonomi c
Holy Grail , 159
homeostatic , 594
hominids , 653
homophone , 757, 772
homo sapiens , 3
homunculus , 819
Hopcroft , J. E., 853,85 9
Hopfield , J., 24, 595, 876
Hopfiel d network , 571
Hopkin s Beast , 810
Horch , K. W., 832, 864
horizon , 726
horizo n problem , 129
Horn , A., 174,292,87 6
Horn , B., 769,770,863 , 876
Horn clause , 174, 310,640,642,66 7
916 Index
guarded , 329
Horn Norma l Form , 270, 290
Horn sentence , 174, 270
Horowitz , M, 313, 888
Horvitz , E., 25, 50,465 , 495, 876
housebuilding , 372, 384
Howard , R., 484,493 , 494, 520,
876, 886
HPP, see Heuristi c Programmin g
Projec t
HPSG , 686
HST, see Hubbl e spac e telescop e
Hsu, F.­H. , 144, 876
Hsu, K., 595, 877
Hu, V, 595, 859
Huang , T., 27, 520, 877,  880
Hubbl e spac e telescope , 114 , 370,
390
Hubel.D . H., 770, 877
Huber , R. J.. 832, 864
Hucka , M., 788, W
Huddleston , R. D., 685, 877
Huffman , D.. 19, 746, 769, 877
Hughes , G., 261 , 877
HUGIN , 465 , 520
huma n judgment , 446, 458,466,47 9
huma n performance , 4
huma n preference , 507
Hume,D. , 9,430 , 877
Hunt , E., 559, 877
Hunter , G., 293,57 7
Hunter , L..877
Hurst , S., 539, 894
Hutchinson , C, 29, 895
Huttenlocher , D., 753, 769, #77
Huygens,C , 431,57 7
Hwang , C. H., 258. 686. 877
Hyatt , R., 144,57 7
hypothesis , 529
approximatel y correct , 553
consistent , 529
null, 542
space , 544, 545
I
IBM, 14, 138
IBM 701 computer , 14
IBM 704 computer , 138
icecream , 418
Ichiyoshi , N., 329. 888
ICON , 856
103,55 9
IDA* . 117
IDA* , 107
IDEAL­PLANNER , 337 , 338
ideal mapping , 34
ideal rationa l agent . 33
identificatio n in the limit , 557identit y relation , 194
IDS, 103
ignorance , 459, 462
practical , 417
theoretical , 416
IJCA I (Internationa l Join t
Conferenc e on AI), 28
ILA, see action , intermediate­leve l
ILP. see logic programming ,
inductiv e
image , 725
imag e formation , 725­731,76 7
imag e processing , 767
imag e reconstructio n
Bayesian , 370
implementatio n level , 153
implication , 167
Implication­Elimination , 172
implicativ e norma l form , 278, 282,
290, 292
implici t representation , 615
in; (sum of inputs) , 568
inaccessible , see environment ,
inaccessibl e
incomplet e information , 392
incompleteness , 277
theorem , 1 1, 288, 824
incorporation , 658, 716
incrementa l learning , 529, 549
independenc e
conditional , 444
indeterminac y
bounded . 401
unbounded , 402
index function , 755
indexical,67 9
indexin g
clever , 302
combined . 301
table­based , 300­30 1
tree­based . 301, 301­30 2
indifference , 432
individuation , 242
INDUCE , 529,53 0
induction , 9. 529
constructive , 638
mathematical , 11
inductiv e inferenc e
pure, 529
inductiv e learning , see learnin g
inductiv e logic programmin g (ILP) ,
646
INK see implicativ e norma l form
INFER , 311,31 1
inference , 152, 163­165,26 5
causal , 447
data­driven , 274
diagnostic , 447intercausal,44 7
logical , 163
mixed , 447
probabilistic , 436
inferenc e procedure , 223
inferenc e rule , 171, 171
inferentia l fram e problem , 213
infinit e horizo n problems , 520
infinit e loop , 203
infinit e regress , 819
influenc e diagram , see decisio n
network , 877
informatio n (theory) , 540
INFORMATION­GATHERING­AGENT .
490
informatio n gain , 541, 542
informatio n highway , 848
informatio n retrieval , 694­69 5
informatio n theory , 540­543,55 9
informatio n valu e theory , 487
informe d search , 73, 92
infrared , 512
Ingerman , P., 685, 877
inheritance , 230,230 , 317, 328, 332
multiple , 320
with exceptions , 319­32 0
inhibitor y synapse , 564
INITIAL­STATE , 60, 73,107 , 110,
112, 113
INITIALIZER . 699,70 0
initializer , 698
initia l state , 60, 85, 123
inpu t functio n (of a neuron) . 567
inpu t generalization , 615
input resolution , 285
input unit , 571
insuranc e premium , 478
INTEGER , 856,85 6
integrability , 745
INTELLECT , 720
intelligenc e test, 90
intentio n (discourse) , 657
intentionality , 831, 839, 840
intentiona l stance , 50, 820, 840
intentiona l state , 820
intercausa l inference , see inferenc e
intercausa l reasoning , see reasonin g
interleaving , 59
intermediat e form , 676
interna l model , 203
interna l state , 42
INTERPLAN , 363
INTERPRET­INPUT , 41
interpretation , 161, 165 , 187
logical , 178
pragmatic , 658, 678­680,684 ,
685
semantic , 658
Index 917
inlet­reflections , 745
interval , 235
intractability , 11, 21, 29
intrinsi c property , 243
introspection , 6, 13, 459
invarian t
geometric , 754
projective , 754
invarian t shap e representation , 755
invers e method , 293
invers e resolution , see resolution ,
invers e
inverte d pendulum . 617
IPEM , 411
IPL, 17
IQtest , 19,2 9
IR, see informatio n retrieva l
irrationa l behavior , 473, 824
irrationality , 4
ISA links , 331
Isis, 369, 390
ITEP , 144
ITERATIVE­DEEPENING­SEARCH , 79 ,
79, 103,10 6
iterativ e deepening , see search ,
iterativ e deepenin g
iterativ e improvement , 111, 115
ITOU , 646
Jackel , L., 586, 595, 881
Jackson , P., 29, 877
Jacobs , P., 696, 877
Jacquard , J., 15
Jacquar d loom , 15, 774
Jaffar , J., 329 , 877
Jaguar , 369
James , H., 13
James , W., 13
Japanese , 703
Jaskowski , S., 291,87 7
Jefferson , Prof. , 830
Jeffrey , R. C, 293,432,493 , 494,
863, 877
Jelinek , E, 687,759 , 770, 862, 877
Jensen , E, 465, 860
Jensen , E V., 465, 860, 877
Jerison,H.J. , 653,87 7
Jessell , T. M., 595,878
Jevons , W., 292
Jochem , T., 587, 877
Johnes , K. E., 832, 864
Johnson , C., 559, 864
Johnson , D. S., 853, 872
Johnson , M., 720, 722, 863, 881
Johnson , W., 86, 878
Johnson­Laird , P., 28,878
Johnston , M., 116, 390, 878, 886joint, see joint probabilit y
distributio n
joint (of a robot) , 777, 809
joint encoder , 782
joint probabilit y distribution , 425,
431,436,43 9
'jokes , 682
Jones , N. D., 645, 878
Joshi , A., 687, 720, 878, 894
Joskowicz , L., 260, 894
Joule , J., 641
JTMS , see truth maintenanc e
system , justification­base d
Juang,B.­H. , 770, 891
Judd,J. , 595,878
judgment , see huma n judgmen t
juggling , 599
Julesz , B., 768, 878
junctio n type , 748
K
k­DL  (decisio n list), 555
i­DT (decisio n tree) , 555
Kaelbling.L . P., 180,520,623,788 ,
864, 868,878
Kahan , W., 466
Kahneman , D., 4, 443,479 , 878,89 9
Kaindl,H. , 117 , 143,87 8
Kalman,R. , 510, 520,87 8
Kalma n filtering , 510, 520, 587
Kambhampati , S., 390,87 8
Kanade , T., 737, 899
Kanal , L. N., 116­117,467,878 ,
880,887,88 7
Kanazawa , K., 520, 868
KandeLE . R., 595,87 8
Kanoui , H., 328,86 6
Kaplan , D., 259,87 8
Kaplan , R., 720, 884
Karger.D. , 87,87 8
Karp , R. M., 12,69,116,853,875 ,
878
Kasami , T., 720, 878
Kasparov,G. , 137, 144
Kassirer , J., 432, 875
Kastner,J. , 325,88 4
Kautz , H. A., 258,466 , 876, 878
Kay, M., 692,770 , 878
KB, see knowledg e base
KB­AGENT , 152,20 2
KBIL , see inductiv e learning ,
knowledge­base d
Kearns , M. J., 560, 878
Kedar­Cabelli , S., 645, 886
Kedzier , D., 539,89 4
Keeler , J., 595, 890
Keeney , R. L., 479, 484,494,878 ,
879Keller , R., 645, 886
Kelly , J., 865
Kemp , M., 768, 879
Kenley,C. , 465,89 5
Kent , C., 244
Kepler , J., 768
Ketchpel , S., 219,87 2
Keynes , J. M., 432, 879
Khorsand,A. , 117,87 8
KIDS , 330
Kierulf , A., 145, 879
Kietz , J.­U. , 646, 879
Kilimanjaro , 651
Kim, J. H., 465,87 9
kind, see categor y
kinematics , 781
King , C., 447
King , R., 641,879 , 887
King , S., 27, 879
kinship , 197
Kirkpatrick , S., 117,87 9
Kirman , J., 520,86 8
Kirousis , L., 749, 879
Kister , J., 143, 879
Kjaerulff , U., 520, 879
KL­ONE , 298,33 2
KL1,32 9
Klein , E., 686, 872
Knight , B., 142,89 4
Knight , K., 5, 328,879 , 892
Knoblock.C , 86,87 9
knowin g what , 246
knowin g whether , 246
knowledg e
acquisition , 23, 217
and action , 10,24 7
background , 152
built­in , 35
commonsense , 18
diagnostic , 427
model­based , 427
knowledge­base d approach , 827
knowledge­base d system , 22­24 ,
615
knowledg e base , 151, 178,21 3
large , 844
properties , 218
knowledg e effect , 247
knowledg e engineer , 440
knowledg e engineering , 217,221 ,
368
for decision­theoreti c systems ,
492
for planning , 359
vs. programming , 219
with uncertainty , 456
knowledg e level , 153, 179
knowledg e map , see belie f networ k
918 Index
knowledg e precondition , 247
knowledg e representation , 5, 15, 18,
23,157,25 7
analogical , 213
language , 152, 178
knowledg e sourc e (KS) , 770
know s that, 246
Knuth.D. , 132, 143,293,685,853 ,
879
Knuth­Bendi x algorithm , 293
KODIAK , 298
Koenderink,J. , 769,57 9
Kohn , W., 329, 879
Kohonen.T. , 595,880
Koko , 653
Roller , D., 27, 87, 432,519 , 520,
596, 860,877,  878,  880,893
Kolmogorov , A. N., 431, 432,520 ,
559, 88 0
Kolmogoro v complexity , 559
Kolodner,! , 23,646,55 0
Konolige , K., 260, 880
Koopmans , T., 520, 880
Korf,R . E., 87, 117 , 880
Kotok,A. , 143, 880
Kowalski , R., 213, 259,292 , 304,
328, 330,880
Koza,J . R., 623, SSO
Kripke , S. A., 260, 880
Krogh , A., 595,575
Krotkov,E. , 778,59 6
Kruppa , E., 768, 880
KRYPTON , 332
Kube , P., 258, 876
Kuehner , D., 293, 880
Kukich , K., 720, 880
Kulikowski , C. A., 560, 901
Kumar , V., 116, 117, 878,880 , 881.
887
Kurzweil , R., 5, 28, 881
Kyburg , H. E., 432, 881
Ladkin , P., 259, 881
Ladner , R., 259, 577
Lafferty , J., 687, 862
Laird , J., 26, 316, 645, 788, 843,
881,887
Laird , P., 116,886
Lakoff , G., 258, 720, 881
lambd a calculus , 329
Lambertia n surface , 729,743 , 771
La Mettrie , J. O. de, 838, 849, 881
landmark , 805, 809
Langley , P., 881
Langlotz,C , 25,876
language , 651
as action , 685analysis , 658
formal , 654
interfaces , 23, 693
model , 760
in disambiguation , 682
module , 653
"natural , 7, 161,65 4
origin , 653
perception , 657
problem­solving , 329
processing , 16
situated , 659, 659, 684
and thought , 162,65 3
translation , 21,632 , 691­693,72 0
understanding , 19, 23
Lansky , A. L., 364, 872
Laplace , P., 12,430,432,45 8
Larson , G., 626
laser rang e finder , 785
Lassez , J.­L. , 328, 329, 877,881
Latin , 668
Latombe , J.­C , 390, 811, 868, 88]
lattic e theory , 313
Lauritzen , S., 465,596 , 577, 881,
897
LAWALY , 389
Lawler.E. , 116,55, ?
Laws , K. I., 685, 876
laziness , 416, 558
leak node , 443
leapin g to conclusions , 626
learning , 153,525 , 823,83 0
action­value , 599
active , 599
architecture , 844
assessin g performance , 538
Bayesian , 588, 588­589,59 3
belie f network , 531, 588­59 2
blocks­world , 19
cart­pol e problem , 617
checkers , 18
curve , 538
decisio n lists, 555­55 6
decisio n trees , 531­536,64 1
determinations , 634
driving , 586­58 7
element , 525,55 8
explanation­based , 788
and failure , 403
to fly, 539
game­playing , 617
handwritin g recognition , 586
human , 624
incremental , 529, 549,62 6
inductive , 558, 566, 829
knowledge­based , 628, 637,
645
knowledg e in, 625—62 8linearl y separabl e functions ,
575­57 7
new predicates , 638, 640
PAC, 553,560 , 632
passive , 599
pronunciation , 585­58 6
Q, 613
rate, 576, 579,604 , 613
restauran t problem , 531, 620
speedup , 527
top­down , 641­64 4
utilit y function , 599
least­commitment , 549
least­constraining­value , see
heuristi c
least commitment , 271, 346, 374
least mea n squares , 601
Le Cun , Y., 586, 595, 881
Lederberg , J., 22, 94, 257, 870, 882
Lee, K.­E , 623, 770, 881,90 0
Lee, R., 294 , 864
Leech , E, 688, 872
Leech , G., 685, 597
Lefkovitz , D., 145, 882
left­corne r parser , 699
legal reasoning , 29
Leibniz , W., 9, 15, 179,432 , 827
Leiserson , C. E., 853, 866
Lemmer , J. E, 467, 878
Lenat , D. B., 242,258 , 263, 646,
647, 828, 844, 867,882
LENGTH , 666,702,85 1
lens, 727
Leon , M., 692,90 0
Leonard , H. S., 258, 882
Leonard , J., 520, 882
LePape , J.­R, 330, 869
Lesh,N. , 411,57 0
Lesniewski , S., 258, 882
Lesser , V. R., 770, 570
level s of description , 152
Levesque,H. , 184,259,261,331 ,
332, 863,882,  894, 895
Levy , D. N., 144, 875
Lewis , D. K., 50, 685, 839, 882
Lewis , R., 641,57 9
LEX, 552,55 9
lexicon , 664,686 , 698, 703­70 4
LEG , see grammar ,
lexical­functiona l
Li, M., 560, 882
liability,  848
Lieberman , L., 769, 860
LIFE , 297, 329
life insurance , 479
Lifschitz , V., 363, 390,466, 872,
882
liftin g lemma , 286,28 9
Index 919
light­bea m sensors , 785
Lighthill , J., 21,882
Lighthil l report , 21,2 4
likelihood , relative , 427
likelihoo d weighting , 456,46 5
limb (in a scene) , 745
limite d rationality , 8
Lin, S., 115
Linden , T. A., 411,85 2
Lindsay , R. K., 257,720, 882
linea r inpu t resolution , 312
linearization , 346, 347
linea r plan , see plan , linea r
linea r resolution , 285
linea r separability , 574,59 3
line labelling , 745
linguistics , 15­16,2 8
computational , 16
Linguisti c Strin g Project , 685
link (in a neura l network) , 567
link (of a robot) , 777, 809
linke d list, 329
linkin g curve , 800
LINKS , 347, 372,37 5
Linnaeus , 258
Lipkis , T., 332, 894
LIPS , see logica l inference s per
secon d
liquid event , see event , liqui d
liquids , 260
Lisp, 18, 197,329,65 4
Common , viii, 329,330 , 855
lists, 200
litera l (sentence) , 167, 182
Littman , M., 520, 864
lizard toasting , 627
LLA , see action , low­leve l
Lloyd , J., 328, 882
LMS , see least mea n square s
LMS­UPDATE , 602,602 , 623
LMT (Lozano­Perez , Mason , and
Taylor) , 795
local encoding , 577
locality , 174,46 0
locall y finit e graph , 100
locall y structure d system , 441
Locke , J., 9
Locke , W. N., 720, 882
locking , 312
locomotion , 777, 778
Lodge , D., 848
logic , 7, 11,151,15 8
autoepistemic , 466
combinatory , 329
default , 459
description , 298
dynamic , 259
first­order , 185,21 1semantics , 186
syntax , 186
first­orde r probabilistic , 843
fuzzy , 166,417,459,463^6 6
higher­order , 195
inductive , 432,43 2
< modal , 245, 258,26 0
nonmonotonic , 321,45 9
notation , 7
prepositional , 151,16 5
limitations , 176
semantics , 168
syntax , 166
temporal , 165,258 , 259
terminological , 298
logica l connective , 16, 165, 189
logica l inference s per second , 307
logica l level , 153
logica l omniscience , 246
logica l piano , 292
logica l positivism , 10
logicism , 7, 16
logic programming , 293, 304­310 ,
328, 343
constraint , 308, 328
inductiv e (ILP) , 628,636­64 5
language , 297
logic sampling , 455,465 , 515
Logic Theorist , 292
Logic Theoris t (LT) , 17
logic variables , 306,30 8
LOGIN , 328,32 9
long­distanc e dependency , 710
long­ter m memory , 316
Longuet­Higgins , H., 769, 882
LOOKUP , 38, 321
looku p table , 38, 572
LOOM , 298
lottery , 473
standard , 478
love, 823
Lovejoy.W. , 520,883
Lovelace , A., 15,82 3
Loveland.D. , 293,294,55 3
Lowe , D., 769,88. 3
Lowenheim , L., 212, 883
Lowerre , B., 770, 883
Lowrance , J., 466, 893
Lowry , M. R., 330, 883
Loyd , S., 86, 883
Lozano­Perez , T., 795, 883
LT, 17
LT (Logi c Theorist) , 17
Luby , M., 465,86 7
Lucas , J., 824, 883
Luger.G . R, 5,88 3
LUNAR , 23,693,72 0
Lunokho d robots , 775Lusk , E., 294, 902
M
M?. (model) , 499
M (model) , 605
MacHac k 6, 143
machin e evolution , 21
machin e learning , 5, 7,463 , 687,
829
machin e translation , see language ,
translatio n
Machover.M. , 213,86 7
Mackworth , A., 749,769 , 853
Macpherson , P., 466, 899
macro­operator , 787
macrop , 389
Maes , P., 36, 553
Magerman , D., 687, 562,58 3
Mahajan , S., 623, 881
Mahanti,A. , 117,88 3
Mahaviracarya , 431
Maher , M., 328, 881
MAJORITY­VALUE , 537
majorit y function , 533,57 3
MAKE­ACTION­QUERY , 152 , 177 ,
202, 663
MAKE­ACTION­SENTENCE , 152 ,
202, 338, 395,40 8
MAKE­GOAL­QUERY , 337,338,40 8
MAKE­MINIMAL­PLAN , 356
MAKE­NODE , 73,107,110,112 ,
113,66 6
MAKE­PERCEPT­SENTENCE , 152,
177,202,338,395,402 ,
408,66 3
MAKE­PLAN , 399,40 8
MAKE­QUEUE , 72,73,11 0
Malik , J., 27,520 , 737, 743, 749,
769, 877,880 , 883
Manhattan , see heuristic , Manhatta n
MANIAC , 587
Manin , Y., 293,55 3
manipulation , 254,725,77 7
manipulator , 780
Mann.W.C. , 719,720 , 553
Manna , Z., 213,293 , 330, 553
manufacturing , 390
MAP , see maximu m a posterior i
map­colorin g problem , 91, 105
mapping , 34
ideal , 34
Marchand , M., 595, 883
Marin , J., 559, 877
Mark MI/HI , 14
Markov , A. A., 500,770 , 553
Marko v blanket , 465
Marko v chain , 514
Marko v decisio n problem , 411,50 0
920 Index
partiall y observable , 500
Marko v decisio n problems , 520
Marko v model , 762
Marko v property , 500,509,762 , 765
Marr , D., 769, 883
Marriott , K., 328, 881
Marsland , A. T., 144, 884
Martelli , A., 116, 143, 328, 884
Martin , C, 28, 884
Martin , ]., 720, 884
Martin , P., 258, 694, 720, 874, 876
MARVEL , 26
Maslov,S. , 293,884
Mason , M., 411, 795, 883,884
mass noun , 242
mass spectrometer , 22
MATCH , 666
matc h phase , 314
materia l advantage , 127
materia l handling , 774
materialism , 9, 819
eliminative , 840
materia l value , 127
Mates , B., 119,884
mathematica l inductio n schema , 288
mathematica l objection , 824, 826
mathematics , 11­1 2
Matheson , J., 484,494 , 576, 886
Mauchly,J. , 14
MAUT , see multiattribut e utilit y
theor y
MAX , 110,13 2
MAX­VALUE , 130­132,13 2
maximu m a posteriori , 588
maximu m expecte d utility , 472,493 ,
502
maximu m likelihood , 589
Maxwell,!. , 458,720,55 4
Mayer.A. , 116,57 4
Mays , E., 325.55 4
McAllester , D., 143, 312. 330, 332,
364, 554
MCC , 24
McCarthy , J., 17, 18,50 . 179,212 ,
213,230,257,259,329 ,
459, 820, 832, 554
McCartney , R. D., 330, 883
McCawleyJ . D., 685,55 5
McClelland , J. L.. 24, 893
McConnell­Ginet , S., 28, 685.56 5
McCorduck , P., 810,55 5
McCulloch , W. S., 16, 19,563 , 570,
594, 555
McCune , W., 310, 330, 555
McDermott , D., 5, 213 , 317, 328,
330,331,390,411,459 ,
565, 555, 596, 595
McDermott , J., 24, 316, 555McDonald , D., 707
McDonald , R., 185
McGuinness , D., 324, 332, 863
MDL , see minimu m descriptio n
lengt h
MDP , see Marko v decisio n proble m
Mead , C., 595,55 5
meal , 251
meaning , 161
means­end s analysis , 10,1 0
mean squar e error , 623
measure , 231, 386
measur e fluent , 386
medica l diagnosis , 22, 23, 27, 465
MeehanJ. , 328,56 5
Meet,  239
meganode , 453
Megaria n school , 179,258 , 291
Megiddo , N., 847,55 5
Melcuk , I., 687,55 5
Mellish , C., 330, 865
MEMBER , 307,307,32 2
MEMBER? , 322
MEMBERSHIPS , 321,32 2
memoization , 87,452 , 629
memor y requirements , 75, 77, 79
meningitis , 426^127,434 , 435
menta l model , in disambiguation ,
682
menta l objects , 243­24 7
menta l states , 819
Mercer , R., 687, S62
mereology , 258
Merkhofer , M., 494, 556
MERLIN , 331
Mero , L., 116,55 5
meta­comment , 718
Meta­DENDRAL , 552, 559,64 1
METALOG , 330
metamer , 730
metaphor , 715, 720, 822
metaphysics , 10
metareasoning , 140,309 , 364
decision­theoretic , 844
metonymy , 714, 720
Metropolis , N., 117,55 5
Metropoli s algorithm , 117
MEU , see maximu m expecte d utilit y
Mezard , M., 573, 595, 555
MGSS* , 143
MGU , see unifier , mos t genera l
Michalski , R. S., 560 , 555
Michaylov.S. , 329,57 7
Michie , D., 28, 70, 86, 115,117 ,
144,221,539,560,618 ,
622,810,569,555,59 4
MICRO­PLANNER , 330
microelectronic s industry , 774micromort , 480,494,49 7
microworld , 19,19 , 21, 827
Middleton , B., 444,456 , 890
Miles , E, 596, 885
Mill, J. S., 10, 546, 555, 556
Miller , A., 494,55 6
Miller , D., 389,56 5
Miller , G., 595, 704, 556
Milne , A. A., 218, 556
MIN, 107, 132
min­conflicts , see heuristic ,
min­conflicts , 116
MIN­VALUE , 130, 132,13 2
mind , 5, 817, 838
conscious , 818
dualisti c view , 838
and mysticism , 565
philosoph y of, 817, 838, 840
as physica l system , 8, 9
and symbolism , 180
theor y of, 6
MlNIMAL­CONSISTENT­DET , 635 ,
635
minimax , see search , minima x
MINIMAX­DECISION , 126, 147
MINIMAX­VALUE , 126,126 , 135,
147
minima x decision , 124
minimizatio n (logical) , 236
minimu m descriptio n length , 560
minimu m spannin g tree, 116,11 9
Minker,J. , 328,572,55 6
Minsky.M . L., 16,18,21,23,24 ,
28,331,465,577,578,594 ,
595, 827, 556
Minton.S. , 116,645,55 6
missin g attribut e values , 543
missionarie s and cannibals , 67, 86,
88
MIT, 17, 18,781,81 0
Mitchell , D., 184,59 5
Mitchell , T., 552, 559,560 , 645,
788,812,843,565,564 ,
555, 556
Mitchell , T. M., 645
MIT1 , 777
ML, see maximu m likelihoo d
ML (programmin g language) , 329
mobil e robots , 520
mobot , 775
moda l logic , see logic , moda l
mode l
causal , 443
(in logic) , 170
(in representation) , 13
theory , 212
trees , 616
model­base d reasoning , 209
INDE X 921
model­base d recognition , 785
modificatio n operator , 346
modularity , 210
Modu s Ponens , 172,245,285 , 290,
291,294,81 9
Generalized , 269,269­27 0
MOLGEN , 390
monke y and bananas , 366
monotonicit y (of a heuristic) , 97,
116
monotonicit y (of a logic) , 173, 321,
459
monotonicit y (of preferences) , 474
Montague , R., 258, 259, 685, 878,
886, 899
Montanari , U., 116, 143,328 , 884
Mont y Python , 159
Mooney.R. , 645,868
Moore , A., 623, 886
Moore,J. , 331, 886
Moore , J.S. , 293,313,328 , 330,
826, 863
Moore , R., 132,143,260,261,466 ,
876, 879, 887
morality , 817
Moravec , H., 835, 849, 887
More,!. , 17
Morgan , J., 720, 866
Morgenstern , L., 260, 887
Morgenstern,O. , 12, 142,493,847 ,
900
morpholog y
analysis , 695, 703
derivational , 703
inflectional , 703
Morris , P., 467, 873
Morrison , E., 142,887
Morrison , P., 142, 887
Mosher.R. , 776
most­constrained­variable , see
heuristi c
most­constraining­variable , see
heuristi c
most genera l unifier , see unifier ,
most genera l
Mostow,J. , 116,118,58 7
Motet , S., 27, 879
motion , 735­73 7
compliant , 803
guarded , 803
motio n parallax , 737, 768
motio n planning , 796­808,81 1
complexity , 811
moto r subsystem , 410
Motzkin , T., 594, 859,887
Mourelatos , A. P., 259, 887
Moussouris , J., 144, 887MPI, see mutua l preferentia l
independenc e
MRS , 297, 309,313,330,45 7
MST (minimu m spannin g tree) , 119
Muggleton , S., 466, 641,646 , 860,
879, 887
jnultiagen t domain , 244
multiattribut e utilit y theory , 480,
480, 494
multilaye r network , see neura l
network , multilaye r
multiple­stat e problem , see proble m
multipl e inheritance , 320
multipl y connecte d network , 453
Mundy.J. , 769,887,89 3
MUNIN , 465
Murphy' s Law , 59, 87
music , 15
mutation , 21, 619,62 0
mutuall y utility­independent , 484
mutua l preferentia l independence ,
483
MYCIN , 23­24,461,46 6
myopi c policy , 490
Myrhaug.B. , 331,862 , 867
mysticism , 565
N
n­arme d bandit , 611
Nadal,J.­P. , 573,595,88 5
Nagel , T., 839, 887
Nalwa,V . S., 13,770,88 7
NAME , 321
namin g policy , 660
narro w content , 821
NASA , 390,693,77 6
NASL , 411
Nasr , R., 328,85 9
natura l deduction , 291
natura l kind , 232, 319
natura l language , see language ,
natura l
Natura l Languag e Inc., 694
natura l languag e processin g (NLP) ,
see languag e
natura l science , 654
natura l stupidity , 317
Nau, D., 870
Nau, D. S., 116,143,389,390,878 ,
887
Naur , P., 685,88 7
Navathe , S., 325, 867
navigation , 252­253,725 , 796­80 8
landmark­based , 796
NavLab , 586
NAVTO , 787
Neal , R., 596,88 7
Neapolitan , R. E., 467, 887neat, 25
needl e in a haystack , 160
negate d literals , 312
negation , 167
negatio n as failure , 304, 343
negativ e example , 534
negligence , 848
Neisser , U., 594,89 5
Nelson , H., 144, 877
NERF , 572
NETL , 298,33 1
NETtalk , 585
Netto , E., 86, 887
NEURAL­NETWORK­LEARNING , 576 ,
577,58 0
NEURAL­NETWORK­OUTPUT , 577 ,
597
neura l computation , see neura l
networ k
neura l network , 16, 19, 24, 128,
139,530,563,563,593,82 9
vs. belie f network , 592
efficiency , 583
expressiveness , 16, 583
feed­forward , 570, 593
hardware , 16
learning , 16
multilayer , 21,571 , 593
and noise , 584
nonlinearit y in, 567
recurrent , 570
secon d best, 585
neurobiology,565 , 769
Neurogammon , 617
neuron , 16,452,563,564,819.833 ,
835
NEW­CLAUSE , 643
NEW­LITERALS , 642­64 4
NEW­VARIABLE , 307
Newborn , M., 117,88 9
Newell , A., 6, 10,17,26,86,94 ,
115, 143,179,292,316 ,
329,331,645 , 843, 587,
886­888,896 . 898
Newton , I., 3, 509
NEXT­SUCCESSOR , 110
Neyman , A., 847, 888
Nicholson , A., 520, 868, 888
Nicholson , A. E., 520
Nievergelt , J., 145, 879
Nilsson , N. J., 28, 87, 115,116,118 ,
143, 144,213,286,293 ,
363,411,594,645,810 ,
8 U, 870,872,  874,888,  900
Nim, 142
Nitta , K., 329, 888
Nixon , R., 320, 682,71 4
Nixo n diamond , 320
922 Index
NL­MENU , 720
NLP , see natura l languag e
processin g
NOAH , 330, 363,41 1
Nobe l prize , 50
node , search , 71
noise , 535, 542­543,552 . 563, 584,
588,636,731.79 5
noisy­OR , 443
nomina l compounds , 706, 721
nominalist , 258
nominativ e case , see subjectiv e
noncomputability , 11
nondeterminism , 855­85 6
nondeterministic , see environment ,
nondeterministi c
NONDETERMINISTIC­CHART­PARSE ,
699
nondeterministi c algorithm , 855
nonepisodic , see environment .
nonepisodi c
nonholonomic , see robot ,
nonholonomi c
NONLIN , 363
NONLIN+ , 389 , 390
nonlinea r plan , see plan , nonlinea r
nonmonotonicity , 321,45 9
nonmonotoni c logic , see logic ,
nonmonotoni c
Nono , 267
nontermina l symbol , 655, 656, 854
normalization , 428
normalizin g constant , 451
Normann , R. A., 832 , 864
normativ e theory , 479
North , O., 267
North,!. , 21, 871
Norvig , P., 328 , 330, 688, 770, 878,
888
notatio n
arithmetic , 7
logical , 7
notationa l variation s for FOL , 196
noun phrase , 655, 709
Nowatzyk.A. , 144 , 876
Nowick , S., 313,888
NP, see nou n phras e
NP­completeness , 12, 21, 852, 853
nuclea r power , 468, 838
NUMBER­OF­SUBKINDS , 322
numbe r theory , 647
Nunberg,G. , 720 , 888
Nussbaum,M.C. , 888
Nygaard.K. , 331,862,867
O
O() notation , 852cP (configuratio n spac e obstacle) ,
791
O'Brien , S., 720. 891
O'Keefe , R., 330. 888
O­PLAN , 369­371.387,39 0
object , 165,185,18 8
^composite , 234
object­oriente d programming , 15
objec t creation , 384
objectiv e case , 668
objectivism , 430
objec t recognition , 725, 751­75 5
observatio n sentences , 10
obstacl e avoidance , 808
OCCUR­CHECK , 303
occur­check,303,30 5
Ochiai , T., 618, 871
Ockham' s razor , 534. 558, 559, 589,
626,64 4
Ockham,W. , 534,55 9
Ockha m algorithm , 560
octant , 747
odometry , 783
Oetzel , R. M., 596, 900
offlin e cost , 61
Ogasawara , G., 27,520 , 877.  880
Oglesby.E , 313, 330,57 4
Olalainty , B., 390, 871
Olawsky.D. , 411,888
Olesen , K., 465, 860,  877, 888
Oliver , R., 494. 888
Olson , C., 754, 888
Olum , P., 769 , 872
omniscience , 32, 558
logical , 246
one­sho t decision , see decision ,
one­sho t
onlin e algorithm , 796, 806, 806­80 9
onlin e cost, 61
onlin e navigation , 813
Ono,N. , 618,57 ;
ONTIC , 312 , 330
ontologica l commitment , 165, 185,
417,45 9
ontologica l engineering , 217, 222
ontologica l promiscuity , 258
ontology , 222, 257
Op (STRIP S Operator) , 344
OP, 299, 303
opacity , 244
open­coding , 306
open class , 664
operationality , 632
operationalit y and generality , 632
operation s research , 86, 87, 116,
117,367,498,50 0
OPERATOR , 72
operator , 60, 85, 123abstract , 372
primitive , 372
operato r expansion , see planning ,
hierarchica l
operato r reduction , see planning ,
hierarchica l
OPERATORS , 60,73,12 6
operato r schema , 344
OPS­5,298,314,31 6
optica l characte r recognition , 657
optica l flow , 735, 750, 769
optimalit y (of a searc h algorithm) ,
73,85
optimall y efficien t algorithm , 99
optima l solution , 76
optimisti c prior , 610
optimizatio n problem , 619
optimizer , peephole , 379
OPTIMUM­AIV , 367,369 , 390
Opus , 320
OR, see operation s researc h
Or­Introduction , 172
oracle , 856
orderability , 474
orderin g constraints , 350
ORDERINGS , 347, 372,37 4
ordina l utility , see utility , ordina l
Organon , 179,25 7
Ortony , A., 720, 888
Osherson , D. N., 559, 889
Othello , 138,62 3
OTTER , 297,310 , 311,311,313 ,
330,33 3
outcom e of a lottery , 473
outpu t unit , 571
Overbeek , R., 294, 902
overfitting , 542, 542­543,572 , 588
overgeneration , 668
overshooting , 577
OWL , 298
P (probability) , 420
P (probabilit y vector) , 421
PAC , see probabl y approximatel y
correc t
packe d forest , 697, 703
packe d tree, 723
Paek , E., 595, 870
PAG E description , 36, 37, 248
Paige , R., 330, 889
Palay , A. J., 143, 889
Palmer , R. G., 595, 875
Palmieri , G., 594, 872,889
Pandemonium , 594
Pang , C., 87, 868
Panini , 15, 685
Pao, C., 26, 903
Index 923
Papadimitriou . C., 749,847 , 879,
889
Papert , S., 21, 24,577 , 578,594 ,
595, 886
PARADISE , 140
paradoxes , 259
paralle l distribute d processing , see
neura l networ k
Paralle l Inferenc e Machine , 308
parallelism , 566
AND­ , 308
OR­, 308
paralle l lines , 726
paralle l search , 117, 117
paramodulation , 284,29 3
PARENT­NODE , 72
paren t node , 72
parity function , 533
Parker , D., 595, 889
PARLOG , 329
Parrod , Y., 390, 859
PARSE , 662,66 3
parse forest , 664
parse tree, 658, 664,70 1
parsing , 658,658 , 664
chart , 697
partia l derivative , 580
partia l order , see planning ,
partial­orde r
partia l plan , see plan , partia l
partition , 231, 795
part of, 233
part of speech , 658
Partridge , D., 29, 889
parts , 252
Pascal' s wager , 432, 493
Pascal , B., 12, 14,43 1
Pascaline , 15
Pasero , R., 328, 866
PASSIVE­RL­AGENT , 602,605,60 7
passiv e learning , 599
passives , 687
Paterson , M., 889
path, 60, 85
PATH­COST , 72
PATH­COST­RlNCTION , 60
path cost, 60, 61,8 5
PATHFINDER , 457,458,46 5
pathmax , 98
Patil, R., 332,702,720 , 865,869
Patrick , B., 117,88 9
patter n matching , 330
Paul, R. P., 811,88 9
paying , 255
payof f function , 124,41 8
PCFG , see grammar , context­fre e
POP, see paralle l distribute d
processin gPeano.G. , 212, 316,889
Pearl , J., 25, 97, 99, 116­117,143 ,
435,437,465,467,497 .
596, 868, 873, 879,  889,895
PEDESTAL , 390
Pednault,E . P., 390,889
PEGASUS , 26
Peirce,C . S., 212, 316.323.685 ,
889
pen, 713
Penberthy , J., 390, 861,889
Peng , J., 623, 889
Pengi , 411
Penrose , R., 825, 889
Pentland , A., 36, 883
Peot, M., 411,465 , 889,895
PERCEPT , 48
percept , 39
perception , 30
perceptron , 19, 571,573 , 593, 594
convergenc e theorem , 20
learnin g rule, 576, 593
representationa l power , 21, 596
percep t sequence , 33
Pereira , E, 304, 306, 685,688 , 693,
694, 720, 874, 889,  890,  900
Pereira , L., 306, 900
PERFORMANCE­ELEMENT , 608 , 609
PERFORMANCE­FN , 48
performanc e element , 525, 526,
558,562,76 6
performanc e measure , 32, 40. 47,
50,416,47 2
Perrault , C., 720, 866
perspective . 767
perspectiv e projection , 726, 735
PERT , 367­36 9
Peters , S., 685,86 9
Peterson , C., 595, 890
Petrie , T., 770, 867
phenomenology , 828
Philips , A., 116, 886
Phillips , M., 26, 903
Phillips , S., 87, 878
Philo , 179
philosophy , 3, 8­10 , 817­84 1
European , 8
phon e (speech) , 757
phoneme , 585
phon e number , 246
phoneti c alphabet , 758
photogrammetry , 768
photometry , 729
photosensitiv e spot, 749
phrase , 655
phras e structure , 655, 685
physicalism , 819,83 9
Picasso , P., 834pick (choic e point) , 856
Pickwick , Mr., 831
piecewis e continuity . 740
pigeons , 13
Pillsbury , 696
PIM, see Paralle l Inferenc e Machin e
ping­pong , 29, 598
pinhol e camera , 725
Pinker , S., 687, 890
Pisa, towe r of, 526
pit, bottomless , 153
Pitt, L., 560, 862
Pitts, W., 16,19 , 563, 570, 594, 885
pixel , 727
place , 236
Place , U., 839, 890
plan, 347
canned , 407
complete , 349
conditional , 410, 806
consistent , 349, 349
fully instantiated , 346
initial , 347
linear , 363
noninterleaved , 363
nonlinear , 363
parameterized , 398
partial , 345
representation , 346
PLAN­ERS1 , 390
PLANEX , 411,78 7
Plankalkul , 14
PLANNER , 23,330,40 2
planning , 42, 140, 211,342 , 539
abstraction , 389
adaptive , 389
ADL formalism , 390
assembly , 792
block s world , 19
bounded­error , 796
case­based , 389
conditional , 392, 393­398,407 ,
412,415,50 0
contingency , 392
andDDNs , 519
deferred , 393
and execution , 403­40 6
fine­motion , 802, 809
formalization , 25
hierarchical , 371­380,38 9
hierarchica l decomposition , 374
history , 363
as logica l inference , 341
menu , 249­25 2
multi­agent , 390
partial­order , 337, 346. 355­356 ,
390,40 7
progression , 345, 365
924 Index
reactive , 411, 411
regression , 345, 356, 363
route , 18
and scheduling , 369
searc h spaces , 345­34 6
situatio n calculus , 341­34 2
situatio n space , 345
speec h acts, 654
symbolic , 788
total order , 346
unde r uncertainty , 795, 843
plannin g agent , see agent , plannin g
plannin g variable , 400
plan recognition , 654,71 8
plasticity , 564
Plato , 8, 178,827,83 8
Plotkin , G. D., 646, 890
ply, 124
Pnueli , A., 259, 890
Podelski , A., 329, 859
poetry , 4, 682
Pohl, I., 87, 116,89 0
poke r hands , 433
Poland , 240
Polguere , A., 687, 885
policy , 411,498,500 , 517,519 , 806
optimal , 500
POLICY­ITERATION , 506,60 8
polic y iteration , 505,505­506,520 ,
603, 843
polic y loss, 505
Polifroni , J., 26, 903
Pollack , M., 720, 866
Pollard , C, 686, 890
Polya , G., 94,89 0
polytree , 448,46 4
POMDP , see Marko v decisio n
proble m
Pomerleau , D. A., 26, 586, 587,750 ,
877, 890
POMP D (partiall y observabl e
Marko v decisio n problem) ,
520
Pooh , Winni e the, 218
POP, 355, 356, 356, 357, 358, 362,
364­367,369,374,384 ,
391,398,404,406,41 2
POP­DUNC , 381, 384, 385, 390,
401
POPLOG , 330
Popper , K. R., 432, 559, 836, 890
Port­Roya l Logic , 471, 493
Portuguese , 627
pose, 734,75 2
positiv e example , 534
positivism , logical , 10
possibilit y theory , 466,46 6
possibl e threat , 354,357 , 357possibl e world , 260,79 5
Post, E. L., 179,89 0
posterio r probability , see
probability , conditiona l
POSTSCRIPT , 706 , 707
Prade , H., 466, 869
Pradhan , M., 444, 456,89 0
pragmati c interpretation , see
interpretation , pragmati c
pragmatics , 658
Prata , A., 595, 870
Pratt, V. R., 259, 890
Prawitz , D., 291,293,89 0
pre­editing , 692
precompilation , 270
precondition , 344
disjunctive , 383
universall y quantified , 383
PRECONDITIONS , 402
predecessor , 80
predicate , 165,67 4
predicat e calculus , see logic ,
first­orde r
predicat e symbol , 187, 211
PREDICT , 723
predictin g the future , 531, 553
prediction­estimatio n process , 515
predictio n phase , 509
PREDICTOR , 698­700,702,70 2
predictor , 698
preference , 418, 473,474 , 483
lexicographic , 496
monotonic , 476
preferenc e independence , 483
preferentiall y independent , 496
premise , 167
prene x form , 292
preposition , 664
president , 240
Presley , E., 240
PRESS , 330, 331
Price Waterhouse , 369
Prieditis , A., 103, 116,118,887,89 0
primar y colors , 730
Princeton , 17,70 4
Principia  Mathematica,  17
PRINT , 856
Prinz , D., 142, 890
Prior , A. N., 258,89 0
prioritize d sweeping , 607, 623
priorit y queue , 623
prior knowledge , 625,627 , 636, 645
prior probability , see probability ,
prior
prismati c motion , 781
Prisoner' s Dilemma , 847
prisoners , three , 435
probabilisti c model , 425probabilisti c network , see belie f
networ k
adaptive , 590
probabilisti c projection , 515
probabilisti c senso r models , 520
probability , 12,23,25 , 127
alternative s to, 458
assessment , 430
axiom s of, 422­42 3
conditional , 418,421 , 421­422 ,
431,44 0
conjunctive , 440
distribution , 139,421,44 5
history , 433
judgments , 443
of sun rising , 430
prior , 420,420­421,43 1
theory , 417, 493
probabl y approximatel y correct ,
553, 556, 560
PROBLEM,6 0
problem , 60, 85
airport­siting , 496
assembl y sequencing , 70
bandit , 623
contingency , 59, 123
cryptarithmetic , 65, 91
datatype , 60
8­queens , 64, 83
8­puzzle , 101,103 , 115
8­queens , 86, 89
exploration , 59
halting , 277, 824
inherentl y hard , 852­85 3
intrinsicall y difficult , 106
map­coloring , 91, 105
missionarie s and cannibals , 67
monke y and bananas , 366
multiple­state , 58, 66
real­world , 63
relaxed , 103
robot navigation , 69
route­finding , 93
shoes­and­sock , 347
shopping , 340
single­state , 58, 60
toy, 63
travellin g salesperson , 69
VLSI layout , 69, 114
problem­solvin g agent , see agent ,
problem­solvin g
proble m formulation , 56, 57
proble m generator , 526, 562
proble m solving , 22
complexity , 342
vs. planning , 338
procedura l attachment , 323
PROCESS , 311,31 1
Index 925
process , 237
production , 40, 854
productio n system , 297, 314
produc t rule, 421
PROGRAM , 48
programmin g language , 14, 160
progra m synthesis , 411
progression  planner , see plannin g
projection , probabilistic , 514
Prolog , 197 , 297, 304, 328, 363,
644, 688
Concurrent , 329
parallel , 308
Prolo g Technolog y Theore m Prover ,
311,33 0
promotion , 353
pronunciation , 585,76 2
proof , 160, 173,26 6
as search , 268
by contradiction , 280
checker , 312
procedure , 11
theory , 160, 165
property , 185
PROPOSITIONAL­KB­AGENT , 177
propositiona l attitude , 244
propositiona l logic , see logic ,
propositiona l
proprioception , 782
PROSPECTOR , 23,46 6
prostheti c limbs , 777
Protagoras , 685
protecte d link , 353
protection , 350
protectio n interval , 347
protei n structure , 641
Provan , 0. M., 444,456 , 890
pruning , 123,130,346 , 542
in contingenc y problems , 136
inEBL , 631
Psaltis , D., 595, 859, 870, 877
pseudo­code , 855
pseudo­experience , 607,61 6
PSPACE , 811,85 3
psychologica l models , 566
psychologica l reasoning , 261
psychology , 3, 12­1 4
experimental , 6, 13
information­processing , 13
psychophysics , 769
publi c key encryption , 313
Puget , J.­E, 646, 893
Pullum , G. K., 162,656,686 , 872,
890
PUMA , 781,78 2
punishment , 528
Purdom , P., 116,89 7Putnam , H., 50, 286, 292, 432, 839,
867, 891
Pylyshyn.Z . W., 839, <S9/
Q(a,  i) (valu e of actio n in state) , 612
Q­learning , 599, 623,62 4
Q­LEARNING­AGENT , 614
Q­value,61 2
QA3, 212,36 3
QALY , 480,49 4
QLlSP , 330
qualia , 821, 836, 840
qualificatio n problem , 207, 213,
415,83 0
qualitativ e physics , 233,26 0
qualitativ e probabilisti c network ,
465, 482
quantifie d term , 676, 677
quantifier , 165,18 9
existential , 191­19 2
nested , 192­19 3
scope , 673,68 6
universal , 189­19 1
quantizatio n factor , 758
quantu m theory , 826
quasi­logica l form , 676,68 6
query (logical) , 201
query variable , 445
question , 711
queue , 72
QUEUING­FN , 72, 73
Quevedo,T. , 142,81 0
quiescence , 129,14 2
Quillian , M. R., 257, 331,891
Quine,W. , 179, 213,232 , 258, 891
Quinlan , E., 720, 891
Quinlan , ]., 540, 559, 561,616 , 642,
644, 646, 891
Quirk , R., 685, 891
R(i) (Reward) , 603
Rl,24,31 6
Rabiner , L. R., 688, 766, 770, 891
racin g cars, 846
Raibert , M. H., 778, 891
Raiffa , H., 479,484 , 494, 879
ramificatio n problem , 207
Ramsay , A., 330, 861
Ramsey , F. P., 432,493 , 891
randomization , 34
rando m restart , 119
rando m variable , 420, 441
Boolean , 420
Rao, B., 27, 520, 877,88 0
Raphael , B., 19, 115, 116,646,810 ,
863, 874,891rapid prototyping , 304
RAPTS , 330
rationa l action , 518
rationa l agent , see agent , rationa l
rationalism , 827
rationality , 4, 847
calculative , 845
limited , 8
perfect , 8, 845
rationa l speaker , 716
Ratner , D., 87, 897
rats, 13
Rau, L., 696, 877
Rayner , M., 632, 894
ray tracing , 729
RBDTL , 635, 636,64 6
RBL , see relevance­base d learnin g
reactiv e planner , see planning ,
reactiv e
readin g signs , 254
Reagan , R., 706
real­tim e AI, see artificia l
intelligence , real­tim e
real­worl d problems , see problem ,
real­worl d
reasoning , 18, 163
abou t location , 206
default , 326, 458­460,46 6
goal­directed , 140
intercausal , 461
metalevel , 330
uncertain , 25,658,68 2
with uncertainty , 760
Reboh , R., 330, 894
recognitio n hypothesis , 755
recognizabl e set, 795, 795
RECOMMENDATION , 57
recor d player , 812
recurrent , see neura l network ,
recurren t
recursiv e definitions , 643
Reddy , R.. 770, 870, 883
Redfield , S., 595, 890
reducti o ad absurdum , 280
reduction , 12, 184,85 3
redundancy , 791
redundan t step, 405
referenc e class , 430,43 2
referentia l opacity , 260
referentia l transparency , 244
refinemen t operator , 345
reflectance , 729, 743
reflectanc e map , 745
REFLEX­AGENT­WITH­STATE , 43,4 9
REFLEX­LEARNING­ELEMENT , 529 ,
530
REFLEX­PERFORMANCE­ELEMENT ,
529, 530, 530
926 Index
refutation , 280, 283, 293, 311
refutatio n completeness , 286
regression , nonlinear , 572
regressio n planner , see plannin g
regret , 479
Reichardt , J., 810, 891
Reichenbach , H., 432, 892
Reif, J., 796, 811,564 , 892
reification , 230,230 , 319,43 5
reinforcement , 528, 528, 598
reinforcemen t learning , 528, 598,
829
Reingold,E. , 116,56 2
Reiter , R., 213,259 , 459,466 , 892
Reitman , W., 145, 892
RELATED­TO? , 322
relation , 185
relativ e clause , 710
relaxe d problem , see proble m
relevance , 169,628,64 6
relevance­base d learning , 628, 634,
645
RELS­IN , 321
RELS­OUT , 321,32 2
REMAINDER , 57
remova l from a KB, 298
REMOVE­FRONT , 72, 73
Remus , H., 145,<W,59 2
renaming , 273
Renyi , A., 592
Renyi , A., 431
repeatability , 783
repeate d states , 82
replanning , 367, 371, 392,40 1 ­403 ,
407,41 2
REPLANNING­AGENT , 402,40 2
representation , see knowledg e
representatio n
representatio n languages , 157
representatio n theorem , 480
REPRODUCTION , 620
REQUEST , 490
Rescher,N. , 261,59 2
RESET­TRAIL , 307
Resnik , P., 688, 592
resolution , 18,21 , 172,172,265 ,
277,290,297,64 8
algorithm , 277
completenes s proof , 286
generalized , 278
input , 285
inverse , 639, 639­641,64 6
linear , 285
strategies , 284­28 6
resolutio n closure , 287
RESOLVE­THREAT , 382RESOLVE­THREATS , 356,356 , 357,
358,374,375,382,382 ,
384,385,38 5
resolvent , 279, 639
resourc e (in planning) , 386­389 ,
391
response , 13
REST , 273,275,303 , 338,395,40 2
restaurant , see learning , restauran t
proble m
restrictiv e languag e (in planning) ,
342
Result  (situatio n calculus) , 342
rete, 314
retina , 724
RETRACT , 321,325,326,41 2
retraction , 325
REWARD , 601
reward , 528, 598,60 3
reward­to­go , 601
rewar d function , 503
rewrit e rule, 310, 333,655 , 854
REWRITES­FOR , 699 , 702
Rice , T, 494, 886
Rich, E., 5, 592
Rieger.C , 23,59 2
Riesbeck , C. K., 23, 328, 565, 894
right thing , doin g the, 4, 8
Ringle , M., 840, 592
risk aversion , 478
risk neutrality , 478
risk seeking , 478
Rissanen,J. , 560, 592
Ristad , E., 690
Ritchie , G. D., 647, 592
Rivest , R.. 560,595 . 853, 562. 566,
592
RMS , see root mea n squar e
Roach , J., 329, 592
roadmap , 800
Roberts , D. D., 331,59 2
Roberts , L., 769, 592
Roberts , M., 143,56 2
Robin , C., 218
Robinson , A., 292
Robinson , G., 293, 902
Robinson , J., 18,277,286,293,328 ,
592
Robo­SoAR , 788
robot , 773, 773, 809
architecture , 786­79 0
autonomous , 773
cleaning , 812
holonomic , 778
mobile , 775,81 2
navigation , 69
nonholonomic , 778
robot game , 811robotics , 6, 363,51 2
behavior­based , 789
fathe r of, 810
laws of, 814
Robo t Institut e of America , 773
robot reply , 832
Rochester , N., 14, 17, 18
Rock , I., 770, S92
rollup(ofaDBN) , 515
Romania , 56
root mea n squar e error , 505
Rorty , R., 840,59 2
Rosenberg , C., 585,59 5
Rosenblatt , E, 19, 576, 594, 769,
572, 592
Rosenblitt , D., 364, 554
Rosenbloom , P., 26, 316,645 , 843,
881, 887,898
Rosenbluth , A., 117,55 5
Rosenbluth , M., 117,55 5
Rosenfeld , E., 28, 560
Rosenholtz , R., 743,769 , 883
Rosenschein , J., 180,59 2
Rosenschein , S. J., 180, 788,575 ,
592
Rosenthal , D. M., 840, 592
Ross , K., 720,59 9
Ross , S. M., 433, 592
rotar y motion , 781
rotation , 734
Rothwell , C., 769, 593
Roukos , S., 687, 562
Roussel , P., 328, 566, 593
route finding , 68
Rouveirol , C., 646, 893
Rowe , N. C., 29,893
RSA (Rivest , Shamir , and
Adelman) , 313
Rubik'scube , 86, 103,73 5
Rujan , P., 595, 553
rule
causal , 212
condition­action , 40, 145,61 2
diagnostic , 212
dotted , 697
if­then , 40,16 7
implication , 167
of thumb , 94
situation­action , 40, 498
uncertain , 461
RULE­ACTION , 41,4 3
rule­base d systems , 458
with uncertainty , 460­46 2
RULE­LHS , 666
RULE­MATCH , 41,4 3
RULE­RHS , 666
rule memory , 314
RULES , 666,69 9
Index 927
rule schema , 671
Rumelhart , D. E., 24, 595, 893
RUN­ENVIRONMENT , 47,4 8
RUN­EVAL­ENVIRONMENT , 47,48 ,
48
RUN­NETWORK , 581
RUNNING­AVERAGE , 602,60 5
runtim e variable , 400
Ruspini , E., 466 , 893
Russell , B., 10, 16, 17,291,292 ,
825, 901
Russell , J., 494, 893
Russell , S. J.. 27, 50, 117 , 143, 309,
330,520,596,646,843 ,
844, 846, 867, 874, 877,
880, 893,  898
Russian , 21
Ruzzo , W. L., 720, 873
Ryder , J. L., 145, 893
S
S­set , 550
sabotage , 661
Sacerdoti , E. D., 330, 363, 389, 893,
894
Sachs , J., 162,59 4
Sacks , E., 260, 894
Saenz , R., 720, 899
safety clearance , 797
Sag, I., 686,720 , 572, 878,  890
Sagalowicz , D., 330, 894
Sager , N., 685, 594
SAINT , 19
St. Petersbur g paradox , 476, 494
Salton , G., 688, 894
SAM, 297 , 313,33 0
Samad , T., 595, 574
Sammut , C, 539, 894
sampl e complexity , see complexity ,
sampl e
samplin g rate, 758
Sampson , G., 688, 572
Samuel , A., 17, 18, 138, 143,559 ,
600,617,622,823,829,59 4
Samuelsson , C., 632,59 4
Sanna , R., 594, 872, 889
Sanskrit , 256, 331,68 5
Sapir­Whor f hypothesis , 162
Saraswat , V. A., 329,59 4
SAT, see satisfiabilit y proble m
satisfiability , 164
satisfiabilit y problem , 182
saturation , 287
Satyanarayana , S., 595, 559
Savage , L. J., 12, 424,432 , 493,59 4
sawing­the­lady­in­hal f trick , 817
SAY, 66 3
Sayre , K., 818, 894scale d orthographi c projection , 726
SCANNER , 699 , 700 , 702 , 702
scanne r (in char t parsing) , 698
scene , 725
scent marking , 653
Schabes , Y., 687, 894
Schaeffer , J.. 138, 142, 144.554 ,
594
Schalkoff , R. J., 5, 594
Schank,R . C., 23,59 4
scheduling , 367, 369
job shop , 369
space missions , 369
Schemes , R., 596, 597
schema , 234
SCHEME , 329,85 6
Scherl , R., 259, 594
Schmolze , J., 332,59 4
Schoenberg . L, 594, 559,55 7
Schoenfinkel.M. , 329,59 4
Schofield , P., 86, 594
Schoppers , M. J., 411,59 4
Schroder , E., 292,59 4
Schubert , L., 258, 686, 577
Schwartz , J. H., 595,57 5
Schwartz , W., 432, 873
Schweikard , A., 794, 901
Schwuttke , U., 26. 594
scientifi c discovery , 559
SCISOR , 696
script , 234
Scriven , M.. 839, 595
scruffy , 25
SEAN , 330
SEARCH,5 7
search , 22, 42, 56, 85
A*. 96­10 1
completeness , 100
complexity , 100
optimality , 99
alpha­beta , 129­133.141 , 143,
844
B*, 143
backtracking , 84, 309,77 1
bidirectional , 80
blind , 73
constrain t satisfaction , 83­8 4
current­best­hypothesis , 546
cuttin g off, 129
general , 85
greedy,  93, 115, 1 18
heuristic , 73, 115
hill­climbing , 111,59 5
IDA* , 106
informed , 73, 92
iterativ e deepening , 79, 85, 87,
129,31 2
iterativ e deepenin g A*, 106iterativ e improvement , 115
minimax , 124­126.130 , 139, 141
quiescence , 129
random­restart , 595
simulate d annealing , 113
SMA* , 107
uninformed , 73
searc h cost , 61
searc h node , see node , searc h
searc h strategy , 70, 73
searc h tree, 71
Searle , J. R., 565,685 , 819.
831­835.837.839,840,59 5
segmen t (of discourse) , 717
segmentatio n (of an image) , 734
segmentatio n (of speech) , 757
Sejnowski , T., 585, 594, 595, 617,
575, 595,59 9
SELECT­NONPRIMITIVE , 374
SELECT­SUB­GOAL , 374, 382­384 ,
385,39 1
SELECT­SUBGOAL , 355,356,35 6
SELECTION , 620
Self, M., 565
Selfridge , M., 687,56 2
Selfridge , O. G., 17,594,59 5
Selfridge , P., 325,56 5
Sells, P., 686,59 5
Selman , B., 117, 184,331,466,575 ,
579, 595
SEM­NET­NODE , 321
semanti c interpretation , 672­678 ,
685, 689, 720
semanti c network , 298, 316
partitioned , 323
SEMANTICS , 662,66 3
semantics , 28,157 , 165,67 2
causal , 821
compositional , 672
intersective , 708
logical , 178
preference , 688
semidecidability , 277
semidynamic , see environment ,
semidynami c
Seneff , S., 26,90 3
sensin g action , see action , sensin g
sensitivit y analysis , 447, 492
sensor , 31, 724
abstract , 795
cross­beam , 785
depth , 785
force , 784
lane position , 513
parallel­beam , 785
structure d light , 785
tactile , 724,78 4
senso r failure , 512
928 Index
senso r fusion , 512
senso r model , 510, 510­51 3
stationary , 510
sensor y stimuli , 724
sentenc e
analytic , 164
atomic , 167, 189, 193,211,27 9
complex , 167, 211
in a KB, 151, 178
in a language , 655
logical , 186
necessaril y true, 163
as physica l configuration , 158
quantified , 211
valid , 163
separability , see utilit y function ,
separabl e
sequenc e prediction , 90
sequent , 291
sequentia l decision , see decision ,
sequentia l
sequentia l decisio n problems , 520
Sergot , M., 259, 880
Sestoft , P., 645, 878
set of support , 285, 310
sets, 199
Settle , L., 313,330,87 4
Shachter , R., 465,480 , 494, 520,
895, 898
shading , 735, 743­74 5
Shafer,G.,466,467,89 5
Shahan , R. W., 840, 862
Shakey , 19,360,363,365,411,787 ,
810
Shalla , L., 293, 902
Shankar.N. , 313, 826,89 5
Shannon , C., 16, 17, 122, 142,540 ,
559, 895
shape , 734
shape from shading , 769
shape from texture , 769
Shapiro , E., 329, 330,646 , 895,897
Shapiro , S. C., 28, 94, 323
Sharp , D., 596, 866
Sharpies , M., 29, 895
Shavlik , J., 560, 869, 896
Shaw , J., 94, 143, 292, 329, 888
sheep shearing , 775
Shenoy , P., 466,89 6
Shepard , E. H., 886
Shieber , S. M., 328,686 , 688, 890,
896
shippin g lanes , 793
Shirayanagi , K., 145,59 6
shoes and socks , 364
Shoham , Y., 213, 258,259 , 466,
876, 896
shopping , 227, 247­255 , 371, 392shoppin g list, 249
short­ter m memory , 316, 570
shortes t path , 87, 119
Shortliffe , E. H., 22,466 , 864,896
SHRDLU , 23, 330
Shrobe , H., 329, 870
SHUNYATA , 826
Shwe , M., 465, 896
Sidner,C , 719,720 , 874
Siekmann , J., 294, 896
Sietsma , J., 595, 896
SIGART , 28
sigmoi d function , 569, 580,58 3
signa l processing , 758­75 9
sign function , 569
Siklossy , L., 389,89 6
silhouett e curve , 800, 800
silhouett e method , 800
similarit y network , 457
Simmons , R., 778,89 6
Simon , H. A., 6, 10, 17, 28, 50, 86,
94, 137, 143,292,313,330 ,
887,888,89 6
SIMPLE­COMMUNICATING­AGENT ,
663
SlMPLE­PLANNING­AGENT , 33 8
SlMPLE­POLICY­AGENT , 501,52 1
SlMPLE­PROBLEM­SOLVING­AGENT ,
57
SIMPLE­REFLEX­AGENT , 41
SIMPLIFY , 311
SIMULATED­ANNEALING , 113
simulate d annealing , 111, 113, 117 ,
119
simulatio n of intelligence , 834
simulatio n of world , 821
simultaneity , 227
Singh , S., 623, 867
single­stat e problem , see problem ,
single­stat e
singl y connecte d network , 447
sins, seve n deadly , 95
SIPE, 371,387,389 , 390
SIR, 19
Siskind , J. M., 687,89 6
SlTUATED­PLANNING­AGENT , 40 8
situate d agents , 830
situate d automaton , 788
situate d language , see language ,
situate d
situatedness , 26, 403
situation , 204,47 2
situation­actio n mapping , 828
situatio n calculus , 204,212 , 213,
216,227,234,259,361 ,
363,368,390,400,65 9
skeleton , 798
SKELETON­AGENT , 38skeletonization , 796, 798, 809
Skinner , B. E, 15,50,89 6
Skolem,T.,212,292,S9 6
Skole m constant , 292, 679
Skole m function , 282, 292,40 0
skolemization,281 , 292
Slagle , J. R., 19, 142­144,896 , 897
slant, 734
Slate , D. J., 87, 897
Slater , E., 142,89 7
sliding­bloc k puzzle , 63
Sloman , A., 330, 839, 861,897
Slavic,  P., 4, 878
SMA* , 107
Smallwood , R., 520,89 7
Smith , D. E., 295, 309, 330, 411,
872, 889,89 7
Smith , D. R., 330, 897
Smith , J., 494,88 8
Smith , R., 559,86 4
Smith , S., 369, 87/
smoothing , 730
SNARC,1 6
SNEPS , 298, 323
SNLP , 364
snow , 162
SOAR , 298,645 , 788,790 , 843, 844
Societ e de Linguistique , 653
Socrates , 6, 8, 9
Socrati c reasoning , 312
Soderland , S., 364, 897
softbot.36,412,77 3
softwar e agent , 36
Solomonoff , R., 17,559,89 7
solution , 56, 60, 85,349,35 8
in planning , 342,34 9
SOLUTION? , 356,358,37 4
soma , 564
Sompolinsky,H. , 595,86 0
sonar , 501, 512, 784
Sondik , E., 520,89 7
sonnet , 830
soul, 838
sound (inference) , 159, 178
sour grapes , 32, 526
space complexity , see complexit y
spacecraf t assembly , 390
SPANAM , 692
Sparc k Jones , K., 688, 874
spars e system , 441
spatia l reasoning , 260
spatia l substance , see substanc e
speaker , 652
speake r identification , 759
specialization , 546, 547
specificit y preference , 459
spectrophotometry,  730
specularl y reflecte d light , 729i
INDE X 929
SPEECH­PART , 663
speec h act, 652, 652­654,684,72 0
indirect , 652
interpretation , 663
speec h recognition , 25, 26, 657,
757,757­76 7
speec h synthesis , 657
speec h understanding , 757
speedu p learning , 527
spellin g correction , 704, 720,72 2
Spiegelhalter , D. J., 465, 560, 596,
881,885,  897
spies , 163
SPIKE , 370,39 0
spin glass , 595
Spirtes , P., 596, 897
Sproull , R. R, x, 495, 870
SPSS , 370
SQRT , 34
squar e root, 34
SRI, 19, 343, 360, 363,494 , 810
Srivas.M. , 313,89 7
SSD, see sum of square d difference s
stabilit y
dynamic , 778
static , 778
Stader , J., 390, 859
staged  search , 117
STAHL , 647
Stamper , R., 466, 899
standardizin g apart , 271,29 4
Stanfor d University , 17, 22, 457
Stanhop e Demonstrator , 292
stapler , 728
START , 405­408,66 6
start symbol , 854
STATE , 72,60 1
state
representation , 339
state (in a chart) , 697
state (in the world) , 56
state (process) , 237
STATE­DESCRIPTION , 337,338,395 ,
402,40 8
state evolutio n model , 514
States , D., 877
state set space , 60
state space , 60, 85, 790
state variable , 508
static , see environment , static
static (variable) , 856
static universe , 383
stationarit y (for rewards) , 507
stationarit y (in PAC learning) , 553
statistica l mechanics , 24, 595
STATLOG , 560
Steel , S., 411,86 0
Steele , G. L., 329, 330, 868,897steerin g vehicles , 780
Stefik , M. J., 390, 897
Stein , P., 143, 879
step function , 569
steppe r motor , 783
STEPS , 372,37 4
stereogram , rando m dot, 768
stereopsis , binocular , 735
Sterling , L., 330, 897
Sternberg , M., 641,879,887
Stevens , K., 769, 897
Stickel , M. E., 258, 293, 311, 330,
720, 876,  897
stimulus , 13
Stob, M., 559, 889
stochasti c dominance , see
dominance , stochastic , 493
stochasti c simulation , 453,455,464 ,
515
Stockman , G., 143,897
Stoic school , 179,258,29 1
Stokes , I., 390, 859
Stolcke , A., 687, 897
Stone , H., 116,89 7
Stone , J., 116,89 7
Stone , P., 559, 877
Stonebraker,M. , 219,89 7
STORE , 299,300,302 , 334
Story , W., 86, 878
slotting , 659
Strachey , C. S., 16, 142,597,89 9
straight­lin e distance , 93
Strat , T., 466,89 3
strateg y (in a game) , 124
strawberries , enjoy , 823
string (in a language) , 655
string (in logic) , 245
STRIPS , 343,360 , 363,365 , 389,
390,400,401,411,412 ,
645,78 7
STRIP S language , 343­345,36 7
Strohm , G., 390, 877
stron g AI, 29,831­834,83 9
structur e (composit e object) , 234
Stubblefield , W. A., 5, 883
Stuckey , P., 329,87 7
STUDENT , 19
stuff, 242
Stutz , J., 865
subcat , see subcategorizatio n list
subcategorization , 670, 669­67 1
list, 670
subevent , 235
subject­au x inversion , 711
subjectiv e case, 668
subjectivism , 13,43 0
subplan s
interactio n between , 341Subramanian , D., 646, 846,893,89 8
SUBS , 321,32 2
SUBSEQUENCE , 666
SUBSET?, 322,322
SUBST , 265,27 3
substance , 228, 241­24 3
spatial , 242
temporal , 242
substitutabilit y (of lotteries) , 474
substitution , 201, 265, 270
subsumptio n (in descriptio n logic) ,
323
subsumptio n (in resolution) , 286
subsumptio n architecture , 411
SUCCESSOR , 76
successor­stat e axiom , see axiom ,
successor­stat e
successo r function , 60
SUCCESSORS , 107,110 , 132
Suermondt , H. J., 465,87 6
Sugihara , K., 749, 769, S98
SUMMATION , 851, 852
summer' s day, 831
sum of square d differences , 735
Sundararajan , R., 329, 892
Sun Microsystems , x
sun rising , 430
Superman , 161, 244
SUPERS , 321,32 2
supervise d learning , 528, 829
SUPPORT­EXCEPT , 451 , 452, 452
sure thing , 478
surfac e patches,  746
surfac e structure , 686
Sussman , G. J., 330, 363, 868, 898
Sussma n anomaly , 365
Sutherland , G., 22, 864
Sutton.R. , 603,623,867,89 8
Svartvik , J., 685, 897
Swade , D. D., 15, 898
Swedish , 29
syllogism , 6, 291
Symantec , 694
symbo l
dynamic , 660
static , 660
symboli c differentiation , 333
symboli c integration , 552
synapse , 564
synchronic , 209
syntacti c ambiguity , 720
syntacti c sugar , 200
syntacti c theor y (of knowledge) , 245
syntax , 23, 165
syntax , logical , 157, 178
synthesis , 313
deductive , 313
synthesi s of algorithms , 313
930 Index
syste m gain , 507,51 9
system s reply , 832
SYSTRAN , 692
Szeliski , R., 736
T(cat,  i) (liqui d event) , 237
T­SCHED , 390
TABLE­DRIVEN­AGENT , 38, 39
table tennis , see ping­pon g
tabu search , 117
TACITUS , 258
tactil e sensor , see sensor , tactil e
Tadepalli , P., 646, 898
TAG (Tree­Adjoinin g Grammar) ,
687
Tail, P., 86,898
Taki, K., 329, 888,898
Talos , 810
Tambe , M., 316, 645,559 , 898
Tanimoto , S., 29, 898
Tarjan , R. E., 853, 898
Tarski' s world , 213
Tarski,A. , 11,212,685,59 8
Tarzan , 68,70 7
Tash, J. K., 520, 898
taskability , 790
task network , 363
Tate, A., 25, 363, 364,369,389 ,
390, 859,861,866
Tatman , J., 520, 898
TAUM­METEO , 692,72 0
tautology , 164
Tawney , G., 142, 866
taxi
in Athens , 435
automated , 526
environment , 39
taxonomi c hierarchy , 23, 228,23 0
taxonomi c information , 252
taxonomy , 230, 257
Taylor , C., 560,885
Taylor , R., 795, 810, 883,898
TD, see temporal­differenc e
TD­UPDATE , 604,60 5
teacher , 528,59 8
TEAM , 694, 720
TEIRESIAS , 330
telepathi c communication , 660
telephon e number , 87,24 6
telepresence , 776
television , 652
TELL , 152, 153,214,245 , 273, 298,
299, 320, 323,66 0
Teller , A., 117,85 5
Teller , E., 117,88 5
Temperley , H., 594,86 6
temporal­differenc e equation , 604tempora l constraints , 388
tempora l differencing , 622
tempora l logic , see logic , tempora l
tempora l substance , see substanc e
Tenenberg,J. , 391,89 9
Tennant , H., 720,89 9
term (in IR), 695
term (in logic) , 186, 188
TERMINAL­TEST , 126
TERMINAL? , 601­602,605,608,61 4
termina l state , 124,59 8
termina l symbol , 655, 854
termina l test, 124
terminatio n condition , 803
terminologica l logic , see logic ,
terminologica l
term rewriting , 293
Tesauro,G. , 139,615,617,89 9
Tesla,N. , 810
test set, 538,53 8
texel , 743
text, 715
text categorization , 695­69 6
text interpretation , 694
texture , 735,742,742­74 3
textur e gradient , 743, 769
Thag , 626
thee and thou , 664
THEN­PART , 395
THEO , 788,790,843,84 4
theorem , 198
theore m prover , 5, 297, 310­31 3
as assistants , 312
theore m proving , 363, 411
mathematical , 20, 29
theor y of informatio n value , 609,
830, 84 4
theor y resolution , 293
thermostat , 820
Theseus , 559
thingification , 230
Thomason , R., 685, 899
Thomere,J. , 27,87 9
Thompson , C., 720, 899
Thompson , K., 137, 144,86 6
Thompson , S., 719,720, 883
Thorndike , E. L., 13
Thorne , J., 686,89 9
Thorpe , C., 587, 877
thought , see reasonin g
laws of, 7
threat , 353
threa t resolution , 353, 382
3­CNF , 182
3­D information , 734
3SAT , 84, 182
Tic­Tac­Toe , 123, 124, 145, 147
tiling algorithm , 573tilt, 734
Timberlin e workshop , 364
time, 258
time complexity , see complexit y
time expressions , 722
time interval , 238, 259, 262
time machine , 365
time slice , 515
Tinsley , M., 138
TMS , see truth maintenanc e syste m
Todd , B., 466,89 9
Todd , P. M., 595,88 6
tokenization , 703
Tomasi , C., 737, 899
tomato , 762
tongue , 762
Torrance , S., 29, 895
TOSCA , 369
total cost, 61
total order , see planning , total orde r
touch , see sensor , tactil e
Touretzky , D. S., 331, 596, 899
tower s of Hanoi , 793
toy problem , see problem , toy
trace , see gap
tractabilit y of inference , 324
trading , 263
trail, 306
trainin g
curve , 580
sequence , 600
set, 534 , 538
on test set, 538
transfe r motion , 793
transfe r path , 793
transitio n function , 606
transitio n model , 499, 502
transitivit y (of preferences),  473,
474
transitivit y of implication , 278
transi t motion , 793
transi t path , 793
translation , 29
travellin g salesperso n problem , 69,
69, 116,11 9
tree model , 132
Treloar , N., 142, 894
triangl e table , 401
trie, 704
trigram , 761
trihedra l solid , 746
tropism , 201
truth , 163, 178,18 9
truth­functionality , 460,46 4
truth­preservin g (inference) , 159
truth maintenance , 326
truth maintenanc e system , 326, 332,
460, 83 9
Index 931
assumption­based , 327, 332
justification­based , 326
truth table , 168, 179, 180
TSP, see travellin g salesperso n
proble m
Tuck , M., 788, 881
tuple , 187
Turing , A., 5, 11, 14, 16, 28, 122,
142,292,465,691,823 ,
824,826,827,831,836 ,
839, 840,89 9
Turin g award , 466, 853
Turin g machine , 11, 560, 827
Turin g Test , 5, 7, 28,37 , 652, 823,
825,83 0
total, 6
Turk , 141
turnin g radius , 780
Tversky , A., 4, 443,479 , 878,899
TWEAK , 363
Tweety , 320
2001 : A Spac e Odyssey , 465
TYPE , 663
type, see categor y
type (in planning) , 383
type predicate , 308
typica l instance , 232
U
U (utility) , 472
UCPOP , 390
Ueda , K., 329, 899
Ulam , S., 143, 879
Ullman , J. D., 328, 853, 859, 899
Ullman , S., 753, 769, 877,899
uncertainty , 23, 25, 66, 229,415 ,
462, 830, 843
in games , 123
model , 805
reasonin g with , see reasonin g
rule­base d approac h to, 458
summarizing , 417
unconditiona l probability , see
probability , prior
unconsciousness , 564
undecidability , 11, 29
understanding , 716
understandin g problem , 654
ungrammatica l sentences , 669
unicorn , 181
unification , 270, 270­271,290 , 314
328, 383
algorithm , 302­30 3
in a DCG , 686
and equality , 284
unifier , 270
most genera l (MGU) , 271,29 4
uniform­cos t search , 85unifor m convergenc e theory , 560
unifor m cost search , 75
unifor m prior , 589
UNIFY , 270, 271,273 , 275, 284,
303, 303,307 , 358, 385
UNIFY­LISTS , 303,303,30 3
UNIFY­VAR , 303,303,30 6
Unimation , 810
uninforme d search , 73
uniqu e main subaction , 378, 389
unit (of a neura l network) , 567
unit clause , 285
unit preference , 285
Unit Resolution , 172
units function , 231
Universa l Elimination , 266
universa l field , AI as a, 4
universa l plan, 411
UNIX , 706
unsatisfiability , 164
unsupervise d learning , 528
UOSAT­II , 370 , 390
UPDATE , 601,60 2
UPDATE­ACTTVE­MODEL , 608
UPDATE­FN , 48
UPDATE­MEMORY , 38
UPDATE­STATE , 42,43,5 7
upwar d solution , 376, 389, 391
URP , 494
Urquhart,A. , 261,892
Uskov , A., 144, 859
Utgoff , P. E., 552,559 , 886
utilitarianism , 10
act, 847
rule, 847
UTILITY , 126
utility , 23, 44, 123,41 8
expected , 50, 135,419,471,472 ,
476,51 6
independence , 484
maximu m expected , 419
of money , 476­47 8
node , 485
normalized , 478
ordinal , 476
theory , 418, 473­475,49 3
multiattribute , 493
utilit y function , 45, 50, 124,471 ,
474,499,61 5
additive , 502
multiplicative , 484
separable , 502
utilit y scale , 478­48 0
utterance , 652
UWL , 411
vacuum , see agent , vacuu mvacuu m tube , 14, 16
vacuu m world , 32,51,87,90,412 ,
624
vagueness , 459,68 1
Valiant , L., 560, 899
valid conclusions , 165
VALIDITY , 182
validity , 163,17 8
valid sentences , 169
VALUE , 111­113,12 6
value , backed­up , 124
VALUE­DETERMINATION , 506
VALUE­ITERATION , 503,504,504 ,
506,60 8
value determination , 505, 603, 624
value function , 476
additive , 483
value iteration , 502, 502­505,52 0
value node , see utilit y node
value of computation , 844
value of information , 487­490,493 ,
497,50 2
value of perfec t information , 488
van Benthem.J. , 261,899
van Doom , A., 769, 879
van Harmelen , E, 645, 899
van Heijenoort , J., 293, 899
vanishin g point , 726
VanLehn , K., 686, 899
van Roy, P., 304, 307, 329, 899
Vapnik , V, 560, 899
variabilizatio n (in EBL) , 630
variabl e (in a CSP) , 83
variabl e (in a gramma r rule) , 668
variabl e (logical) , 190
VARIABLE? , 303
variabl e binding , 357, 374
Vasconcellos , M., 692, 900
Vaucanson , J., 810
Vecchi.M. , 117,57 9
vector­spac e model , 695
vecto r quantization , 759, 762, 763
Veloso , M., 646, 900
Vendler , Z., 259, 900
Venn diagram , 422
verb phrase , 655
Vere, S. A., 389, 900
verification , 313
hardware , 226
VERSION­SPACE­LEARNING , 549 ,
552
VERSION­SPACE­UPDATE , 549,549 ,
550
version­spac e learning , 530
versio n space , 549, 550
versio n spac e collapse , 551
verte x (of a chart) , 697
verte x (polyhedron) , 746
932
verve t monkeys , 651
very long names , 218
virtua l reality , 776
visibilit y graph , 799
vision , 6, 7, 12, 19,725­76 7
Vitanyi , P. M. B., 560, 882
Viterb i algorithm , 765,77 2
VLSI layout , 69, 114
vocabular y words , 664
vocal tract , 762
vonLinne , C., 258
von Mises , R., 432, 900
Von Neumann,). , 12, 14, 16, 142,
493, 594, 847, 900
von Winterfeldt , D., 493, 900
Voorhees,E. , 720,90 0
Vorono i diagram , 800
Voyager , 26, 370
VP, see verb phras e
VPI, see valu e of perfec t
informatio n
W
Wj,t (weigh t on a link) , 568
Waibel , A., 770, 900
Walden , W., 143,87 9
Waldinger , R., 213, 293, 330, 363,
894, 900
Wall , R., 685, 869
Wall Stree t Journal , 696
Walter , G., 810
Waltz , D., 19, 749, 900
WAM , see Warre n Abstrac t Machin e
Wand.M , 146, 900
Wang , E., 260, 898
Wang , H., 6, 179,90 0
Wanner , E., 162,687,90 0
war games , 849
Warmuth , M., 560, 862, 891
WARPLAN , 363,41 1
WARPLAN­C , 411
Warren , D. H. D., 304, 306, 328,
363,411,685,590,90 0
Warre n Abstrac t Machine , 306, 328
washin g clothes , 722
Washington , G., 240
Wasserman , P. D., 596, 900
Watkins , C,, 623, 900
Watson , C., 832,90 0
Watson , J., 13
Watson , L., 329, 892
wavelength , 730
weak AI, 29, 839
weak method , 22
weather  reports , 692Weaver , S., 777
Weaver , W., 540,559 , 895
Webber , B. L., 28, 688, 720, 874,
878, 900
Weber , J., 27, 520, 737, 877, 880
Wefald , E. H., 50, 143, 844, 893
Wegman.M. , 328,889
weigh t (in a neura l network) , 567
weighte d linea r function , 127
Weinstein , S., 559, 889
Weiss , I., 770, 859
Weiss , S. M., 560, 90/
Weizenbaum , J., 20, 839, 849, 907
Weld , D., 260, 364, 390, 411, 814,
848, 861,869,  870,889,
897, 901
well­forme d formula , T93 , 193
Wellman,  M. P., 364, 465,494 , 520,
843,868,874,  901
Wells , M., 143, 879
Werbos , P., 595, 889,90 1
Wermuth , N., 465, 881
West , Col. , 267
Westinghouse , 369, 390
wff, see well­forme d formul a
Wheatstone , C., 768, 907
White , T., 594
Whitehead , A. N., 8, 16, 291, 292,
629, 907
Whiter , A., 389, 898
Whittaker , W,, 778, 896
Whorf,B. , 162,90 7
WHY , 721
wide content , 821
Widrow , B., 19, 594,601 , 622, 907
Wiener , N., 142, 520, 594, 907
Wigderson , A., 847, 885
Wilber , B., 894
Wilcox , B., 145, 892
Wilensky , R., x, 23, 720, 826. 832,
907
Wilkins , D. E., 140, 389, 907
Wilks,Y , 688,90 7
Williams , R., 496, 595, 623, 889,
893
Williamson , M., 411,S7 0
Wilson , R. H., 794, 90/
Winker , S., 313, 330,90 2
Winograd , S., 19, 902
Winograd , T., 19, 23, 330, 827, 898,
902
Winston , P. H., 5, 19, 548,559 , 902
Wittgenstein , L., 158, 179,232,685 ,
822, 902Woehr , J., 466, 902
Wohler , R, 834
Wojciechowski , W., 313, 902
Wojcik , A., 313, 902
Wood,D. , 116.SS 7
Woods , W. A., x, 23, 331, 686, 693,
720, 902
word , 652
Wordnet , 704, 720
workin g memory , 314, 645
world model , in disambiguation , 682
world state , 56
wors t possibl e catastrophe , 478
Wos, L., 293,294 , 313. 330, 902
Wright , S., 464, 902
Wrightson , G., 294, 896
Wu, D., 688,720 , 902
wumpu s world , 153, 206, 216, 227,
412,65 2
Wundt , W., 12
X
XCON , 316
Xerox , 692
xor, see exclusiv e or
Yager , E., 788, 881
Yakimovsky , Y., 494, 870
Yang , Q., 389, 903
Yannakakis,M. , 847.SS 9
Yap.R. , 329,877
Yarowsky , D., 688, 903
yield (in parsing) , 658
Yip, K., 260, 903
Yoshikawa.T. , 811,90 3
Young , D., 29, 895
Younger , D., 720. 903
Z­3,1 4
Zadeh,L . A.,466 , 903
Zapp , A., 587, 750,770,56 9
Zermelo.E. , 142,826,90 3
ZFC, 826
Zhivotovsky,A. , 144,859
Zilberstein , S., 844 , 903
Zimmermann , H.­J. , 466, 903
zip code , 572,58 6
Zisserman , A., 769, 871,887 , 893
Zobrist , A., 145, 903
Zue, V., 26, 903
Zuse , K., 14, 142, 903
Zytkow.J . M., 881
